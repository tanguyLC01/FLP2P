[2025-09-11 17:06:36,244][flp2p.graph_runner][INFO] - Train, Round 0 : loss => 2.304904338816801,  accuracy: 0.10404666666666668, gradient_norm : 0.17273967064835294
[2025-09-11 17:06:49,917][flp2p.graph_runner][INFO] - Test, Round 0 : loss => 2.3047734740018844,  accuracy: 0.1053
[2025-09-11 17:07:24,406][flp2p.graph_runner][INFO] - Train, Round 1 : loss => 2.3038786726196596,  accuracy: 0.1088933333333333, gradient_norm : 0.17242690292554413
[2025-09-11 17:07:38,556][flp2p.graph_runner][INFO] - Test, Round 1 : loss => 2.3038294197916986,  accuracy: 0.1106
[2025-09-11 17:08:13,006][flp2p.graph_runner][INFO] - Train, Round 2 : loss => 2.3028407368063926,  accuracy: 0.11320000000000002, gradient_norm : 0.17494838897245457
[2025-09-11 17:08:27,059][flp2p.graph_runner][INFO] - Test, Round 2 : loss => 2.3029325046062468,  accuracy: 0.1135
[2025-09-11 17:09:02,117][flp2p.graph_runner][INFO] - Train, Round 3 : loss => 2.3018631446361533,  accuracy: 0.11630666666666666, gradient_norm : 0.17347331426238521
[2025-09-11 17:09:15,946][flp2p.graph_runner][INFO] - Test, Round 3 : loss => 2.302071819984913,  accuracy: 0.1207
[2025-09-11 17:09:50,425][flp2p.graph_runner][INFO] - Train, Round 4 : loss => 2.3009367916981387,  accuracy: 0.11955333333333334, gradient_norm : 0.17203687244803048
[2025-09-11 17:10:04,196][flp2p.graph_runner][INFO] - Test, Round 4 : loss => 2.3012446896672247,  accuracy: 0.1213
[2025-09-11 17:10:38,927][flp2p.graph_runner][INFO] - Train, Round 5 : loss => 2.300036047796409,  accuracy: 0.12225333333333332, gradient_norm : 0.17447042945486987
[2025-09-11 17:10:52,798][flp2p.graph_runner][INFO] - Test, Round 5 : loss => 2.3004401389956475,  accuracy: 0.123
[2025-09-11 17:11:28,014][flp2p.graph_runner][INFO] - Train, Round 6 : loss => 2.2991358166933065,  accuracy: 0.12319333333333334, gradient_norm : 0.17994731215582643
[2025-09-11 17:11:41,922][flp2p.graph_runner][INFO] - Test, Round 6 : loss => 2.2996524708747863,  accuracy: 0.1227
[2025-09-11 17:12:16,641][flp2p.graph_runner][INFO] - Train, Round 7 : loss => 2.29823423196872,  accuracy: 0.12408, gradient_norm : 0.17634610145488833
[2025-09-11 17:12:30,431][flp2p.graph_runner][INFO] - Test, Round 7 : loss => 2.2988751925826074,  accuracy: 0.1254
[2025-09-11 17:13:05,114][flp2p.graph_runner][INFO] - Train, Round 8 : loss => 2.2973253916700664,  accuracy: 0.12564000000000003, gradient_norm : 0.1819188363309782
[2025-09-11 17:13:19,108][flp2p.graph_runner][INFO] - Test, Round 8 : loss => 2.2980986114501953,  accuracy: 0.1276
[2025-09-11 17:13:53,892][flp2p.graph_runner][INFO] - Train, Round 9 : loss => 2.2964457181096076,  accuracy: 0.12799999999999997, gradient_norm : 0.18390993101952866
[2025-09-11 17:14:07,667][flp2p.graph_runner][INFO] - Test, Round 9 : loss => 2.29731118131876,  accuracy: 0.1279
[2025-09-11 17:14:42,575][flp2p.graph_runner][INFO] - Train, Round 10 : loss => 2.295539173086485,  accuracy: 0.12984666666666667, gradient_norm : 0.18439288906097132
[2025-09-11 17:14:56,355][flp2p.graph_runner][INFO] - Test, Round 10 : loss => 2.296511535692215,  accuracy: 0.1288
[2025-09-11 17:15:31,167][flp2p.graph_runner][INFO] - Train, Round 11 : loss => 2.294614368875822,  accuracy: 0.13090666666666667, gradient_norm : 0.18423471805825617
[2025-09-11 17:15:44,762][flp2p.graph_runner][INFO] - Test, Round 11 : loss => 2.2956874863147734,  accuracy: 0.1295
[2025-09-11 17:16:19,417][flp2p.graph_runner][INFO] - Train, Round 12 : loss => 2.293625970383485,  accuracy: 0.13252000000000003, gradient_norm : 0.1888582644134244
[2025-09-11 17:16:33,068][flp2p.graph_runner][INFO] - Test, Round 12 : loss => 2.2948303352475166,  accuracy: 0.1299
[2025-09-11 17:17:07,873][flp2p.graph_runner][INFO] - Train, Round 13 : loss => 2.292622810900212,  accuracy: 0.13362, gradient_norm : 0.19017633929689975
[2025-09-11 17:17:21,679][flp2p.graph_runner][INFO] - Test, Round 13 : loss => 2.2939320824027063,  accuracy: 0.131
[2025-09-11 17:17:56,369][flp2p.graph_runner][INFO] - Train, Round 14 : loss => 2.2915264784296348,  accuracy: 0.13577333333333336, gradient_norm : 0.20354452135376117
[2025-09-11 17:18:10,038][flp2p.graph_runner][INFO] - Test, Round 14 : loss => 2.2929749203562735,  accuracy: 0.1332
[2025-09-11 17:18:44,917][flp2p.graph_runner][INFO] - Train, Round 15 : loss => 2.2903719649712246,  accuracy: 0.13824000000000003, gradient_norm : 0.19567018627879354
[2025-09-11 17:18:58,622][flp2p.graph_runner][INFO] - Test, Round 15 : loss => 2.291949161589146,  accuracy: 0.1354
[2025-09-11 17:19:33,433][flp2p.graph_runner][INFO] - Train, Round 16 : loss => 2.289073074062665,  accuracy: 0.14037999999999995, gradient_norm : 0.20425354224183073
[2025-09-11 17:19:47,092][flp2p.graph_runner][INFO] - Test, Round 16 : loss => 2.2908336161017417,  accuracy: 0.1355
[2025-09-11 17:20:21,924][flp2p.graph_runner][INFO] - Train, Round 17 : loss => 2.287682062685491,  accuracy: 0.1426533333333333, gradient_norm : 0.2184018336674759
[2025-09-11 17:20:35,706][flp2p.graph_runner][INFO] - Test, Round 17 : loss => 2.2896118819117546,  accuracy: 0.1371
[2025-09-11 17:21:10,718][flp2p.graph_runner][INFO] - Train, Round 18 : loss => 2.286136913498243,  accuracy: 0.14523333333333333, gradient_norm : 0.21925880359538655
[2025-09-11 17:21:24,525][flp2p.graph_runner][INFO] - Test, Round 18 : loss => 2.2882599616885186,  accuracy: 0.1389
[2025-09-11 17:21:59,465][flp2p.graph_runner][INFO] - Train, Round 19 : loss => 2.2843914742271103,  accuracy: 0.14772666666666662, gradient_norm : 0.2320296331152455
[2025-09-11 17:22:13,319][flp2p.graph_runner][INFO] - Test, Round 19 : loss => 2.2867534190654752,  accuracy: 0.1404
[2025-09-11 17:22:48,035][flp2p.graph_runner][INFO] - Train, Round 20 : loss => 2.28243879099687,  accuracy: 0.15014000000000002, gradient_norm : 0.24292177480881322
[2025-09-11 17:23:01,760][flp2p.graph_runner][INFO] - Test, Round 20 : loss => 2.285053142130375,  accuracy: 0.1415
[2025-09-11 17:23:36,524][flp2p.graph_runner][INFO] - Train, Round 21 : loss => 2.2802119283874833,  accuracy: 0.15315999999999994, gradient_norm : 0.24501171038418082
[2025-09-11 17:23:50,217][flp2p.graph_runner][INFO] - Test, Round 21 : loss => 2.28311439640522,  accuracy: 0.144
[2025-09-11 17:24:25,177][flp2p.graph_runner][INFO] - Train, Round 22 : loss => 2.2776927019158997,  accuracy: 0.15619333333333335, gradient_norm : 0.2654225542978642
[2025-09-11 17:24:38,785][flp2p.graph_runner][INFO] - Test, Round 22 : loss => 2.28088526250124,  accuracy: 0.1451
[2025-09-11 17:25:13,524][flp2p.graph_runner][INFO] - Train, Round 23 : loss => 2.2747452050447463,  accuracy: 0.15931333333333333, gradient_norm : 0.28497198465694856
[2025-09-11 17:25:26,981][flp2p.graph_runner][INFO] - Test, Round 23 : loss => 2.278316989791393,  accuracy: 0.1499
[2025-09-11 17:26:02,042][flp2p.graph_runner][INFO] - Train, Round 24 : loss => 2.2713026031851764,  accuracy: 0.16310000000000002, gradient_norm : 0.2972752975314661
[2025-09-11 17:26:15,625][flp2p.graph_runner][INFO] - Test, Round 24 : loss => 2.2753386033058165,  accuracy: 0.1522
[2025-09-11 17:26:50,345][flp2p.graph_runner][INFO] - Train, Round 25 : loss => 2.2672683918476104,  accuracy: 0.16608000000000003, gradient_norm : 0.3268137403305854
[2025-09-11 17:27:03,567][flp2p.graph_runner][INFO] - Test, Round 25 : loss => 2.2718588305950167,  accuracy: 0.1572
[2025-09-11 17:27:38,087][flp2p.graph_runner][INFO] - Train, Round 26 : loss => 2.262529605726401,  accuracy: 0.16881333333333337, gradient_norm : 0.36816645776942686
[2025-09-11 17:27:52,900][flp2p.graph_runner][INFO] - Test, Round 26 : loss => 2.2678072992801668,  accuracy: 0.1587
[2025-09-11 17:28:27,875][flp2p.graph_runner][INFO] - Train, Round 27 : loss => 2.257003641525903,  accuracy: 0.17286000000000004, gradient_norm : 0.385343667576488
[2025-09-11 17:28:46,824][flp2p.graph_runner][INFO] - Test, Round 27 : loss => 2.2630741698503494,  accuracy: 0.1603
[2025-09-11 17:29:21,522][flp2p.graph_runner][INFO] - Train, Round 28 : loss => 2.2505257659157123,  accuracy: 0.17631333333333332, gradient_norm : 0.41538165995973286
[2025-09-11 17:29:42,214][flp2p.graph_runner][INFO] - Test, Round 28 : loss => 2.25760139786005,  accuracy: 0.1627
[2025-09-11 17:30:16,936][flp2p.graph_runner][INFO] - Train, Round 29 : loss => 2.242914537688097,  accuracy: 0.17963333333333342, gradient_norm : 0.4655498706531905
[2025-09-11 17:30:34,440][flp2p.graph_runner][INFO] - Test, Round 29 : loss => 2.2512373022675516,  accuracy: 0.1657
[2025-09-11 17:31:09,314][flp2p.graph_runner][INFO] - Train, Round 30 : loss => 2.234123676717281,  accuracy: 0.18368666666666675, gradient_norm : 0.52744179114653
[2025-09-11 17:31:25,281][flp2p.graph_runner][INFO] - Test, Round 30 : loss => 2.243924146091938,  accuracy: 0.169
[2025-09-11 17:32:00,106][flp2p.graph_runner][INFO] - Train, Round 31 : loss => 2.224026819219192,  accuracy: 0.18772666666666657, gradient_norm : 0.590839500202003
[2025-09-11 17:32:14,000][flp2p.graph_runner][INFO] - Test, Round 31 : loss => 2.2356626993060114,  accuracy: 0.1717
[2025-09-11 17:32:48,964][flp2p.graph_runner][INFO] - Train, Round 32 : loss => 2.2124923598766326,  accuracy: 0.19264666666666674, gradient_norm : 0.60456405470096
[2025-09-11 17:33:02,929][flp2p.graph_runner][INFO] - Test, Round 32 : loss => 2.2261652408719064,  accuracy: 0.1774
[2025-09-11 17:33:37,574][flp2p.graph_runner][INFO] - Train, Round 33 : loss => 2.1997615029414495,  accuracy: 0.19856000000000004, gradient_norm : 0.7172075372265045
[2025-09-11 17:33:51,585][flp2p.graph_runner][INFO] - Test, Round 33 : loss => 2.2158150493502617,  accuracy: 0.1813
[2025-09-11 17:34:26,562][flp2p.graph_runner][INFO] - Train, Round 34 : loss => 2.1853690539797146,  accuracy: 0.2040266666666667, gradient_norm : 0.8022083958705822
[2025-09-11 17:34:40,450][flp2p.graph_runner][INFO] - Test, Round 34 : loss => 2.2045343383073805,  accuracy: 0.1875
[2025-09-11 17:35:15,295][flp2p.graph_runner][INFO] - Train, Round 35 : loss => 2.169698554476102,  accuracy: 0.21178, gradient_norm : 0.836027951336918
[2025-09-11 17:35:29,420][flp2p.graph_runner][INFO] - Test, Round 35 : loss => 2.192368029820919,  accuracy: 0.1928
[2025-09-11 17:36:04,406][flp2p.graph_runner][INFO] - Train, Round 36 : loss => 2.15292549992601,  accuracy: 0.21890000000000004, gradient_norm : 0.9634230277194757
[2025-09-11 17:36:18,346][flp2p.graph_runner][INFO] - Test, Round 36 : loss => 2.180004315531254,  accuracy: 0.2006
[2025-09-11 17:36:53,200][flp2p.graph_runner][INFO] - Train, Round 37 : loss => 2.135324731469154,  accuracy: 0.2268133333333333, gradient_norm : 1.0696768348760346
[2025-09-11 17:37:07,042][flp2p.graph_runner][INFO] - Test, Round 37 : loss => 2.1669137713968754,  accuracy: 0.2078
[2025-09-11 17:37:41,538][flp2p.graph_runner][INFO] - Train, Round 38 : loss => 2.1167915998895954,  accuracy: 0.23526, gradient_norm : 1.1324249890196452
[2025-09-11 17:37:55,539][flp2p.graph_runner][INFO] - Test, Round 38 : loss => 2.1542705395936967,  accuracy: 0.2154
[2025-09-11 17:38:30,553][flp2p.graph_runner][INFO] - Train, Round 39 : loss => 2.0981921734412516,  accuracy: 0.24354000000000003, gradient_norm : 1.2767216604902907
[2025-09-11 17:38:44,621][flp2p.graph_runner][INFO] - Test, Round 39 : loss => 2.1410664544165137,  accuracy: 0.2218
[2025-09-11 17:39:19,221][flp2p.graph_runner][INFO] - Train, Round 40 : loss => 2.0787809596955773,  accuracy: 0.25349333333333324, gradient_norm : 1.3997124002290766
[2025-09-11 17:39:33,172][flp2p.graph_runner][INFO] - Test, Round 40 : loss => 2.1282341462314127,  accuracy: 0.2257
[2025-09-11 17:40:07,793][flp2p.graph_runner][INFO] - Train, Round 41 : loss => 2.05944684262077,  accuracy: 0.26430000000000003, gradient_norm : 1.444447145981626
[2025-09-11 17:40:21,553][flp2p.graph_runner][INFO] - Test, Round 41 : loss => 2.115953680115938,  accuracy: 0.233
[2025-09-11 17:40:56,422][flp2p.graph_runner][INFO] - Train, Round 42 : loss => 2.0396565136810136,  accuracy: 0.27352, gradient_norm : 1.6437442060681178
[2025-09-11 17:41:10,010][flp2p.graph_runner][INFO] - Test, Round 42 : loss => 2.1049735109090806,  accuracy: 0.2356
[2025-09-11 17:41:44,804][flp2p.graph_runner][INFO] - Train, Round 43 : loss => 2.0205745451152324,  accuracy: 0.28214000000000006, gradient_norm : 1.7830906826841777
[2025-09-11 17:41:58,642][flp2p.graph_runner][INFO] - Test, Round 43 : loss => 2.091589571130276,  accuracy: 0.2451
[2025-09-11 17:42:33,583][flp2p.graph_runner][INFO] - Train, Round 44 : loss => 2.0015472561617687,  accuracy: 0.2894733333333332, gradient_norm : 1.8737257305178423
[2025-09-11 17:42:47,250][flp2p.graph_runner][INFO] - Test, Round 44 : loss => 2.079889682126045,  accuracy: 0.2499
[2025-09-11 17:43:21,970][flp2p.graph_runner][INFO] - Train, Round 45 : loss => 1.9824965432286268,  accuracy: 0.29828, gradient_norm : 2.022404396682038
[2025-09-11 17:43:35,811][flp2p.graph_runner][INFO] - Test, Round 45 : loss => 2.068271579670906,  accuracy: 0.2602
[2025-09-11 17:44:10,775][flp2p.graph_runner][INFO] - Train, Round 46 : loss => 1.9637570421894395,  accuracy: 0.30484666666666665, gradient_norm : 2.074941478380777
[2025-09-11 17:44:24,575][flp2p.graph_runner][INFO] - Test, Round 46 : loss => 2.057402243411541,  accuracy: 0.2626
[2025-09-11 17:44:59,585][flp2p.graph_runner][INFO] - Train, Round 47 : loss => 1.945650126685699,  accuracy: 0.3128599999999998, gradient_norm : 2.2372343418841143
[2025-09-11 17:45:13,253][flp2p.graph_runner][INFO] - Test, Round 47 : loss => 2.046919129437208,  accuracy: 0.2661
[2025-09-11 17:45:48,064][flp2p.graph_runner][INFO] - Train, Round 48 : loss => 1.9274923143287495,  accuracy: 0.31958000000000003, gradient_norm : 2.3527453322698717
[2025-09-11 17:46:01,868][flp2p.graph_runner][INFO] - Test, Round 48 : loss => 2.0389097589969634,  accuracy: 0.274
[2025-09-11 17:46:36,698][flp2p.graph_runner][INFO] - Train, Round 49 : loss => 1.9101605491836857,  accuracy: 0.32575333333333334, gradient_norm : 2.494479010804376
[2025-09-11 17:46:50,283][flp2p.graph_runner][INFO] - Test, Round 49 : loss => 2.0318708917081354,  accuracy: 0.2694
[2025-09-11 17:47:25,239][flp2p.graph_runner][INFO] - Train, Round 50 : loss => 1.8934833711882428,  accuracy: 0.33126000000000005, gradient_norm : 2.620564360958548
[2025-09-11 17:47:38,865][flp2p.graph_runner][INFO] - Test, Round 50 : loss => 2.023847130525112,  accuracy: 0.2788
[2025-09-11 17:48:13,751][flp2p.graph_runner][INFO] - Train, Round 51 : loss => 1.8755158298710997,  accuracy: 0.3381199999999999, gradient_norm : 2.7550020615398245
[2025-09-11 17:48:27,612][flp2p.graph_runner][INFO] - Test, Round 51 : loss => 2.016956892222166,  accuracy: 0.2748
[2025-09-11 17:49:02,491][flp2p.graph_runner][INFO] - Train, Round 52 : loss => 1.8600197950998945,  accuracy: 0.3431866666666668, gradient_norm : 3.023054489673557
[2025-09-11 17:49:16,295][flp2p.graph_runner][INFO] - Test, Round 52 : loss => 2.011884151953459,  accuracy: 0.2796
[2025-09-11 17:49:51,259][flp2p.graph_runner][INFO] - Train, Round 53 : loss => 1.843613796780507,  accuracy: 0.34882, gradient_norm : 3.0472868832419056
[2025-09-11 17:50:05,016][flp2p.graph_runner][INFO] - Test, Round 53 : loss => 2.004972793507576,  accuracy: 0.2794
[2025-09-11 17:50:40,085][flp2p.graph_runner][INFO] - Train, Round 54 : loss => 1.828149841229121,  accuracy: 0.3529933333333334, gradient_norm : 3.2719497209481316
[2025-09-11 17:50:53,906][flp2p.graph_runner][INFO] - Test, Round 54 : loss => 2.005650111532211,  accuracy: 0.283
[2025-09-11 17:51:28,933][flp2p.graph_runner][INFO] - Train, Round 55 : loss => 1.8120041601856536,  accuracy: 0.35900666666666664, gradient_norm : 3.293162052835568
[2025-09-11 17:51:42,634][flp2p.graph_runner][INFO] - Test, Round 55 : loss => 1.9984676025032997,  accuracy: 0.2851
[2025-09-11 17:52:17,397][flp2p.graph_runner][INFO] - Train, Round 56 : loss => 1.7965397613743934,  accuracy: 0.36458, gradient_norm : 3.496572897560634
[2025-09-11 17:52:30,995][flp2p.graph_runner][INFO] - Test, Round 56 : loss => 2.0027382271885874,  accuracy: 0.2889
[2025-09-11 17:53:05,493][flp2p.graph_runner][INFO] - Train, Round 57 : loss => 1.7830510906378427,  accuracy: 0.3696066666666667, gradient_norm : 3.7565545732183794
[2025-09-11 17:53:19,055][flp2p.graph_runner][INFO] - Test, Round 57 : loss => 1.9919413093149663,  accuracy: 0.2903
[2025-09-11 17:53:53,837][flp2p.graph_runner][INFO] - Train, Round 58 : loss => 1.7663918564220273,  accuracy: 0.37475333333333344, gradient_norm : 3.579867193754582
[2025-09-11 17:54:07,421][flp2p.graph_runner][INFO] - Test, Round 58 : loss => 1.9892284006893635,  accuracy: 0.2906
[2025-09-11 17:54:42,225][flp2p.graph_runner][INFO] - Train, Round 59 : loss => 1.7517752940456068,  accuracy: 0.38031333333333345, gradient_norm : 3.7786822354912295
[2025-09-11 17:54:55,417][flp2p.graph_runner][INFO] - Test, Round 59 : loss => 1.9876985581636428,  accuracy: 0.2916
[2025-09-11 17:55:30,063][flp2p.graph_runner][INFO] - Train, Round 60 : loss => 1.7331108080844078,  accuracy: 0.3885066666666667, gradient_norm : 4.185025653233492
[2025-09-11 17:55:44,558][flp2p.graph_runner][INFO] - Test, Round 60 : loss => 1.9947342381358146,  accuracy: 0.2939
[2025-09-11 17:56:19,306][flp2p.graph_runner][INFO] - Train, Round 61 : loss => 1.7215885568161804,  accuracy: 0.3912266666666668, gradient_norm : 4.16139483591278
[2025-09-11 17:56:38,180][flp2p.graph_runner][INFO] - Test, Round 61 : loss => 1.993821885818243,  accuracy: 0.2948
[2025-09-11 17:57:13,010][flp2p.graph_runner][INFO] - Train, Round 62 : loss => 1.7049796016017595,  accuracy: 0.3977466666666666, gradient_norm : 4.2120474330045985
[2025-09-11 17:57:33,619][flp2p.graph_runner][INFO] - Test, Round 62 : loss => 1.9920282132923604,  accuracy: 0.2901
[2025-09-11 17:58:08,567][flp2p.graph_runner][INFO] - Train, Round 63 : loss => 1.6901303108533219,  accuracy: 0.4035600000000001, gradient_norm : 4.359432714011523
[2025-09-11 17:58:27,411][flp2p.graph_runner][INFO] - Test, Round 63 : loss => 1.984044697511196,  accuracy: 0.2956
[2025-09-11 17:59:02,122][flp2p.graph_runner][INFO] - Train, Round 64 : loss => 1.6682617266476147,  accuracy: 0.4125199999999997, gradient_norm : 4.290032343456218
[2025-09-11 17:59:18,846][flp2p.graph_runner][INFO] - Test, Round 64 : loss => 1.9871318951666355,  accuracy: 0.2943
[2025-09-11 17:59:53,565][flp2p.graph_runner][INFO] - Train, Round 65 : loss => 1.65295988470316,  accuracy: 0.4165, gradient_norm : 4.589010587721861
[2025-09-11 18:00:09,003][flp2p.graph_runner][INFO] - Test, Round 65 : loss => 1.990474093580246,  accuracy: 0.2963
[2025-09-11 18:00:43,694][flp2p.graph_runner][INFO] - Train, Round 66 : loss => 1.6355094814797244,  accuracy: 0.42450666666666664, gradient_norm : 4.534419246156185
[2025-09-11 18:00:57,965][flp2p.graph_runner][INFO] - Test, Round 66 : loss => 1.9859380789220333,  accuracy: 0.294
[2025-09-11 18:01:32,635][flp2p.graph_runner][INFO] - Train, Round 67 : loss => 1.620433816313744,  accuracy: 0.4303266666666665, gradient_norm : 4.76379156616313
[2025-09-11 18:01:46,826][flp2p.graph_runner][INFO] - Test, Round 67 : loss => 1.990558223849535,  accuracy: 0.2942
[2025-09-11 18:02:21,711][flp2p.graph_runner][INFO] - Train, Round 68 : loss => 1.6037714758018649,  accuracy: 0.43549333333333334, gradient_norm : 4.801590097137057
[2025-09-11 18:02:35,669][flp2p.graph_runner][INFO] - Test, Round 68 : loss => 1.9915417839705944,  accuracy: 0.3025
[2025-09-11 18:03:10,475][flp2p.graph_runner][INFO] - Train, Round 69 : loss => 1.5792312424381576,  accuracy: 0.44438, gradient_norm : 4.96101315701416
[2025-09-11 18:03:24,359][flp2p.graph_runner][INFO] - Test, Round 69 : loss => 1.9929490446209908,  accuracy: 0.2989
[2025-09-11 18:03:59,190][flp2p.graph_runner][INFO] - Train, Round 70 : loss => 1.5625683687627308,  accuracy: 0.45121999999999995, gradient_norm : 5.082861991092521
[2025-09-11 18:04:13,134][flp2p.graph_runner][INFO] - Test, Round 70 : loss => 1.9994165951013565,  accuracy: 0.299
[2025-09-11 18:04:48,156][flp2p.graph_runner][INFO] - Train, Round 71 : loss => 1.5421860328316686,  accuracy: 0.45926666666666677, gradient_norm : 5.031176624448045
[2025-09-11 18:05:02,152][flp2p.graph_runner][INFO] - Test, Round 71 : loss => 1.9994784952044486,  accuracy: 0.3015
[2025-09-11 18:05:37,195][flp2p.graph_runner][INFO] - Train, Round 72 : loss => 1.5241683419793841,  accuracy: 0.4656266666666665, gradient_norm : 5.174075951761368
[2025-09-11 18:05:51,024][flp2p.graph_runner][INFO] - Test, Round 72 : loss => 1.998616041237116,  accuracy: 0.3025
[2025-09-11 18:06:25,958][flp2p.graph_runner][INFO] - Train, Round 73 : loss => 1.5022096790373323,  accuracy: 0.4737466666666665, gradient_norm : 5.38748087953024
[2025-09-11 18:06:39,962][flp2p.graph_runner][INFO] - Test, Round 73 : loss => 2.0006293976426126,  accuracy: 0.2996
[2025-09-11 18:07:14,805][flp2p.graph_runner][INFO] - Train, Round 74 : loss => 1.4782075863083202,  accuracy: 0.4818066666666667, gradient_norm : 5.685019986752053
[2025-09-11 18:07:28,638][flp2p.graph_runner][INFO] - Test, Round 74 : loss => 2.0247432398855687,  accuracy: 0.3042
[2025-09-11 18:08:03,263][flp2p.graph_runner][INFO] - Train, Round 75 : loss => 1.460251190041502,  accuracy: 0.48753333333333326, gradient_norm : 5.620063560087634
[2025-09-11 18:08:16,904][flp2p.graph_runner][INFO] - Test, Round 75 : loss => 2.018368959981203,  accuracy: 0.3003
[2025-09-11 18:08:52,032][flp2p.graph_runner][INFO] - Train, Round 76 : loss => 1.4337246963133419,  accuracy: 0.49624000000000024, gradient_norm : 5.498751016437305
[2025-09-11 18:09:05,771][flp2p.graph_runner][INFO] - Test, Round 76 : loss => 2.0171313431441784,  accuracy: 0.3021
[2025-09-11 18:09:40,675][flp2p.graph_runner][INFO] - Train, Round 77 : loss => 1.4162643116960918,  accuracy: 0.5040466666666666, gradient_norm : 5.511261730144831
[2025-09-11 18:09:54,440][flp2p.graph_runner][INFO] - Test, Round 77 : loss => 2.0231638913929464,  accuracy: 0.3005
[2025-09-11 18:10:29,050][flp2p.graph_runner][INFO] - Train, Round 78 : loss => 1.3893194713443509,  accuracy: 0.5140800000000001, gradient_norm : 5.980025478259183
[2025-09-11 18:10:42,899][flp2p.graph_runner][INFO] - Test, Round 78 : loss => 2.0417087339520457,  accuracy: 0.3059
[2025-09-11 18:11:17,978][flp2p.graph_runner][INFO] - Train, Round 79 : loss => 1.3652345291525128,  accuracy: 0.52394, gradient_norm : 6.011927140667078
[2025-09-11 18:11:31,678][flp2p.graph_runner][INFO] - Test, Round 79 : loss => 2.0512406663656235,  accuracy: 0.305
[2025-09-11 18:12:06,594][flp2p.graph_runner][INFO] - Train, Round 80 : loss => 1.3428206908206144,  accuracy: 0.5315600000000003, gradient_norm : 6.094586548897739
[2025-09-11 18:12:20,229][flp2p.graph_runner][INFO] - Test, Round 80 : loss => 2.0646540894687178,  accuracy: 0.3014
[2025-09-11 18:12:55,153][flp2p.graph_runner][INFO] - Train, Round 81 : loss => 1.317154759913683,  accuracy: 0.5413133333333332, gradient_norm : 6.48989452341108
[2025-09-11 18:13:08,759][flp2p.graph_runner][INFO] - Test, Round 81 : loss => 2.0707189935684203,  accuracy: 0.3062
[2025-09-11 18:13:43,620][flp2p.graph_runner][INFO] - Train, Round 82 : loss => 1.2951585863282287,  accuracy: 0.5504666666666667, gradient_norm : 6.487225321943491
[2025-09-11 18:13:57,302][flp2p.graph_runner][INFO] - Test, Round 82 : loss => 2.066572027271986,  accuracy: 0.3045
[2025-09-11 18:14:32,371][flp2p.graph_runner][INFO] - Train, Round 83 : loss => 1.2640408292661112,  accuracy: 0.5614399999999996, gradient_norm : 6.697494237240282
[2025-09-11 18:14:46,071][flp2p.graph_runner][INFO] - Test, Round 83 : loss => 2.0936872417628765,  accuracy: 0.3024
[2025-09-11 18:15:20,681][flp2p.graph_runner][INFO] - Train, Round 84 : loss => 1.2415045801550146,  accuracy: 0.5700400000000001, gradient_norm : 6.84778428132434
[2025-09-11 18:15:34,691][flp2p.graph_runner][INFO] - Test, Round 84 : loss => 2.106819394040108,  accuracy: 0.3077
[2025-09-11 18:16:09,528][flp2p.graph_runner][INFO] - Train, Round 85 : loss => 1.212260276352366,  accuracy: 0.5813599999999999, gradient_norm : 6.659314889128756
[2025-09-11 18:16:23,361][flp2p.graph_runner][INFO] - Test, Round 85 : loss => 2.115162897324562,  accuracy: 0.3111
[2025-09-11 18:16:57,879][flp2p.graph_runner][INFO] - Train, Round 86 : loss => 1.1908341864993173,  accuracy: 0.5884866666666667, gradient_norm : 7.077219499498605
[2025-09-11 18:17:11,534][flp2p.graph_runner][INFO] - Test, Round 86 : loss => 2.137417422759533,  accuracy: 0.3066
[2025-09-11 18:17:46,585][flp2p.graph_runner][INFO] - Train, Round 87 : loss => 1.1577722976605098,  accuracy: 0.60232, gradient_norm : 6.93442531847807
[2025-09-11 18:18:00,185][flp2p.graph_runner][INFO] - Test, Round 87 : loss => 2.135609493470192,  accuracy: 0.3159
[2025-09-11 18:18:34,913][flp2p.graph_runner][INFO] - Train, Round 88 : loss => 1.1300269200901192,  accuracy: 0.6121666666666666, gradient_norm : 6.991031750248628
[2025-09-11 18:18:48,586][flp2p.graph_runner][INFO] - Test, Round 88 : loss => 2.164757368963957,  accuracy: 0.3126
[2025-09-11 18:19:23,373][flp2p.graph_runner][INFO] - Train, Round 89 : loss => 1.1059510329862439,  accuracy: 0.62136, gradient_norm : 7.255774528637692
[2025-09-11 18:19:37,025][flp2p.graph_runner][INFO] - Test, Round 89 : loss => 2.1917447465538977,  accuracy: 0.3055
[2025-09-11 18:20:11,959][flp2p.graph_runner][INFO] - Train, Round 90 : loss => 1.077915155664087,  accuracy: 0.6320799999999999, gradient_norm : 7.681908702636896
[2025-09-11 18:20:25,493][flp2p.graph_runner][INFO] - Test, Round 90 : loss => 2.231691150701046,  accuracy: 0.3066
[2025-09-11 18:21:00,392][flp2p.graph_runner][INFO] - Train, Round 91 : loss => 1.0465387434760733,  accuracy: 0.6424533333333334, gradient_norm : 7.529478002474411
[2025-09-11 18:21:13,858][flp2p.graph_runner][INFO] - Test, Round 91 : loss => 2.2247534631848334,  accuracy: 0.3087
[2025-09-11 18:21:48,674][flp2p.graph_runner][INFO] - Train, Round 92 : loss => 1.0108262873813516,  accuracy: 0.6568066666666665, gradient_norm : 6.92393273831433
[2025-09-11 18:22:02,131][flp2p.graph_runner][INFO] - Test, Round 92 : loss => 2.2351679709196093,  accuracy: 0.3141
[2025-09-11 18:22:37,280][flp2p.graph_runner][INFO] - Train, Round 93 : loss => 0.9892784654100736,  accuracy: 0.6652466666666668, gradient_norm : 7.550834926963559
[2025-09-11 18:22:50,699][flp2p.graph_runner][INFO] - Test, Round 93 : loss => 2.2713424332439898,  accuracy: 0.3098
[2025-09-11 18:23:25,265][flp2p.graph_runner][INFO] - Train, Round 94 : loss => 0.9603679616625106,  accuracy: 0.6770533333333333, gradient_norm : 7.5085072180328885
[2025-09-11 18:23:41,619][flp2p.graph_runner][INFO] - Test, Round 94 : loss => 2.2966966233730317,  accuracy: 0.3109
[2025-09-11 18:24:16,399][flp2p.graph_runner][INFO] - Train, Round 95 : loss => 0.9268842285747333,  accuracy: 0.68896, gradient_norm : 7.908255836719657
[2025-09-11 18:24:36,078][flp2p.graph_runner][INFO] - Test, Round 95 : loss => 2.344851263523102,  accuracy: 0.3097
[2025-09-11 18:25:10,740][flp2p.graph_runner][INFO] - Train, Round 96 : loss => 0.8945711292947336,  accuracy: 0.6999666666666667, gradient_norm : 7.937292549575063
[2025-09-11 18:25:30,788][flp2p.graph_runner][INFO] - Test, Round 96 : loss => 2.36789213552475,  accuracy: 0.3068
[2025-09-11 18:26:05,615][flp2p.graph_runner][INFO] - Train, Round 97 : loss => 0.8661768592894079,  accuracy: 0.7129000000000002, gradient_norm : 8.040877762883703
[2025-09-11 18:26:22,330][flp2p.graph_runner][INFO] - Test, Round 97 : loss => 2.384530472671986,  accuracy: 0.309
[2025-09-11 18:26:56,877][flp2p.graph_runner][INFO] - Train, Round 98 : loss => 0.8320806041111549,  accuracy: 0.7248399999999998, gradient_norm : 7.886298047371286
[2025-09-11 18:27:11,404][flp2p.graph_runner][INFO] - Test, Round 98 : loss => 2.412095751416683,  accuracy: 0.3118
[2025-09-11 18:27:46,230][flp2p.graph_runner][INFO] - Train, Round 99 : loss => 0.8008101112395523,  accuracy: 0.7392733333333331, gradient_norm : 7.880648725857751
[2025-09-11 18:28:00,260][flp2p.graph_runner][INFO] - Test, Round 99 : loss => 2.45765735629797,  accuracy: 0.3057
[2025-09-11 18:28:34,963][flp2p.graph_runner][INFO] - Train, Round 100 : loss => 0.7750860210632281,  accuracy: 0.7468933333333334, gradient_norm : 8.295162101473572
[2025-09-11 18:28:48,988][flp2p.graph_runner][INFO] - Test, Round 100 : loss => 2.5143280774772165,  accuracy: 0.3082
[2025-09-11 18:29:23,781][flp2p.graph_runner][INFO] - Train, Round 101 : loss => 0.7424196802886821,  accuracy: 0.7598066666666664, gradient_norm : 8.204902155986042
[2025-09-11 18:29:37,808][flp2p.graph_runner][INFO] - Test, Round 101 : loss => 2.548852800220251,  accuracy: 0.3101
[2025-09-11 18:30:12,496][flp2p.graph_runner][INFO] - Train, Round 102 : loss => 0.7081952839593093,  accuracy: 0.7734, gradient_norm : 7.809257836290735
[2025-09-11 18:30:26,528][flp2p.graph_runner][INFO] - Test, Round 102 : loss => 2.546147800588608,  accuracy: 0.3108
[2025-09-11 18:31:01,398][flp2p.graph_runner][INFO] - Train, Round 103 : loss => 0.6736158913063507,  accuracy: 0.7867333333333332, gradient_norm : 7.798529794561208
[2025-09-11 18:31:15,199][flp2p.graph_runner][INFO] - Test, Round 103 : loss => 2.5823499635219576,  accuracy: 0.3094
[2025-09-11 18:31:50,048][flp2p.graph_runner][INFO] - Train, Round 104 : loss => 0.6483766063675283,  accuracy: 0.7960933333333334, gradient_norm : 7.728013876614474
[2025-09-11 18:32:03,926][flp2p.graph_runner][INFO] - Test, Round 104 : loss => 2.6301611414015293,  accuracy: 0.3136
[2025-09-11 18:32:38,895][flp2p.graph_runner][INFO] - Train, Round 105 : loss => 0.6119223144153755,  accuracy: 0.8096933333333333, gradient_norm : 7.907970673519251
[2025-09-11 18:32:52,840][flp2p.graph_runner][INFO] - Test, Round 105 : loss => 2.7022932062208653,  accuracy: 0.3059
[2025-09-11 18:33:27,722][flp2p.graph_runner][INFO] - Train, Round 106 : loss => 0.5800383204780521,  accuracy: 0.8220466666666668, gradient_norm : 7.906748553957235
[2025-09-11 18:33:41,452][flp2p.graph_runner][INFO] - Test, Round 106 : loss => 2.7362603375017645,  accuracy: 0.3085
[2025-09-11 18:34:16,318][flp2p.graph_runner][INFO] - Train, Round 107 : loss => 0.5593721937574448,  accuracy: 0.8304666666666665, gradient_norm : 8.017735495174225
[2025-09-11 18:34:30,030][flp2p.graph_runner][INFO] - Test, Round 107 : loss => 2.805126259458065,  accuracy: 0.3115
[2025-09-11 18:35:04,922][flp2p.graph_runner][INFO] - Train, Round 108 : loss => 0.5264811017457397,  accuracy: 0.8442133333333335, gradient_norm : 6.925168202296264
[2025-09-11 18:35:18,620][flp2p.graph_runner][INFO] - Test, Round 108 : loss => 2.805501138395071,  accuracy: 0.3092
[2025-09-11 18:35:53,213][flp2p.graph_runner][INFO] - Train, Round 109 : loss => 0.494491787372778,  accuracy: 0.8562200000000001, gradient_norm : 7.4651210165861155
[2025-09-11 18:36:06,707][flp2p.graph_runner][INFO] - Test, Round 109 : loss => 2.890172600102425,  accuracy: 0.3058
[2025-09-11 18:36:41,551][flp2p.graph_runner][INFO] - Train, Round 110 : loss => 0.46490234861150365,  accuracy: 0.8701333333333335, gradient_norm : 7.411308735668624
[2025-09-11 18:36:55,239][flp2p.graph_runner][INFO] - Test, Round 110 : loss => 2.9933038953006266,  accuracy: 0.3047
[2025-09-11 18:37:30,138][flp2p.graph_runner][INFO] - Train, Round 111 : loss => 0.43938054193742543,  accuracy: 0.8797199999999998, gradient_norm : 6.470280626397388
[2025-09-11 18:37:43,992][flp2p.graph_runner][INFO] - Test, Round 111 : loss => 2.933155686903,  accuracy: 0.312
[2025-09-11 18:38:18,967][flp2p.graph_runner][INFO] - Train, Round 112 : loss => 0.40994564491324126,  accuracy: 0.8906333333333332, gradient_norm : 6.954050093585047
[2025-09-11 18:38:33,034][flp2p.graph_runner][INFO] - Test, Round 112 : loss => 3.0238510466575623,  accuracy: 0.3134
[2025-09-11 18:39:07,661][flp2p.graph_runner][INFO] - Train, Round 113 : loss => 0.3727316505834461,  accuracy: 0.9033533333333331, gradient_norm : 6.48188960342571
[2025-09-11 18:39:21,419][flp2p.graph_runner][INFO] - Test, Round 113 : loss => 3.0896679735422135,  accuracy: 0.3091
[2025-09-11 18:39:56,230][flp2p.graph_runner][INFO] - Train, Round 114 : loss => 0.36021198720205566,  accuracy: 0.9093066666666666, gradient_norm : 6.402432644861197
[2025-09-11 18:40:09,892][flp2p.graph_runner][INFO] - Test, Round 114 : loss => 3.1262314639806745,  accuracy: 0.3073
[2025-09-11 18:40:44,788][flp2p.graph_runner][INFO] - Train, Round 115 : loss => 0.3290972155689573,  accuracy: 0.9193533333333332, gradient_norm : 6.172473367119138
[2025-09-11 18:40:58,548][flp2p.graph_runner][INFO] - Test, Round 115 : loss => 3.191509212356806,  accuracy: 0.3121
[2025-09-11 18:41:33,676][flp2p.graph_runner][INFO] - Train, Round 116 : loss => 0.31922545122603574,  accuracy: 0.9243133333333335, gradient_norm : 5.747927122035805
[2025-09-11 18:41:47,523][flp2p.graph_runner][INFO] - Test, Round 116 : loss => 3.2293482865095138,  accuracy: 0.3084
[2025-09-11 18:42:22,355][flp2p.graph_runner][INFO] - Train, Round 117 : loss => 0.282500476151084,  accuracy: 0.935853333333333, gradient_norm : 5.86754972455339
[2025-09-11 18:42:36,143][flp2p.graph_runner][INFO] - Test, Round 117 : loss => 3.3052684634447096,  accuracy: 0.3129
[2025-09-11 18:43:11,077][flp2p.graph_runner][INFO] - Train, Round 118 : loss => 0.2753996417733529,  accuracy: 0.9395133333333335, gradient_norm : 5.686855815441298
[2025-09-11 18:43:24,788][flp2p.graph_runner][INFO] - Test, Round 118 : loss => 3.3508780682325363,  accuracy: 0.3117
[2025-09-11 18:43:59,641][flp2p.graph_runner][INFO] - Train, Round 119 : loss => 0.23606989204262693,  accuracy: 0.9540333333333332, gradient_norm : 4.935457351463033
[2025-09-11 18:44:13,374][flp2p.graph_runner][INFO] - Test, Round 119 : loss => 3.3910722863078115,  accuracy: 0.313
[2025-09-11 18:44:48,113][flp2p.graph_runner][INFO] - Train, Round 120 : loss => 0.2384748342167586,  accuracy: 0.9521666666666664, gradient_norm : 5.035910532502758
[2025-09-11 18:45:01,699][flp2p.graph_runner][INFO] - Test, Round 120 : loss => 3.4458149799346924,  accuracy: 0.3094
[2025-09-11 18:45:36,470][flp2p.graph_runner][INFO] - Train, Round 121 : loss => 0.20027700972898557,  accuracy: 0.96396, gradient_norm : 4.510527586781361
[2025-09-11 18:45:50,036][flp2p.graph_runner][INFO] - Test, Round 121 : loss => 3.480830199956894,  accuracy: 0.3138
[2025-09-11 18:46:25,089][flp2p.graph_runner][INFO] - Train, Round 122 : loss => 0.180869163608489,  accuracy: 0.9696466666666671, gradient_norm : 3.8515871537425137
[2025-09-11 18:46:38,819][flp2p.graph_runner][INFO] - Test, Round 122 : loss => 3.5426182821512224,  accuracy: 0.3138
[2025-09-11 18:47:13,493][flp2p.graph_runner][INFO] - Train, Round 123 : loss => 0.15733447735275452,  accuracy: 0.9765866666666666, gradient_norm : 4.194655889958476
[2025-09-11 18:47:27,096][flp2p.graph_runner][INFO] - Test, Round 123 : loss => 3.650475623130798,  accuracy: 0.3102
[2025-09-11 18:48:02,085][flp2p.graph_runner][INFO] - Train, Round 124 : loss => 0.1614992507679077,  accuracy: 0.9743333333333336, gradient_norm : 3.33543071272282
[2025-09-11 18:48:15,496][flp2p.graph_runner][INFO] - Test, Round 124 : loss => 3.6817250163555144,  accuracy: 0.3128
[2025-09-11 18:48:50,411][flp2p.graph_runner][INFO] - Train, Round 125 : loss => 0.13643896471709016,  accuracy: 0.9819066666666667, gradient_norm : 3.511432968661294
[2025-09-11 18:49:06,008][flp2p.graph_runner][INFO] - Test, Round 125 : loss => 3.746151686489582,  accuracy: 0.3148
[2025-09-11 18:49:40,738][flp2p.graph_runner][INFO] - Train, Round 126 : loss => 0.12109668422723185,  accuracy: 0.9846133333333336, gradient_norm : 3.0631714989815726
[2025-09-11 18:50:00,444][flp2p.graph_runner][INFO] - Test, Round 126 : loss => 3.807196423971653,  accuracy: 0.3132
[2025-09-11 18:50:35,194][flp2p.graph_runner][INFO] - Train, Round 127 : loss => 0.10445200519093004,  accuracy: 0.9884466666666663, gradient_norm : 3.016458948735917
[2025-09-11 18:50:56,278][flp2p.graph_runner][INFO] - Test, Round 127 : loss => 3.8917398411273956,  accuracy: 0.3088
[2025-09-11 18:51:31,098][flp2p.graph_runner][INFO] - Train, Round 128 : loss => 0.10158622948142387,  accuracy: 0.9882266666666663, gradient_norm : 2.5739286479156083
[2025-09-11 18:51:48,799][flp2p.graph_runner][INFO] - Test, Round 128 : loss => 3.9051158999919893,  accuracy: 0.3088
[2025-09-11 18:52:23,900][flp2p.graph_runner][INFO] - Train, Round 129 : loss => 0.08569006783111643,  accuracy: 0.99268, gradient_norm : 2.6188651604524797
[2025-09-11 18:52:39,756][flp2p.graph_runner][INFO] - Test, Round 129 : loss => 3.9718444800138473,  accuracy: 0.3129
[2025-09-11 18:53:14,644][flp2p.graph_runner][INFO] - Train, Round 130 : loss => 0.09200463011899654,  accuracy: 0.9908933333333334, gradient_norm : 2.788670442449484
[2025-09-11 18:53:28,599][flp2p.graph_runner][INFO] - Test, Round 130 : loss => 4.04131062040329,  accuracy: 0.3134
[2025-09-11 18:54:03,503][flp2p.graph_runner][INFO] - Train, Round 131 : loss => 0.08894547349812156,  accuracy: 0.9912200000000001, gradient_norm : 2.0001128018675898
[2025-09-11 18:54:17,642][flp2p.graph_runner][INFO] - Test, Round 131 : loss => 4.041355518305302,  accuracy: 0.3128
[2025-09-11 18:54:52,370][flp2p.graph_runner][INFO] - Train, Round 132 : loss => 0.0676965268169685,  accuracy: 0.9948666666666667, gradient_norm : 2.0817174960623444
[2025-09-11 18:55:06,706][flp2p.graph_runner][INFO] - Test, Round 132 : loss => 4.112890415668487,  accuracy: 0.3126
[2025-09-11 18:55:41,403][flp2p.graph_runner][INFO] - Train, Round 133 : loss => 0.0626263136808605,  accuracy: 0.9958199999999994, gradient_norm : 1.9400019755719242
[2025-09-11 18:55:55,304][flp2p.graph_runner][INFO] - Test, Round 133 : loss => 4.163380396497249,  accuracy: 0.3135
[2025-09-11 18:56:29,945][flp2p.graph_runner][INFO] - Train, Round 134 : loss => 0.05720697182890338,  accuracy: 0.9964333333333336, gradient_norm : 1.626626048200688
[2025-09-11 18:56:43,891][flp2p.graph_runner][INFO] - Test, Round 134 : loss => 4.206483591485023,  accuracy: 0.3114
[2025-09-11 18:57:18,824][flp2p.graph_runner][INFO] - Train, Round 135 : loss => 0.046813247420359395,  accuracy: 0.9984666666666666, gradient_norm : 1.40311869363801
[2025-09-11 18:57:32,694][flp2p.graph_runner][INFO] - Test, Round 135 : loss => 4.253422663593292,  accuracy: 0.3131
[2025-09-11 18:58:07,651][flp2p.graph_runner][INFO] - Train, Round 136 : loss => 0.04749336743882548,  accuracy: 0.9976466666666661, gradient_norm : 1.585475262060419
[2025-09-11 18:58:21,621][flp2p.graph_runner][INFO] - Test, Round 136 : loss => 4.30511563975811,  accuracy: 0.313
[2025-09-11 18:58:56,506][flp2p.graph_runner][INFO] - Train, Round 137 : loss => 0.04897602094414954,  accuracy: 0.9968866666666667, gradient_norm : 1.5842403354326497
[2025-09-11 18:59:10,129][flp2p.graph_runner][INFO] - Test, Round 137 : loss => 4.368860981321335,  accuracy: 0.3116
[2025-09-11 18:59:44,918][flp2p.graph_runner][INFO] - Train, Round 138 : loss => 0.055880556924191006,  accuracy: 0.9950933333333333, gradient_norm : 1.3180244404864951
[2025-09-11 18:59:58,708][flp2p.graph_runner][INFO] - Test, Round 138 : loss => 4.373561991488933,  accuracy: 0.3113
[2025-09-11 19:00:33,252][flp2p.graph_runner][INFO] - Train, Round 139 : loss => 0.03489499552951506,  accuracy: 0.9994866666666667, gradient_norm : 1.1774710231905798
[2025-09-11 19:00:47,165][flp2p.graph_runner][INFO] - Test, Round 139 : loss => 4.424751151967048,  accuracy: 0.3104
[2025-09-11 19:01:21,787][flp2p.graph_runner][INFO] - Train, Round 140 : loss => 0.0319489513671336,  accuracy: 0.9996, gradient_norm : 1.0732259202209566
[2025-09-11 19:01:35,545][flp2p.graph_runner][INFO] - Test, Round 140 : loss => 4.471028940296173,  accuracy: 0.3126
[2025-09-11 19:02:10,322][flp2p.graph_runner][INFO] - Train, Round 141 : loss => 0.029681549139398453,  accuracy: 0.9996933333333333, gradient_norm : 0.9930644956586989
[2025-09-11 19:02:24,078][flp2p.graph_runner][INFO] - Test, Round 141 : loss => 4.513670231986046,  accuracy: 0.3112
[2025-09-11 19:02:58,924][flp2p.graph_runner][INFO] - Train, Round 142 : loss => 0.02778527649507547,  accuracy: 0.9997999999999998, gradient_norm : 0.95418952729934
[2025-09-11 19:03:12,588][flp2p.graph_runner][INFO] - Test, Round 142 : loss => 4.550834456574917,  accuracy: 0.3133
[2025-09-11 19:03:47,217][flp2p.graph_runner][INFO] - Train, Round 143 : loss => 0.031074682088801632,  accuracy: 0.9988599999999999, gradient_norm : 0.9281663252514869
[2025-09-11 19:04:01,280][flp2p.graph_runner][INFO] - Test, Round 143 : loss => 4.590361930596829,  accuracy: 0.3107
[2025-09-11 19:04:36,034][flp2p.graph_runner][INFO] - Train, Round 144 : loss => 0.025499553399082893,  accuracy: 0.9996866666666667, gradient_norm : 0.8709355002105332
[2025-09-11 19:04:49,736][flp2p.graph_runner][INFO] - Test, Round 144 : loss => 4.6241043097257615,  accuracy: 0.3119
[2025-09-11 19:05:24,414][flp2p.graph_runner][INFO] - Train, Round 145 : loss => 0.02342020798455148,  accuracy: 0.9998933333333334, gradient_norm : 0.7726661705089348
[2025-09-11 19:05:38,051][flp2p.graph_runner][INFO] - Test, Round 145 : loss => 4.664510758459568,  accuracy: 0.3108
[2025-09-11 19:06:12,704][flp2p.graph_runner][INFO] - Train, Round 146 : loss => 0.02217336388227219,  accuracy: 0.9999199999999998, gradient_norm : 0.7771897264242321
[2025-09-11 19:06:26,489][flp2p.graph_runner][INFO] - Test, Round 146 : loss => 4.6995650419473645,  accuracy: 0.3104
[2025-09-11 19:07:01,140][flp2p.graph_runner][INFO] - Train, Round 147 : loss => 0.02106846008061742,  accuracy: 0.9999533333333335, gradient_norm : 0.7865277780905114
[2025-09-11 19:07:14,997][flp2p.graph_runner][INFO] - Test, Round 147 : loss => 4.733808765757084,  accuracy: 0.3103
[2025-09-11 19:07:49,896][flp2p.graph_runner][INFO] - Train, Round 148 : loss => 0.035476644456891034,  accuracy: 0.9975266666666667, gradient_norm : 0.7891997004304558
[2025-09-11 19:08:03,754][flp2p.graph_runner][INFO] - Test, Round 148 : loss => 4.756164126634598,  accuracy: 0.3103
[2025-09-11 19:08:38,416][flp2p.graph_runner][INFO] - Train, Round 149 : loss => 0.02219800362809717,  accuracy: 0.9992533333333333, gradient_norm : 0.7114432308200033
[2025-09-11 19:08:52,312][flp2p.graph_runner][INFO] - Test, Round 149 : loss => 4.788687133717537,  accuracy: 0.3103
[2025-09-11 19:09:27,040][flp2p.graph_runner][INFO] - Train, Round 150 : loss => 0.019546871220421362,  accuracy: 0.9998133333333334, gradient_norm : 0.6660807023181132
[2025-09-11 19:09:40,583][flp2p.graph_runner][INFO] - Test, Round 150 : loss => 4.823445907986164,  accuracy: 0.3098
[2025-09-11 19:10:15,462][flp2p.graph_runner][INFO] - Train, Round 151 : loss => 0.018210496084842213,  accuracy: 0.9999133333333333, gradient_norm : 0.6595868007699097
[2025-09-11 19:10:28,929][flp2p.graph_runner][INFO] - Test, Round 151 : loss => 4.85238643847704,  accuracy: 0.3095
[2025-09-11 19:11:03,863][flp2p.graph_runner][INFO] - Train, Round 152 : loss => 0.017245256185027154,  accuracy: 0.9999533333333332, gradient_norm : 0.6799901548612871
[2025-09-11 19:11:17,203][flp2p.graph_runner][INFO] - Test, Round 152 : loss => 4.8814582226514815,  accuracy: 0.311
[2025-09-11 19:11:51,967][flp2p.graph_runner][INFO] - Train, Round 153 : loss => 0.016429359944110423,  accuracy: 0.9999533333333331, gradient_norm : 0.6197216102226415
[2025-09-11 19:12:05,294][flp2p.graph_runner][INFO] - Test, Round 153 : loss => 4.913570440661907,  accuracy: 0.3111
[2025-09-11 19:12:40,134][flp2p.graph_runner][INFO] - Train, Round 154 : loss => 0.033273345799922624,  accuracy: 0.99842, gradient_norm : 0.8100946321338434
[2025-09-11 19:12:56,582][flp2p.graph_runner][INFO] - Test, Round 154 : loss => 4.925191479802131,  accuracy: 0.3105
[2025-09-11 19:13:31,450][flp2p.graph_runner][INFO] - Train, Round 155 : loss => 0.023077681845946545,  accuracy: 0.99762, gradient_norm : 0.5901307546774648
[2025-09-11 19:13:50,923][flp2p.graph_runner][INFO] - Test, Round 155 : loss => 4.943966794800758,  accuracy: 0.3114
[2025-09-11 19:14:25,862][flp2p.graph_runner][INFO] - Train, Round 156 : loss => 0.017441773218452,  accuracy: 0.9994866666666665, gradient_norm : 0.5551530377290155
[2025-09-11 19:14:46,603][flp2p.graph_runner][INFO] - Test, Round 156 : loss => 4.97226686193943,  accuracy: 0.3111
[2025-09-11 19:15:21,585][flp2p.graph_runner][INFO] - Train, Round 157 : loss => 0.015398571339319456,  accuracy: 0.9998333333333332, gradient_norm : 0.5252658545974801
[2025-09-11 19:15:40,654][flp2p.graph_runner][INFO] - Test, Round 157 : loss => 4.999358314180374,  accuracy: 0.311
[2025-09-11 19:16:15,324][flp2p.graph_runner][INFO] - Train, Round 158 : loss => 0.01425606151305449,  accuracy: 0.9999533333333332, gradient_norm : 0.5394370479951739
[2025-09-11 19:16:31,001][flp2p.graph_runner][INFO] - Test, Round 158 : loss => 5.028801076209545,  accuracy: 0.3109
[2025-09-11 19:17:05,925][flp2p.graph_runner][INFO] - Train, Round 159 : loss => 0.01356010701313304,  accuracy: 0.99998, gradient_norm : 0.48407002300048063
[2025-09-11 19:17:20,088][flp2p.graph_runner][INFO] - Test, Round 159 : loss => 5.054183440208435,  accuracy: 0.3096
[2025-09-11 19:17:54,890][flp2p.graph_runner][INFO] - Train, Round 160 : loss => 0.01294442948089757,  accuracy: 0.9999933333333333, gradient_norm : 0.48046565229143084
[2025-09-11 19:18:08,612][flp2p.graph_runner][INFO] - Test, Round 160 : loss => 5.077118859302997,  accuracy: 0.309
[2025-09-11 19:18:43,529][flp2p.graph_runner][INFO] - Train, Round 161 : loss => 0.012550837233623801,  accuracy: 0.9999733333333333, gradient_norm : 0.4675388554910168
[2025-09-11 19:18:57,230][flp2p.graph_runner][INFO] - Test, Round 161 : loss => 5.103006534576416,  accuracy: 0.3089
[2025-09-11 19:19:32,137][flp2p.graph_runner][INFO] - Train, Round 162 : loss => 0.012003577425881911,  accuracy: 0.9999933333333333, gradient_norm : 0.4645700973874882
[2025-09-11 19:19:46,074][flp2p.graph_runner][INFO] - Test, Round 162 : loss => 5.1241384380579,  accuracy: 0.3092
[2025-09-11 19:20:21,000][flp2p.graph_runner][INFO] - Train, Round 163 : loss => 0.011576385525986548,  accuracy: 0.9999933333333333, gradient_norm : 0.4521566336507699
[2025-09-11 19:20:35,050][flp2p.graph_runner][INFO] - Test, Round 163 : loss => 5.145856526851654,  accuracy: 0.3102
[2025-09-11 19:21:09,613][flp2p.graph_runner][INFO] - Train, Round 164 : loss => 0.011180506240731724,  accuracy: 1.0, gradient_norm : 0.4296463349035463
[2025-09-11 19:21:23,489][flp2p.graph_runner][INFO] - Test, Round 164 : loss => 5.170039584481716,  accuracy: 0.3106
[2025-09-11 19:21:58,217][flp2p.graph_runner][INFO] - Train, Round 165 : loss => 0.01083859726709003,  accuracy: 0.9999866666666667, gradient_norm : 0.4236482419808059
[2025-09-11 19:22:12,196][flp2p.graph_runner][INFO] - Test, Round 165 : loss => 5.188373961210251,  accuracy: 0.3104
[2025-09-11 19:22:47,165][flp2p.graph_runner][INFO] - Train, Round 166 : loss => 0.010490103212650863,  accuracy: 1.0, gradient_norm : 0.422287402346505
[2025-09-11 19:23:01,007][flp2p.graph_runner][INFO] - Test, Round 166 : loss => 5.209846304702759,  accuracy: 0.3096
[2025-09-11 19:23:35,860][flp2p.graph_runner][INFO] - Train, Round 167 : loss => 0.010176882821251636,  accuracy: 1.0, gradient_norm : 0.388146634896531
[2025-09-11 19:23:49,748][flp2p.graph_runner][INFO] - Test, Round 167 : loss => 5.230318327820301,  accuracy: 0.3093
[2025-09-11 19:24:24,480][flp2p.graph_runner][INFO] - Train, Round 168 : loss => 0.009886781114231174,  accuracy: 1.0, gradient_norm : 0.3791751103643265
[2025-09-11 19:24:38,324][flp2p.graph_runner][INFO] - Test, Round 168 : loss => 5.251830390167236,  accuracy: 0.3082
[2025-09-11 19:25:12,922][flp2p.graph_runner][INFO] - Train, Round 169 : loss => 0.009631230299322243,  accuracy: 0.9999866666666667, gradient_norm : 0.3923870488935555
[2025-09-11 19:25:26,385][flp2p.graph_runner][INFO] - Test, Round 169 : loss => 5.270351703715324,  accuracy: 0.3102
[2025-09-11 19:26:01,140][flp2p.graph_runner][INFO] - Train, Round 170 : loss => 0.009345656529573412,  accuracy: 1.0, gradient_norm : 0.36081007016258454
[2025-09-11 19:26:15,049][flp2p.graph_runner][INFO] - Test, Round 170 : loss => 5.290307889413834,  accuracy: 0.3092
[2025-09-11 19:26:50,216][flp2p.graph_runner][INFO] - Train, Round 171 : loss => 0.009103420139678445,  accuracy: 1.0, gradient_norm : 0.3613783812102021
[2025-09-11 19:27:04,055][flp2p.graph_runner][INFO] - Test, Round 171 : loss => 5.307146877467632,  accuracy: 0.3093
[2025-09-11 19:27:39,119][flp2p.graph_runner][INFO] - Train, Round 172 : loss => 0.008875690218410455,  accuracy: 1.0, gradient_norm : 0.3655247992664631
[2025-09-11 19:27:52,756][flp2p.graph_runner][INFO] - Test, Round 172 : loss => 5.325156786513329,  accuracy: 0.3095
[2025-09-11 19:28:27,220][flp2p.graph_runner][INFO] - Train, Round 173 : loss => 0.008645520341815429,  accuracy: 1.0, gradient_norm : 0.33293923108791035
[2025-09-11 19:28:40,955][flp2p.graph_runner][INFO] - Test, Round 173 : loss => 5.3441872745752335,  accuracy: 0.3091
[2025-09-11 19:29:15,932][flp2p.graph_runner][INFO] - Train, Round 174 : loss => 0.008432277768733914,  accuracy: 1.0, gradient_norm : 0.32208483345607086
[2025-09-11 19:29:29,770][flp2p.graph_runner][INFO] - Test, Round 174 : loss => 5.360960812294484,  accuracy: 0.3098
[2025-09-11 19:30:04,714][flp2p.graph_runner][INFO] - Train, Round 175 : loss => 0.008229454417984618,  accuracy: 1.0, gradient_norm : 0.32875579593450616
[2025-09-11 19:30:18,378][flp2p.graph_runner][INFO] - Test, Round 175 : loss => 5.375179721367359,  accuracy: 0.3106
[2025-09-11 19:30:53,501][flp2p.graph_runner][INFO] - Train, Round 176 : loss => 0.008031865890467694,  accuracy: 1.0, gradient_norm : 0.33060462328773044
[2025-09-11 19:31:07,404][flp2p.graph_runner][INFO] - Test, Round 176 : loss => 5.392844822287559,  accuracy: 0.3094
[2025-09-11 19:31:42,142][flp2p.graph_runner][INFO] - Train, Round 177 : loss => 0.007846731797617393,  accuracy: 1.0, gradient_norm : 0.31520522103886095
[2025-09-11 19:31:55,912][flp2p.graph_runner][INFO] - Test, Round 177 : loss => 5.409552119553089,  accuracy: 0.3095
[2025-09-11 19:32:30,972][flp2p.graph_runner][INFO] - Train, Round 178 : loss => 0.007671631929697464,  accuracy: 1.0, gradient_norm : 0.3198716431780116
[2025-09-11 19:32:44,793][flp2p.graph_runner][INFO] - Test, Round 178 : loss => 5.426456640219689,  accuracy: 0.3095
[2025-09-11 19:33:19,452][flp2p.graph_runner][INFO] - Train, Round 179 : loss => 0.007500154904361503,  accuracy: 1.0, gradient_norm : 0.31377325163006875
[2025-09-11 19:33:33,268][flp2p.graph_runner][INFO] - Test, Round 179 : loss => 5.441083399403095,  accuracy: 0.3087
[2025-09-11 19:34:08,206][flp2p.graph_runner][INFO] - Train, Round 180 : loss => 0.007325544192960174,  accuracy: 1.0, gradient_norm : 0.3004866215022782
[2025-09-11 19:34:21,896][flp2p.graph_runner][INFO] - Test, Round 180 : loss => 5.456881082689762,  accuracy: 0.3087
[2025-09-11 19:34:56,944][flp2p.graph_runner][INFO] - Train, Round 181 : loss => 0.007176993860533304,  accuracy: 1.0, gradient_norm : 0.29150735147234835
[2025-09-11 19:35:10,816][flp2p.graph_runner][INFO] - Test, Round 181 : loss => 5.471610545837879,  accuracy: 0.3096
[2025-09-11 19:35:45,507][flp2p.graph_runner][INFO] - Train, Round 182 : loss => 0.0070224471752104965,  accuracy: 1.0, gradient_norm : 0.2924849883916135
[2025-09-11 19:35:58,888][flp2p.graph_runner][INFO] - Test, Round 182 : loss => 5.485991533207893,  accuracy: 0.3089
[2025-09-11 19:36:33,842][flp2p.graph_runner][INFO] - Train, Round 183 : loss => 0.006881634380357961,  accuracy: 1.0, gradient_norm : 0.2935395594603363
[2025-09-11 19:36:47,121][flp2p.graph_runner][INFO] - Test, Round 183 : loss => 5.501184112226963,  accuracy: 0.3094
[2025-09-11 19:37:21,979][flp2p.graph_runner][INFO] - Train, Round 184 : loss => 0.006734265140063748,  accuracy: 1.0, gradient_norm : 0.28244227057719984
[2025-09-11 19:37:35,394][flp2p.graph_runner][INFO] - Test, Round 184 : loss => 5.514810889494419,  accuracy: 0.3091
[2025-09-11 19:38:10,017][flp2p.graph_runner][INFO] - Train, Round 185 : loss => 0.006608319606554383,  accuracy: 1.0, gradient_norm : 0.2685759338030933
[2025-09-11 19:38:23,533][flp2p.graph_runner][INFO] - Test, Round 185 : loss => 5.531215750038624,  accuracy: 0.3089
[2025-09-11 19:38:58,357][flp2p.graph_runner][INFO] - Train, Round 186 : loss => 0.006476509308461875,  accuracy: 1.0, gradient_norm : 0.2768546918068485
[2025-09-11 19:39:15,308][flp2p.graph_runner][INFO] - Test, Round 186 : loss => 5.544379074406624,  accuracy: 0.3108
[2025-09-11 19:39:49,997][flp2p.graph_runner][INFO] - Train, Round 187 : loss => 0.006352003227026822,  accuracy: 1.0, gradient_norm : 0.2680882521176899
[2025-09-11 19:40:10,413][flp2p.graph_runner][INFO] - Test, Round 187 : loss => 5.556944804167747,  accuracy: 0.31
[2025-09-11 19:40:45,338][flp2p.graph_runner][INFO] - Train, Round 188 : loss => 0.006230031149461864,  accuracy: 1.0, gradient_norm : 0.25809999104769715
[2025-09-11 19:41:05,760][flp2p.graph_runner][INFO] - Test, Round 188 : loss => 5.570228120660782,  accuracy: 0.3087
[2025-09-11 19:41:40,398][flp2p.graph_runner][INFO] - Train, Round 189 : loss => 0.0061164011968260976,  accuracy: 1.0, gradient_norm : 0.26512968874114223
[2025-09-11 19:41:57,895][flp2p.graph_runner][INFO] - Test, Round 189 : loss => 5.5825849155545235,  accuracy: 0.3104
[2025-09-11 19:42:32,778][flp2p.graph_runner][INFO] - Train, Round 190 : loss => 0.006004813111697636,  accuracy: 1.0, gradient_norm : 0.2564805116654452
[2025-09-11 19:42:46,669][flp2p.graph_runner][INFO] - Test, Round 190 : loss => 5.595627166354657,  accuracy: 0.3092
[2025-09-11 19:43:21,631][flp2p.graph_runner][INFO] - Train, Round 191 : loss => 0.005898965367232447,  accuracy: 1.0, gradient_norm : 0.2565976735763921
[2025-09-11 19:43:35,419][flp2p.graph_runner][INFO] - Test, Round 191 : loss => 5.608111811828613,  accuracy: 0.3087
[2025-09-11 19:44:10,276][flp2p.graph_runner][INFO] - Train, Round 192 : loss => 0.005794583606669522,  accuracy: 1.0, gradient_norm : 0.24808103336971413
[2025-09-11 19:44:24,231][flp2p.graph_runner][INFO] - Test, Round 192 : loss => 5.619034179282188,  accuracy: 0.3098
[2025-09-11 19:44:58,991][flp2p.graph_runner][INFO] - Train, Round 193 : loss => 0.005690478551647783,  accuracy: 1.0, gradient_norm : 0.23524547041665522
[2025-09-11 19:45:12,973][flp2p.graph_runner][INFO] - Test, Round 193 : loss => 5.633045001757145,  accuracy: 0.3097
[2025-09-11 19:45:47,865][flp2p.graph_runner][INFO] - Train, Round 194 : loss => 0.00559357761851667,  accuracy: 1.0, gradient_norm : 0.24484030623163694
[2025-09-11 19:46:01,919][flp2p.graph_runner][INFO] - Test, Round 194 : loss => 5.643468237829208,  accuracy: 0.3088
[2025-09-11 19:46:36,731][flp2p.graph_runner][INFO] - Train, Round 195 : loss => 0.005498652188786458,  accuracy: 1.0, gradient_norm : 0.23210410978531765
[2025-09-11 19:46:50,770][flp2p.graph_runner][INFO] - Test, Round 195 : loss => 5.655547982561588,  accuracy: 0.308
[2025-09-11 19:47:25,721][flp2p.graph_runner][INFO] - Train, Round 196 : loss => 0.005411943546399317,  accuracy: 1.0, gradient_norm : 0.23341524538046343
[2025-09-11 19:47:39,537][flp2p.graph_runner][INFO] - Test, Round 196 : loss => 5.668931870067119,  accuracy: 0.3096
[2025-09-11 19:48:14,638][flp2p.graph_runner][INFO] - Train, Round 197 : loss => 0.005323192299110813,  accuracy: 1.0, gradient_norm : 0.2232082235296728
[2025-09-11 19:48:28,588][flp2p.graph_runner][INFO] - Test, Round 197 : loss => 5.678702904617786,  accuracy: 0.309
[2025-09-11 19:49:03,677][flp2p.graph_runner][INFO] - Train, Round 198 : loss => 0.005232349307334517,  accuracy: 1.0, gradient_norm : 0.2195488722203209
[2025-09-11 19:49:17,332][flp2p.graph_runner][INFO] - Test, Round 198 : loss => 5.689906603491306,  accuracy: 0.309
[2025-09-11 19:49:52,088][flp2p.graph_runner][INFO] - Train, Round 199 : loss => 0.00515366550340938,  accuracy: 1.0, gradient_norm : 0.2379517447106919
[2025-09-11 19:50:05,944][flp2p.graph_runner][INFO] - Test, Round 199 : loss => 5.7002166926383975,  accuracy: 0.3087
[2025-09-11 19:50:40,630][flp2p.graph_runner][INFO] - Train, Round 200 : loss => 0.005072462175254867,  accuracy: 1.0, gradient_norm : 0.21815124576548242
[2025-09-11 19:50:54,407][flp2p.graph_runner][INFO] - Test, Round 200 : loss => 5.711234843885898,  accuracy: 0.3089
[2025-09-11 19:51:29,509][flp2p.graph_runner][INFO] - Train, Round 201 : loss => 0.004992899433224619,  accuracy: 1.0, gradient_norm : 0.22201123266350828
[2025-09-11 19:51:43,006][flp2p.graph_runner][INFO] - Test, Round 201 : loss => 5.721802402639389,  accuracy: 0.3093
[2025-09-11 19:52:17,667][flp2p.graph_runner][INFO] - Train, Round 202 : loss => 0.004917328311421443,  accuracy: 1.0, gradient_norm : 0.214252499986116
[2025-09-11 19:52:31,459][flp2p.graph_runner][INFO] - Test, Round 202 : loss => 5.732619822728634,  accuracy: 0.3089
[2025-09-11 19:53:06,018][flp2p.graph_runner][INFO] - Train, Round 203 : loss => 0.004845820278472576,  accuracy: 1.0, gradient_norm : 0.20710969283008676
[2025-09-11 19:53:19,714][flp2p.graph_runner][INFO] - Test, Round 203 : loss => 5.743129002690315,  accuracy: 0.309
[2025-09-11 19:53:54,681][flp2p.graph_runner][INFO] - Train, Round 204 : loss => 0.004772355520011234,  accuracy: 1.0, gradient_norm : 0.20943237211827853
[2025-09-11 19:54:08,437][flp2p.graph_runner][INFO] - Test, Round 204 : loss => 5.752630896008014,  accuracy: 0.3099
[2025-09-11 19:54:43,660][flp2p.graph_runner][INFO] - Train, Round 205 : loss => 0.004705052181573896,  accuracy: 1.0, gradient_norm : 0.20343897782721712
[2025-09-11 19:54:57,442][flp2p.graph_runner][INFO] - Test, Round 205 : loss => 5.764018750762939,  accuracy: 0.3094
[2025-09-11 19:55:32,318][flp2p.graph_runner][INFO] - Train, Round 206 : loss => 0.00463585662112261,  accuracy: 1.0, gradient_norm : 0.1906203995568109
[2025-09-11 19:55:45,898][flp2p.graph_runner][INFO] - Test, Round 206 : loss => 5.773969336807728,  accuracy: 0.3084
[2025-09-11 19:56:20,824][flp2p.graph_runner][INFO] - Train, Round 207 : loss => 0.004569532342041687,  accuracy: 1.0, gradient_norm : 0.2010322867513803
[2025-09-11 19:56:34,521][flp2p.graph_runner][INFO] - Test, Round 207 : loss => 5.783048593866825,  accuracy: 0.3089
[2025-09-11 19:57:09,528][flp2p.graph_runner][INFO] - Train, Round 208 : loss => 0.004507249182885667,  accuracy: 1.0, gradient_norm : 0.19353964481077457
[2025-09-11 19:57:23,499][flp2p.graph_runner][INFO] - Test, Round 208 : loss => 5.792593980121612,  accuracy: 0.309
[2025-09-11 19:57:58,629][flp2p.graph_runner][INFO] - Train, Round 209 : loss => 0.004444934358956136,  accuracy: 1.0, gradient_norm : 0.19818106253667508
[2025-09-11 19:58:12,449][flp2p.graph_runner][INFO] - Test, Round 209 : loss => 5.802612167096138,  accuracy: 0.3092
[2025-09-11 19:58:47,277][flp2p.graph_runner][INFO] - Train, Round 210 : loss => 0.00438434693166831,  accuracy: 1.0, gradient_norm : 0.17493016775285283
[2025-09-11 19:59:00,953][flp2p.graph_runner][INFO] - Test, Round 210 : loss => 5.811945275712013,  accuracy: 0.3083
[2025-09-11 19:59:35,988][flp2p.graph_runner][INFO] - Train, Round 211 : loss => 0.004324823772379507,  accuracy: 1.0, gradient_norm : 0.18635666849346152
[2025-09-11 19:59:49,585][flp2p.graph_runner][INFO] - Test, Round 211 : loss => 5.821152815616131,  accuracy: 0.3087
[2025-09-11 20:00:24,585][flp2p.graph_runner][INFO] - Train, Round 212 : loss => 0.004270003752802343,  accuracy: 1.0, gradient_norm : 0.19031600366300874
[2025-09-11 20:00:38,172][flp2p.graph_runner][INFO] - Test, Round 212 : loss => 5.829744054567814,  accuracy: 0.308
[2025-09-11 20:01:13,067][flp2p.graph_runner][INFO] - Train, Round 213 : loss => 0.0042154506125370965,  accuracy: 1.0, gradient_norm : 0.19713212130025173
[2025-09-11 20:01:26,430][flp2p.graph_runner][INFO] - Test, Round 213 : loss => 5.839378873157501,  accuracy: 0.3089
[2025-09-11 20:02:01,439][flp2p.graph_runner][INFO] - Train, Round 214 : loss => 0.004160458704379077,  accuracy: 1.0, gradient_norm : 0.17693102214041098
[2025-09-11 20:02:15,241][flp2p.graph_runner][INFO] - Test, Round 214 : loss => 5.847934525644779,  accuracy: 0.3084
[2025-09-11 20:02:50,076][flp2p.graph_runner][INFO] - Train, Round 215 : loss => 0.004103440396332492,  accuracy: 1.0, gradient_norm : 0.18000070426057857
[2025-09-11 20:03:03,470][flp2p.graph_runner][INFO] - Test, Round 215 : loss => 5.858070669341087,  accuracy: 0.3076
[2025-09-11 20:03:38,374][flp2p.graph_runner][INFO] - Train, Round 216 : loss => 0.004055499139746341,  accuracy: 1.0, gradient_norm : 0.18498551800972288
[2025-09-11 20:03:55,676][flp2p.graph_runner][INFO] - Test, Round 216 : loss => 5.8667334408640865,  accuracy: 0.3093
[2025-09-11 20:04:30,285][flp2p.graph_runner][INFO] - Train, Round 217 : loss => 0.004005129040257693,  accuracy: 1.0, gradient_norm : 0.1777771462600796
[2025-09-11 20:04:49,991][flp2p.graph_runner][INFO] - Test, Round 217 : loss => 5.874203580391407,  accuracy: 0.3084
[2025-09-11 20:05:24,820][flp2p.graph_runner][INFO] - Train, Round 218 : loss => 0.003957123035118759,  accuracy: 1.0, gradient_norm : 0.17780143692216993
[2025-09-11 20:05:45,444][flp2p.graph_runner][INFO] - Test, Round 218 : loss => 5.882131031310558,  accuracy: 0.3085
[2025-09-11 20:06:20,192][flp2p.graph_runner][INFO] - Train, Round 219 : loss => 0.0039076586748706165,  accuracy: 1.0, gradient_norm : 0.1739217336371609
[2025-09-11 20:06:37,948][flp2p.graph_runner][INFO] - Test, Round 219 : loss => 5.89093107560873,  accuracy: 0.3079
[2025-09-11 20:07:12,583][flp2p.graph_runner][INFO] - Train, Round 220 : loss => 0.0038601117716946932,  accuracy: 1.0, gradient_norm : 0.17160409744456487
[2025-09-11 20:07:27,655][flp2p.graph_runner][INFO] - Test, Round 220 : loss => 5.8988908436059955,  accuracy: 0.3085
[2025-09-11 20:08:02,388][flp2p.graph_runner][INFO] - Train, Round 221 : loss => 0.0038177168757344304,  accuracy: 1.0, gradient_norm : 0.17249683105349767
[2025-09-11 20:08:16,405][flp2p.graph_runner][INFO] - Test, Round 221 : loss => 5.906725105321407,  accuracy: 0.3077
[2025-09-11 20:08:51,156][flp2p.graph_runner][INFO] - Train, Round 222 : loss => 0.003771781550215868,  accuracy: 1.0, gradient_norm : 0.1700147124140949
[2025-09-11 20:09:05,143][flp2p.graph_runner][INFO] - Test, Round 222 : loss => 5.915321275901794,  accuracy: 0.3081
[2025-09-11 20:09:39,896][flp2p.graph_runner][INFO] - Train, Round 223 : loss => 0.0037300228744182574,  accuracy: 1.0, gradient_norm : 0.1694194304683408
[2025-09-11 20:09:53,902][flp2p.graph_runner][INFO] - Test, Round 223 : loss => 5.923697929596901,  accuracy: 0.3075
[2025-09-11 20:10:28,945][flp2p.graph_runner][INFO] - Train, Round 224 : loss => 0.0036832244921242807,  accuracy: 1.0, gradient_norm : 0.15775235860259662
[2025-09-11 20:10:42,945][flp2p.graph_runner][INFO] - Test, Round 224 : loss => 5.930970473718643,  accuracy: 0.3079
[2025-09-11 20:11:17,947][flp2p.graph_runner][INFO] - Train, Round 225 : loss => 0.003645433045069998,  accuracy: 1.0, gradient_norm : 0.16427102905689117
[2025-09-11 20:11:31,745][flp2p.graph_runner][INFO] - Test, Round 225 : loss => 5.93870531232357,  accuracy: 0.3081
[2025-09-11 20:12:06,774][flp2p.graph_runner][INFO] - Train, Round 226 : loss => 0.0036049617805595825,  accuracy: 1.0, gradient_norm : 0.164259675688916
[2025-09-11 20:12:20,861][flp2p.graph_runner][INFO] - Test, Round 226 : loss => 5.945586634612083,  accuracy: 0.3083
[2025-09-11 20:12:55,375][flp2p.graph_runner][INFO] - Train, Round 227 : loss => 0.00356486814765958,  accuracy: 1.0, gradient_norm : 0.15520248879318932
[2025-09-11 20:13:09,059][flp2p.graph_runner][INFO] - Test, Round 227 : loss => 5.953785904037953,  accuracy: 0.308
[2025-09-11 20:13:43,854][flp2p.graph_runner][INFO] - Train, Round 228 : loss => 0.0035240873931616074,  accuracy: 1.0, gradient_norm : 0.1616871732911241
[2025-09-11 20:13:57,644][flp2p.graph_runner][INFO] - Test, Round 228 : loss => 5.961784197032451,  accuracy: 0.308
[2025-09-11 20:14:32,364][flp2p.graph_runner][INFO] - Train, Round 229 : loss => 0.0034884581701286766,  accuracy: 1.0, gradient_norm : 0.1580587917700175
[2025-09-11 20:14:46,188][flp2p.graph_runner][INFO] - Test, Round 229 : loss => 5.968296482348442,  accuracy: 0.3079
[2025-09-11 20:15:20,982][flp2p.graph_runner][INFO] - Train, Round 230 : loss => 0.003452386512217346,  accuracy: 1.0, gradient_norm : 0.1589419718639206
[2025-09-11 20:15:35,037][flp2p.graph_runner][INFO] - Test, Round 230 : loss => 5.976731402373314,  accuracy: 0.308
[2025-09-11 20:16:09,946][flp2p.graph_runner][INFO] - Train, Round 231 : loss => 0.0034140766054042626,  accuracy: 1.0, gradient_norm : 0.15161310852914586
[2025-09-11 20:16:23,695][flp2p.graph_runner][INFO] - Test, Round 231 : loss => 5.982606532847881,  accuracy: 0.3078
[2025-09-11 20:16:58,563][flp2p.graph_runner][INFO] - Train, Round 232 : loss => 0.003378533993382007,  accuracy: 1.0, gradient_norm : 0.15329148157829
[2025-09-11 20:17:12,064][flp2p.graph_runner][INFO] - Test, Round 232 : loss => 5.989746230113506,  accuracy: 0.3085
[2025-09-11 20:17:46,931][flp2p.graph_runner][INFO] - Train, Round 233 : loss => 0.003344152085725606,  accuracy: 1.0, gradient_norm : 0.1483247997292221
[2025-09-11 20:18:00,630][flp2p.graph_runner][INFO] - Test, Round 233 : loss => 5.997256095051766,  accuracy: 0.3077
[2025-09-11 20:18:35,470][flp2p.graph_runner][INFO] - Train, Round 234 : loss => 0.003311151389837808,  accuracy: 1.0, gradient_norm : 0.15433452721061328
[2025-09-11 20:18:49,266][flp2p.graph_runner][INFO] - Test, Round 234 : loss => 6.004271191263199,  accuracy: 0.308
[2025-09-11 20:19:24,059][flp2p.graph_runner][INFO] - Train, Round 235 : loss => 0.0032773833260095375,  accuracy: 1.0, gradient_norm : 0.15539674261460293
[2025-09-11 20:19:37,997][flp2p.graph_runner][INFO] - Test, Round 235 : loss => 6.010996808028221,  accuracy: 0.3074
[2025-09-11 20:20:13,051][flp2p.graph_runner][INFO] - Train, Round 236 : loss => 0.0032450294199225017,  accuracy: 1.0, gradient_norm : 0.14929559853772045
[2025-09-11 20:20:26,753][flp2p.graph_runner][INFO] - Test, Round 236 : loss => 6.017111235237121,  accuracy: 0.3091
[2025-09-11 20:21:01,312][flp2p.graph_runner][INFO] - Train, Round 237 : loss => 0.0032127594404058376,  accuracy: 1.0, gradient_norm : 0.14602118251540777
[2025-09-11 20:21:15,227][flp2p.graph_runner][INFO] - Test, Round 237 : loss => 6.023748543608189,  accuracy: 0.308
[2025-09-11 20:21:49,992][flp2p.graph_runner][INFO] - Train, Round 238 : loss => 0.00318363032929483,  accuracy: 1.0, gradient_norm : 0.14385509186344925
[2025-09-11 20:22:03,837][flp2p.graph_runner][INFO] - Test, Round 238 : loss => 6.030962442481518,  accuracy: 0.3081
[2025-09-11 20:22:38,780][flp2p.graph_runner][INFO] - Train, Round 239 : loss => 0.003152236138751807,  accuracy: 1.0, gradient_norm : 0.14118346550244032
[2025-09-11 20:22:52,521][flp2p.graph_runner][INFO] - Test, Round 239 : loss => 6.038039080047607,  accuracy: 0.3084
[2025-09-11 20:23:27,231][flp2p.graph_runner][INFO] - Train, Round 240 : loss => 0.003122603063675344,  accuracy: 1.0, gradient_norm : 0.14715319938986623
[2025-09-11 20:23:40,842][flp2p.graph_runner][INFO] - Test, Round 240 : loss => 6.044334644246101,  accuracy: 0.3081
[2025-09-11 20:24:15,784][flp2p.graph_runner][INFO] - Train, Round 241 : loss => 0.003092852722572085,  accuracy: 1.0, gradient_norm : 0.143084810181593
[2025-09-11 20:24:29,526][flp2p.graph_runner][INFO] - Test, Round 241 : loss => 6.050963910710812,  accuracy: 0.3085
[2025-09-11 20:25:04,385][flp2p.graph_runner][INFO] - Train, Round 242 : loss => 0.0030651463357692895,  accuracy: 1.0, gradient_norm : 0.14489590187849852
[2025-09-11 20:25:17,804][flp2p.graph_runner][INFO] - Test, Round 242 : loss => 6.057393977570534,  accuracy: 0.3079
[2025-09-11 20:25:52,636][flp2p.graph_runner][INFO] - Train, Round 243 : loss => 0.0030357508283244284,  accuracy: 1.0, gradient_norm : 0.1373406438206005
[2025-09-11 20:26:06,132][flp2p.graph_runner][INFO] - Test, Round 243 : loss => 6.062860440897942,  accuracy: 0.3085
[2025-09-11 20:26:41,183][flp2p.graph_runner][INFO] - Train, Round 244 : loss => 0.0030074131961252233,  accuracy: 1.0, gradient_norm : 0.1370931426959243
[2025-09-11 20:26:54,570][flp2p.graph_runner][INFO] - Test, Round 244 : loss => 6.0691530608654025,  accuracy: 0.3081
[2025-09-11 20:27:29,273][flp2p.graph_runner][INFO] - Train, Round 245 : loss => 0.002980854051905529,  accuracy: 1.0, gradient_norm : 0.13832679615885987
[2025-09-11 20:27:46,231][flp2p.graph_runner][INFO] - Test, Round 245 : loss => 6.074616384756565,  accuracy: 0.3079
[2025-09-11 20:28:21,230][flp2p.graph_runner][INFO] - Train, Round 246 : loss => 0.0029546742810149834,  accuracy: 1.0, gradient_norm : 0.14098631252894905
[2025-09-11 20:28:41,958][flp2p.graph_runner][INFO] - Test, Round 246 : loss => 6.080771774172783,  accuracy: 0.3081
[2025-09-11 20:29:16,705][flp2p.graph_runner][INFO] - Train, Round 247 : loss => 0.002929012038851701,  accuracy: 1.0, gradient_norm : 0.1356426530663738
[2025-09-11 20:29:35,424][flp2p.graph_runner][INFO] - Test, Round 247 : loss => 6.087884358787536,  accuracy: 0.3079
[2025-09-11 20:30:10,141][flp2p.graph_runner][INFO] - Train, Round 248 : loss => 0.0029019472286260384,  accuracy: 1.0, gradient_norm : 0.13590377533256082
[2025-09-11 20:30:27,905][flp2p.graph_runner][INFO] - Test, Round 248 : loss => 6.092040778422356,  accuracy: 0.3082
[2025-09-11 20:31:02,807][flp2p.graph_runner][INFO] - Train, Round 249 : loss => 0.0028757681796075,  accuracy: 1.0, gradient_norm : 0.13139454508831966
[2025-09-11 20:31:17,251][flp2p.graph_runner][INFO] - Test, Round 249 : loss => 6.098969927918911,  accuracy: 0.3086
[2025-09-11 20:31:52,019][flp2p.graph_runner][INFO] - Train, Round 250 : loss => 0.0028537524068572863,  accuracy: 1.0, gradient_norm : 0.12922970557815253
[2025-09-11 20:32:05,864][flp2p.graph_runner][INFO] - Test, Round 250 : loss => 6.104510108935833,  accuracy: 0.3076
[2025-09-11 20:32:40,745][flp2p.graph_runner][INFO] - Train, Round 251 : loss => 0.0028271123146987533,  accuracy: 1.0, gradient_norm : 0.12510649303564467
[2025-09-11 20:32:54,620][flp2p.graph_runner][INFO] - Test, Round 251 : loss => 6.109957739448547,  accuracy: 0.3079
[2025-09-11 20:33:29,412][flp2p.graph_runner][INFO] - Train, Round 252 : loss => 0.0028037538941134703,  accuracy: 1.0, gradient_norm : 0.130597283259795
[2025-09-11 20:33:43,451][flp2p.graph_runner][INFO] - Test, Round 252 : loss => 6.11563147405386,  accuracy: 0.3077
[2025-09-11 20:34:18,164][flp2p.graph_runner][INFO] - Train, Round 253 : loss => 0.002780785833504828,  accuracy: 1.0, gradient_norm : 0.12406089528035738
[2025-09-11 20:34:32,167][flp2p.graph_runner][INFO] - Test, Round 253 : loss => 6.120903434288501,  accuracy: 0.3078
[2025-09-11 20:35:07,121][flp2p.graph_runner][INFO] - Train, Round 254 : loss => 0.002759901301857705,  accuracy: 1.0, gradient_norm : 0.12864548720710287
[2025-09-11 20:35:21,118][flp2p.graph_runner][INFO] - Test, Round 254 : loss => 6.127090322363377,  accuracy: 0.3076
[2025-09-11 20:35:55,951][flp2p.graph_runner][INFO] - Train, Round 255 : loss => 0.002734701877974051,  accuracy: 1.0, gradient_norm : 0.13087334099571765
[2025-09-11 20:36:09,853][flp2p.graph_runner][INFO] - Test, Round 255 : loss => 6.132856894218921,  accuracy: 0.3078
[2025-09-11 20:36:44,831][flp2p.graph_runner][INFO] - Train, Round 256 : loss => 0.0027136157157656277,  accuracy: 1.0, gradient_norm : 0.1267751848492056
[2025-09-11 20:36:58,792][flp2p.graph_runner][INFO] - Test, Round 256 : loss => 6.137629330170155,  accuracy: 0.3081
[2025-09-11 20:37:33,647][flp2p.graph_runner][INFO] - Train, Round 257 : loss => 0.0026920710519577062,  accuracy: 1.0, gradient_norm : 0.12602499867132796
[2025-09-11 20:37:47,546][flp2p.graph_runner][INFO] - Test, Round 257 : loss => 6.142934389305115,  accuracy: 0.3079
[2025-09-11 20:38:22,391][flp2p.graph_runner][INFO] - Train, Round 258 : loss => 0.0026712611008163855,  accuracy: 1.0, gradient_norm : 0.12515279679188315
[2025-09-11 20:38:36,151][flp2p.graph_runner][INFO] - Test, Round 258 : loss => 6.147798342847824,  accuracy: 0.308
[2025-09-11 20:39:10,710][flp2p.graph_runner][INFO] - Train, Round 259 : loss => 0.002649595430654397,  accuracy: 1.0, gradient_norm : 0.12497982931570423
[2025-09-11 20:39:24,500][flp2p.graph_runner][INFO] - Test, Round 259 : loss => 6.1536563408017155,  accuracy: 0.3082
[2025-09-11 20:39:59,401][flp2p.graph_runner][INFO] - Train, Round 260 : loss => 0.002629579863375208,  accuracy: 1.0, gradient_norm : 0.12204870427442165
[2025-09-11 20:40:13,208][flp2p.graph_runner][INFO] - Test, Round 260 : loss => 6.1587525703072545,  accuracy: 0.3076
[2025-09-11 20:40:48,456][flp2p.graph_runner][INFO] - Train, Round 261 : loss => 0.0026084918353687196,  accuracy: 1.0, gradient_norm : 0.12244497507346978
[2025-09-11 20:41:02,170][flp2p.graph_runner][INFO] - Test, Round 261 : loss => 6.164200134146213,  accuracy: 0.3079
[2025-09-11 20:41:36,938][flp2p.graph_runner][INFO] - Train, Round 262 : loss => 0.0025879744160435315,  accuracy: 1.0, gradient_norm : 0.11967333445772085
[2025-09-11 20:41:50,702][flp2p.graph_runner][INFO] - Test, Round 262 : loss => 6.168882976043224,  accuracy: 0.3078
[2025-09-11 20:42:25,782][flp2p.graph_runner][INFO] - Train, Round 263 : loss => 0.002569814390153624,  accuracy: 1.0, gradient_norm : 0.11599989813677594
[2025-09-11 20:42:39,466][flp2p.graph_runner][INFO] - Test, Round 263 : loss => 6.1747283627867695,  accuracy: 0.3073
[2025-09-11 20:43:14,554][flp2p.graph_runner][INFO] - Train, Round 264 : loss => 0.0025514580518453535,  accuracy: 1.0, gradient_norm : 0.11978888173085625
[2025-09-11 20:43:28,222][flp2p.graph_runner][INFO] - Test, Round 264 : loss => 6.178631047153473,  accuracy: 0.3074
[2025-09-11 20:44:03,087][flp2p.graph_runner][INFO] - Train, Round 265 : loss => 0.0025319147973884054,  accuracy: 1.0, gradient_norm : 0.1193323691207948
[2025-09-11 20:44:16,914][flp2p.graph_runner][INFO] - Test, Round 265 : loss => 6.183907700872421,  accuracy: 0.3081
[2025-09-11 20:44:51,908][flp2p.graph_runner][INFO] - Train, Round 266 : loss => 0.0025131689349655057,  accuracy: 1.0, gradient_norm : 0.11532997879739434
[2025-09-11 20:45:05,492][flp2p.graph_runner][INFO] - Test, Round 266 : loss => 6.188948096001148,  accuracy: 0.3074
[2025-09-11 20:45:40,308][flp2p.graph_runner][INFO] - Train, Round 267 : loss => 0.002494953676990311,  accuracy: 1.0, gradient_norm : 0.11899190648953654
[2025-09-11 20:45:53,989][flp2p.graph_runner][INFO] - Test, Round 267 : loss => 6.194167821800709,  accuracy: 0.3074
[2025-09-11 20:46:28,936][flp2p.graph_runner][INFO] - Train, Round 268 : loss => 0.0024766185588183964,  accuracy: 1.0, gradient_norm : 0.11364007778938424
[2025-09-11 20:46:42,847][flp2p.graph_runner][INFO] - Test, Round 268 : loss => 6.198328402972221,  accuracy: 0.3073
[2025-09-11 20:47:17,927][flp2p.graph_runner][INFO] - Train, Round 269 : loss => 0.0024591832553657383,  accuracy: 1.0, gradient_norm : 0.11848849939823433
[2025-09-11 20:47:31,645][flp2p.graph_runner][INFO] - Test, Round 269 : loss => 6.203574717569351,  accuracy: 0.3085
[2025-09-11 20:48:06,260][flp2p.graph_runner][INFO] - Train, Round 270 : loss => 0.002439819495581711,  accuracy: 1.0, gradient_norm : 0.11689983464189463
[2025-09-11 20:48:19,891][flp2p.graph_runner][INFO] - Test, Round 270 : loss => 6.20828218921423,  accuracy: 0.308
[2025-09-11 20:48:54,591][flp2p.graph_runner][INFO] - Train, Round 271 : loss => 0.0024247540583019153,  accuracy: 1.0, gradient_norm : 0.1184945470494595
[2025-09-11 20:49:08,049][flp2p.graph_runner][INFO] - Test, Round 271 : loss => 6.212053359937668,  accuracy: 0.3083
[2025-09-11 20:49:42,615][flp2p.graph_runner][INFO] - Train, Round 272 : loss => 0.0024076534025759123,  accuracy: 1.0, gradient_norm : 0.11416062355134508
[2025-09-11 20:49:56,044][flp2p.graph_runner][INFO] - Test, Round 272 : loss => 6.217213451862335,  accuracy: 0.3083
[2025-09-11 20:50:31,055][flp2p.graph_runner][INFO] - Train, Round 273 : loss => 0.0023904715376556855,  accuracy: 1.0, gradient_norm : 0.1127350326598084
[2025-09-11 20:50:44,508][flp2p.graph_runner][INFO] - Test, Round 273 : loss => 6.221487806296349,  accuracy: 0.308
[2025-09-11 20:51:19,290][flp2p.graph_runner][INFO] - Train, Round 274 : loss => 0.0023734084842726585,  accuracy: 1.0, gradient_norm : 0.11176307627738222
[2025-09-11 20:51:34,394][flp2p.graph_runner][INFO] - Test, Round 274 : loss => 6.226101112794876,  accuracy: 0.3079
[2025-09-11 20:52:09,319][flp2p.graph_runner][INFO] - Train, Round 275 : loss => 0.0023585571230311563,  accuracy: 1.0, gradient_norm : 0.1090126821269933
[2025-09-11 20:52:29,491][flp2p.graph_runner][INFO] - Test, Round 275 : loss => 6.231022245216369,  accuracy: 0.308
[2025-09-11 20:53:04,225][flp2p.graph_runner][INFO] - Train, Round 276 : loss => 0.0023412050985886406,  accuracy: 1.0, gradient_norm : 0.10666535593668203
[2025-09-11 20:53:24,315][flp2p.graph_runner][INFO] - Test, Round 276 : loss => 6.234914470458031,  accuracy: 0.3081
[2025-09-11 20:53:59,144][flp2p.graph_runner][INFO] - Train, Round 277 : loss => 0.002326632735785097,  accuracy: 1.0, gradient_norm : 0.112820612281209
[2025-09-11 20:54:16,869][flp2p.graph_runner][INFO] - Test, Round 277 : loss => 6.2390396101474765,  accuracy: 0.308
[2025-09-11 20:54:51,671][flp2p.graph_runner][INFO] - Train, Round 278 : loss => 0.002310235674570624,  accuracy: 1.0, gradient_norm : 0.10823196306267874
[2025-09-11 20:55:06,950][flp2p.graph_runner][INFO] - Test, Round 278 : loss => 6.243305520200729,  accuracy: 0.308
[2025-09-11 20:55:41,891][flp2p.graph_runner][INFO] - Train, Round 279 : loss => 0.00229511641455853,  accuracy: 1.0, gradient_norm : 0.11264781655554289
[2025-09-11 20:55:55,936][flp2p.graph_runner][INFO] - Test, Round 279 : loss => 6.247453413319588,  accuracy: 0.3079
[2025-09-11 20:56:30,593][flp2p.graph_runner][INFO] - Train, Round 280 : loss => 0.0022806362582196015,  accuracy: 1.0, gradient_norm : 0.10569167918892929
[2025-09-11 20:56:44,395][flp2p.graph_runner][INFO] - Test, Round 280 : loss => 6.252420249843597,  accuracy: 0.3078
[2025-09-11 20:57:19,003][flp2p.graph_runner][INFO] - Train, Round 281 : loss => 0.0022667453983255354,  accuracy: 1.0, gradient_norm : 0.10920122969753883
[2025-09-11 20:57:32,931][flp2p.graph_runner][INFO] - Test, Round 281 : loss => 6.256266435003281,  accuracy: 0.3079
[2025-09-11 20:58:07,494][flp2p.graph_runner][INFO] - Train, Round 282 : loss => 0.002250861450399194,  accuracy: 1.0, gradient_norm : 0.10699706285966361
[2025-09-11 20:58:21,423][flp2p.graph_runner][INFO] - Test, Round 282 : loss => 6.260226734650135,  accuracy: 0.3081
[2025-09-11 20:58:56,035][flp2p.graph_runner][INFO] - Train, Round 283 : loss => 0.0022372975209145793,  accuracy: 1.0, gradient_norm : 0.10810744063244523
[2025-09-11 20:59:09,933][flp2p.graph_runner][INFO] - Test, Round 283 : loss => 6.265038436996937,  accuracy: 0.3079
[2025-09-11 20:59:44,529][flp2p.graph_runner][INFO] - Train, Round 284 : loss => 0.002223034605655508,  accuracy: 1.0, gradient_norm : 0.10003905808565612
[2025-09-11 20:59:58,458][flp2p.graph_runner][INFO] - Test, Round 284 : loss => 6.2686991261005405,  accuracy: 0.3079
[2025-09-11 21:00:33,181][flp2p.graph_runner][INFO] - Train, Round 285 : loss => 0.0022095295017061293,  accuracy: 1.0, gradient_norm : 0.10344633634191674
[2025-09-11 21:00:46,960][flp2p.graph_runner][INFO] - Test, Round 285 : loss => 6.272974287223816,  accuracy: 0.3082
[2025-09-11 21:01:21,526][flp2p.graph_runner][INFO] - Train, Round 286 : loss => 0.0021961464498114463,  accuracy: 1.0, gradient_norm : 0.10194701584221195
[2025-09-11 21:01:35,287][flp2p.graph_runner][INFO] - Test, Round 286 : loss => 6.277003229427337,  accuracy: 0.3082
[2025-09-11 21:02:09,853][flp2p.graph_runner][INFO] - Train, Round 287 : loss => 0.0021814709993486768,  accuracy: 1.0, gradient_norm : 0.1016214101707495
[2025-09-11 21:02:23,526][flp2p.graph_runner][INFO] - Test, Round 287 : loss => 6.281017145311832,  accuracy: 0.3076
[2025-09-11 21:02:58,386][flp2p.graph_runner][INFO] - Train, Round 288 : loss => 0.0021686919150427767,  accuracy: 1.0, gradient_norm : 0.10519712780366985
[2025-09-11 21:03:12,070][flp2p.graph_runner][INFO] - Test, Round 288 : loss => 6.284712497210503,  accuracy: 0.3081
[2025-09-11 21:03:46,583][flp2p.graph_runner][INFO] - Train, Round 289 : loss => 0.0021565968953170043,  accuracy: 1.0, gradient_norm : 0.10745816147459315
[2025-09-11 21:04:00,487][flp2p.graph_runner][INFO] - Test, Round 289 : loss => 6.28832320176363,  accuracy: 0.3077
[2025-09-11 21:04:35,305][flp2p.graph_runner][INFO] - Train, Round 290 : loss => 0.0021437516792987775,  accuracy: 1.0, gradient_norm : 0.10549598114156412
[2025-09-11 21:04:49,071][flp2p.graph_runner][INFO] - Test, Round 290 : loss => 6.291875531697273,  accuracy: 0.3077
[2025-09-11 21:05:24,080][flp2p.graph_runner][INFO] - Train, Round 291 : loss => 0.0021294527573142356,  accuracy: 1.0, gradient_norm : 0.09819778252727138
[2025-09-11 21:05:37,762][flp2p.graph_runner][INFO] - Test, Round 291 : loss => 6.2963038396120075,  accuracy: 0.3073
[2025-09-11 21:06:12,225][flp2p.graph_runner][INFO] - Train, Round 292 : loss => 0.0021167687985386386,  accuracy: 1.0, gradient_norm : 0.09749533003070238
[2025-09-11 21:06:25,981][flp2p.graph_runner][INFO] - Test, Round 292 : loss => 6.300075043416023,  accuracy: 0.3082
[2025-09-11 21:07:00,860][flp2p.graph_runner][INFO] - Train, Round 293 : loss => 0.002104434020729968,  accuracy: 1.0, gradient_norm : 0.09605486031947627
[2025-09-11 21:07:14,625][flp2p.graph_runner][INFO] - Test, Round 293 : loss => 6.303891686272621,  accuracy: 0.3082
[2025-09-11 21:07:49,567][flp2p.graph_runner][INFO] - Train, Round 294 : loss => 0.0020923001289096044,  accuracy: 1.0, gradient_norm : 0.09723875766447643
[2025-09-11 21:08:03,204][flp2p.graph_runner][INFO] - Test, Round 294 : loss => 6.306893956923485,  accuracy: 0.3078
[2025-09-11 21:08:37,906][flp2p.graph_runner][INFO] - Train, Round 295 : loss => 0.0020805001074040773,  accuracy: 1.0, gradient_norm : 0.1006643816393748
[2025-09-11 21:08:51,735][flp2p.graph_runner][INFO] - Test, Round 295 : loss => 6.311255880999565,  accuracy: 0.3086
[2025-09-11 21:09:26,526][flp2p.graph_runner][INFO] - Train, Round 296 : loss => 0.0020696756699180685,  accuracy: 1.0, gradient_norm : 0.09935930254577015
[2025-09-11 21:09:40,323][flp2p.graph_runner][INFO] - Test, Round 296 : loss => 6.3153619022130965,  accuracy: 0.3073
[2025-09-11 21:10:15,307][flp2p.graph_runner][INFO] - Train, Round 297 : loss => 0.0020568812375616596,  accuracy: 1.0, gradient_norm : 0.09523533783760867
[2025-09-11 21:10:28,906][flp2p.graph_runner][INFO] - Test, Round 297 : loss => 6.318080837535859,  accuracy: 0.3078
[2025-09-11 21:11:03,745][flp2p.graph_runner][INFO] - Train, Round 298 : loss => 0.0020455508394904127,  accuracy: 1.0, gradient_norm : 0.0946060191406292
[2025-09-11 21:11:17,299][flp2p.graph_runner][INFO] - Test, Round 298 : loss => 6.322097702479362,  accuracy: 0.3075
[2025-09-11 21:11:51,905][flp2p.graph_runner][INFO] - Train, Round 299 : loss => 0.002034869158378569,  accuracy: 1.0, gradient_norm : 0.0990474720130697
[2025-09-11 21:12:05,333][flp2p.graph_runner][INFO] - Test, Round 299 : loss => 6.325825537610054,  accuracy: 0.3077
[2025-09-11 21:12:40,096][flp2p.graph_runner][INFO] - Train, Round 300 : loss => 0.0020240791352504547,  accuracy: 1.0, gradient_norm : 0.09852105044785323
[2025-09-11 21:12:53,673][flp2p.graph_runner][INFO] - Test, Round 300 : loss => 6.328795326328278,  accuracy: 0.3076
[2025-09-11 21:13:28,551][flp2p.graph_runner][INFO] - Train, Round 301 : loss => 0.0020126757631563426,  accuracy: 1.0, gradient_norm : 0.09482347594840533
[2025-09-11 21:13:43,042][flp2p.graph_runner][INFO] - Test, Round 301 : loss => 6.332501350569725,  accuracy: 0.3081
[2025-09-11 21:14:18,301][flp2p.graph_runner][INFO] - Train, Round 302 : loss => 0.002001347899640678,  accuracy: 1.0, gradient_norm : 0.09249879212011022
[2025-09-11 21:14:36,920][flp2p.graph_runner][INFO] - Test, Round 302 : loss => 6.335877009391785,  accuracy: 0.307
[2025-09-11 21:15:11,656][flp2p.graph_runner][INFO] - Train, Round 303 : loss => 0.0019903236689181843,  accuracy: 1.0, gradient_norm : 0.09556955059436206
[2025-09-11 21:15:33,304][flp2p.graph_runner][INFO] - Test, Round 303 : loss => 6.3392470869779585,  accuracy: 0.3074
[2025-09-11 21:16:08,278][flp2p.graph_runner][INFO] - Train, Round 304 : loss => 0.0019810362865003606,  accuracy: 1.0, gradient_norm : 0.09736501734553114
[2025-09-11 21:16:27,519][flp2p.graph_runner][INFO] - Test, Round 304 : loss => 6.343641454648972,  accuracy: 0.307
[2025-09-11 21:17:02,231][flp2p.graph_runner][INFO] - Train, Round 305 : loss => 0.001969432963427001,  accuracy: 1.0, gradient_norm : 0.0946790701532958
[2025-09-11 21:17:18,739][flp2p.graph_runner][INFO] - Test, Round 305 : loss => 6.346156017899514,  accuracy: 0.3075
[2025-09-11 21:17:53,557][flp2p.graph_runner][INFO] - Train, Round 306 : loss => 0.001959402330636901,  accuracy: 1.0, gradient_norm : 0.09450953969288704
[2025-09-11 21:18:08,169][flp2p.graph_runner][INFO] - Test, Round 306 : loss => 6.34997978386879,  accuracy: 0.3072
[2025-09-11 21:18:43,117][flp2p.graph_runner][INFO] - Train, Round 307 : loss => 0.0019484303543868004,  accuracy: 1.0, gradient_norm : 0.09342234958644693
[2025-09-11 21:18:57,184][flp2p.graph_runner][INFO] - Test, Round 307 : loss => 6.35327488849163,  accuracy: 0.3075
[2025-09-11 21:19:31,980][flp2p.graph_runner][INFO] - Train, Round 308 : loss => 0.0019382190196968931,  accuracy: 1.0, gradient_norm : 0.08959449497441438
[2025-09-11 21:19:45,962][flp2p.graph_runner][INFO] - Test, Round 308 : loss => 6.357031297850609,  accuracy: 0.308
[2025-09-11 21:20:20,969][flp2p.graph_runner][INFO] - Train, Round 309 : loss => 0.0019281961010225732,  accuracy: 1.0, gradient_norm : 0.09033997581278289
[2025-09-11 21:20:34,774][flp2p.graph_runner][INFO] - Test, Round 309 : loss => 6.359454008817672,  accuracy: 0.3079
[2025-09-11 21:21:09,595][flp2p.graph_runner][INFO] - Train, Round 310 : loss => 0.0019185437896521759,  accuracy: 1.0, gradient_norm : 0.0949817558448131
[2025-09-11 21:21:23,504][flp2p.graph_runner][INFO] - Test, Round 310 : loss => 6.362617508840561,  accuracy: 0.3073
[2025-09-11 21:21:58,342][flp2p.graph_runner][INFO] - Train, Round 311 : loss => 0.0019089050895127009,  accuracy: 1.0, gradient_norm : 0.08968709096579941
[2025-09-11 21:22:12,487][flp2p.graph_runner][INFO] - Test, Round 311 : loss => 6.365490010690689,  accuracy: 0.3076
[2025-09-11 21:22:47,287][flp2p.graph_runner][INFO] - Train, Round 312 : loss => 0.0018994572435137044,  accuracy: 1.0, gradient_norm : 0.08965596390698599
[2025-09-11 21:23:01,474][flp2p.graph_runner][INFO] - Test, Round 312 : loss => 6.368749992895126,  accuracy: 0.3076
[2025-09-11 21:23:36,544][flp2p.graph_runner][INFO] - Train, Round 313 : loss => 0.001889982899932268,  accuracy: 1.0, gradient_norm : 0.09111946925943773
[2025-09-11 21:23:50,315][flp2p.graph_runner][INFO] - Test, Round 313 : loss => 6.372136213231086,  accuracy: 0.3072
[2025-09-11 21:24:25,359][flp2p.graph_runner][INFO] - Train, Round 314 : loss => 0.0018811044346754599,  accuracy: 1.0, gradient_norm : 0.0880711023227205
[2025-09-11 21:24:39,075][flp2p.graph_runner][INFO] - Test, Round 314 : loss => 6.375227683281898,  accuracy: 0.3076
[2025-09-11 21:25:14,044][flp2p.graph_runner][INFO] - Train, Round 315 : loss => 0.0018717893232436228,  accuracy: 1.0, gradient_norm : 0.08731089930778398
[2025-09-11 21:25:27,853][flp2p.graph_runner][INFO] - Test, Round 315 : loss => 6.379099298882484,  accuracy: 0.3071
[2025-09-11 21:26:02,760][flp2p.graph_runner][INFO] - Train, Round 316 : loss => 0.0018635389286646381,  accuracy: 1.0, gradient_norm : 0.0865643989315128
[2025-09-11 21:26:16,434][flp2p.graph_runner][INFO] - Test, Round 316 : loss => 6.381893077611923,  accuracy: 0.307
[2025-09-11 21:26:51,112][flp2p.graph_runner][INFO] - Train, Round 317 : loss => 0.0018538274371288333,  accuracy: 1.0, gradient_norm : 0.09148308925016002
[2025-09-11 21:27:05,011][flp2p.graph_runner][INFO] - Test, Round 317 : loss => 6.384879577279091,  accuracy: 0.307
[2025-09-11 21:27:40,086][flp2p.graph_runner][INFO] - Train, Round 318 : loss => 0.001845005350915016,  accuracy: 1.0, gradient_norm : 0.09055582325337477
[2025-09-11 21:27:53,955][flp2p.graph_runner][INFO] - Test, Round 318 : loss => 6.387485196733475,  accuracy: 0.3073
[2025-09-11 21:28:28,549][flp2p.graph_runner][INFO] - Train, Round 319 : loss => 0.001836049030340898,  accuracy: 1.0, gradient_norm : 0.0919092987644097
[2025-09-11 21:28:42,371][flp2p.graph_runner][INFO] - Test, Round 319 : loss => 6.39062716012001,  accuracy: 0.3072
[2025-09-11 21:29:17,092][flp2p.graph_runner][INFO] - Train, Round 320 : loss => 0.001827203220794521,  accuracy: 1.0, gradient_norm : 0.08826346304984847
[2025-09-11 21:29:30,681][flp2p.graph_runner][INFO] - Test, Round 320 : loss => 6.393388508534431,  accuracy: 0.3075
[2025-09-11 21:30:05,846][flp2p.graph_runner][INFO] - Train, Round 321 : loss => 0.0018188554253720213,  accuracy: 1.0, gradient_norm : 0.08567942449893388
[2025-09-11 21:30:19,520][flp2p.graph_runner][INFO] - Test, Round 321 : loss => 6.3969841615438465,  accuracy: 0.3075
[2025-09-11 21:30:54,466][flp2p.graph_runner][INFO] - Train, Round 322 : loss => 0.0018092933574613806,  accuracy: 1.0, gradient_norm : 0.08537224764164465
[2025-09-11 21:31:08,186][flp2p.graph_runner][INFO] - Test, Round 322 : loss => 6.399410765600204,  accuracy: 0.3072
[2025-09-11 21:31:42,633][flp2p.graph_runner][INFO] - Train, Round 323 : loss => 0.0018021382613126957,  accuracy: 1.0, gradient_norm : 0.08653863383760334
[2025-09-11 21:31:56,190][flp2p.graph_runner][INFO] - Test, Round 323 : loss => 6.402117500972748,  accuracy: 0.3071
[2025-09-11 21:32:30,911][flp2p.graph_runner][INFO] - Train, Round 324 : loss => 0.0017930732568493112,  accuracy: 1.0, gradient_norm : 0.08716125029577636
[2025-09-11 21:32:44,757][flp2p.graph_runner][INFO] - Test, Round 324 : loss => 6.405059732460976,  accuracy: 0.3074
[2025-09-11 21:33:19,357][flp2p.graph_runner][INFO] - Train, Round 325 : loss => 0.0017852225045013868,  accuracy: 1.0, gradient_norm : 0.08620944619325935
[2025-09-11 21:33:33,162][flp2p.graph_runner][INFO] - Test, Round 325 : loss => 6.407435718798637,  accuracy: 0.307
[2025-09-11 21:34:07,989][flp2p.graph_runner][INFO] - Train, Round 326 : loss => 0.0017770099627523448,  accuracy: 1.0, gradient_norm : 0.0874368577405232
[2025-09-11 21:34:21,715][flp2p.graph_runner][INFO] - Test, Round 326 : loss => 6.410626779294014,  accuracy: 0.3072
[2025-09-11 21:34:56,537][flp2p.graph_runner][INFO] - Train, Round 327 : loss => 0.0017686575691429126,  accuracy: 1.0, gradient_norm : 0.08310068258861346
[2025-09-11 21:35:10,048][flp2p.graph_runner][INFO] - Test, Round 327 : loss => 6.413651978969574,  accuracy: 0.307
[2025-09-11 21:35:45,126][flp2p.graph_runner][INFO] - Train, Round 328 : loss => 0.00176035859088491,  accuracy: 1.0, gradient_norm : 0.08918004356976166
[2025-09-11 21:35:58,507][flp2p.graph_runner][INFO] - Test, Round 328 : loss => 6.416798998308182,  accuracy: 0.307
[2025-09-11 21:36:33,119][flp2p.graph_runner][INFO] - Train, Round 329 : loss => 0.0017525317083345732,  accuracy: 1.0, gradient_norm : 0.08558618927228805
[2025-09-11 21:36:46,642][flp2p.graph_runner][INFO] - Test, Round 329 : loss => 6.41925604519844,  accuracy: 0.3068
[2025-09-11 21:37:21,624][flp2p.graph_runner][INFO] - Train, Round 330 : loss => 0.0017453147005289796,  accuracy: 1.0, gradient_norm : 0.08446152626473138
[2025-09-11 21:37:34,968][flp2p.graph_runner][INFO] - Test, Round 330 : loss => 6.421552588605881,  accuracy: 0.3072
[2025-09-11 21:38:09,829][flp2p.graph_runner][INFO] - Train, Round 331 : loss => 0.0017379963338316879,  accuracy: 1.0, gradient_norm : 0.08643636766282844
[2025-09-11 21:38:26,284][flp2p.graph_runner][INFO] - Test, Round 331 : loss => 6.424568739128113,  accuracy: 0.3067
[2025-09-11 21:39:01,150][flp2p.graph_runner][INFO] - Train, Round 332 : loss => 0.001730106881053265,  accuracy: 1.0, gradient_norm : 0.0864457347388214
[2025-09-11 21:39:21,279][flp2p.graph_runner][INFO] - Test, Round 332 : loss => 6.427325020885467,  accuracy: 0.3074
[2025-09-11 21:39:56,244][flp2p.graph_runner][INFO] - Train, Round 333 : loss => 0.0017229387378514122,  accuracy: 1.0, gradient_norm : 0.0868580726934412
[2025-09-11 21:40:16,620][flp2p.graph_runner][INFO] - Test, Round 333 : loss => 6.429562586545944,  accuracy: 0.3074
[2025-09-11 21:40:51,506][flp2p.graph_runner][INFO] - Train, Round 334 : loss => 0.0017155276128808814,  accuracy: 1.0, gradient_norm : 0.08071249242719665
[2025-09-11 21:41:09,221][flp2p.graph_runner][INFO] - Test, Round 334 : loss => 6.432487756824494,  accuracy: 0.3066
[2025-09-11 21:41:44,084][flp2p.graph_runner][INFO] - Train, Round 335 : loss => 0.0017080407966568604,  accuracy: 1.0, gradient_norm : 0.08339582888230274
[2025-09-11 21:41:59,524][flp2p.graph_runner][INFO] - Test, Round 335 : loss => 6.43474352889061,  accuracy: 0.307
[2025-09-11 21:42:34,661][flp2p.graph_runner][INFO] - Train, Round 336 : loss => 0.0016997030139464192,  accuracy: 1.0, gradient_norm : 0.08256621333423297
[2025-09-11 21:42:48,677][flp2p.graph_runner][INFO] - Test, Round 336 : loss => 6.437478371334076,  accuracy: 0.3067
[2025-09-11 21:43:23,360][flp2p.graph_runner][INFO] - Train, Round 337 : loss => 0.0016940489516127845,  accuracy: 1.0, gradient_norm : 0.08266325543046366
[2025-09-11 21:43:37,535][flp2p.graph_runner][INFO] - Test, Round 337 : loss => 6.440335956978798,  accuracy: 0.3068
[2025-09-11 21:44:12,347][flp2p.graph_runner][INFO] - Train, Round 338 : loss => 0.001686628330304908,  accuracy: 1.0, gradient_norm : 0.07959969545172758
[2025-09-11 21:44:26,462][flp2p.graph_runner][INFO] - Test, Round 338 : loss => 6.442044296550751,  accuracy: 0.3074
[2025-09-11 21:45:01,232][flp2p.graph_runner][INFO] - Train, Round 339 : loss => 0.0016792037662526123,  accuracy: 1.0, gradient_norm : 0.08006442008789633
[2025-09-11 21:45:15,120][flp2p.graph_runner][INFO] - Test, Round 339 : loss => 6.444977142620087,  accuracy: 0.307
[2025-09-11 21:45:49,935][flp2p.graph_runner][INFO] - Train, Round 340 : loss => 0.0016726231588593992,  accuracy: 1.0, gradient_norm : 0.08138362369673642
[2025-09-11 21:46:04,163][flp2p.graph_runner][INFO] - Test, Round 340 : loss => 6.44771952226162,  accuracy: 0.3072
[2025-09-11 21:46:38,873][flp2p.graph_runner][INFO] - Train, Round 341 : loss => 0.0016658736968626423,  accuracy: 1.0, gradient_norm : 0.08040192688518397
[2025-09-11 21:46:52,846][flp2p.graph_runner][INFO] - Test, Round 341 : loss => 6.4499924324035645,  accuracy: 0.3065
[2025-09-11 21:47:27,583][flp2p.graph_runner][INFO] - Train, Round 342 : loss => 0.001659684383072697,  accuracy: 1.0, gradient_norm : 0.08080710641522926
[2025-09-11 21:47:41,309][flp2p.graph_runner][INFO] - Test, Round 342 : loss => 6.452482770991326,  accuracy: 0.3072
[2025-09-11 21:48:16,526][flp2p.graph_runner][INFO] - Train, Round 343 : loss => 0.0016529579822963567,  accuracy: 1.0, gradient_norm : 0.08141682118352316
[2025-09-11 21:48:30,496][flp2p.graph_runner][INFO] - Test, Round 343 : loss => 6.455152978563309,  accuracy: 0.3069
[2025-09-11 21:49:05,392][flp2p.graph_runner][INFO] - Train, Round 344 : loss => 0.0016450077146873815,  accuracy: 1.0, gradient_norm : 0.07922298058448721
[2025-09-11 21:49:19,170][flp2p.graph_runner][INFO] - Test, Round 344 : loss => 6.457212597703934,  accuracy: 0.3074
[2025-09-11 21:49:54,208][flp2p.graph_runner][INFO] - Train, Round 345 : loss => 0.001639894915739812,  accuracy: 1.0, gradient_norm : 0.08206343115741452
[2025-09-11 21:50:07,839][flp2p.graph_runner][INFO] - Test, Round 345 : loss => 6.459902283382416,  accuracy: 0.3073
[2025-09-11 21:50:42,730][flp2p.graph_runner][INFO] - Train, Round 346 : loss => 0.0016327373082943557,  accuracy: 1.0, gradient_norm : 0.07908641176189848
[2025-09-11 21:50:56,466][flp2p.graph_runner][INFO] - Test, Round 346 : loss => 6.461418320512772,  accuracy: 0.3071
[2025-09-11 21:51:31,659][flp2p.graph_runner][INFO] - Train, Round 347 : loss => 0.0016267793171088373,  accuracy: 1.0, gradient_norm : 0.08021746773803468
[2025-09-11 21:51:45,228][flp2p.graph_runner][INFO] - Test, Round 347 : loss => 6.464249113869667,  accuracy: 0.3069
[2025-09-11 21:52:20,224][flp2p.graph_runner][INFO] - Train, Round 348 : loss => 0.0016199482059649512,  accuracy: 1.0, gradient_norm : 0.07637038775197148
[2025-09-11 21:52:33,930][flp2p.graph_runner][INFO] - Test, Round 348 : loss => 6.466809690284729,  accuracy: 0.3074
[2025-09-11 21:53:08,717][flp2p.graph_runner][INFO] - Train, Round 349 : loss => 0.0016128411001651933,  accuracy: 1.0, gradient_norm : 0.08036481994336103
[2025-09-11 21:53:22,511][flp2p.graph_runner][INFO] - Test, Round 349 : loss => 6.469359954190254,  accuracy: 0.3069
[2025-09-11 21:53:57,458][flp2p.graph_runner][INFO] - Train, Round 350 : loss => 0.0016080329415126473,  accuracy: 1.0, gradient_norm : 0.07559047421551875
[2025-09-11 21:54:11,252][flp2p.graph_runner][INFO] - Test, Round 350 : loss => 6.4712887478351595,  accuracy: 0.3069
[2025-09-11 21:54:46,461][flp2p.graph_runner][INFO] - Train, Round 351 : loss => 0.0016014111742939955,  accuracy: 1.0, gradient_norm : 0.07778122953630685
[2025-09-11 21:55:00,167][flp2p.graph_runner][INFO] - Test, Round 351 : loss => 6.473764298319817,  accuracy: 0.3066
[2025-09-11 21:55:35,259][flp2p.graph_runner][INFO] - Train, Round 352 : loss => 0.001596077892803199,  accuracy: 1.0, gradient_norm : 0.07927441949203659
[2025-09-11 21:55:48,959][flp2p.graph_runner][INFO] - Test, Round 352 : loss => 6.4753834259033205,  accuracy: 0.3067
[2025-09-11 21:56:23,875][flp2p.graph_runner][INFO] - Train, Round 353 : loss => 0.0015891274747021583,  accuracy: 1.0, gradient_norm : 0.07587092717986217
[2025-09-11 21:56:37,618][flp2p.graph_runner][INFO] - Test, Round 353 : loss => 6.478074136018753,  accuracy: 0.3067
[2025-09-11 21:57:12,476][flp2p.graph_runner][INFO] - Train, Round 354 : loss => 0.0015833236006922856,  accuracy: 1.0, gradient_norm : 0.07751169404543214
[2025-09-11 21:57:26,137][flp2p.graph_runner][INFO] - Test, Round 354 : loss => 6.480609528326988,  accuracy: 0.3066
[2025-09-11 21:58:00,970][flp2p.graph_runner][INFO] - Train, Round 355 : loss => 0.0015769363572083724,  accuracy: 1.0, gradient_norm : 0.07586473670524979
[2025-09-11 21:58:14,919][flp2p.graph_runner][INFO] - Test, Round 355 : loss => 6.4827611563920975,  accuracy: 0.3067
[2025-09-11 21:58:49,764][flp2p.graph_runner][INFO] - Train, Round 356 : loss => 0.0015726011328661108,  accuracy: 1.0, gradient_norm : 0.07894787614373669
[2025-09-11 21:59:03,470][flp2p.graph_runner][INFO] - Test, Round 356 : loss => 6.48419085392952,  accuracy: 0.3068
[2025-09-11 21:59:38,216][flp2p.graph_runner][INFO] - Train, Round 357 : loss => 0.0015667178569371268,  accuracy: 1.0, gradient_norm : 0.07647777321862427
[2025-09-11 21:59:52,066][flp2p.graph_runner][INFO] - Test, Round 357 : loss => 6.486809138178826,  accuracy: 0.3068
[2025-09-11 22:00:26,948][flp2p.graph_runner][INFO] - Train, Round 358 : loss => 0.0015611171220371032,  accuracy: 1.0, gradient_norm : 0.07551910121701308
[2025-09-11 22:00:40,535][flp2p.graph_runner][INFO] - Test, Round 358 : loss => 6.488700728392601,  accuracy: 0.3068
[2025-09-11 22:01:15,784][flp2p.graph_runner][INFO] - Train, Round 359 : loss => 0.0015547297814191559,  accuracy: 1.0, gradient_norm : 0.07392053056005475
[2025-09-11 22:01:29,077][flp2p.graph_runner][INFO] - Test, Round 359 : loss => 6.491312175774574,  accuracy: 0.3069
[2025-09-11 22:02:04,182][flp2p.graph_runner][INFO] - Train, Round 360 : loss => 0.0015491255070082838,  accuracy: 1.0, gradient_norm : 0.07748735167965108
[2025-09-11 22:02:17,746][flp2p.graph_runner][INFO] - Test, Round 360 : loss => 6.492604358482361,  accuracy: 0.3064
[2025-09-11 22:02:52,677][flp2p.graph_runner][INFO] - Train, Round 361 : loss => 0.001543586280580106,  accuracy: 1.0, gradient_norm : 0.0733044581027804
[2025-09-11 22:03:06,153][flp2p.graph_runner][INFO] - Test, Round 361 : loss => 6.495147003698349,  accuracy: 0.3067
[2025-09-11 22:03:40,787][flp2p.graph_runner][INFO] - Train, Round 362 : loss => 0.0015386665895008875,  accuracy: 1.0, gradient_norm : 0.07570790802229452
[2025-09-11 22:03:55,696][flp2p.graph_runner][INFO] - Test, Round 362 : loss => 6.497059161114692,  accuracy: 0.3068
[2025-09-11 22:04:30,229][flp2p.graph_runner][INFO] - Train, Round 363 : loss => 0.0015328057866766658,  accuracy: 1.0, gradient_norm : 0.07420436823898263
[2025-09-11 22:04:49,319][flp2p.graph_runner][INFO] - Test, Round 363 : loss => 6.499242022418976,  accuracy: 0.3069
[2025-09-11 22:05:24,151][flp2p.graph_runner][INFO] - Train, Round 364 : loss => 0.0015270275507403613,  accuracy: 1.0, gradient_norm : 0.07193898610310367
[2025-09-11 22:05:44,675][flp2p.graph_runner][INFO] - Test, Round 364 : loss => 6.501423771595955,  accuracy: 0.3071
[2025-09-11 22:06:19,510][flp2p.graph_runner][INFO] - Train, Round 365 : loss => 0.0015224784062350717,  accuracy: 1.0, gradient_norm : 0.0765666493167363
[2025-09-11 22:06:37,066][flp2p.graph_runner][INFO] - Test, Round 365 : loss => 6.503316666221619,  accuracy: 0.3069
[2025-09-11 22:07:11,780][flp2p.graph_runner][INFO] - Train, Round 366 : loss => 0.0015166429524348744,  accuracy: 1.0, gradient_norm : 0.07273396012296389
[2025-09-11 22:07:28,078][flp2p.graph_runner][INFO] - Test, Round 366 : loss => 6.504711227536202,  accuracy: 0.3069
[2025-09-11 22:08:03,120][flp2p.graph_runner][INFO] - Train, Round 367 : loss => 0.0015122171794791943,  accuracy: 1.0, gradient_norm : 0.0753479021749118
[2025-09-11 22:08:16,994][flp2p.graph_runner][INFO] - Test, Round 367 : loss => 6.507674299526214,  accuracy: 0.3068
[2025-09-11 22:08:51,792][flp2p.graph_runner][INFO] - Train, Round 368 : loss => 0.0015068840405001531,  accuracy: 1.0, gradient_norm : 0.07637329851181421
[2025-09-11 22:09:05,835][flp2p.graph_runner][INFO] - Test, Round 368 : loss => 6.509482015275955,  accuracy: 0.3066
[2025-09-11 22:09:40,559][flp2p.graph_runner][INFO] - Train, Round 369 : loss => 0.0015016484047616053,  accuracy: 1.0, gradient_norm : 0.07121561761821811
[2025-09-11 22:09:54,408][flp2p.graph_runner][INFO] - Test, Round 369 : loss => 6.511180912947655,  accuracy: 0.3066
[2025-09-11 22:10:29,260][flp2p.graph_runner][INFO] - Train, Round 370 : loss => 0.0014964969603897774,  accuracy: 1.0, gradient_norm : 0.07233469787767667
[2025-09-11 22:10:43,225][flp2p.graph_runner][INFO] - Test, Round 370 : loss => 6.513222542977333,  accuracy: 0.3063
[2025-09-11 22:11:18,312][flp2p.graph_runner][INFO] - Train, Round 371 : loss => 0.0014912015976490988,  accuracy: 1.0, gradient_norm : 0.07139543339862904
[2025-09-11 22:11:32,678][flp2p.graph_runner][INFO] - Test, Round 371 : loss => 6.514750079679489,  accuracy: 0.3067
[2025-09-11 22:12:07,822][flp2p.graph_runner][INFO] - Train, Round 372 : loss => 0.0014858106155346237,  accuracy: 1.0, gradient_norm : 0.073567582857199
[2025-09-11 22:12:21,717][flp2p.graph_runner][INFO] - Test, Round 372 : loss => 6.516951929211617,  accuracy: 0.3073
[2025-09-11 22:12:56,588][flp2p.graph_runner][INFO] - Train, Round 373 : loss => 0.0014810612138292836,  accuracy: 1.0, gradient_norm : 0.0714029101898555
[2025-09-11 22:13:10,480][flp2p.graph_runner][INFO] - Test, Round 373 : loss => 6.518769486546517,  accuracy: 0.3072
[2025-09-11 22:13:45,296][flp2p.graph_runner][INFO] - Train, Round 374 : loss => 0.0014766233425810545,  accuracy: 1.0, gradient_norm : 0.07386728002046716
[2025-09-11 22:13:58,884][flp2p.graph_runner][INFO] - Test, Round 374 : loss => 6.520999936389923,  accuracy: 0.3063
[2025-09-11 22:14:33,778][flp2p.graph_runner][INFO] - Train, Round 375 : loss => 0.001471844891881726,  accuracy: 1.0, gradient_norm : 0.0730945034782484
[2025-09-11 22:14:47,358][flp2p.graph_runner][INFO] - Test, Round 375 : loss => 6.522589860630036,  accuracy: 0.3066
[2025-09-11 22:15:22,126][flp2p.graph_runner][INFO] - Train, Round 376 : loss => 0.0014670914655774442,  accuracy: 1.0, gradient_norm : 0.07448406255051812
[2025-09-11 22:15:35,973][flp2p.graph_runner][INFO] - Test, Round 376 : loss => 6.524264325976372,  accuracy: 0.3063
[2025-09-11 22:16:10,828][flp2p.graph_runner][INFO] - Train, Round 377 : loss => 0.0014626195703507016,  accuracy: 1.0, gradient_norm : 0.07133739858906994
[2025-09-11 22:16:24,568][flp2p.graph_runner][INFO] - Test, Round 377 : loss => 6.5263011206388475,  accuracy: 0.3065
[2025-09-11 22:16:59,445][flp2p.graph_runner][INFO] - Train, Round 378 : loss => 0.0014580893764165616,  accuracy: 1.0, gradient_norm : 0.07339619945874341
[2025-09-11 22:17:13,203][flp2p.graph_runner][INFO] - Test, Round 378 : loss => 6.5279968972206115,  accuracy: 0.3067
[2025-09-11 22:17:48,177][flp2p.graph_runner][INFO] - Train, Round 379 : loss => 0.0014529523356031856,  accuracy: 1.0, gradient_norm : 0.07080568044940626
[2025-09-11 22:18:01,644][flp2p.graph_runner][INFO] - Test, Round 379 : loss => 6.529651500153541,  accuracy: 0.3066
[2025-09-11 22:18:36,706][flp2p.graph_runner][INFO] - Train, Round 380 : loss => 0.0014484763794462194,  accuracy: 1.0, gradient_norm : 0.07143285368990299
[2025-09-11 22:18:50,527][flp2p.graph_runner][INFO] - Test, Round 380 : loss => 6.531514423513412,  accuracy: 0.3065
[2025-09-11 22:19:25,312][flp2p.graph_runner][INFO] - Train, Round 381 : loss => 0.001444090886143385,  accuracy: 1.0, gradient_norm : 0.07301839107896232
[2025-09-11 22:19:39,063][flp2p.graph_runner][INFO] - Test, Round 381 : loss => 6.53329965903759,  accuracy: 0.3062
[2025-09-11 22:20:13,938][flp2p.graph_runner][INFO] - Train, Round 382 : loss => 0.0014395375331757053,  accuracy: 1.0, gradient_norm : 0.06866732546487687
[2025-09-11 22:20:27,827][flp2p.graph_runner][INFO] - Test, Round 382 : loss => 6.535330418014526,  accuracy: 0.3066
[2025-09-11 22:21:02,768][flp2p.graph_runner][INFO] - Train, Round 383 : loss => 0.001435310547700889,  accuracy: 1.0, gradient_norm : 0.07235300978229506
[2025-09-11 22:21:16,474][flp2p.graph_runner][INFO] - Test, Round 383 : loss => 6.537111630535126,  accuracy: 0.3063
[2025-09-11 22:21:51,518][flp2p.graph_runner][INFO] - Train, Round 384 : loss => 0.001430104444759005,  accuracy: 1.0, gradient_norm : 0.06665696183565925
[2025-09-11 22:22:05,165][flp2p.graph_runner][INFO] - Test, Round 384 : loss => 6.5386551579236984,  accuracy: 0.3068
[2025-09-11 22:22:39,759][flp2p.graph_runner][INFO] - Train, Round 385 : loss => 0.0014264776009804336,  accuracy: 1.0, gradient_norm : 0.0723250569644781
[2025-09-11 22:22:53,499][flp2p.graph_runner][INFO] - Test, Round 385 : loss => 6.540148873972893,  accuracy: 0.3066
[2025-09-11 22:23:28,720][flp2p.graph_runner][INFO] - Train, Round 386 : loss => 0.0014213024212222087,  accuracy: 1.0, gradient_norm : 0.07005711459971076
[2025-09-11 22:23:42,487][flp2p.graph_runner][INFO] - Test, Round 386 : loss => 6.541853166651726,  accuracy: 0.3069
[2025-09-11 22:24:17,276][flp2p.graph_runner][INFO] - Train, Round 387 : loss => 0.001417169745642846,  accuracy: 1.0, gradient_norm : 0.07030030329242534
[2025-09-11 22:24:31,188][flp2p.graph_runner][INFO] - Test, Round 387 : loss => 6.543692429852485,  accuracy: 0.3065
[2025-09-11 22:25:05,868][flp2p.graph_runner][INFO] - Train, Round 388 : loss => 0.0014134195804460137,  accuracy: 1.0, gradient_norm : 0.06742611919672509
[2025-09-11 22:25:19,452][flp2p.graph_runner][INFO] - Test, Round 388 : loss => 6.545239659357071,  accuracy: 0.3065
[2025-09-11 22:25:54,083][flp2p.graph_runner][INFO] - Train, Round 389 : loss => 0.001409669372393788,  accuracy: 1.0, gradient_norm : 0.0695153960579351
[2025-09-11 22:26:07,663][flp2p.graph_runner][INFO] - Test, Round 389 : loss => 6.547060429096222,  accuracy: 0.3069
[2025-09-11 22:26:42,557][flp2p.graph_runner][INFO] - Train, Round 390 : loss => 0.0014055469601347184,  accuracy: 1.0, gradient_norm : 0.06672947024954551
[2025-09-11 22:26:56,075][flp2p.graph_runner][INFO] - Test, Round 390 : loss => 6.548500442028046,  accuracy: 0.3063
[2025-09-11 22:27:30,848][flp2p.graph_runner][INFO] - Train, Round 391 : loss => 0.00140178519039182,  accuracy: 1.0, gradient_norm : 0.07128792587090492
[2025-09-11 22:27:44,629][flp2p.graph_runner][INFO] - Test, Round 391 : loss => 6.550341725158692,  accuracy: 0.3068
[2025-09-11 22:28:19,410][flp2p.graph_runner][INFO] - Train, Round 392 : loss => 0.0013971627230542564,  accuracy: 1.0, gradient_norm : 0.06844098007568293
[2025-09-11 22:28:37,031][flp2p.graph_runner][INFO] - Test, Round 392 : loss => 6.551762518978119,  accuracy: 0.3066
[2025-09-11 22:29:11,853][flp2p.graph_runner][INFO] - Train, Round 393 : loss => 0.0013927696494162476,  accuracy: 1.0, gradient_norm : 0.06895101669098021
[2025-09-11 22:29:34,111][flp2p.graph_runner][INFO] - Test, Round 393 : loss => 6.553266527533531,  accuracy: 0.3066
[2025-09-11 22:30:08,793][flp2p.graph_runner][INFO] - Train, Round 394 : loss => 0.0013886423550866312,  accuracy: 1.0, gradient_norm : 0.06832029113248274
[2025-09-11 22:30:27,108][flp2p.graph_runner][INFO] - Test, Round 394 : loss => 6.555214885616302,  accuracy: 0.3069
[2025-09-11 22:31:01,790][flp2p.graph_runner][INFO] - Train, Round 395 : loss => 0.0013840389362061007,  accuracy: 1.0, gradient_norm : 0.0668330104623938
[2025-09-11 22:31:18,954][flp2p.graph_runner][INFO] - Test, Round 395 : loss => 6.556382917761803,  accuracy: 0.3068
[2025-09-11 22:31:53,705][flp2p.graph_runner][INFO] - Train, Round 396 : loss => 0.001380611793041074,  accuracy: 1.0, gradient_norm : 0.06684543198104845
[2025-09-11 22:32:09,326][flp2p.graph_runner][INFO] - Test, Round 396 : loss => 6.558300258874893,  accuracy: 0.3064
[2025-09-11 22:32:44,082][flp2p.graph_runner][INFO] - Train, Round 397 : loss => 0.0013769721517746802,  accuracy: 1.0, gradient_norm : 0.06556768645064329
[2025-09-11 22:32:58,159][flp2p.graph_runner][INFO] - Test, Round 397 : loss => 6.55958492834568,  accuracy: 0.3067
[2025-09-11 22:33:33,241][flp2p.graph_runner][INFO] - Train, Round 398 : loss => 0.001372798338828337,  accuracy: 1.0, gradient_norm : 0.06591566734284776
[2025-09-11 22:33:47,172][flp2p.graph_runner][INFO] - Test, Round 398 : loss => 6.561512653660774,  accuracy: 0.3065
[2025-09-11 22:34:22,090][flp2p.graph_runner][INFO] - Train, Round 399 : loss => 0.001368795741194238,  accuracy: 1.0, gradient_norm : 0.06917362786881433
[2025-09-11 22:34:36,066][flp2p.graph_runner][INFO] - Test, Round 399 : loss => 6.563070580530167,  accuracy: 0.3063
[2025-09-11 22:35:10,908][flp2p.graph_runner][INFO] - Train, Round 400 : loss => 0.0013649173095958151,  accuracy: 1.0, gradient_norm : 0.06911709863504355
[2025-09-11 22:35:25,009][flp2p.graph_runner][INFO] - Test, Round 400 : loss => 6.564672458195687,  accuracy: 0.3067
[2025-09-11 22:35:59,889][flp2p.graph_runner][INFO] - Train, Round 401 : loss => 0.0013611506533682887,  accuracy: 1.0, gradient_norm : 0.06380464394490094
[2025-09-11 22:36:13,752][flp2p.graph_runner][INFO] - Test, Round 401 : loss => 6.565502796959877,  accuracy: 0.3067
[2025-09-11 22:36:48,715][flp2p.graph_runner][INFO] - Train, Round 402 : loss => 0.0013580535667521564,  accuracy: 1.0, gradient_norm : 0.06504710113689459
[2025-09-11 22:37:02,694][flp2p.graph_runner][INFO] - Test, Round 402 : loss => 6.567255978751183,  accuracy: 0.3064
[2025-09-11 22:37:37,463][flp2p.graph_runner][INFO] - Train, Round 403 : loss => 0.001353874263877515,  accuracy: 1.0, gradient_norm : 0.06652295188633243
[2025-09-11 22:37:51,391][flp2p.graph_runner][INFO] - Test, Round 403 : loss => 6.569207951116562,  accuracy: 0.3067
[2025-09-11 22:38:26,201][flp2p.graph_runner][INFO] - Train, Round 404 : loss => 0.0013500651064532576,  accuracy: 1.0, gradient_norm : 0.06587059637839857
[2025-09-11 22:38:40,199][flp2p.graph_runner][INFO] - Test, Round 404 : loss => 6.570504201865196,  accuracy: 0.3066
[2025-09-11 22:39:15,267][flp2p.graph_runner][INFO] - Train, Round 405 : loss => 0.001346728373149138,  accuracy: 1.0, gradient_norm : 0.06650298069110748
[2025-09-11 22:39:28,952][flp2p.graph_runner][INFO] - Test, Round 405 : loss => 6.571381356430054,  accuracy: 0.3067
[2025-09-11 22:40:04,139][flp2p.graph_runner][INFO] - Train, Round 406 : loss => 0.001343085009866627,  accuracy: 1.0, gradient_norm : 0.06570323183124709
[2025-09-11 22:40:17,764][flp2p.graph_runner][INFO] - Test, Round 406 : loss => 6.573303501224518,  accuracy: 0.3068
[2025-09-11 22:40:52,601][flp2p.graph_runner][INFO] - Train, Round 407 : loss => 0.001339367163309362,  accuracy: 1.0, gradient_norm : 0.0665884357232762
[2025-09-11 22:41:06,403][flp2p.graph_runner][INFO] - Test, Round 407 : loss => 6.574495487976074,  accuracy: 0.3065
[2025-09-11 22:41:41,461][flp2p.graph_runner][INFO] - Train, Round 408 : loss => 0.0013356993935546297,  accuracy: 1.0, gradient_norm : 0.06552733233120425
[2025-09-11 22:41:55,392][flp2p.graph_runner][INFO] - Test, Round 408 : loss => 6.576213614439964,  accuracy: 0.3068
[2025-09-11 22:42:30,206][flp2p.graph_runner][INFO] - Train, Round 409 : loss => 0.0013318618822086143,  accuracy: 1.0, gradient_norm : 0.06266877202786039
[2025-09-11 22:42:44,140][flp2p.graph_runner][INFO] - Test, Round 409 : loss => 6.577394414496422,  accuracy: 0.3067
[2025-09-11 22:43:19,046][flp2p.graph_runner][INFO] - Train, Round 410 : loss => 0.0013286999155631444,  accuracy: 1.0, gradient_norm : 0.06581315031804161
[2025-09-11 22:43:32,786][flp2p.graph_runner][INFO] - Test, Round 410 : loss => 6.578938887453079,  accuracy: 0.3066
[2025-09-11 22:44:07,418][flp2p.graph_runner][INFO] - Train, Round 411 : loss => 0.0013249819477399191,  accuracy: 1.0, gradient_norm : 0.06469448312931585
[2025-09-11 22:44:21,266][flp2p.graph_runner][INFO] - Test, Round 411 : loss => 6.5804092092037205,  accuracy: 0.3065
[2025-09-11 22:44:56,095][flp2p.graph_runner][INFO] - Train, Round 412 : loss => 0.0013214888394092368,  accuracy: 1.0, gradient_norm : 0.06430818976514216
[2025-09-11 22:45:09,829][flp2p.graph_runner][INFO] - Test, Round 412 : loss => 6.581456655406952,  accuracy: 0.3065
[2025-09-11 22:45:44,983][flp2p.graph_runner][INFO] - Train, Round 413 : loss => 0.001318554136790529,  accuracy: 1.0, gradient_norm : 0.06589410196634486
[2025-09-11 22:45:58,738][flp2p.graph_runner][INFO] - Test, Round 413 : loss => 6.5830642761230465,  accuracy: 0.3064
[2025-09-11 22:46:33,457][flp2p.graph_runner][INFO] - Train, Round 414 : loss => 0.0013152463542792242,  accuracy: 1.0, gradient_norm : 0.06685188021845302
[2025-09-11 22:46:47,398][flp2p.graph_runner][INFO] - Test, Round 414 : loss => 6.58440041744709,  accuracy: 0.3065
[2025-09-11 22:47:22,437][flp2p.graph_runner][INFO] - Train, Round 415 : loss => 0.0013123710830647423,  accuracy: 1.0, gradient_norm : 0.06537592091380036
[2025-09-11 22:47:36,292][flp2p.graph_runner][INFO] - Test, Round 415 : loss => 6.585790426111221,  accuracy: 0.3065
[2025-09-11 22:48:11,238][flp2p.graph_runner][INFO] - Train, Round 416 : loss => 0.001308784534824857,  accuracy: 1.0, gradient_norm : 0.06521321317249677
[2025-09-11 22:48:24,923][flp2p.graph_runner][INFO] - Test, Round 416 : loss => 6.586893888068199,  accuracy: 0.3066
[2025-09-11 22:48:59,641][flp2p.graph_runner][INFO] - Train, Round 417 : loss => 0.001305388339339212,  accuracy: 1.0, gradient_norm : 0.06459823430837433
[2025-09-11 22:49:13,168][flp2p.graph_runner][INFO] - Test, Round 417 : loss => 6.588301811504364,  accuracy: 0.3067
[2025-09-11 22:49:48,140][flp2p.graph_runner][INFO] - Train, Round 418 : loss => 0.0013017934695138437,  accuracy: 1.0, gradient_norm : 0.06423860035514868
[2025-09-11 22:50:01,915][flp2p.graph_runner][INFO] - Test, Round 418 : loss => 6.589593345189095,  accuracy: 0.3065
[2025-09-11 22:50:36,696][flp2p.graph_runner][INFO] - Train, Round 419 : loss => 0.0012987158517595767,  accuracy: 1.0, gradient_norm : 0.06432090222700064
[2025-09-11 22:50:50,160][flp2p.graph_runner][INFO] - Test, Round 419 : loss => 6.591079916858673,  accuracy: 0.3064
[2025-09-11 22:51:24,965][flp2p.graph_runner][INFO] - Train, Round 420 : loss => 0.0012950697022703636,  accuracy: 1.0, gradient_norm : 0.06378788873297235
[2025-09-11 22:51:38,624][flp2p.graph_runner][INFO] - Test, Round 420 : loss => 6.592054328775406,  accuracy: 0.3061
[2025-09-11 22:52:13,617][flp2p.graph_runner][INFO] - Train, Round 421 : loss => 0.0012917373918268516,  accuracy: 1.0, gradient_norm : 0.06223124895630597
[2025-09-11 22:52:27,073][flp2p.graph_runner][INFO] - Test, Round 421 : loss => 6.5935700322389605,  accuracy: 0.3065
[2025-09-11 22:53:01,810][flp2p.graph_runner][INFO] - Train, Round 422 : loss => 0.001289159841714233,  accuracy: 1.0, gradient_norm : 0.06388490925335699
[2025-09-11 22:53:15,200][flp2p.graph_runner][INFO] - Test, Round 422 : loss => 6.594627790284157,  accuracy: 0.3066
[2025-09-11 22:53:50,148][flp2p.graph_runner][INFO] - Train, Round 423 : loss => 0.001285255948023405,  accuracy: 1.0, gradient_norm : 0.06452110955548516
[2025-09-11 22:54:06,397][flp2p.graph_runner][INFO] - Test, Round 423 : loss => 6.595875146317482,  accuracy: 0.3066
[2025-09-11 22:54:41,176][flp2p.graph_runner][INFO] - Train, Round 424 : loss => 0.0012828811325744024,  accuracy: 1.0, gradient_norm : 0.06274647436854128
[2025-09-11 22:55:01,944][flp2p.graph_runner][INFO] - Test, Round 424 : loss => 6.597201839971542,  accuracy: 0.3066
[2025-09-11 22:55:36,755][flp2p.graph_runner][INFO] - Train, Round 425 : loss => 0.0012801172360680842,  accuracy: 1.0, gradient_norm : 0.06209706623897783
[2025-09-11 22:55:56,415][flp2p.graph_runner][INFO] - Test, Round 425 : loss => 6.598444723510743,  accuracy: 0.3064
[2025-09-11 22:56:31,337][flp2p.graph_runner][INFO] - Train, Round 426 : loss => 0.001276998201162011,  accuracy: 1.0, gradient_norm : 0.06298384011249902
[2025-09-11 22:56:48,547][flp2p.graph_runner][INFO] - Test, Round 426 : loss => 6.599683986163139,  accuracy: 0.3069
[2025-09-11 22:57:23,360][flp2p.graph_runner][INFO] - Train, Round 427 : loss => 0.0012736763663269816,  accuracy: 1.0, gradient_norm : 0.06163860890707843
[2025-09-11 22:57:38,357][flp2p.graph_runner][INFO] - Test, Round 427 : loss => 6.601356812167167,  accuracy: 0.3065
[2025-09-11 22:58:13,456][flp2p.graph_runner][INFO] - Train, Round 428 : loss => 0.0012708388730970908,  accuracy: 1.0, gradient_norm : 0.06422021740645456
[2025-09-11 22:58:27,193][flp2p.graph_runner][INFO] - Test, Round 428 : loss => 6.602440385365486,  accuracy: 0.3066
[2025-09-11 22:59:01,893][flp2p.graph_runner][INFO] - Train, Round 429 : loss => 0.0012679319317733943,  accuracy: 1.0, gradient_norm : 0.06428929270831324
[2025-09-11 22:59:15,898][flp2p.graph_runner][INFO] - Test, Round 429 : loss => 6.6035967311382295,  accuracy: 0.3067
[2025-09-11 22:59:50,682][flp2p.graph_runner][INFO] - Train, Round 430 : loss => 0.001264686367600613,  accuracy: 1.0, gradient_norm : 0.06030007130917664
[2025-09-11 23:00:04,540][flp2p.graph_runner][INFO] - Test, Round 430 : loss => 6.604978031802178,  accuracy: 0.3064
[2025-09-11 23:00:39,390][flp2p.graph_runner][INFO] - Train, Round 431 : loss => 0.0012618690059025535,  accuracy: 1.0, gradient_norm : 0.06352859785671412
[2025-09-11 23:00:53,507][flp2p.graph_runner][INFO] - Test, Round 431 : loss => 6.6058044109582905,  accuracy: 0.3065
[2025-09-11 23:01:28,471][flp2p.graph_runner][INFO] - Train, Round 432 : loss => 0.0012589179069861223,  accuracy: 1.0, gradient_norm : 0.062122092883568944
[2025-09-11 23:01:42,454][flp2p.graph_runner][INFO] - Test, Round 432 : loss => 6.60695170545578,  accuracy: 0.3063
[2025-09-11 23:02:17,289][flp2p.graph_runner][INFO] - Train, Round 433 : loss => 0.0012559829858219012,  accuracy: 1.0, gradient_norm : 0.06187712307922875
[2025-09-11 23:02:31,275][flp2p.graph_runner][INFO] - Test, Round 433 : loss => 6.608192057967186,  accuracy: 0.3064
[2025-09-11 23:03:06,484][flp2p.graph_runner][INFO] - Train, Round 434 : loss => 0.0012527291240257909,  accuracy: 1.0, gradient_norm : 0.06306309588205694
[2025-09-11 23:03:20,362][flp2p.graph_runner][INFO] - Test, Round 434 : loss => 6.609031081962585,  accuracy: 0.3063
[2025-09-11 23:03:55,209][flp2p.graph_runner][INFO] - Train, Round 435 : loss => 0.001249932960272418,  accuracy: 1.0, gradient_norm : 0.06187528640498902
[2025-09-11 23:04:09,158][flp2p.graph_runner][INFO] - Test, Round 435 : loss => 6.610471501708031,  accuracy: 0.3064
[2025-09-11 23:04:44,223][flp2p.graph_runner][INFO] - Train, Round 436 : loss => 0.001247443804653206,  accuracy: 1.0, gradient_norm : 0.0615904104745592
[2025-09-11 23:04:58,250][flp2p.graph_runner][INFO] - Test, Round 436 : loss => 6.611675304245948,  accuracy: 0.3062
[2025-09-11 23:05:33,412][flp2p.graph_runner][INFO] - Train, Round 437 : loss => 0.0012446740558516467,  accuracy: 1.0, gradient_norm : 0.06364841127523202
[2025-09-11 23:05:47,192][flp2p.graph_runner][INFO] - Test, Round 437 : loss => 6.612740415453911,  accuracy: 0.3063
[2025-09-11 23:06:22,161][flp2p.graph_runner][INFO] - Train, Round 438 : loss => 0.0012417816927336384,  accuracy: 1.0, gradient_norm : 0.05984233269461128
[2025-09-11 23:06:35,722][flp2p.graph_runner][INFO] - Test, Round 438 : loss => 6.613914929199218,  accuracy: 0.3063
[2025-09-11 23:07:10,522][flp2p.graph_runner][INFO] - Train, Round 439 : loss => 0.001238775232801951,  accuracy: 1.0, gradient_norm : 0.06152509928680461
[2025-09-11 23:07:24,329][flp2p.graph_runner][INFO] - Test, Round 439 : loss => 6.614882904791832,  accuracy: 0.3065
[2025-09-11 23:07:59,034][flp2p.graph_runner][INFO] - Train, Round 440 : loss => 0.0012363381975349817,  accuracy: 1.0, gradient_norm : 0.06019925930707742
[2025-09-11 23:08:12,689][flp2p.graph_runner][INFO] - Test, Round 440 : loss => 6.616147289085388,  accuracy: 0.306
[2025-09-11 23:08:47,744][flp2p.graph_runner][INFO] - Train, Round 441 : loss => 0.0012334144967462632,  accuracy: 1.0, gradient_norm : 0.060099413510387174
[2025-09-11 23:09:01,456][flp2p.graph_runner][INFO] - Test, Round 441 : loss => 6.617005539083481,  accuracy: 0.3068
[2025-09-11 23:09:36,314][flp2p.graph_runner][INFO] - Train, Round 442 : loss => 0.0012312208445897943,  accuracy: 1.0, gradient_norm : 0.059380051260433137
[2025-09-11 23:09:50,150][flp2p.graph_runner][INFO] - Test, Round 442 : loss => 6.618161961841583,  accuracy: 0.3065
[2025-09-11 23:10:24,997][flp2p.graph_runner][INFO] - Train, Round 443 : loss => 0.001227934005340406,  accuracy: 1.0, gradient_norm : 0.062314740227932236
[2025-09-11 23:10:38,640][flp2p.graph_runner][INFO] - Test, Round 443 : loss => 6.61962108566761,  accuracy: 0.3064
[2025-09-11 23:11:13,455][flp2p.graph_runner][INFO] - Train, Round 444 : loss => 0.001225866461851789,  accuracy: 1.0, gradient_norm : 0.05989778105567078
[2025-09-11 23:11:27,065][flp2p.graph_runner][INFO] - Test, Round 444 : loss => 6.620644871044159,  accuracy: 0.306
[2025-09-11 23:12:02,048][flp2p.graph_runner][INFO] - Train, Round 445 : loss => 0.0012227595502675586,  accuracy: 1.0, gradient_norm : 0.06024284515843772
[2025-09-11 23:12:15,672][flp2p.graph_runner][INFO] - Test, Round 445 : loss => 6.621617461585998,  accuracy: 0.3066
[2025-09-11 23:12:50,690][flp2p.graph_runner][INFO] - Train, Round 446 : loss => 0.001220419509045314,  accuracy: 1.0, gradient_norm : 0.0637447108309589
[2025-09-11 23:13:04,315][flp2p.graph_runner][INFO] - Test, Round 446 : loss => 6.622535618948937,  accuracy: 0.3069
[2025-09-11 23:13:39,096][flp2p.graph_runner][INFO] - Train, Round 447 : loss => 0.0012178643586473,  accuracy: 1.0, gradient_norm : 0.059130807242418364
[2025-09-11 23:13:52,995][flp2p.graph_runner][INFO] - Test, Round 447 : loss => 6.623004125881195,  accuracy: 0.3059
[2025-09-11 23:14:27,811][flp2p.graph_runner][INFO] - Train, Round 448 : loss => 0.0012155769108843136,  accuracy: 1.0, gradient_norm : 0.059515037341389576
[2025-09-11 23:14:41,490][flp2p.graph_runner][INFO] - Test, Round 448 : loss => 6.624520322918892,  accuracy: 0.3067
[2025-09-11 23:15:16,796][flp2p.graph_runner][INFO] - Train, Round 449 : loss => 0.0012125397278820549,  accuracy: 1.0, gradient_norm : 0.05905661142921726
[2025-09-11 23:15:30,369][flp2p.graph_runner][INFO] - Test, Round 449 : loss => 6.625476523828507,  accuracy: 0.3064
[2025-09-11 23:16:05,451][flp2p.graph_runner][INFO] - Train, Round 450 : loss => 0.0012096748130958684,  accuracy: 1.0, gradient_norm : 0.05764423413768778
[2025-09-11 23:16:18,891][flp2p.graph_runner][INFO] - Test, Round 450 : loss => 6.626459521842003,  accuracy: 0.3066
[2025-09-11 23:16:53,858][flp2p.graph_runner][INFO] - Train, Round 451 : loss => 0.00120750011272321,  accuracy: 1.0, gradient_norm : 0.060488238003344616
[2025-09-11 23:17:07,321][flp2p.graph_runner][INFO] - Test, Round 451 : loss => 6.627860226917267,  accuracy: 0.3067
[2025-09-11 23:17:42,176][flp2p.graph_runner][INFO] - Train, Round 452 : loss => 0.0012048845887329663,  accuracy: 1.0, gradient_norm : 0.059078155998747224
[2025-09-11 23:17:55,835][flp2p.graph_runner][INFO] - Test, Round 452 : loss => 6.628478093791008,  accuracy: 0.3064
[2025-09-11 23:18:30,866][flp2p.graph_runner][INFO] - Train, Round 453 : loss => 0.0012027002109001234,  accuracy: 1.0, gradient_norm : 0.05914766169445135
[2025-09-11 23:18:44,345][flp2p.graph_runner][INFO] - Test, Round 453 : loss => 6.629373766303062,  accuracy: 0.3065
[2025-09-11 23:19:19,401][flp2p.graph_runner][INFO] - Train, Round 454 : loss => 0.0011995483045757283,  accuracy: 1.0, gradient_norm : 0.05662975829789687
[2025-09-11 23:19:33,225][flp2p.graph_runner][INFO] - Test, Round 454 : loss => 6.6306033203125,  accuracy: 0.3063
[2025-09-11 23:20:08,106][flp2p.graph_runner][INFO] - Train, Round 455 : loss => 0.0011975796105980407,  accuracy: 1.0, gradient_norm : 0.058844617971939996
[2025-09-11 23:20:26,741][flp2p.graph_runner][INFO] - Test, Round 455 : loss => 6.631352346467972,  accuracy: 0.3065
[2025-09-11 23:21:01,655][flp2p.graph_runner][INFO] - Train, Round 456 : loss => 0.0011954999009321907,  accuracy: 1.0, gradient_norm : 0.059660728025671086
[2025-09-11 23:21:23,111][flp2p.graph_runner][INFO] - Test, Round 456 : loss => 6.6324484083890916,  accuracy: 0.3061
[2025-09-11 23:21:58,030][flp2p.graph_runner][INFO] - Train, Round 457 : loss => 0.0011932762400101641,  accuracy: 1.0, gradient_norm : 0.059907751019041465
[2025-09-11 23:22:17,625][flp2p.graph_runner][INFO] - Test, Round 457 : loss => 6.633234135866165,  accuracy: 0.3063
[2025-09-11 23:22:52,174][flp2p.graph_runner][INFO] - Train, Round 458 : loss => 0.0011903340839004761,  accuracy: 1.0, gradient_norm : 0.06051020390502513
[2025-09-11 23:23:09,437][flp2p.graph_runner][INFO] - Test, Round 458 : loss => 6.6343657223701475,  accuracy: 0.3067
[2025-09-11 23:23:44,280][flp2p.graph_runner][INFO] - Train, Round 459 : loss => 0.0011882186566799645,  accuracy: 1.0, gradient_norm : 0.059040405317578044
[2025-09-11 23:23:58,686][flp2p.graph_runner][INFO] - Test, Round 459 : loss => 6.635302131605148,  accuracy: 0.3064
[2025-09-11 23:24:33,332][flp2p.graph_runner][INFO] - Train, Round 460 : loss => 0.0011852052972244564,  accuracy: 1.0, gradient_norm : 0.056837812700348156
[2025-09-11 23:24:47,234][flp2p.graph_runner][INFO] - Test, Round 460 : loss => 6.636122589015961,  accuracy: 0.3062
[2025-09-11 23:25:21,823][flp2p.graph_runner][INFO] - Train, Round 461 : loss => 0.0011836162739200515,  accuracy: 1.0, gradient_norm : 0.05695725780841168
[2025-09-11 23:25:35,744][flp2p.graph_runner][INFO] - Test, Round 461 : loss => 6.6370294667482375,  accuracy: 0.3066
[2025-09-11 23:26:10,463][flp2p.graph_runner][INFO] - Train, Round 462 : loss => 0.0011812181897888274,  accuracy: 1.0, gradient_norm : 0.05841867962349335
[2025-09-11 23:26:24,353][flp2p.graph_runner][INFO] - Test, Round 462 : loss => 6.637796614193916,  accuracy: 0.3058
[2025-09-11 23:26:59,101][flp2p.graph_runner][INFO] - Train, Round 463 : loss => 0.0011786613389752653,  accuracy: 1.0, gradient_norm : 0.05896929145998456
[2025-09-11 23:27:13,046][flp2p.graph_runner][INFO] - Test, Round 463 : loss => 6.639086580729485,  accuracy: 0.3066
[2025-09-11 23:27:47,932][flp2p.graph_runner][INFO] - Train, Round 464 : loss => 0.001176426330348477,  accuracy: 1.0, gradient_norm : 0.05618161606927377
[2025-09-11 23:28:02,017][flp2p.graph_runner][INFO] - Test, Round 464 : loss => 6.63990735783577,  accuracy: 0.3064
[2025-09-11 23:28:36,903][flp2p.graph_runner][INFO] - Train, Round 465 : loss => 0.0011746322162313537,  accuracy: 1.0, gradient_norm : 0.058996168607055324
[2025-09-11 23:28:50,624][flp2p.graph_runner][INFO] - Test, Round 465 : loss => 6.640784861946106,  accuracy: 0.3066
[2025-09-11 23:29:25,316][flp2p.graph_runner][INFO] - Train, Round 466 : loss => 0.0011720645938961146,  accuracy: 1.0, gradient_norm : 0.06007877678624775
[2025-09-11 23:29:39,158][flp2p.graph_runner][INFO] - Test, Round 466 : loss => 6.641451415777206,  accuracy: 0.3065
[2025-09-11 23:30:14,182][flp2p.graph_runner][INFO] - Train, Round 467 : loss => 0.0011699945589983447,  accuracy: 1.0, gradient_norm : 0.057971339396668486
[2025-09-11 23:30:27,766][flp2p.graph_runner][INFO] - Test, Round 467 : loss => 6.642552018308639,  accuracy: 0.3063
[2025-09-11 23:31:02,537][flp2p.graph_runner][INFO] - Train, Round 468 : loss => 0.001167422664099528,  accuracy: 1.0, gradient_norm : 0.05593526123973676
[2025-09-11 23:31:16,106][flp2p.graph_runner][INFO] - Test, Round 468 : loss => 6.643164322113991,  accuracy: 0.3065
[2025-09-11 23:31:50,993][flp2p.graph_runner][INFO] - Train, Round 469 : loss => 0.0011655642097321104,  accuracy: 1.0, gradient_norm : 0.0570888370392434
[2025-09-11 23:32:04,871][flp2p.graph_runner][INFO] - Test, Round 469 : loss => 6.644088327789307,  accuracy: 0.3063
[2025-09-11 23:32:39,869][flp2p.graph_runner][INFO] - Train, Round 470 : loss => 0.001163561839978987,  accuracy: 1.0, gradient_norm : 0.05591157640224467
[2025-09-11 23:32:53,585][flp2p.graph_runner][INFO] - Test, Round 470 : loss => 6.645087115073204,  accuracy: 0.3064
[2025-09-11 23:33:28,560][flp2p.graph_runner][INFO] - Train, Round 471 : loss => 0.0011613827110947266,  accuracy: 1.0, gradient_norm : 0.056288206908379296
[2025-09-11 23:33:42,011][flp2p.graph_runner][INFO] - Test, Round 471 : loss => 6.645937749528885,  accuracy: 0.3066
[2025-09-11 23:34:17,094][flp2p.graph_runner][INFO] - Train, Round 472 : loss => 0.0011589486500694572,  accuracy: 1.0, gradient_norm : 0.05785512073689871
[2025-09-11 23:34:30,797][flp2p.graph_runner][INFO] - Test, Round 472 : loss => 6.64705636844635,  accuracy: 0.3066
[2025-09-11 23:35:05,787][flp2p.graph_runner][INFO] - Train, Round 473 : loss => 0.0011569356073596283,  accuracy: 1.0, gradient_norm : 0.057850082066099644
[2025-09-11 23:35:19,601][flp2p.graph_runner][INFO] - Test, Round 473 : loss => 6.648096405339241,  accuracy: 0.3067
[2025-09-11 23:35:54,428][flp2p.graph_runner][INFO] - Train, Round 474 : loss => 0.0011542763264636353,  accuracy: 1.0, gradient_norm : 0.05667236517582843
[2025-09-11 23:36:08,262][flp2p.graph_runner][INFO] - Test, Round 474 : loss => 6.648114794969559,  accuracy: 0.3064
[2025-09-11 23:36:43,008][flp2p.graph_runner][INFO] - Train, Round 475 : loss => 0.0011525551727148318,  accuracy: 1.0, gradient_norm : 0.05640141100302691
[2025-09-11 23:36:56,673][flp2p.graph_runner][INFO] - Test, Round 475 : loss => 6.64899884364605,  accuracy: 0.3069
[2025-09-11 23:37:31,686][flp2p.graph_runner][INFO] - Train, Round 476 : loss => 0.0011505201434677782,  accuracy: 1.0, gradient_norm : 0.057627674320800175
[2025-09-11 23:37:45,301][flp2p.graph_runner][INFO] - Test, Round 476 : loss => 6.6501264948606496,  accuracy: 0.3064
[2025-09-11 23:38:20,233][flp2p.graph_runner][INFO] - Train, Round 477 : loss => 0.0011483836988433417,  accuracy: 1.0, gradient_norm : 0.054898073607912885
[2025-09-11 23:38:33,928][flp2p.graph_runner][INFO] - Test, Round 477 : loss => 6.651074355769158,  accuracy: 0.3068
[2025-09-11 23:39:08,755][flp2p.graph_runner][INFO] - Train, Round 478 : loss => 0.0011466024180117526,  accuracy: 1.0, gradient_norm : 0.06019571082861372
[2025-09-11 23:39:22,323][flp2p.graph_runner][INFO] - Test, Round 478 : loss => 6.651485171842575,  accuracy: 0.3064
[2025-09-11 23:39:57,193][flp2p.graph_runner][INFO] - Train, Round 479 : loss => 0.0011440385972431005,  accuracy: 1.0, gradient_norm : 0.05632509791652222
[2025-09-11 23:40:10,625][flp2p.graph_runner][INFO] - Test, Round 479 : loss => 6.652385184240341,  accuracy: 0.3067
[2025-09-11 23:40:45,499][flp2p.graph_runner][INFO] - Train, Round 480 : loss => 0.0011420199888380013,  accuracy: 1.0, gradient_norm : 0.05491561736164243
[2025-09-11 23:40:58,885][flp2p.graph_runner][INFO] - Test, Round 480 : loss => 6.653242552471161,  accuracy: 0.3067
[2025-09-11 23:41:33,788][flp2p.graph_runner][INFO] - Train, Round 481 : loss => 0.0011408512212801724,  accuracy: 1.0, gradient_norm : 0.05761069212552515
[2025-09-11 23:41:47,109][flp2p.graph_runner][INFO] - Test, Round 481 : loss => 6.653750601673126,  accuracy: 0.3066
[2025-09-11 23:42:22,006][flp2p.graph_runner][INFO] - Train, Round 482 : loss => 0.0011386465098621556,  accuracy: 1.0, gradient_norm : 0.05761048176230381
[2025-09-11 23:42:35,185][flp2p.graph_runner][INFO] - Test, Round 482 : loss => 6.654720413804054,  accuracy: 0.3067
[2025-09-11 23:43:09,774][flp2p.graph_runner][INFO] - Train, Round 483 : loss => 0.0011363375391374578,  accuracy: 1.0, gradient_norm : 0.0583899851214493
[2025-09-11 23:43:27,764][flp2p.graph_runner][INFO] - Test, Round 483 : loss => 6.6555627621650695,  accuracy: 0.3067
[2025-09-11 23:44:02,495][flp2p.graph_runner][INFO] - Train, Round 484 : loss => 0.001134404892703363,  accuracy: 1.0, gradient_norm : 0.0572373795374205
[2025-09-11 23:44:24,634][flp2p.graph_runner][INFO] - Test, Round 484 : loss => 6.656133499455452,  accuracy: 0.3065
[2025-09-11 23:44:59,339][flp2p.graph_runner][INFO] - Train, Round 485 : loss => 0.0011327901577897136,  accuracy: 1.0, gradient_norm : 0.06098934660121934
[2025-09-11 23:45:18,117][flp2p.graph_runner][INFO] - Test, Round 485 : loss => 6.656917845702171,  accuracy: 0.3064
[2025-09-11 23:45:53,041][flp2p.graph_runner][INFO] - Train, Round 486 : loss => 0.0011309691622833878,  accuracy: 1.0, gradient_norm : 0.056418890700246534
[2025-09-11 23:46:09,392][flp2p.graph_runner][INFO] - Test, Round 486 : loss => 6.657616280531883,  accuracy: 0.3064
[2025-09-11 23:46:44,284][flp2p.graph_runner][INFO] - Train, Round 487 : loss => 0.0011286296166993755,  accuracy: 1.0, gradient_norm : 0.05436215924167124
[2025-09-11 23:46:58,098][flp2p.graph_runner][INFO] - Test, Round 487 : loss => 6.658379303669929,  accuracy: 0.3067
[2025-09-11 23:47:32,748][flp2p.graph_runner][INFO] - Train, Round 488 : loss => 0.0011268354209702624,  accuracy: 1.0, gradient_norm : 0.056159963761121236
[2025-09-11 23:47:46,811][flp2p.graph_runner][INFO] - Test, Round 488 : loss => 6.658809420251846,  accuracy: 0.3069
[2025-09-11 23:48:21,586][flp2p.graph_runner][INFO] - Train, Round 489 : loss => 0.0011246301334419209,  accuracy: 1.0, gradient_norm : 0.057220192730286897
[2025-09-11 23:48:35,541][flp2p.graph_runner][INFO] - Test, Round 489 : loss => 6.659767358660698,  accuracy: 0.3067
[2025-09-11 23:49:10,282][flp2p.graph_runner][INFO] - Train, Round 490 : loss => 0.0011231712481821886,  accuracy: 1.0, gradient_norm : 0.0565279593916246
[2025-09-11 23:49:24,483][flp2p.graph_runner][INFO] - Test, Round 490 : loss => 6.660359095788002,  accuracy: 0.3068
[2025-09-11 23:49:59,307][flp2p.graph_runner][INFO] - Train, Round 491 : loss => 0.0011211078728714102,  accuracy: 1.0, gradient_norm : 0.05543011055282629
[2025-09-11 23:50:13,314][flp2p.graph_runner][INFO] - Test, Round 491 : loss => 6.661308961081505,  accuracy: 0.3066
[2025-09-11 23:50:48,210][flp2p.graph_runner][INFO] - Train, Round 492 : loss => 0.0011194649148334674,  accuracy: 1.0, gradient_norm : 0.05611730829443597
[2025-09-11 23:51:01,995][flp2p.graph_runner][INFO] - Test, Round 492 : loss => 6.662211435389518,  accuracy: 0.3066
[2025-09-11 23:51:36,884][flp2p.graph_runner][INFO] - Train, Round 493 : loss => 0.00111749546090626,  accuracy: 1.0, gradient_norm : 0.05532589961282037
[2025-09-11 23:51:50,997][flp2p.graph_runner][INFO] - Test, Round 493 : loss => 6.66288009827137,  accuracy: 0.3063
[2025-09-11 23:52:25,776][flp2p.graph_runner][INFO] - Train, Round 494 : loss => 0.0011156103897277108,  accuracy: 1.0, gradient_norm : 0.05503267132067637
[2025-09-11 23:52:39,528][flp2p.graph_runner][INFO] - Test, Round 494 : loss => 6.663049798440933,  accuracy: 0.3067
[2025-09-11 23:53:14,330][flp2p.graph_runner][INFO] - Train, Round 495 : loss => 0.0011140088667404295,  accuracy: 1.0, gradient_norm : 0.054981637046961705
[2025-09-11 23:53:28,088][flp2p.graph_runner][INFO] - Test, Round 495 : loss => 6.663939484333992,  accuracy: 0.3065
[2025-09-11 23:54:02,944][flp2p.graph_runner][INFO] - Train, Round 496 : loss => 0.0011115975326780843,  accuracy: 1.0, gradient_norm : 0.05409071314645513
[2025-09-11 23:54:16,993][flp2p.graph_runner][INFO] - Test, Round 496 : loss => 6.6646345888376235,  accuracy: 0.3067
[2025-09-11 23:54:51,841][flp2p.graph_runner][INFO] - Train, Round 497 : loss => 0.0011104050295155806,  accuracy: 1.0, gradient_norm : 0.05580173645038851
[2025-09-11 23:55:05,627][flp2p.graph_runner][INFO] - Test, Round 497 : loss => 6.665099288892746,  accuracy: 0.3068
[2025-09-11 23:55:39,109][flp2p.graph_runner][INFO] - Train, Round 498 : loss => 0.0011083111531479516,  accuracy: 1.0, gradient_norm : 0.054859450028841156
[2025-09-11 23:55:50,613][flp2p.graph_runner][INFO] - Test, Round 498 : loss => 6.666209612298012,  accuracy: 0.3066
[2025-09-11 23:56:21,857][flp2p.graph_runner][INFO] - Train, Round 499 : loss => 0.0011064013409243976,  accuracy: 1.0, gradient_norm : 0.05602416123166801
[2025-09-11 23:56:33,341][flp2p.graph_runner][INFO] - Test, Round 499 : loss => 6.666525838065147,  accuracy: 0.3065
[2025-09-11 23:56:33,345][__main__][INFO] - Train, Round 001: loss=2.3049, accuracy=0.1040, gradient_norm=0.1727, 
[2025-09-11 23:56:33,345][__main__][INFO] - Train, Round 002: loss=2.3039, accuracy=0.1089, gradient_norm=0.1724, 
[2025-09-11 23:56:33,345][__main__][INFO] - Train, Round 003: loss=2.3028, accuracy=0.1132, gradient_norm=0.1749, 
[2025-09-11 23:56:33,345][__main__][INFO] - Train, Round 004: loss=2.3019, accuracy=0.1163, gradient_norm=0.1735, 
[2025-09-11 23:56:33,345][__main__][INFO] - Train, Round 005: loss=2.3009, accuracy=0.1196, gradient_norm=0.1720, 
[2025-09-11 23:56:33,345][__main__][INFO] - Train, Round 006: loss=2.3000, accuracy=0.1223, gradient_norm=0.1745, 
[2025-09-11 23:56:33,345][__main__][INFO] - Train, Round 007: loss=2.2991, accuracy=0.1232, gradient_norm=0.1799, 
[2025-09-11 23:56:33,345][__main__][INFO] - Train, Round 008: loss=2.2982, accuracy=0.1241, gradient_norm=0.1763, 
[2025-09-11 23:56:33,345][__main__][INFO] - Train, Round 009: loss=2.2973, accuracy=0.1256, gradient_norm=0.1819, 
[2025-09-11 23:56:33,345][__main__][INFO] - Train, Round 010: loss=2.2964, accuracy=0.1280, gradient_norm=0.1839, 
[2025-09-11 23:56:33,345][__main__][INFO] - Train, Round 011: loss=2.2955, accuracy=0.1298, gradient_norm=0.1844, 
[2025-09-11 23:56:33,345][__main__][INFO] - Train, Round 012: loss=2.2946, accuracy=0.1309, gradient_norm=0.1842, 
[2025-09-11 23:56:33,345][__main__][INFO] - Train, Round 013: loss=2.2936, accuracy=0.1325, gradient_norm=0.1889, 
[2025-09-11 23:56:33,345][__main__][INFO] - Train, Round 014: loss=2.2926, accuracy=0.1336, gradient_norm=0.1902, 
[2025-09-11 23:56:33,345][__main__][INFO] - Train, Round 015: loss=2.2915, accuracy=0.1358, gradient_norm=0.2035, 
[2025-09-11 23:56:33,345][__main__][INFO] - Train, Round 016: loss=2.2904, accuracy=0.1382, gradient_norm=0.1957, 
[2025-09-11 23:56:33,345][__main__][INFO] - Train, Round 017: loss=2.2891, accuracy=0.1404, gradient_norm=0.2043, 
[2025-09-11 23:56:33,345][__main__][INFO] - Train, Round 018: loss=2.2877, accuracy=0.1427, gradient_norm=0.2184, 
[2025-09-11 23:56:33,345][__main__][INFO] - Train, Round 019: loss=2.2861, accuracy=0.1452, gradient_norm=0.2193, 
[2025-09-11 23:56:33,345][__main__][INFO] - Train, Round 020: loss=2.2844, accuracy=0.1477, gradient_norm=0.2320, 
[2025-09-11 23:56:33,345][__main__][INFO] - Train, Round 021: loss=2.2824, accuracy=0.1501, gradient_norm=0.2429, 
[2025-09-11 23:56:33,345][__main__][INFO] - Train, Round 022: loss=2.2802, accuracy=0.1532, gradient_norm=0.2450, 
[2025-09-11 23:56:33,345][__main__][INFO] - Train, Round 023: loss=2.2777, accuracy=0.1562, gradient_norm=0.2654, 
[2025-09-11 23:56:33,345][__main__][INFO] - Train, Round 024: loss=2.2747, accuracy=0.1593, gradient_norm=0.2850, 
[2025-09-11 23:56:33,345][__main__][INFO] - Train, Round 025: loss=2.2713, accuracy=0.1631, gradient_norm=0.2973, 
[2025-09-11 23:56:33,345][__main__][INFO] - Train, Round 026: loss=2.2673, accuracy=0.1661, gradient_norm=0.3268, 
[2025-09-11 23:56:33,345][__main__][INFO] - Train, Round 027: loss=2.2625, accuracy=0.1688, gradient_norm=0.3682, 
[2025-09-11 23:56:33,345][__main__][INFO] - Train, Round 028: loss=2.2570, accuracy=0.1729, gradient_norm=0.3853, 
[2025-09-11 23:56:33,345][__main__][INFO] - Train, Round 029: loss=2.2505, accuracy=0.1763, gradient_norm=0.4154, 
[2025-09-11 23:56:33,345][__main__][INFO] - Train, Round 030: loss=2.2429, accuracy=0.1796, gradient_norm=0.4655, 
[2025-09-11 23:56:33,345][__main__][INFO] - Train, Round 031: loss=2.2341, accuracy=0.1837, gradient_norm=0.5274, 
[2025-09-11 23:56:33,345][__main__][INFO] - Train, Round 032: loss=2.2240, accuracy=0.1877, gradient_norm=0.5908, 
[2025-09-11 23:56:33,345][__main__][INFO] - Train, Round 033: loss=2.2125, accuracy=0.1926, gradient_norm=0.6046, 
[2025-09-11 23:56:33,345][__main__][INFO] - Train, Round 034: loss=2.1998, accuracy=0.1986, gradient_norm=0.7172, 
[2025-09-11 23:56:33,345][__main__][INFO] - Train, Round 035: loss=2.1854, accuracy=0.2040, gradient_norm=0.8022, 
[2025-09-11 23:56:33,345][__main__][INFO] - Train, Round 036: loss=2.1697, accuracy=0.2118, gradient_norm=0.8360, 
[2025-09-11 23:56:33,345][__main__][INFO] - Train, Round 037: loss=2.1529, accuracy=0.2189, gradient_norm=0.9634, 
[2025-09-11 23:56:33,345][__main__][INFO] - Train, Round 038: loss=2.1353, accuracy=0.2268, gradient_norm=1.0697, 
[2025-09-11 23:56:33,345][__main__][INFO] - Train, Round 039: loss=2.1168, accuracy=0.2353, gradient_norm=1.1324, 
[2025-09-11 23:56:33,346][__main__][INFO] - Train, Round 040: loss=2.0982, accuracy=0.2435, gradient_norm=1.2767, 
[2025-09-11 23:56:33,346][__main__][INFO] - Train, Round 041: loss=2.0788, accuracy=0.2535, gradient_norm=1.3997, 
[2025-09-11 23:56:33,346][__main__][INFO] - Train, Round 042: loss=2.0594, accuracy=0.2643, gradient_norm=1.4444, 
[2025-09-11 23:56:33,346][__main__][INFO] - Train, Round 043: loss=2.0397, accuracy=0.2735, gradient_norm=1.6437, 
[2025-09-11 23:56:33,346][__main__][INFO] - Train, Round 044: loss=2.0206, accuracy=0.2821, gradient_norm=1.7831, 
[2025-09-11 23:56:33,346][__main__][INFO] - Train, Round 045: loss=2.0015, accuracy=0.2895, gradient_norm=1.8737, 
[2025-09-11 23:56:33,346][__main__][INFO] - Train, Round 046: loss=1.9825, accuracy=0.2983, gradient_norm=2.0224, 
[2025-09-11 23:56:33,346][__main__][INFO] - Train, Round 047: loss=1.9638, accuracy=0.3048, gradient_norm=2.0749, 
[2025-09-11 23:56:33,346][__main__][INFO] - Train, Round 048: loss=1.9457, accuracy=0.3129, gradient_norm=2.2372, 
[2025-09-11 23:56:33,346][__main__][INFO] - Train, Round 049: loss=1.9275, accuracy=0.3196, gradient_norm=2.3527, 
[2025-09-11 23:56:33,346][__main__][INFO] - Train, Round 050: loss=1.9102, accuracy=0.3258, gradient_norm=2.4945, 
[2025-09-11 23:56:33,346][__main__][INFO] - Train, Round 051: loss=1.8935, accuracy=0.3313, gradient_norm=2.6206, 
[2025-09-11 23:56:33,346][__main__][INFO] - Train, Round 052: loss=1.8755, accuracy=0.3381, gradient_norm=2.7550, 
[2025-09-11 23:56:33,346][__main__][INFO] - Train, Round 053: loss=1.8600, accuracy=0.3432, gradient_norm=3.0231, 
[2025-09-11 23:56:33,346][__main__][INFO] - Train, Round 054: loss=1.8436, accuracy=0.3488, gradient_norm=3.0473, 
[2025-09-11 23:56:33,346][__main__][INFO] - Train, Round 055: loss=1.8281, accuracy=0.3530, gradient_norm=3.2719, 
[2025-09-11 23:56:33,346][__main__][INFO] - Train, Round 056: loss=1.8120, accuracy=0.3590, gradient_norm=3.2932, 
[2025-09-11 23:56:33,346][__main__][INFO] - Train, Round 057: loss=1.7965, accuracy=0.3646, gradient_norm=3.4966, 
[2025-09-11 23:56:33,346][__main__][INFO] - Train, Round 058: loss=1.7831, accuracy=0.3696, gradient_norm=3.7566, 
[2025-09-11 23:56:33,346][__main__][INFO] - Train, Round 059: loss=1.7664, accuracy=0.3748, gradient_norm=3.5799, 
[2025-09-11 23:56:33,346][__main__][INFO] - Train, Round 060: loss=1.7518, accuracy=0.3803, gradient_norm=3.7787, 
[2025-09-11 23:56:33,346][__main__][INFO] - Train, Round 061: loss=1.7331, accuracy=0.3885, gradient_norm=4.1850, 
[2025-09-11 23:56:33,346][__main__][INFO] - Train, Round 062: loss=1.7216, accuracy=0.3912, gradient_norm=4.1614, 
[2025-09-11 23:56:33,346][__main__][INFO] - Train, Round 063: loss=1.7050, accuracy=0.3977, gradient_norm=4.2120, 
[2025-09-11 23:56:33,346][__main__][INFO] - Train, Round 064: loss=1.6901, accuracy=0.4036, gradient_norm=4.3594, 
[2025-09-11 23:56:33,346][__main__][INFO] - Train, Round 065: loss=1.6683, accuracy=0.4125, gradient_norm=4.2900, 
[2025-09-11 23:56:33,346][__main__][INFO] - Train, Round 066: loss=1.6530, accuracy=0.4165, gradient_norm=4.5890, 
[2025-09-11 23:56:33,346][__main__][INFO] - Train, Round 067: loss=1.6355, accuracy=0.4245, gradient_norm=4.5344, 
[2025-09-11 23:56:33,346][__main__][INFO] - Train, Round 068: loss=1.6204, accuracy=0.4303, gradient_norm=4.7638, 
[2025-09-11 23:56:33,346][__main__][INFO] - Train, Round 069: loss=1.6038, accuracy=0.4355, gradient_norm=4.8016, 
[2025-09-11 23:56:33,346][__main__][INFO] - Train, Round 070: loss=1.5792, accuracy=0.4444, gradient_norm=4.9610, 
[2025-09-11 23:56:33,346][__main__][INFO] - Train, Round 071: loss=1.5626, accuracy=0.4512, gradient_norm=5.0829, 
[2025-09-11 23:56:33,346][__main__][INFO] - Train, Round 072: loss=1.5422, accuracy=0.4593, gradient_norm=5.0312, 
[2025-09-11 23:56:33,346][__main__][INFO] - Train, Round 073: loss=1.5242, accuracy=0.4656, gradient_norm=5.1741, 
[2025-09-11 23:56:33,346][__main__][INFO] - Train, Round 074: loss=1.5022, accuracy=0.4737, gradient_norm=5.3875, 
[2025-09-11 23:56:33,346][__main__][INFO] - Train, Round 075: loss=1.4782, accuracy=0.4818, gradient_norm=5.6850, 
[2025-09-11 23:56:33,346][__main__][INFO] - Train, Round 076: loss=1.4603, accuracy=0.4875, gradient_norm=5.6201, 
[2025-09-11 23:56:33,346][__main__][INFO] - Train, Round 077: loss=1.4337, accuracy=0.4962, gradient_norm=5.4988, 
[2025-09-11 23:56:33,346][__main__][INFO] - Train, Round 078: loss=1.4163, accuracy=0.5040, gradient_norm=5.5113, 
[2025-09-11 23:56:33,346][__main__][INFO] - Train, Round 079: loss=1.3893, accuracy=0.5141, gradient_norm=5.9800, 
[2025-09-11 23:56:33,346][__main__][INFO] - Train, Round 080: loss=1.3652, accuracy=0.5239, gradient_norm=6.0119, 
[2025-09-11 23:56:33,346][__main__][INFO] - Train, Round 081: loss=1.3428, accuracy=0.5316, gradient_norm=6.0946, 
[2025-09-11 23:56:33,346][__main__][INFO] - Train, Round 082: loss=1.3172, accuracy=0.5413, gradient_norm=6.4899, 
[2025-09-11 23:56:33,346][__main__][INFO] - Train, Round 083: loss=1.2952, accuracy=0.5505, gradient_norm=6.4872, 
[2025-09-11 23:56:33,346][__main__][INFO] - Train, Round 084: loss=1.2640, accuracy=0.5614, gradient_norm=6.6975, 
[2025-09-11 23:56:33,346][__main__][INFO] - Train, Round 085: loss=1.2415, accuracy=0.5700, gradient_norm=6.8478, 
[2025-09-11 23:56:33,346][__main__][INFO] - Train, Round 086: loss=1.2123, accuracy=0.5814, gradient_norm=6.6593, 
[2025-09-11 23:56:33,346][__main__][INFO] - Train, Round 087: loss=1.1908, accuracy=0.5885, gradient_norm=7.0772, 
[2025-09-11 23:56:33,346][__main__][INFO] - Train, Round 088: loss=1.1578, accuracy=0.6023, gradient_norm=6.9344, 
[2025-09-11 23:56:33,346][__main__][INFO] - Train, Round 089: loss=1.1300, accuracy=0.6122, gradient_norm=6.9910, 
[2025-09-11 23:56:33,346][__main__][INFO] - Train, Round 090: loss=1.1060, accuracy=0.6214, gradient_norm=7.2558, 
[2025-09-11 23:56:33,346][__main__][INFO] - Train, Round 091: loss=1.0779, accuracy=0.6321, gradient_norm=7.6819, 
[2025-09-11 23:56:33,346][__main__][INFO] - Train, Round 092: loss=1.0465, accuracy=0.6425, gradient_norm=7.5295, 
[2025-09-11 23:56:33,346][__main__][INFO] - Train, Round 093: loss=1.0108, accuracy=0.6568, gradient_norm=6.9239, 
[2025-09-11 23:56:33,346][__main__][INFO] - Train, Round 094: loss=0.9893, accuracy=0.6652, gradient_norm=7.5508, 
[2025-09-11 23:56:33,346][__main__][INFO] - Train, Round 095: loss=0.9604, accuracy=0.6771, gradient_norm=7.5085, 
[2025-09-11 23:56:33,346][__main__][INFO] - Train, Round 096: loss=0.9269, accuracy=0.6890, gradient_norm=7.9083, 
[2025-09-11 23:56:33,346][__main__][INFO] - Train, Round 097: loss=0.8946, accuracy=0.7000, gradient_norm=7.9373, 
[2025-09-11 23:56:33,346][__main__][INFO] - Train, Round 098: loss=0.8662, accuracy=0.7129, gradient_norm=8.0409, 
[2025-09-11 23:56:33,346][__main__][INFO] - Train, Round 099: loss=0.8321, accuracy=0.7248, gradient_norm=7.8863, 
[2025-09-11 23:56:33,346][__main__][INFO] - Train, Round 100: loss=0.8008, accuracy=0.7393, gradient_norm=7.8806, 
[2025-09-11 23:56:33,346][__main__][INFO] - Train, Round 101: loss=0.7751, accuracy=0.7469, gradient_norm=8.2952, 
[2025-09-11 23:56:33,346][__main__][INFO] - Train, Round 102: loss=0.7424, accuracy=0.7598, gradient_norm=8.2049, 
[2025-09-11 23:56:33,346][__main__][INFO] - Train, Round 103: loss=0.7082, accuracy=0.7734, gradient_norm=7.8093, 
[2025-09-11 23:56:33,346][__main__][INFO] - Train, Round 104: loss=0.6736, accuracy=0.7867, gradient_norm=7.7985, 
[2025-09-11 23:56:33,346][__main__][INFO] - Train, Round 105: loss=0.6484, accuracy=0.7961, gradient_norm=7.7280, 
[2025-09-11 23:56:33,346][__main__][INFO] - Train, Round 106: loss=0.6119, accuracy=0.8097, gradient_norm=7.9080, 
[2025-09-11 23:56:33,346][__main__][INFO] - Train, Round 107: loss=0.5800, accuracy=0.8220, gradient_norm=7.9067, 
[2025-09-11 23:56:33,346][__main__][INFO] - Train, Round 108: loss=0.5594, accuracy=0.8305, gradient_norm=8.0177, 
[2025-09-11 23:56:33,346][__main__][INFO] - Train, Round 109: loss=0.5265, accuracy=0.8442, gradient_norm=6.9252, 
[2025-09-11 23:56:33,346][__main__][INFO] - Train, Round 110: loss=0.4945, accuracy=0.8562, gradient_norm=7.4651, 
[2025-09-11 23:56:33,346][__main__][INFO] - Train, Round 111: loss=0.4649, accuracy=0.8701, gradient_norm=7.4113, 
[2025-09-11 23:56:33,346][__main__][INFO] - Train, Round 112: loss=0.4394, accuracy=0.8797, gradient_norm=6.4703, 
[2025-09-11 23:56:33,346][__main__][INFO] - Train, Round 113: loss=0.4099, accuracy=0.8906, gradient_norm=6.9541, 
[2025-09-11 23:56:33,346][__main__][INFO] - Train, Round 114: loss=0.3727, accuracy=0.9034, gradient_norm=6.4819, 
[2025-09-11 23:56:33,346][__main__][INFO] - Train, Round 115: loss=0.3602, accuracy=0.9093, gradient_norm=6.4024, 
[2025-09-11 23:56:33,346][__main__][INFO] - Train, Round 116: loss=0.3291, accuracy=0.9194, gradient_norm=6.1725, 
[2025-09-11 23:56:33,346][__main__][INFO] - Train, Round 117: loss=0.3192, accuracy=0.9243, gradient_norm=5.7479, 
[2025-09-11 23:56:33,346][__main__][INFO] - Train, Round 118: loss=0.2825, accuracy=0.9359, gradient_norm=5.8675, 
[2025-09-11 23:56:33,346][__main__][INFO] - Train, Round 119: loss=0.2754, accuracy=0.9395, gradient_norm=5.6869, 
[2025-09-11 23:56:33,346][__main__][INFO] - Train, Round 120: loss=0.2361, accuracy=0.9540, gradient_norm=4.9355, 
[2025-09-11 23:56:33,346][__main__][INFO] - Train, Round 121: loss=0.2385, accuracy=0.9522, gradient_norm=5.0359, 
[2025-09-11 23:56:33,347][__main__][INFO] - Train, Round 122: loss=0.2003, accuracy=0.9640, gradient_norm=4.5105, 
[2025-09-11 23:56:33,347][__main__][INFO] - Train, Round 123: loss=0.1809, accuracy=0.9696, gradient_norm=3.8516, 
[2025-09-11 23:56:33,347][__main__][INFO] - Train, Round 124: loss=0.1573, accuracy=0.9766, gradient_norm=4.1947, 
[2025-09-11 23:56:33,347][__main__][INFO] - Train, Round 125: loss=0.1615, accuracy=0.9743, gradient_norm=3.3354, 
[2025-09-11 23:56:33,347][__main__][INFO] - Train, Round 126: loss=0.1364, accuracy=0.9819, gradient_norm=3.5114, 
[2025-09-11 23:56:33,347][__main__][INFO] - Train, Round 127: loss=0.1211, accuracy=0.9846, gradient_norm=3.0632, 
[2025-09-11 23:56:33,347][__main__][INFO] - Train, Round 128: loss=0.1045, accuracy=0.9884, gradient_norm=3.0165, 
[2025-09-11 23:56:33,347][__main__][INFO] - Train, Round 129: loss=0.1016, accuracy=0.9882, gradient_norm=2.5739, 
[2025-09-11 23:56:33,347][__main__][INFO] - Train, Round 130: loss=0.0857, accuracy=0.9927, gradient_norm=2.6189, 
[2025-09-11 23:56:33,347][__main__][INFO] - Train, Round 131: loss=0.0920, accuracy=0.9909, gradient_norm=2.7887, 
[2025-09-11 23:56:33,347][__main__][INFO] - Train, Round 132: loss=0.0889, accuracy=0.9912, gradient_norm=2.0001, 
[2025-09-11 23:56:33,347][__main__][INFO] - Train, Round 133: loss=0.0677, accuracy=0.9949, gradient_norm=2.0817, 
[2025-09-11 23:56:33,347][__main__][INFO] - Train, Round 134: loss=0.0626, accuracy=0.9958, gradient_norm=1.9400, 
[2025-09-11 23:56:33,347][__main__][INFO] - Train, Round 135: loss=0.0572, accuracy=0.9964, gradient_norm=1.6266, 
[2025-09-11 23:56:33,347][__main__][INFO] - Train, Round 136: loss=0.0468, accuracy=0.9985, gradient_norm=1.4031, 
[2025-09-11 23:56:33,347][__main__][INFO] - Train, Round 137: loss=0.0475, accuracy=0.9976, gradient_norm=1.5855, 
[2025-09-11 23:56:33,347][__main__][INFO] - Train, Round 138: loss=0.0490, accuracy=0.9969, gradient_norm=1.5842, 
[2025-09-11 23:56:33,347][__main__][INFO] - Train, Round 139: loss=0.0559, accuracy=0.9951, gradient_norm=1.3180, 
[2025-09-11 23:56:33,347][__main__][INFO] - Train, Round 140: loss=0.0349, accuracy=0.9995, gradient_norm=1.1775, 
[2025-09-11 23:56:33,347][__main__][INFO] - Train, Round 141: loss=0.0319, accuracy=0.9996, gradient_norm=1.0732, 
[2025-09-11 23:56:33,347][__main__][INFO] - Train, Round 142: loss=0.0297, accuracy=0.9997, gradient_norm=0.9931, 
[2025-09-11 23:56:33,347][__main__][INFO] - Train, Round 143: loss=0.0278, accuracy=0.9998, gradient_norm=0.9542, 
[2025-09-11 23:56:33,347][__main__][INFO] - Train, Round 144: loss=0.0311, accuracy=0.9989, gradient_norm=0.9282, 
[2025-09-11 23:56:33,347][__main__][INFO] - Train, Round 145: loss=0.0255, accuracy=0.9997, gradient_norm=0.8709, 
[2025-09-11 23:56:33,347][__main__][INFO] - Train, Round 146: loss=0.0234, accuracy=0.9999, gradient_norm=0.7727, 
[2025-09-11 23:56:33,347][__main__][INFO] - Train, Round 147: loss=0.0222, accuracy=0.9999, gradient_norm=0.7772, 
[2025-09-11 23:56:33,347][__main__][INFO] - Train, Round 148: loss=0.0211, accuracy=1.0000, gradient_norm=0.7865, 
[2025-09-11 23:56:33,347][__main__][INFO] - Train, Round 149: loss=0.0355, accuracy=0.9975, gradient_norm=0.7892, 
[2025-09-11 23:56:33,347][__main__][INFO] - Train, Round 150: loss=0.0222, accuracy=0.9993, gradient_norm=0.7114, 
[2025-09-11 23:56:33,347][__main__][INFO] - Train, Round 151: loss=0.0195, accuracy=0.9998, gradient_norm=0.6661, 
[2025-09-11 23:56:33,347][__main__][INFO] - Train, Round 152: loss=0.0182, accuracy=0.9999, gradient_norm=0.6596, 
[2025-09-11 23:56:33,347][__main__][INFO] - Train, Round 153: loss=0.0172, accuracy=1.0000, gradient_norm=0.6800, 
[2025-09-11 23:56:33,347][__main__][INFO] - Train, Round 154: loss=0.0164, accuracy=1.0000, gradient_norm=0.6197, 
[2025-09-11 23:56:33,347][__main__][INFO] - Train, Round 155: loss=0.0333, accuracy=0.9984, gradient_norm=0.8101, 
[2025-09-11 23:56:33,347][__main__][INFO] - Train, Round 156: loss=0.0231, accuracy=0.9976, gradient_norm=0.5901, 
[2025-09-11 23:56:33,347][__main__][INFO] - Train, Round 157: loss=0.0174, accuracy=0.9995, gradient_norm=0.5552, 
[2025-09-11 23:56:33,347][__main__][INFO] - Train, Round 158: loss=0.0154, accuracy=0.9998, gradient_norm=0.5253, 
[2025-09-11 23:56:33,347][__main__][INFO] - Train, Round 159: loss=0.0143, accuracy=1.0000, gradient_norm=0.5394, 
[2025-09-11 23:56:33,347][__main__][INFO] - Train, Round 160: loss=0.0136, accuracy=1.0000, gradient_norm=0.4841, 
[2025-09-11 23:56:33,347][__main__][INFO] - Train, Round 161: loss=0.0129, accuracy=1.0000, gradient_norm=0.4805, 
[2025-09-11 23:56:33,347][__main__][INFO] - Train, Round 162: loss=0.0126, accuracy=1.0000, gradient_norm=0.4675, 
[2025-09-11 23:56:33,347][__main__][INFO] - Train, Round 163: loss=0.0120, accuracy=1.0000, gradient_norm=0.4646, 
[2025-09-11 23:56:33,347][__main__][INFO] - Train, Round 164: loss=0.0116, accuracy=1.0000, gradient_norm=0.4522, 
[2025-09-11 23:56:33,347][__main__][INFO] - Train, Round 165: loss=0.0112, accuracy=1.0000, gradient_norm=0.4296, 
[2025-09-11 23:56:33,347][__main__][INFO] - Train, Round 166: loss=0.0108, accuracy=1.0000, gradient_norm=0.4236, 
[2025-09-11 23:56:33,347][__main__][INFO] - Train, Round 167: loss=0.0105, accuracy=1.0000, gradient_norm=0.4223, 
[2025-09-11 23:56:33,347][__main__][INFO] - Train, Round 168: loss=0.0102, accuracy=1.0000, gradient_norm=0.3881, 
[2025-09-11 23:56:33,347][__main__][INFO] - Train, Round 169: loss=0.0099, accuracy=1.0000, gradient_norm=0.3792, 
[2025-09-11 23:56:33,347][__main__][INFO] - Train, Round 170: loss=0.0096, accuracy=1.0000, gradient_norm=0.3924, 
[2025-09-11 23:56:33,347][__main__][INFO] - Train, Round 171: loss=0.0093, accuracy=1.0000, gradient_norm=0.3608, 
[2025-09-11 23:56:33,347][__main__][INFO] - Train, Round 172: loss=0.0091, accuracy=1.0000, gradient_norm=0.3614, 
[2025-09-11 23:56:33,347][__main__][INFO] - Train, Round 173: loss=0.0089, accuracy=1.0000, gradient_norm=0.3655, 
[2025-09-11 23:56:33,347][__main__][INFO] - Train, Round 174: loss=0.0086, accuracy=1.0000, gradient_norm=0.3329, 
[2025-09-11 23:56:33,347][__main__][INFO] - Train, Round 175: loss=0.0084, accuracy=1.0000, gradient_norm=0.3221, 
[2025-09-11 23:56:33,347][__main__][INFO] - Train, Round 176: loss=0.0082, accuracy=1.0000, gradient_norm=0.3288, 
[2025-09-11 23:56:33,347][__main__][INFO] - Train, Round 177: loss=0.0080, accuracy=1.0000, gradient_norm=0.3306, 
[2025-09-11 23:56:33,347][__main__][INFO] - Train, Round 178: loss=0.0078, accuracy=1.0000, gradient_norm=0.3152, 
[2025-09-11 23:56:33,347][__main__][INFO] - Train, Round 179: loss=0.0077, accuracy=1.0000, gradient_norm=0.3199, 
[2025-09-11 23:56:33,347][__main__][INFO] - Train, Round 180: loss=0.0075, accuracy=1.0000, gradient_norm=0.3138, 
[2025-09-11 23:56:33,347][__main__][INFO] - Train, Round 181: loss=0.0073, accuracy=1.0000, gradient_norm=0.3005, 
[2025-09-11 23:56:33,347][__main__][INFO] - Train, Round 182: loss=0.0072, accuracy=1.0000, gradient_norm=0.2915, 
[2025-09-11 23:56:33,347][__main__][INFO] - Train, Round 183: loss=0.0070, accuracy=1.0000, gradient_norm=0.2925, 
[2025-09-11 23:56:33,347][__main__][INFO] - Train, Round 184: loss=0.0069, accuracy=1.0000, gradient_norm=0.2935, 
[2025-09-11 23:56:33,347][__main__][INFO] - Train, Round 185: loss=0.0067, accuracy=1.0000, gradient_norm=0.2824, 
[2025-09-11 23:56:33,347][__main__][INFO] - Train, Round 186: loss=0.0066, accuracy=1.0000, gradient_norm=0.2686, 
[2025-09-11 23:56:33,347][__main__][INFO] - Train, Round 187: loss=0.0065, accuracy=1.0000, gradient_norm=0.2769, 
[2025-09-11 23:56:33,347][__main__][INFO] - Train, Round 188: loss=0.0064, accuracy=1.0000, gradient_norm=0.2681, 
[2025-09-11 23:56:33,347][__main__][INFO] - Train, Round 189: loss=0.0062, accuracy=1.0000, gradient_norm=0.2581, 
[2025-09-11 23:56:33,347][__main__][INFO] - Train, Round 190: loss=0.0061, accuracy=1.0000, gradient_norm=0.2651, 
[2025-09-11 23:56:33,347][__main__][INFO] - Train, Round 191: loss=0.0060, accuracy=1.0000, gradient_norm=0.2565, 
[2025-09-11 23:56:33,347][__main__][INFO] - Train, Round 192: loss=0.0059, accuracy=1.0000, gradient_norm=0.2566, 
[2025-09-11 23:56:33,347][__main__][INFO] - Train, Round 193: loss=0.0058, accuracy=1.0000, gradient_norm=0.2481, 
[2025-09-11 23:56:33,347][__main__][INFO] - Train, Round 194: loss=0.0057, accuracy=1.0000, gradient_norm=0.2352, 
[2025-09-11 23:56:33,347][__main__][INFO] - Train, Round 195: loss=0.0056, accuracy=1.0000, gradient_norm=0.2448, 
[2025-09-11 23:56:33,347][__main__][INFO] - Train, Round 196: loss=0.0055, accuracy=1.0000, gradient_norm=0.2321, 
[2025-09-11 23:56:33,347][__main__][INFO] - Train, Round 197: loss=0.0054, accuracy=1.0000, gradient_norm=0.2334, 
[2025-09-11 23:56:33,347][__main__][INFO] - Train, Round 198: loss=0.0053, accuracy=1.0000, gradient_norm=0.2232, 
[2025-09-11 23:56:33,347][__main__][INFO] - Train, Round 199: loss=0.0052, accuracy=1.0000, gradient_norm=0.2195, 
[2025-09-11 23:56:33,347][__main__][INFO] - Train, Round 200: loss=0.0052, accuracy=1.0000, gradient_norm=0.2380, 
[2025-09-11 23:56:33,347][__main__][INFO] - Train, Round 201: loss=0.0051, accuracy=1.0000, gradient_norm=0.2182, 
[2025-09-11 23:56:33,347][__main__][INFO] - Train, Round 202: loss=0.0050, accuracy=1.0000, gradient_norm=0.2220, 
[2025-09-11 23:56:33,347][__main__][INFO] - Train, Round 203: loss=0.0049, accuracy=1.0000, gradient_norm=0.2143, 
[2025-09-11 23:56:33,347][__main__][INFO] - Train, Round 204: loss=0.0048, accuracy=1.0000, gradient_norm=0.2071, 
[2025-09-11 23:56:33,348][__main__][INFO] - Train, Round 205: loss=0.0048, accuracy=1.0000, gradient_norm=0.2094, 
[2025-09-11 23:56:33,348][__main__][INFO] - Train, Round 206: loss=0.0047, accuracy=1.0000, gradient_norm=0.2034, 
[2025-09-11 23:56:33,348][__main__][INFO] - Train, Round 207: loss=0.0046, accuracy=1.0000, gradient_norm=0.1906, 
[2025-09-11 23:56:33,348][__main__][INFO] - Train, Round 208: loss=0.0046, accuracy=1.0000, gradient_norm=0.2010, 
[2025-09-11 23:56:33,348][__main__][INFO] - Train, Round 209: loss=0.0045, accuracy=1.0000, gradient_norm=0.1935, 
[2025-09-11 23:56:33,348][__main__][INFO] - Train, Round 210: loss=0.0044, accuracy=1.0000, gradient_norm=0.1982, 
[2025-09-11 23:56:33,348][__main__][INFO] - Train, Round 211: loss=0.0044, accuracy=1.0000, gradient_norm=0.1749, 
[2025-09-11 23:56:33,348][__main__][INFO] - Train, Round 212: loss=0.0043, accuracy=1.0000, gradient_norm=0.1864, 
[2025-09-11 23:56:33,348][__main__][INFO] - Train, Round 213: loss=0.0043, accuracy=1.0000, gradient_norm=0.1903, 
[2025-09-11 23:56:33,348][__main__][INFO] - Train, Round 214: loss=0.0042, accuracy=1.0000, gradient_norm=0.1971, 
[2025-09-11 23:56:33,348][__main__][INFO] - Train, Round 215: loss=0.0042, accuracy=1.0000, gradient_norm=0.1769, 
[2025-09-11 23:56:33,348][__main__][INFO] - Train, Round 216: loss=0.0041, accuracy=1.0000, gradient_norm=0.1800, 
[2025-09-11 23:56:33,348][__main__][INFO] - Train, Round 217: loss=0.0041, accuracy=1.0000, gradient_norm=0.1850, 
[2025-09-11 23:56:33,348][__main__][INFO] - Train, Round 218: loss=0.0040, accuracy=1.0000, gradient_norm=0.1778, 
[2025-09-11 23:56:33,348][__main__][INFO] - Train, Round 219: loss=0.0040, accuracy=1.0000, gradient_norm=0.1778, 
[2025-09-11 23:56:33,348][__main__][INFO] - Train, Round 220: loss=0.0039, accuracy=1.0000, gradient_norm=0.1739, 
[2025-09-11 23:56:33,348][__main__][INFO] - Train, Round 221: loss=0.0039, accuracy=1.0000, gradient_norm=0.1716, 
[2025-09-11 23:56:33,348][__main__][INFO] - Train, Round 222: loss=0.0038, accuracy=1.0000, gradient_norm=0.1725, 
[2025-09-11 23:56:33,348][__main__][INFO] - Train, Round 223: loss=0.0038, accuracy=1.0000, gradient_norm=0.1700, 
[2025-09-11 23:56:33,348][__main__][INFO] - Train, Round 224: loss=0.0037, accuracy=1.0000, gradient_norm=0.1694, 
[2025-09-11 23:56:33,348][__main__][INFO] - Train, Round 225: loss=0.0037, accuracy=1.0000, gradient_norm=0.1578, 
[2025-09-11 23:56:33,348][__main__][INFO] - Train, Round 226: loss=0.0036, accuracy=1.0000, gradient_norm=0.1643, 
[2025-09-11 23:56:33,348][__main__][INFO] - Train, Round 227: loss=0.0036, accuracy=1.0000, gradient_norm=0.1643, 
[2025-09-11 23:56:33,348][__main__][INFO] - Train, Round 228: loss=0.0036, accuracy=1.0000, gradient_norm=0.1552, 
[2025-09-11 23:56:33,348][__main__][INFO] - Train, Round 229: loss=0.0035, accuracy=1.0000, gradient_norm=0.1617, 
[2025-09-11 23:56:33,348][__main__][INFO] - Train, Round 230: loss=0.0035, accuracy=1.0000, gradient_norm=0.1581, 
[2025-09-11 23:56:33,348][__main__][INFO] - Train, Round 231: loss=0.0035, accuracy=1.0000, gradient_norm=0.1589, 
[2025-09-11 23:56:33,348][__main__][INFO] - Train, Round 232: loss=0.0034, accuracy=1.0000, gradient_norm=0.1516, 
[2025-09-11 23:56:33,348][__main__][INFO] - Train, Round 233: loss=0.0034, accuracy=1.0000, gradient_norm=0.1533, 
[2025-09-11 23:56:33,348][__main__][INFO] - Train, Round 234: loss=0.0033, accuracy=1.0000, gradient_norm=0.1483, 
[2025-09-11 23:56:33,348][__main__][INFO] - Train, Round 235: loss=0.0033, accuracy=1.0000, gradient_norm=0.1543, 
[2025-09-11 23:56:33,348][__main__][INFO] - Train, Round 236: loss=0.0033, accuracy=1.0000, gradient_norm=0.1554, 
[2025-09-11 23:56:33,348][__main__][INFO] - Train, Round 237: loss=0.0032, accuracy=1.0000, gradient_norm=0.1493, 
[2025-09-11 23:56:33,348][__main__][INFO] - Train, Round 238: loss=0.0032, accuracy=1.0000, gradient_norm=0.1460, 
[2025-09-11 23:56:33,348][__main__][INFO] - Train, Round 239: loss=0.0032, accuracy=1.0000, gradient_norm=0.1439, 
[2025-09-11 23:56:33,348][__main__][INFO] - Train, Round 240: loss=0.0032, accuracy=1.0000, gradient_norm=0.1412, 
[2025-09-11 23:56:33,348][__main__][INFO] - Train, Round 241: loss=0.0031, accuracy=1.0000, gradient_norm=0.1472, 
[2025-09-11 23:56:33,348][__main__][INFO] - Train, Round 242: loss=0.0031, accuracy=1.0000, gradient_norm=0.1431, 
[2025-09-11 23:56:33,348][__main__][INFO] - Train, Round 243: loss=0.0031, accuracy=1.0000, gradient_norm=0.1449, 
[2025-09-11 23:56:33,348][__main__][INFO] - Train, Round 244: loss=0.0030, accuracy=1.0000, gradient_norm=0.1373, 
[2025-09-11 23:56:33,348][__main__][INFO] - Train, Round 245: loss=0.0030, accuracy=1.0000, gradient_norm=0.1371, 
[2025-09-11 23:56:33,348][__main__][INFO] - Train, Round 246: loss=0.0030, accuracy=1.0000, gradient_norm=0.1383, 
[2025-09-11 23:56:33,348][__main__][INFO] - Train, Round 247: loss=0.0030, accuracy=1.0000, gradient_norm=0.1410, 
[2025-09-11 23:56:33,348][__main__][INFO] - Train, Round 248: loss=0.0029, accuracy=1.0000, gradient_norm=0.1356, 
[2025-09-11 23:56:33,348][__main__][INFO] - Train, Round 249: loss=0.0029, accuracy=1.0000, gradient_norm=0.1359, 
[2025-09-11 23:56:33,348][__main__][INFO] - Train, Round 250: loss=0.0029, accuracy=1.0000, gradient_norm=0.1314, 
[2025-09-11 23:56:33,348][__main__][INFO] - Train, Round 251: loss=0.0029, accuracy=1.0000, gradient_norm=0.1292, 
[2025-09-11 23:56:33,348][__main__][INFO] - Train, Round 252: loss=0.0028, accuracy=1.0000, gradient_norm=0.1251, 
[2025-09-11 23:56:33,348][__main__][INFO] - Train, Round 253: loss=0.0028, accuracy=1.0000, gradient_norm=0.1306, 
[2025-09-11 23:56:33,348][__main__][INFO] - Train, Round 254: loss=0.0028, accuracy=1.0000, gradient_norm=0.1241, 
[2025-09-11 23:56:33,348][__main__][INFO] - Train, Round 255: loss=0.0028, accuracy=1.0000, gradient_norm=0.1286, 
[2025-09-11 23:56:33,348][__main__][INFO] - Train, Round 256: loss=0.0027, accuracy=1.0000, gradient_norm=0.1309, 
[2025-09-11 23:56:33,348][__main__][INFO] - Train, Round 257: loss=0.0027, accuracy=1.0000, gradient_norm=0.1268, 
[2025-09-11 23:56:33,348][__main__][INFO] - Train, Round 258: loss=0.0027, accuracy=1.0000, gradient_norm=0.1260, 
[2025-09-11 23:56:33,348][__main__][INFO] - Train, Round 259: loss=0.0027, accuracy=1.0000, gradient_norm=0.1252, 
[2025-09-11 23:56:33,348][__main__][INFO] - Train, Round 260: loss=0.0026, accuracy=1.0000, gradient_norm=0.1250, 
[2025-09-11 23:56:33,348][__main__][INFO] - Train, Round 261: loss=0.0026, accuracy=1.0000, gradient_norm=0.1220, 
[2025-09-11 23:56:33,348][__main__][INFO] - Train, Round 262: loss=0.0026, accuracy=1.0000, gradient_norm=0.1224, 
[2025-09-11 23:56:33,348][__main__][INFO] - Train, Round 263: loss=0.0026, accuracy=1.0000, gradient_norm=0.1197, 
[2025-09-11 23:56:33,348][__main__][INFO] - Train, Round 264: loss=0.0026, accuracy=1.0000, gradient_norm=0.1160, 
[2025-09-11 23:56:33,348][__main__][INFO] - Train, Round 265: loss=0.0026, accuracy=1.0000, gradient_norm=0.1198, 
[2025-09-11 23:56:33,348][__main__][INFO] - Train, Round 266: loss=0.0025, accuracy=1.0000, gradient_norm=0.1193, 
[2025-09-11 23:56:33,348][__main__][INFO] - Train, Round 267: loss=0.0025, accuracy=1.0000, gradient_norm=0.1153, 
[2025-09-11 23:56:33,348][__main__][INFO] - Train, Round 268: loss=0.0025, accuracy=1.0000, gradient_norm=0.1190, 
[2025-09-11 23:56:33,348][__main__][INFO] - Train, Round 269: loss=0.0025, accuracy=1.0000, gradient_norm=0.1136, 
[2025-09-11 23:56:33,348][__main__][INFO] - Train, Round 270: loss=0.0025, accuracy=1.0000, gradient_norm=0.1185, 
[2025-09-11 23:56:33,348][__main__][INFO] - Train, Round 271: loss=0.0024, accuracy=1.0000, gradient_norm=0.1169, 
[2025-09-11 23:56:33,348][__main__][INFO] - Train, Round 272: loss=0.0024, accuracy=1.0000, gradient_norm=0.1185, 
[2025-09-11 23:56:33,348][__main__][INFO] - Train, Round 273: loss=0.0024, accuracy=1.0000, gradient_norm=0.1142, 
[2025-09-11 23:56:33,348][__main__][INFO] - Train, Round 274: loss=0.0024, accuracy=1.0000, gradient_norm=0.1127, 
[2025-09-11 23:56:33,348][__main__][INFO] - Train, Round 275: loss=0.0024, accuracy=1.0000, gradient_norm=0.1118, 
[2025-09-11 23:56:33,348][__main__][INFO] - Train, Round 276: loss=0.0024, accuracy=1.0000, gradient_norm=0.1090, 
[2025-09-11 23:56:33,348][__main__][INFO] - Train, Round 277: loss=0.0023, accuracy=1.0000, gradient_norm=0.1067, 
[2025-09-11 23:56:33,348][__main__][INFO] - Train, Round 278: loss=0.0023, accuracy=1.0000, gradient_norm=0.1128, 
[2025-09-11 23:56:33,348][__main__][INFO] - Train, Round 279: loss=0.0023, accuracy=1.0000, gradient_norm=0.1082, 
[2025-09-11 23:56:33,348][__main__][INFO] - Train, Round 280: loss=0.0023, accuracy=1.0000, gradient_norm=0.1126, 
[2025-09-11 23:56:33,348][__main__][INFO] - Train, Round 281: loss=0.0023, accuracy=1.0000, gradient_norm=0.1057, 
[2025-09-11 23:56:33,348][__main__][INFO] - Train, Round 282: loss=0.0023, accuracy=1.0000, gradient_norm=0.1092, 
[2025-09-11 23:56:33,348][__main__][INFO] - Train, Round 283: loss=0.0023, accuracy=1.0000, gradient_norm=0.1070, 
[2025-09-11 23:56:33,348][__main__][INFO] - Train, Round 284: loss=0.0022, accuracy=1.0000, gradient_norm=0.1081, 
[2025-09-11 23:56:33,348][__main__][INFO] - Train, Round 285: loss=0.0022, accuracy=1.0000, gradient_norm=0.1000, 
[2025-09-11 23:56:33,348][__main__][INFO] - Train, Round 286: loss=0.0022, accuracy=1.0000, gradient_norm=0.1034, 
[2025-09-11 23:56:33,348][__main__][INFO] - Train, Round 287: loss=0.0022, accuracy=1.0000, gradient_norm=0.1019, 
[2025-09-11 23:56:33,349][__main__][INFO] - Train, Round 288: loss=0.0022, accuracy=1.0000, gradient_norm=0.1016, 
[2025-09-11 23:56:33,349][__main__][INFO] - Train, Round 289: loss=0.0022, accuracy=1.0000, gradient_norm=0.1052, 
[2025-09-11 23:56:33,349][__main__][INFO] - Train, Round 290: loss=0.0022, accuracy=1.0000, gradient_norm=0.1075, 
[2025-09-11 23:56:33,349][__main__][INFO] - Train, Round 291: loss=0.0021, accuracy=1.0000, gradient_norm=0.1055, 
[2025-09-11 23:56:33,349][__main__][INFO] - Train, Round 292: loss=0.0021, accuracy=1.0000, gradient_norm=0.0982, 
[2025-09-11 23:56:33,349][__main__][INFO] - Train, Round 293: loss=0.0021, accuracy=1.0000, gradient_norm=0.0975, 
[2025-09-11 23:56:33,349][__main__][INFO] - Train, Round 294: loss=0.0021, accuracy=1.0000, gradient_norm=0.0961, 
[2025-09-11 23:56:33,349][__main__][INFO] - Train, Round 295: loss=0.0021, accuracy=1.0000, gradient_norm=0.0972, 
[2025-09-11 23:56:33,349][__main__][INFO] - Train, Round 296: loss=0.0021, accuracy=1.0000, gradient_norm=0.1007, 
[2025-09-11 23:56:33,349][__main__][INFO] - Train, Round 297: loss=0.0021, accuracy=1.0000, gradient_norm=0.0994, 
[2025-09-11 23:56:33,349][__main__][INFO] - Train, Round 298: loss=0.0021, accuracy=1.0000, gradient_norm=0.0952, 
[2025-09-11 23:56:33,349][__main__][INFO] - Train, Round 299: loss=0.0020, accuracy=1.0000, gradient_norm=0.0946, 
[2025-09-11 23:56:33,349][__main__][INFO] - Train, Round 300: loss=0.0020, accuracy=1.0000, gradient_norm=0.0990, 
[2025-09-11 23:56:33,349][__main__][INFO] - Train, Round 301: loss=0.0020, accuracy=1.0000, gradient_norm=0.0985, 
[2025-09-11 23:56:33,349][__main__][INFO] - Train, Round 302: loss=0.0020, accuracy=1.0000, gradient_norm=0.0948, 
[2025-09-11 23:56:33,349][__main__][INFO] - Train, Round 303: loss=0.0020, accuracy=1.0000, gradient_norm=0.0925, 
[2025-09-11 23:56:33,349][__main__][INFO] - Train, Round 304: loss=0.0020, accuracy=1.0000, gradient_norm=0.0956, 
[2025-09-11 23:56:33,349][__main__][INFO] - Train, Round 305: loss=0.0020, accuracy=1.0000, gradient_norm=0.0974, 
[2025-09-11 23:56:33,349][__main__][INFO] - Train, Round 306: loss=0.0020, accuracy=1.0000, gradient_norm=0.0947, 
[2025-09-11 23:56:33,349][__main__][INFO] - Train, Round 307: loss=0.0020, accuracy=1.0000, gradient_norm=0.0945, 
[2025-09-11 23:56:33,349][__main__][INFO] - Train, Round 308: loss=0.0019, accuracy=1.0000, gradient_norm=0.0934, 
[2025-09-11 23:56:33,349][__main__][INFO] - Train, Round 309: loss=0.0019, accuracy=1.0000, gradient_norm=0.0896, 
[2025-09-11 23:56:33,349][__main__][INFO] - Train, Round 310: loss=0.0019, accuracy=1.0000, gradient_norm=0.0903, 
[2025-09-11 23:56:33,349][__main__][INFO] - Train, Round 311: loss=0.0019, accuracy=1.0000, gradient_norm=0.0950, 
[2025-09-11 23:56:33,349][__main__][INFO] - Train, Round 312: loss=0.0019, accuracy=1.0000, gradient_norm=0.0897, 
[2025-09-11 23:56:33,349][__main__][INFO] - Train, Round 313: loss=0.0019, accuracy=1.0000, gradient_norm=0.0897, 
[2025-09-11 23:56:33,349][__main__][INFO] - Train, Round 314: loss=0.0019, accuracy=1.0000, gradient_norm=0.0911, 
[2025-09-11 23:56:33,349][__main__][INFO] - Train, Round 315: loss=0.0019, accuracy=1.0000, gradient_norm=0.0881, 
[2025-09-11 23:56:33,349][__main__][INFO] - Train, Round 316: loss=0.0019, accuracy=1.0000, gradient_norm=0.0873, 
[2025-09-11 23:56:33,349][__main__][INFO] - Train, Round 317: loss=0.0019, accuracy=1.0000, gradient_norm=0.0866, 
[2025-09-11 23:56:33,349][__main__][INFO] - Train, Round 318: loss=0.0019, accuracy=1.0000, gradient_norm=0.0915, 
[2025-09-11 23:56:33,349][__main__][INFO] - Train, Round 319: loss=0.0018, accuracy=1.0000, gradient_norm=0.0906, 
[2025-09-11 23:56:33,349][__main__][INFO] - Train, Round 320: loss=0.0018, accuracy=1.0000, gradient_norm=0.0919, 
[2025-09-11 23:56:33,349][__main__][INFO] - Train, Round 321: loss=0.0018, accuracy=1.0000, gradient_norm=0.0883, 
[2025-09-11 23:56:33,349][__main__][INFO] - Train, Round 322: loss=0.0018, accuracy=1.0000, gradient_norm=0.0857, 
[2025-09-11 23:56:33,349][__main__][INFO] - Train, Round 323: loss=0.0018, accuracy=1.0000, gradient_norm=0.0854, 
[2025-09-11 23:56:33,349][__main__][INFO] - Train, Round 324: loss=0.0018, accuracy=1.0000, gradient_norm=0.0865, 
[2025-09-11 23:56:33,349][__main__][INFO] - Train, Round 325: loss=0.0018, accuracy=1.0000, gradient_norm=0.0872, 
[2025-09-11 23:56:33,349][__main__][INFO] - Train, Round 326: loss=0.0018, accuracy=1.0000, gradient_norm=0.0862, 
[2025-09-11 23:56:33,349][__main__][INFO] - Train, Round 327: loss=0.0018, accuracy=1.0000, gradient_norm=0.0874, 
[2025-09-11 23:56:33,349][__main__][INFO] - Train, Round 328: loss=0.0018, accuracy=1.0000, gradient_norm=0.0831, 
[2025-09-11 23:56:33,349][__main__][INFO] - Train, Round 329: loss=0.0018, accuracy=1.0000, gradient_norm=0.0892, 
[2025-09-11 23:56:33,349][__main__][INFO] - Train, Round 330: loss=0.0018, accuracy=1.0000, gradient_norm=0.0856, 
[2025-09-11 23:56:33,349][__main__][INFO] - Train, Round 331: loss=0.0017, accuracy=1.0000, gradient_norm=0.0845, 
[2025-09-11 23:56:33,349][__main__][INFO] - Train, Round 332: loss=0.0017, accuracy=1.0000, gradient_norm=0.0864, 
[2025-09-11 23:56:33,349][__main__][INFO] - Train, Round 333: loss=0.0017, accuracy=1.0000, gradient_norm=0.0864, 
[2025-09-11 23:56:33,349][__main__][INFO] - Train, Round 334: loss=0.0017, accuracy=1.0000, gradient_norm=0.0869, 
[2025-09-11 23:56:33,349][__main__][INFO] - Train, Round 335: loss=0.0017, accuracy=1.0000, gradient_norm=0.0807, 
[2025-09-11 23:56:33,349][__main__][INFO] - Train, Round 336: loss=0.0017, accuracy=1.0000, gradient_norm=0.0834, 
[2025-09-11 23:56:33,349][__main__][INFO] - Train, Round 337: loss=0.0017, accuracy=1.0000, gradient_norm=0.0826, 
[2025-09-11 23:56:33,349][__main__][INFO] - Train, Round 338: loss=0.0017, accuracy=1.0000, gradient_norm=0.0827, 
[2025-09-11 23:56:33,349][__main__][INFO] - Train, Round 339: loss=0.0017, accuracy=1.0000, gradient_norm=0.0796, 
[2025-09-11 23:56:33,349][__main__][INFO] - Train, Round 340: loss=0.0017, accuracy=1.0000, gradient_norm=0.0801, 
[2025-09-11 23:56:33,349][__main__][INFO] - Train, Round 341: loss=0.0017, accuracy=1.0000, gradient_norm=0.0814, 
[2025-09-11 23:56:33,349][__main__][INFO] - Train, Round 342: loss=0.0017, accuracy=1.0000, gradient_norm=0.0804, 
[2025-09-11 23:56:33,349][__main__][INFO] - Train, Round 343: loss=0.0017, accuracy=1.0000, gradient_norm=0.0808, 
[2025-09-11 23:56:33,349][__main__][INFO] - Train, Round 344: loss=0.0017, accuracy=1.0000, gradient_norm=0.0814, 
[2025-09-11 23:56:33,349][__main__][INFO] - Train, Round 345: loss=0.0016, accuracy=1.0000, gradient_norm=0.0792, 
[2025-09-11 23:56:33,349][__main__][INFO] - Train, Round 346: loss=0.0016, accuracy=1.0000, gradient_norm=0.0821, 
[2025-09-11 23:56:33,349][__main__][INFO] - Train, Round 347: loss=0.0016, accuracy=1.0000, gradient_norm=0.0791, 
[2025-09-11 23:56:33,349][__main__][INFO] - Train, Round 348: loss=0.0016, accuracy=1.0000, gradient_norm=0.0802, 
[2025-09-11 23:56:33,349][__main__][INFO] - Train, Round 349: loss=0.0016, accuracy=1.0000, gradient_norm=0.0764, 
[2025-09-11 23:56:33,349][__main__][INFO] - Train, Round 350: loss=0.0016, accuracy=1.0000, gradient_norm=0.0804, 
[2025-09-11 23:56:33,349][__main__][INFO] - Train, Round 351: loss=0.0016, accuracy=1.0000, gradient_norm=0.0756, 
[2025-09-11 23:56:33,349][__main__][INFO] - Train, Round 352: loss=0.0016, accuracy=1.0000, gradient_norm=0.0778, 
[2025-09-11 23:56:33,349][__main__][INFO] - Train, Round 353: loss=0.0016, accuracy=1.0000, gradient_norm=0.0793, 
[2025-09-11 23:56:33,349][__main__][INFO] - Train, Round 354: loss=0.0016, accuracy=1.0000, gradient_norm=0.0759, 
[2025-09-11 23:56:33,349][__main__][INFO] - Train, Round 355: loss=0.0016, accuracy=1.0000, gradient_norm=0.0775, 
[2025-09-11 23:56:33,349][__main__][INFO] - Train, Round 356: loss=0.0016, accuracy=1.0000, gradient_norm=0.0759, 
[2025-09-11 23:56:33,349][__main__][INFO] - Train, Round 357: loss=0.0016, accuracy=1.0000, gradient_norm=0.0789, 
[2025-09-11 23:56:33,349][__main__][INFO] - Train, Round 358: loss=0.0016, accuracy=1.0000, gradient_norm=0.0765, 
[2025-09-11 23:56:33,349][__main__][INFO] - Train, Round 359: loss=0.0016, accuracy=1.0000, gradient_norm=0.0755, 
[2025-09-11 23:56:33,349][__main__][INFO] - Train, Round 360: loss=0.0016, accuracy=1.0000, gradient_norm=0.0739, 
[2025-09-11 23:56:33,349][__main__][INFO] - Train, Round 361: loss=0.0015, accuracy=1.0000, gradient_norm=0.0775, 
[2025-09-11 23:56:33,349][__main__][INFO] - Train, Round 362: loss=0.0015, accuracy=1.0000, gradient_norm=0.0733, 
[2025-09-11 23:56:33,349][__main__][INFO] - Train, Round 363: loss=0.0015, accuracy=1.0000, gradient_norm=0.0757, 
[2025-09-11 23:56:33,349][__main__][INFO] - Train, Round 364: loss=0.0015, accuracy=1.0000, gradient_norm=0.0742, 
[2025-09-11 23:56:33,349][__main__][INFO] - Train, Round 365: loss=0.0015, accuracy=1.0000, gradient_norm=0.0719, 
[2025-09-11 23:56:33,349][__main__][INFO] - Train, Round 366: loss=0.0015, accuracy=1.0000, gradient_norm=0.0766, 
[2025-09-11 23:56:33,349][__main__][INFO] - Train, Round 367: loss=0.0015, accuracy=1.0000, gradient_norm=0.0727, 
[2025-09-11 23:56:33,349][__main__][INFO] - Train, Round 368: loss=0.0015, accuracy=1.0000, gradient_norm=0.0753, 
[2025-09-11 23:56:33,349][__main__][INFO] - Train, Round 369: loss=0.0015, accuracy=1.0000, gradient_norm=0.0764, 
[2025-09-11 23:56:33,349][__main__][INFO] - Train, Round 370: loss=0.0015, accuracy=1.0000, gradient_norm=0.0712, 
[2025-09-11 23:56:33,350][__main__][INFO] - Train, Round 371: loss=0.0015, accuracy=1.0000, gradient_norm=0.0723, 
[2025-09-11 23:56:33,350][__main__][INFO] - Train, Round 372: loss=0.0015, accuracy=1.0000, gradient_norm=0.0714, 
[2025-09-11 23:56:33,350][__main__][INFO] - Train, Round 373: loss=0.0015, accuracy=1.0000, gradient_norm=0.0736, 
[2025-09-11 23:56:33,350][__main__][INFO] - Train, Round 374: loss=0.0015, accuracy=1.0000, gradient_norm=0.0714, 
[2025-09-11 23:56:33,350][__main__][INFO] - Train, Round 375: loss=0.0015, accuracy=1.0000, gradient_norm=0.0739, 
[2025-09-11 23:56:33,350][__main__][INFO] - Train, Round 376: loss=0.0015, accuracy=1.0000, gradient_norm=0.0731, 
[2025-09-11 23:56:33,350][__main__][INFO] - Train, Round 377: loss=0.0015, accuracy=1.0000, gradient_norm=0.0745, 
[2025-09-11 23:56:33,350][__main__][INFO] - Train, Round 378: loss=0.0015, accuracy=1.0000, gradient_norm=0.0713, 
[2025-09-11 23:56:33,350][__main__][INFO] - Train, Round 379: loss=0.0015, accuracy=1.0000, gradient_norm=0.0734, 
[2025-09-11 23:56:33,350][__main__][INFO] - Train, Round 380: loss=0.0015, accuracy=1.0000, gradient_norm=0.0708, 
[2025-09-11 23:56:33,350][__main__][INFO] - Train, Round 381: loss=0.0014, accuracy=1.0000, gradient_norm=0.0714, 
[2025-09-11 23:56:33,350][__main__][INFO] - Train, Round 382: loss=0.0014, accuracy=1.0000, gradient_norm=0.0730, 
[2025-09-11 23:56:33,350][__main__][INFO] - Train, Round 383: loss=0.0014, accuracy=1.0000, gradient_norm=0.0687, 
[2025-09-11 23:56:33,350][__main__][INFO] - Train, Round 384: loss=0.0014, accuracy=1.0000, gradient_norm=0.0724, 
[2025-09-11 23:56:33,350][__main__][INFO] - Train, Round 385: loss=0.0014, accuracy=1.0000, gradient_norm=0.0667, 
[2025-09-11 23:56:33,350][__main__][INFO] - Train, Round 386: loss=0.0014, accuracy=1.0000, gradient_norm=0.0723, 
[2025-09-11 23:56:33,350][__main__][INFO] - Train, Round 387: loss=0.0014, accuracy=1.0000, gradient_norm=0.0701, 
[2025-09-11 23:56:33,350][__main__][INFO] - Train, Round 388: loss=0.0014, accuracy=1.0000, gradient_norm=0.0703, 
[2025-09-11 23:56:33,350][__main__][INFO] - Train, Round 389: loss=0.0014, accuracy=1.0000, gradient_norm=0.0674, 
[2025-09-11 23:56:33,350][__main__][INFO] - Train, Round 390: loss=0.0014, accuracy=1.0000, gradient_norm=0.0695, 
[2025-09-11 23:56:33,350][__main__][INFO] - Train, Round 391: loss=0.0014, accuracy=1.0000, gradient_norm=0.0667, 
[2025-09-11 23:56:33,350][__main__][INFO] - Train, Round 392: loss=0.0014, accuracy=1.0000, gradient_norm=0.0713, 
[2025-09-11 23:56:33,350][__main__][INFO] - Train, Round 393: loss=0.0014, accuracy=1.0000, gradient_norm=0.0684, 
[2025-09-11 23:56:33,350][__main__][INFO] - Train, Round 394: loss=0.0014, accuracy=1.0000, gradient_norm=0.0690, 
[2025-09-11 23:56:33,350][__main__][INFO] - Train, Round 395: loss=0.0014, accuracy=1.0000, gradient_norm=0.0683, 
[2025-09-11 23:56:33,350][__main__][INFO] - Train, Round 396: loss=0.0014, accuracy=1.0000, gradient_norm=0.0668, 
[2025-09-11 23:56:33,350][__main__][INFO] - Train, Round 397: loss=0.0014, accuracy=1.0000, gradient_norm=0.0668, 
[2025-09-11 23:56:33,350][__main__][INFO] - Train, Round 398: loss=0.0014, accuracy=1.0000, gradient_norm=0.0656, 
[2025-09-11 23:56:33,350][__main__][INFO] - Train, Round 399: loss=0.0014, accuracy=1.0000, gradient_norm=0.0659, 
[2025-09-11 23:56:33,350][__main__][INFO] - Train, Round 400: loss=0.0014, accuracy=1.0000, gradient_norm=0.0692, 
[2025-09-11 23:56:33,350][__main__][INFO] - Train, Round 401: loss=0.0014, accuracy=1.0000, gradient_norm=0.0691, 
[2025-09-11 23:56:33,350][__main__][INFO] - Train, Round 402: loss=0.0014, accuracy=1.0000, gradient_norm=0.0638, 
[2025-09-11 23:56:33,350][__main__][INFO] - Train, Round 403: loss=0.0014, accuracy=1.0000, gradient_norm=0.0650, 
[2025-09-11 23:56:33,350][__main__][INFO] - Train, Round 404: loss=0.0014, accuracy=1.0000, gradient_norm=0.0665, 
[2025-09-11 23:56:33,350][__main__][INFO] - Train, Round 405: loss=0.0014, accuracy=1.0000, gradient_norm=0.0659, 
[2025-09-11 23:56:33,350][__main__][INFO] - Train, Round 406: loss=0.0013, accuracy=1.0000, gradient_norm=0.0665, 
[2025-09-11 23:56:33,350][__main__][INFO] - Train, Round 407: loss=0.0013, accuracy=1.0000, gradient_norm=0.0657, 
[2025-09-11 23:56:33,350][__main__][INFO] - Train, Round 408: loss=0.0013, accuracy=1.0000, gradient_norm=0.0666, 
[2025-09-11 23:56:33,350][__main__][INFO] - Train, Round 409: loss=0.0013, accuracy=1.0000, gradient_norm=0.0655, 
[2025-09-11 23:56:33,350][__main__][INFO] - Train, Round 410: loss=0.0013, accuracy=1.0000, gradient_norm=0.0627, 
[2025-09-11 23:56:33,350][__main__][INFO] - Train, Round 411: loss=0.0013, accuracy=1.0000, gradient_norm=0.0658, 
[2025-09-11 23:56:33,350][__main__][INFO] - Train, Round 412: loss=0.0013, accuracy=1.0000, gradient_norm=0.0647, 
[2025-09-11 23:56:33,350][__main__][INFO] - Train, Round 413: loss=0.0013, accuracy=1.0000, gradient_norm=0.0643, 
[2025-09-11 23:56:33,350][__main__][INFO] - Train, Round 414: loss=0.0013, accuracy=1.0000, gradient_norm=0.0659, 
[2025-09-11 23:56:33,350][__main__][INFO] - Train, Round 415: loss=0.0013, accuracy=1.0000, gradient_norm=0.0669, 
[2025-09-11 23:56:33,350][__main__][INFO] - Train, Round 416: loss=0.0013, accuracy=1.0000, gradient_norm=0.0654, 
[2025-09-11 23:56:33,350][__main__][INFO] - Train, Round 417: loss=0.0013, accuracy=1.0000, gradient_norm=0.0652, 
[2025-09-11 23:56:33,350][__main__][INFO] - Train, Round 418: loss=0.0013, accuracy=1.0000, gradient_norm=0.0646, 
[2025-09-11 23:56:33,350][__main__][INFO] - Train, Round 419: loss=0.0013, accuracy=1.0000, gradient_norm=0.0642, 
[2025-09-11 23:56:33,350][__main__][INFO] - Train, Round 420: loss=0.0013, accuracy=1.0000, gradient_norm=0.0643, 
[2025-09-11 23:56:33,350][__main__][INFO] - Train, Round 421: loss=0.0013, accuracy=1.0000, gradient_norm=0.0638, 
[2025-09-11 23:56:33,350][__main__][INFO] - Train, Round 422: loss=0.0013, accuracy=1.0000, gradient_norm=0.0622, 
[2025-09-11 23:56:33,350][__main__][INFO] - Train, Round 423: loss=0.0013, accuracy=1.0000, gradient_norm=0.0639, 
[2025-09-11 23:56:33,350][__main__][INFO] - Train, Round 424: loss=0.0013, accuracy=1.0000, gradient_norm=0.0645, 
[2025-09-11 23:56:33,350][__main__][INFO] - Train, Round 425: loss=0.0013, accuracy=1.0000, gradient_norm=0.0627, 
[2025-09-11 23:56:33,350][__main__][INFO] - Train, Round 426: loss=0.0013, accuracy=1.0000, gradient_norm=0.0621, 
[2025-09-11 23:56:33,350][__main__][INFO] - Train, Round 427: loss=0.0013, accuracy=1.0000, gradient_norm=0.0630, 
[2025-09-11 23:56:33,350][__main__][INFO] - Train, Round 428: loss=0.0013, accuracy=1.0000, gradient_norm=0.0616, 
[2025-09-11 23:56:33,350][__main__][INFO] - Train, Round 429: loss=0.0013, accuracy=1.0000, gradient_norm=0.0642, 
[2025-09-11 23:56:33,350][__main__][INFO] - Train, Round 430: loss=0.0013, accuracy=1.0000, gradient_norm=0.0643, 
[2025-09-11 23:56:33,350][__main__][INFO] - Train, Round 431: loss=0.0013, accuracy=1.0000, gradient_norm=0.0603, 
[2025-09-11 23:56:33,350][__main__][INFO] - Train, Round 432: loss=0.0013, accuracy=1.0000, gradient_norm=0.0635, 
[2025-09-11 23:56:33,350][__main__][INFO] - Train, Round 433: loss=0.0013, accuracy=1.0000, gradient_norm=0.0621, 
[2025-09-11 23:56:33,350][__main__][INFO] - Train, Round 434: loss=0.0013, accuracy=1.0000, gradient_norm=0.0619, 
[2025-09-11 23:56:33,350][__main__][INFO] - Train, Round 435: loss=0.0013, accuracy=1.0000, gradient_norm=0.0631, 
[2025-09-11 23:56:33,350][__main__][INFO] - Train, Round 436: loss=0.0012, accuracy=1.0000, gradient_norm=0.0619, 
[2025-09-11 23:56:33,350][__main__][INFO] - Train, Round 437: loss=0.0012, accuracy=1.0000, gradient_norm=0.0616, 
[2025-09-11 23:56:33,350][__main__][INFO] - Train, Round 438: loss=0.0012, accuracy=1.0000, gradient_norm=0.0636, 
[2025-09-11 23:56:33,350][__main__][INFO] - Train, Round 439: loss=0.0012, accuracy=1.0000, gradient_norm=0.0598, 
[2025-09-11 23:56:33,350][__main__][INFO] - Train, Round 440: loss=0.0012, accuracy=1.0000, gradient_norm=0.0615, 
[2025-09-11 23:56:33,350][__main__][INFO] - Train, Round 441: loss=0.0012, accuracy=1.0000, gradient_norm=0.0602, 
[2025-09-11 23:56:33,350][__main__][INFO] - Train, Round 442: loss=0.0012, accuracy=1.0000, gradient_norm=0.0601, 
[2025-09-11 23:56:33,350][__main__][INFO] - Train, Round 443: loss=0.0012, accuracy=1.0000, gradient_norm=0.0594, 
[2025-09-11 23:56:33,350][__main__][INFO] - Train, Round 444: loss=0.0012, accuracy=1.0000, gradient_norm=0.0623, 
[2025-09-11 23:56:33,350][__main__][INFO] - Train, Round 445: loss=0.0012, accuracy=1.0000, gradient_norm=0.0599, 
[2025-09-11 23:56:33,350][__main__][INFO] - Train, Round 446: loss=0.0012, accuracy=1.0000, gradient_norm=0.0602, 
[2025-09-11 23:56:33,350][__main__][INFO] - Train, Round 447: loss=0.0012, accuracy=1.0000, gradient_norm=0.0637, 
[2025-09-11 23:56:33,350][__main__][INFO] - Train, Round 448: loss=0.0012, accuracy=1.0000, gradient_norm=0.0591, 
[2025-09-11 23:56:33,350][__main__][INFO] - Train, Round 449: loss=0.0012, accuracy=1.0000, gradient_norm=0.0595, 
[2025-09-11 23:56:33,350][__main__][INFO] - Train, Round 450: loss=0.0012, accuracy=1.0000, gradient_norm=0.0591, 
[2025-09-11 23:56:33,350][__main__][INFO] - Train, Round 451: loss=0.0012, accuracy=1.0000, gradient_norm=0.0576, 
[2025-09-11 23:56:33,350][__main__][INFO] - Train, Round 452: loss=0.0012, accuracy=1.0000, gradient_norm=0.0605, 
[2025-09-11 23:56:33,350][__main__][INFO] - Train, Round 453: loss=0.0012, accuracy=1.0000, gradient_norm=0.0591, 
[2025-09-11 23:56:33,351][__main__][INFO] - Train, Round 454: loss=0.0012, accuracy=1.0000, gradient_norm=0.0591, 
[2025-09-11 23:56:33,351][__main__][INFO] - Train, Round 455: loss=0.0012, accuracy=1.0000, gradient_norm=0.0566, 
[2025-09-11 23:56:33,351][__main__][INFO] - Train, Round 456: loss=0.0012, accuracy=1.0000, gradient_norm=0.0588, 
[2025-09-11 23:56:33,351][__main__][INFO] - Train, Round 457: loss=0.0012, accuracy=1.0000, gradient_norm=0.0597, 
[2025-09-11 23:56:33,351][__main__][INFO] - Train, Round 458: loss=0.0012, accuracy=1.0000, gradient_norm=0.0599, 
[2025-09-11 23:56:33,351][__main__][INFO] - Train, Round 459: loss=0.0012, accuracy=1.0000, gradient_norm=0.0605, 
[2025-09-11 23:56:33,351][__main__][INFO] - Train, Round 460: loss=0.0012, accuracy=1.0000, gradient_norm=0.0590, 
[2025-09-11 23:56:33,351][__main__][INFO] - Train, Round 461: loss=0.0012, accuracy=1.0000, gradient_norm=0.0568, 
[2025-09-11 23:56:33,351][__main__][INFO] - Train, Round 462: loss=0.0012, accuracy=1.0000, gradient_norm=0.0570, 
[2025-09-11 23:56:33,351][__main__][INFO] - Train, Round 463: loss=0.0012, accuracy=1.0000, gradient_norm=0.0584, 
[2025-09-11 23:56:33,351][__main__][INFO] - Train, Round 464: loss=0.0012, accuracy=1.0000, gradient_norm=0.0590, 
[2025-09-11 23:56:33,351][__main__][INFO] - Train, Round 465: loss=0.0012, accuracy=1.0000, gradient_norm=0.0562, 
[2025-09-11 23:56:33,351][__main__][INFO] - Train, Round 466: loss=0.0012, accuracy=1.0000, gradient_norm=0.0590, 
[2025-09-11 23:56:33,351][__main__][INFO] - Train, Round 467: loss=0.0012, accuracy=1.0000, gradient_norm=0.0601, 
[2025-09-11 23:56:33,351][__main__][INFO] - Train, Round 468: loss=0.0012, accuracy=1.0000, gradient_norm=0.0580, 
[2025-09-11 23:56:33,351][__main__][INFO] - Train, Round 469: loss=0.0012, accuracy=1.0000, gradient_norm=0.0559, 
[2025-09-11 23:56:33,351][__main__][INFO] - Train, Round 470: loss=0.0012, accuracy=1.0000, gradient_norm=0.0571, 
[2025-09-11 23:56:33,351][__main__][INFO] - Train, Round 471: loss=0.0012, accuracy=1.0000, gradient_norm=0.0559, 
[2025-09-11 23:56:33,351][__main__][INFO] - Train, Round 472: loss=0.0012, accuracy=1.0000, gradient_norm=0.0563, 
[2025-09-11 23:56:33,351][__main__][INFO] - Train, Round 473: loss=0.0012, accuracy=1.0000, gradient_norm=0.0579, 
[2025-09-11 23:56:33,351][__main__][INFO] - Train, Round 474: loss=0.0012, accuracy=1.0000, gradient_norm=0.0579, 
[2025-09-11 23:56:33,351][__main__][INFO] - Train, Round 475: loss=0.0012, accuracy=1.0000, gradient_norm=0.0567, 
[2025-09-11 23:56:33,351][__main__][INFO] - Train, Round 476: loss=0.0012, accuracy=1.0000, gradient_norm=0.0564, 
[2025-09-11 23:56:33,351][__main__][INFO] - Train, Round 477: loss=0.0012, accuracy=1.0000, gradient_norm=0.0576, 
[2025-09-11 23:56:33,351][__main__][INFO] - Train, Round 478: loss=0.0011, accuracy=1.0000, gradient_norm=0.0549, 
[2025-09-11 23:56:33,351][__main__][INFO] - Train, Round 479: loss=0.0011, accuracy=1.0000, gradient_norm=0.0602, 
[2025-09-11 23:56:33,351][__main__][INFO] - Train, Round 480: loss=0.0011, accuracy=1.0000, gradient_norm=0.0563, 
[2025-09-11 23:56:33,351][__main__][INFO] - Train, Round 481: loss=0.0011, accuracy=1.0000, gradient_norm=0.0549, 
[2025-09-11 23:56:33,351][__main__][INFO] - Train, Round 482: loss=0.0011, accuracy=1.0000, gradient_norm=0.0576, 
[2025-09-11 23:56:33,351][__main__][INFO] - Train, Round 483: loss=0.0011, accuracy=1.0000, gradient_norm=0.0576, 
[2025-09-11 23:56:33,351][__main__][INFO] - Train, Round 484: loss=0.0011, accuracy=1.0000, gradient_norm=0.0584, 
[2025-09-11 23:56:33,351][__main__][INFO] - Train, Round 485: loss=0.0011, accuracy=1.0000, gradient_norm=0.0572, 
[2025-09-11 23:56:33,351][__main__][INFO] - Train, Round 486: loss=0.0011, accuracy=1.0000, gradient_norm=0.0610, 
[2025-09-11 23:56:33,351][__main__][INFO] - Train, Round 487: loss=0.0011, accuracy=1.0000, gradient_norm=0.0564, 
[2025-09-11 23:56:33,351][__main__][INFO] - Train, Round 488: loss=0.0011, accuracy=1.0000, gradient_norm=0.0544, 
[2025-09-11 23:56:33,351][__main__][INFO] - Train, Round 489: loss=0.0011, accuracy=1.0000, gradient_norm=0.0562, 
[2025-09-11 23:56:33,351][__main__][INFO] - Train, Round 490: loss=0.0011, accuracy=1.0000, gradient_norm=0.0572, 
[2025-09-11 23:56:33,351][__main__][INFO] - Train, Round 491: loss=0.0011, accuracy=1.0000, gradient_norm=0.0565, 
[2025-09-11 23:56:33,351][__main__][INFO] - Train, Round 492: loss=0.0011, accuracy=1.0000, gradient_norm=0.0554, 
[2025-09-11 23:56:33,351][__main__][INFO] - Train, Round 493: loss=0.0011, accuracy=1.0000, gradient_norm=0.0561, 
[2025-09-11 23:56:33,351][__main__][INFO] - Train, Round 494: loss=0.0011, accuracy=1.0000, gradient_norm=0.0553, 
[2025-09-11 23:56:33,351][__main__][INFO] - Train, Round 495: loss=0.0011, accuracy=1.0000, gradient_norm=0.0550, 
[2025-09-11 23:56:33,351][__main__][INFO] - Train, Round 496: loss=0.0011, accuracy=1.0000, gradient_norm=0.0550, 
[2025-09-11 23:56:33,351][__main__][INFO] - Train, Round 497: loss=0.0011, accuracy=1.0000, gradient_norm=0.0541, 
[2025-09-11 23:56:33,351][__main__][INFO] - Train, Round 498: loss=0.0011, accuracy=1.0000, gradient_norm=0.0558, 
[2025-09-11 23:56:33,351][__main__][INFO] - Train, Round 499: loss=0.0011, accuracy=1.0000, gradient_norm=0.0549, 
[2025-09-11 23:56:33,351][__main__][INFO] - Train, Round 500: loss=0.0011, accuracy=1.0000, gradient_norm=0.0560, 
[2025-09-11 23:56:33,351][__main__][INFO] - Test, Round 001: loss=2.3048, accuracy=0.1053, 
[2025-09-11 23:56:33,351][__main__][INFO] - Test, Round 002: loss=2.3038, accuracy=0.1106, 
[2025-09-11 23:56:33,351][__main__][INFO] - Test, Round 003: loss=2.3029, accuracy=0.1135, 
[2025-09-11 23:56:33,351][__main__][INFO] - Test, Round 004: loss=2.3021, accuracy=0.1207, 
[2025-09-11 23:56:33,351][__main__][INFO] - Test, Round 005: loss=2.3012, accuracy=0.1213, 
[2025-09-11 23:56:33,351][__main__][INFO] - Test, Round 006: loss=2.3004, accuracy=0.1230, 
[2025-09-11 23:56:33,351][__main__][INFO] - Test, Round 007: loss=2.2997, accuracy=0.1227, 
[2025-09-11 23:56:33,351][__main__][INFO] - Test, Round 008: loss=2.2989, accuracy=0.1254, 
[2025-09-11 23:56:33,351][__main__][INFO] - Test, Round 009: loss=2.2981, accuracy=0.1276, 
[2025-09-11 23:56:33,351][__main__][INFO] - Test, Round 010: loss=2.2973, accuracy=0.1279, 
[2025-09-11 23:56:33,351][__main__][INFO] - Test, Round 011: loss=2.2965, accuracy=0.1288, 
[2025-09-11 23:56:33,351][__main__][INFO] - Test, Round 012: loss=2.2957, accuracy=0.1295, 
[2025-09-11 23:56:33,351][__main__][INFO] - Test, Round 013: loss=2.2948, accuracy=0.1299, 
[2025-09-11 23:56:33,351][__main__][INFO] - Test, Round 014: loss=2.2939, accuracy=0.1310, 
[2025-09-11 23:56:33,351][__main__][INFO] - Test, Round 015: loss=2.2930, accuracy=0.1332, 
[2025-09-11 23:56:33,351][__main__][INFO] - Test, Round 016: loss=2.2919, accuracy=0.1354, 
[2025-09-11 23:56:33,351][__main__][INFO] - Test, Round 017: loss=2.2908, accuracy=0.1355, 
[2025-09-11 23:56:33,351][__main__][INFO] - Test, Round 018: loss=2.2896, accuracy=0.1371, 
[2025-09-11 23:56:33,351][__main__][INFO] - Test, Round 019: loss=2.2883, accuracy=0.1389, 
[2025-09-11 23:56:33,351][__main__][INFO] - Test, Round 020: loss=2.2868, accuracy=0.1404, 
[2025-09-11 23:56:33,351][__main__][INFO] - Test, Round 021: loss=2.2851, accuracy=0.1415, 
[2025-09-11 23:56:33,351][__main__][INFO] - Test, Round 022: loss=2.2831, accuracy=0.1440, 
[2025-09-11 23:56:33,351][__main__][INFO] - Test, Round 023: loss=2.2809, accuracy=0.1451, 
[2025-09-11 23:56:33,351][__main__][INFO] - Test, Round 024: loss=2.2783, accuracy=0.1499, 
[2025-09-11 23:56:33,351][__main__][INFO] - Test, Round 025: loss=2.2753, accuracy=0.1522, 
[2025-09-11 23:56:33,351][__main__][INFO] - Test, Round 026: loss=2.2719, accuracy=0.1572, 
[2025-09-11 23:56:33,351][__main__][INFO] - Test, Round 027: loss=2.2678, accuracy=0.1587, 
[2025-09-11 23:56:33,351][__main__][INFO] - Test, Round 028: loss=2.2631, accuracy=0.1603, 
[2025-09-11 23:56:33,351][__main__][INFO] - Test, Round 029: loss=2.2576, accuracy=0.1627, 
[2025-09-11 23:56:33,351][__main__][INFO] - Test, Round 030: loss=2.2512, accuracy=0.1657, 
[2025-09-11 23:56:33,351][__main__][INFO] - Test, Round 031: loss=2.2439, accuracy=0.1690, 
[2025-09-11 23:56:33,351][__main__][INFO] - Test, Round 032: loss=2.2357, accuracy=0.1717, 
[2025-09-11 23:56:33,351][__main__][INFO] - Test, Round 033: loss=2.2262, accuracy=0.1774, 
[2025-09-11 23:56:33,351][__main__][INFO] - Test, Round 034: loss=2.2158, accuracy=0.1813, 
[2025-09-11 23:56:33,351][__main__][INFO] - Test, Round 035: loss=2.2045, accuracy=0.1875, 
[2025-09-11 23:56:33,351][__main__][INFO] - Test, Round 036: loss=2.1924, accuracy=0.1928, 
[2025-09-11 23:56:33,351][__main__][INFO] - Test, Round 037: loss=2.1800, accuracy=0.2006, 
[2025-09-11 23:56:33,352][__main__][INFO] - Test, Round 038: loss=2.1669, accuracy=0.2078, 
[2025-09-11 23:56:33,352][__main__][INFO] - Test, Round 039: loss=2.1543, accuracy=0.2154, 
[2025-09-11 23:56:33,352][__main__][INFO] - Test, Round 040: loss=2.1411, accuracy=0.2218, 
[2025-09-11 23:56:33,352][__main__][INFO] - Test, Round 041: loss=2.1282, accuracy=0.2257, 
[2025-09-11 23:56:33,352][__main__][INFO] - Test, Round 042: loss=2.1160, accuracy=0.2330, 
[2025-09-11 23:56:33,352][__main__][INFO] - Test, Round 043: loss=2.1050, accuracy=0.2356, 
[2025-09-11 23:56:33,352][__main__][INFO] - Test, Round 044: loss=2.0916, accuracy=0.2451, 
[2025-09-11 23:56:33,352][__main__][INFO] - Test, Round 045: loss=2.0799, accuracy=0.2499, 
[2025-09-11 23:56:33,352][__main__][INFO] - Test, Round 046: loss=2.0683, accuracy=0.2602, 
[2025-09-11 23:56:33,352][__main__][INFO] - Test, Round 047: loss=2.0574, accuracy=0.2626, 
[2025-09-11 23:56:33,352][__main__][INFO] - Test, Round 048: loss=2.0469, accuracy=0.2661, 
[2025-09-11 23:56:33,352][__main__][INFO] - Test, Round 049: loss=2.0389, accuracy=0.2740, 
[2025-09-11 23:56:33,352][__main__][INFO] - Test, Round 050: loss=2.0319, accuracy=0.2694, 
[2025-09-11 23:56:33,352][__main__][INFO] - Test, Round 051: loss=2.0238, accuracy=0.2788, 
[2025-09-11 23:56:33,352][__main__][INFO] - Test, Round 052: loss=2.0170, accuracy=0.2748, 
[2025-09-11 23:56:33,352][__main__][INFO] - Test, Round 053: loss=2.0119, accuracy=0.2796, 
[2025-09-11 23:56:33,352][__main__][INFO] - Test, Round 054: loss=2.0050, accuracy=0.2794, 
[2025-09-11 23:56:33,352][__main__][INFO] - Test, Round 055: loss=2.0057, accuracy=0.2830, 
[2025-09-11 23:56:33,352][__main__][INFO] - Test, Round 056: loss=1.9985, accuracy=0.2851, 
[2025-09-11 23:56:33,352][__main__][INFO] - Test, Round 057: loss=2.0027, accuracy=0.2889, 
[2025-09-11 23:56:33,352][__main__][INFO] - Test, Round 058: loss=1.9919, accuracy=0.2903, 
[2025-09-11 23:56:33,352][__main__][INFO] - Test, Round 059: loss=1.9892, accuracy=0.2906, 
[2025-09-11 23:56:33,352][__main__][INFO] - Test, Round 060: loss=1.9877, accuracy=0.2916, 
[2025-09-11 23:56:33,352][__main__][INFO] - Test, Round 061: loss=1.9947, accuracy=0.2939, 
[2025-09-11 23:56:33,352][__main__][INFO] - Test, Round 062: loss=1.9938, accuracy=0.2948, 
[2025-09-11 23:56:33,352][__main__][INFO] - Test, Round 063: loss=1.9920, accuracy=0.2901, 
[2025-09-11 23:56:33,352][__main__][INFO] - Test, Round 064: loss=1.9840, accuracy=0.2956, 
[2025-09-11 23:56:33,352][__main__][INFO] - Test, Round 065: loss=1.9871, accuracy=0.2943, 
[2025-09-11 23:56:33,352][__main__][INFO] - Test, Round 066: loss=1.9905, accuracy=0.2963, 
[2025-09-11 23:56:33,352][__main__][INFO] - Test, Round 067: loss=1.9859, accuracy=0.2940, 
[2025-09-11 23:56:33,352][__main__][INFO] - Test, Round 068: loss=1.9906, accuracy=0.2942, 
[2025-09-11 23:56:33,352][__main__][INFO] - Test, Round 069: loss=1.9915, accuracy=0.3025, 
[2025-09-11 23:56:33,352][__main__][INFO] - Test, Round 070: loss=1.9929, accuracy=0.2989, 
[2025-09-11 23:56:33,352][__main__][INFO] - Test, Round 071: loss=1.9994, accuracy=0.2990, 
[2025-09-11 23:56:33,352][__main__][INFO] - Test, Round 072: loss=1.9995, accuracy=0.3015, 
[2025-09-11 23:56:33,352][__main__][INFO] - Test, Round 073: loss=1.9986, accuracy=0.3025, 
[2025-09-11 23:56:33,352][__main__][INFO] - Test, Round 074: loss=2.0006, accuracy=0.2996, 
[2025-09-11 23:56:33,352][__main__][INFO] - Test, Round 075: loss=2.0247, accuracy=0.3042, 
[2025-09-11 23:56:33,352][__main__][INFO] - Test, Round 076: loss=2.0184, accuracy=0.3003, 
[2025-09-11 23:56:33,352][__main__][INFO] - Test, Round 077: loss=2.0171, accuracy=0.3021, 
[2025-09-11 23:56:33,352][__main__][INFO] - Test, Round 078: loss=2.0232, accuracy=0.3005, 
[2025-09-11 23:56:33,352][__main__][INFO] - Test, Round 079: loss=2.0417, accuracy=0.3059, 
[2025-09-11 23:56:33,352][__main__][INFO] - Test, Round 080: loss=2.0512, accuracy=0.3050, 
[2025-09-11 23:56:33,352][__main__][INFO] - Test, Round 081: loss=2.0647, accuracy=0.3014, 
[2025-09-11 23:56:33,352][__main__][INFO] - Test, Round 082: loss=2.0707, accuracy=0.3062, 
[2025-09-11 23:56:33,352][__main__][INFO] - Test, Round 083: loss=2.0666, accuracy=0.3045, 
[2025-09-11 23:56:33,352][__main__][INFO] - Test, Round 084: loss=2.0937, accuracy=0.3024, 
[2025-09-11 23:56:33,352][__main__][INFO] - Test, Round 085: loss=2.1068, accuracy=0.3077, 
[2025-09-11 23:56:33,352][__main__][INFO] - Test, Round 086: loss=2.1152, accuracy=0.3111, 
[2025-09-11 23:56:33,352][__main__][INFO] - Test, Round 087: loss=2.1374, accuracy=0.3066, 
[2025-09-11 23:56:33,352][__main__][INFO] - Test, Round 088: loss=2.1356, accuracy=0.3159, 
[2025-09-11 23:56:33,352][__main__][INFO] - Test, Round 089: loss=2.1648, accuracy=0.3126, 
[2025-09-11 23:56:33,352][__main__][INFO] - Test, Round 090: loss=2.1917, accuracy=0.3055, 
[2025-09-11 23:56:33,352][__main__][INFO] - Test, Round 091: loss=2.2317, accuracy=0.3066, 
[2025-09-11 23:56:33,352][__main__][INFO] - Test, Round 092: loss=2.2248, accuracy=0.3087, 
[2025-09-11 23:56:33,352][__main__][INFO] - Test, Round 093: loss=2.2352, accuracy=0.3141, 
[2025-09-11 23:56:33,352][__main__][INFO] - Test, Round 094: loss=2.2713, accuracy=0.3098, 
[2025-09-11 23:56:33,352][__main__][INFO] - Test, Round 095: loss=2.2967, accuracy=0.3109, 
[2025-09-11 23:56:33,352][__main__][INFO] - Test, Round 096: loss=2.3449, accuracy=0.3097, 
[2025-09-11 23:56:33,352][__main__][INFO] - Test, Round 097: loss=2.3679, accuracy=0.3068, 
[2025-09-11 23:56:33,352][__main__][INFO] - Test, Round 098: loss=2.3845, accuracy=0.3090, 
[2025-09-11 23:56:33,352][__main__][INFO] - Test, Round 099: loss=2.4121, accuracy=0.3118, 
[2025-09-11 23:56:33,352][__main__][INFO] - Test, Round 100: loss=2.4577, accuracy=0.3057, 
[2025-09-11 23:56:33,352][__main__][INFO] - Test, Round 101: loss=2.5143, accuracy=0.3082, 
[2025-09-11 23:56:33,352][__main__][INFO] - Test, Round 102: loss=2.5489, accuracy=0.3101, 
[2025-09-11 23:56:33,352][__main__][INFO] - Test, Round 103: loss=2.5461, accuracy=0.3108, 
[2025-09-11 23:56:33,352][__main__][INFO] - Test, Round 104: loss=2.5823, accuracy=0.3094, 
[2025-09-11 23:56:33,352][__main__][INFO] - Test, Round 105: loss=2.6302, accuracy=0.3136, 
[2025-09-11 23:56:33,352][__main__][INFO] - Test, Round 106: loss=2.7023, accuracy=0.3059, 
[2025-09-11 23:56:33,352][__main__][INFO] - Test, Round 107: loss=2.7363, accuracy=0.3085, 
[2025-09-11 23:56:33,352][__main__][INFO] - Test, Round 108: loss=2.8051, accuracy=0.3115, 
[2025-09-11 23:56:33,352][__main__][INFO] - Test, Round 109: loss=2.8055, accuracy=0.3092, 
[2025-09-11 23:56:33,352][__main__][INFO] - Test, Round 110: loss=2.8902, accuracy=0.3058, 
[2025-09-11 23:56:33,352][__main__][INFO] - Test, Round 111: loss=2.9933, accuracy=0.3047, 
[2025-09-11 23:56:33,352][__main__][INFO] - Test, Round 112: loss=2.9332, accuracy=0.3120, 
[2025-09-11 23:56:33,352][__main__][INFO] - Test, Round 113: loss=3.0239, accuracy=0.3134, 
[2025-09-11 23:56:33,352][__main__][INFO] - Test, Round 114: loss=3.0897, accuracy=0.3091, 
[2025-09-11 23:56:33,352][__main__][INFO] - Test, Round 115: loss=3.1262, accuracy=0.3073, 
[2025-09-11 23:56:33,352][__main__][INFO] - Test, Round 116: loss=3.1915, accuracy=0.3121, 
[2025-09-11 23:56:33,352][__main__][INFO] - Test, Round 117: loss=3.2293, accuracy=0.3084, 
[2025-09-11 23:56:33,352][__main__][INFO] - Test, Round 118: loss=3.3053, accuracy=0.3129, 
[2025-09-11 23:56:33,352][__main__][INFO] - Test, Round 119: loss=3.3509, accuracy=0.3117, 
[2025-09-11 23:56:33,352][__main__][INFO] - Test, Round 120: loss=3.3911, accuracy=0.3130, 
[2025-09-11 23:56:33,352][__main__][INFO] - Test, Round 121: loss=3.4458, accuracy=0.3094, 
[2025-09-11 23:56:33,352][__main__][INFO] - Test, Round 122: loss=3.4808, accuracy=0.3138, 
[2025-09-11 23:56:33,352][__main__][INFO] - Test, Round 123: loss=3.5426, accuracy=0.3138, 
[2025-09-11 23:56:33,352][__main__][INFO] - Test, Round 124: loss=3.6505, accuracy=0.3102, 
[2025-09-11 23:56:33,353][__main__][INFO] - Test, Round 125: loss=3.6817, accuracy=0.3128, 
[2025-09-11 23:56:33,353][__main__][INFO] - Test, Round 126: loss=3.7462, accuracy=0.3148, 
[2025-09-11 23:56:33,353][__main__][INFO] - Test, Round 127: loss=3.8072, accuracy=0.3132, 
[2025-09-11 23:56:33,353][__main__][INFO] - Test, Round 128: loss=3.8917, accuracy=0.3088, 
[2025-09-11 23:56:33,353][__main__][INFO] - Test, Round 129: loss=3.9051, accuracy=0.3088, 
[2025-09-11 23:56:33,353][__main__][INFO] - Test, Round 130: loss=3.9718, accuracy=0.3129, 
[2025-09-11 23:56:33,353][__main__][INFO] - Test, Round 131: loss=4.0413, accuracy=0.3134, 
[2025-09-11 23:56:33,353][__main__][INFO] - Test, Round 132: loss=4.0414, accuracy=0.3128, 
[2025-09-11 23:56:33,353][__main__][INFO] - Test, Round 133: loss=4.1129, accuracy=0.3126, 
[2025-09-11 23:56:33,353][__main__][INFO] - Test, Round 134: loss=4.1634, accuracy=0.3135, 
[2025-09-11 23:56:33,353][__main__][INFO] - Test, Round 135: loss=4.2065, accuracy=0.3114, 
[2025-09-11 23:56:33,353][__main__][INFO] - Test, Round 136: loss=4.2534, accuracy=0.3131, 
[2025-09-11 23:56:33,353][__main__][INFO] - Test, Round 137: loss=4.3051, accuracy=0.3130, 
[2025-09-11 23:56:33,353][__main__][INFO] - Test, Round 138: loss=4.3689, accuracy=0.3116, 
[2025-09-11 23:56:33,353][__main__][INFO] - Test, Round 139: loss=4.3736, accuracy=0.3113, 
[2025-09-11 23:56:33,353][__main__][INFO] - Test, Round 140: loss=4.4248, accuracy=0.3104, 
[2025-09-11 23:56:33,353][__main__][INFO] - Test, Round 141: loss=4.4710, accuracy=0.3126, 
[2025-09-11 23:56:33,353][__main__][INFO] - Test, Round 142: loss=4.5137, accuracy=0.3112, 
[2025-09-11 23:56:33,353][__main__][INFO] - Test, Round 143: loss=4.5508, accuracy=0.3133, 
[2025-09-11 23:56:33,353][__main__][INFO] - Test, Round 144: loss=4.5904, accuracy=0.3107, 
[2025-09-11 23:56:33,353][__main__][INFO] - Test, Round 145: loss=4.6241, accuracy=0.3119, 
[2025-09-11 23:56:33,353][__main__][INFO] - Test, Round 146: loss=4.6645, accuracy=0.3108, 
[2025-09-11 23:56:33,353][__main__][INFO] - Test, Round 147: loss=4.6996, accuracy=0.3104, 
[2025-09-11 23:56:33,353][__main__][INFO] - Test, Round 148: loss=4.7338, accuracy=0.3103, 
[2025-09-11 23:56:33,353][__main__][INFO] - Test, Round 149: loss=4.7562, accuracy=0.3103, 
[2025-09-11 23:56:33,353][__main__][INFO] - Test, Round 150: loss=4.7887, accuracy=0.3103, 
[2025-09-11 23:56:33,353][__main__][INFO] - Test, Round 151: loss=4.8234, accuracy=0.3098, 
[2025-09-11 23:56:33,353][__main__][INFO] - Test, Round 152: loss=4.8524, accuracy=0.3095, 
[2025-09-11 23:56:33,353][__main__][INFO] - Test, Round 153: loss=4.8815, accuracy=0.3110, 
[2025-09-11 23:56:33,353][__main__][INFO] - Test, Round 154: loss=4.9136, accuracy=0.3111, 
[2025-09-11 23:56:33,353][__main__][INFO] - Test, Round 155: loss=4.9252, accuracy=0.3105, 
[2025-09-11 23:56:33,353][__main__][INFO] - Test, Round 156: loss=4.9440, accuracy=0.3114, 
[2025-09-11 23:56:33,353][__main__][INFO] - Test, Round 157: loss=4.9723, accuracy=0.3111, 
[2025-09-11 23:56:33,353][__main__][INFO] - Test, Round 158: loss=4.9994, accuracy=0.3110, 
[2025-09-11 23:56:33,353][__main__][INFO] - Test, Round 159: loss=5.0288, accuracy=0.3109, 
[2025-09-11 23:56:33,353][__main__][INFO] - Test, Round 160: loss=5.0542, accuracy=0.3096, 
[2025-09-11 23:56:33,353][__main__][INFO] - Test, Round 161: loss=5.0771, accuracy=0.3090, 
[2025-09-11 23:56:33,353][__main__][INFO] - Test, Round 162: loss=5.1030, accuracy=0.3089, 
[2025-09-11 23:56:33,353][__main__][INFO] - Test, Round 163: loss=5.1241, accuracy=0.3092, 
[2025-09-11 23:56:33,353][__main__][INFO] - Test, Round 164: loss=5.1459, accuracy=0.3102, 
[2025-09-11 23:56:33,353][__main__][INFO] - Test, Round 165: loss=5.1700, accuracy=0.3106, 
[2025-09-11 23:56:33,353][__main__][INFO] - Test, Round 166: loss=5.1884, accuracy=0.3104, 
[2025-09-11 23:56:33,353][__main__][INFO] - Test, Round 167: loss=5.2098, accuracy=0.3096, 
[2025-09-11 23:56:33,353][__main__][INFO] - Test, Round 168: loss=5.2303, accuracy=0.3093, 
[2025-09-11 23:56:33,353][__main__][INFO] - Test, Round 169: loss=5.2518, accuracy=0.3082, 
[2025-09-11 23:56:33,353][__main__][INFO] - Test, Round 170: loss=5.2704, accuracy=0.3102, 
[2025-09-11 23:56:33,353][__main__][INFO] - Test, Round 171: loss=5.2903, accuracy=0.3092, 
[2025-09-11 23:56:33,353][__main__][INFO] - Test, Round 172: loss=5.3071, accuracy=0.3093, 
[2025-09-11 23:56:33,353][__main__][INFO] - Test, Round 173: loss=5.3252, accuracy=0.3095, 
[2025-09-11 23:56:33,353][__main__][INFO] - Test, Round 174: loss=5.3442, accuracy=0.3091, 
[2025-09-11 23:56:33,353][__main__][INFO] - Test, Round 175: loss=5.3610, accuracy=0.3098, 
[2025-09-11 23:56:33,353][__main__][INFO] - Test, Round 176: loss=5.3752, accuracy=0.3106, 
[2025-09-11 23:56:33,353][__main__][INFO] - Test, Round 177: loss=5.3928, accuracy=0.3094, 
[2025-09-11 23:56:33,353][__main__][INFO] - Test, Round 178: loss=5.4096, accuracy=0.3095, 
[2025-09-11 23:56:33,353][__main__][INFO] - Test, Round 179: loss=5.4265, accuracy=0.3095, 
[2025-09-11 23:56:33,353][__main__][INFO] - Test, Round 180: loss=5.4411, accuracy=0.3087, 
[2025-09-11 23:56:33,353][__main__][INFO] - Test, Round 181: loss=5.4569, accuracy=0.3087, 
[2025-09-11 23:56:33,353][__main__][INFO] - Test, Round 182: loss=5.4716, accuracy=0.3096, 
[2025-09-11 23:56:33,353][__main__][INFO] - Test, Round 183: loss=5.4860, accuracy=0.3089, 
[2025-09-11 23:56:33,353][__main__][INFO] - Test, Round 184: loss=5.5012, accuracy=0.3094, 
[2025-09-11 23:56:33,353][__main__][INFO] - Test, Round 185: loss=5.5148, accuracy=0.3091, 
[2025-09-11 23:56:33,353][__main__][INFO] - Test, Round 186: loss=5.5312, accuracy=0.3089, 
[2025-09-11 23:56:33,353][__main__][INFO] - Test, Round 187: loss=5.5444, accuracy=0.3108, 
[2025-09-11 23:56:33,353][__main__][INFO] - Test, Round 188: loss=5.5569, accuracy=0.3100, 
[2025-09-11 23:56:33,353][__main__][INFO] - Test, Round 189: loss=5.5702, accuracy=0.3087, 
[2025-09-11 23:56:33,353][__main__][INFO] - Test, Round 190: loss=5.5826, accuracy=0.3104, 
[2025-09-11 23:56:33,353][__main__][INFO] - Test, Round 191: loss=5.5956, accuracy=0.3092, 
[2025-09-11 23:56:33,353][__main__][INFO] - Test, Round 192: loss=5.6081, accuracy=0.3087, 
[2025-09-11 23:56:33,353][__main__][INFO] - Test, Round 193: loss=5.6190, accuracy=0.3098, 
[2025-09-11 23:56:33,353][__main__][INFO] - Test, Round 194: loss=5.6330, accuracy=0.3097, 
[2025-09-11 23:56:33,353][__main__][INFO] - Test, Round 195: loss=5.6435, accuracy=0.3088, 
[2025-09-11 23:56:33,353][__main__][INFO] - Test, Round 196: loss=5.6555, accuracy=0.3080, 
[2025-09-11 23:56:33,353][__main__][INFO] - Test, Round 197: loss=5.6689, accuracy=0.3096, 
[2025-09-11 23:56:33,353][__main__][INFO] - Test, Round 198: loss=5.6787, accuracy=0.3090, 
[2025-09-11 23:56:33,353][__main__][INFO] - Test, Round 199: loss=5.6899, accuracy=0.3090, 
[2025-09-11 23:56:33,353][__main__][INFO] - Test, Round 200: loss=5.7002, accuracy=0.3087, 
[2025-09-11 23:56:33,353][__main__][INFO] - Test, Round 201: loss=5.7112, accuracy=0.3089, 
[2025-09-11 23:56:33,353][__main__][INFO] - Test, Round 202: loss=5.7218, accuracy=0.3093, 
[2025-09-11 23:56:33,353][__main__][INFO] - Test, Round 203: loss=5.7326, accuracy=0.3089, 
[2025-09-11 23:56:33,353][__main__][INFO] - Test, Round 204: loss=5.7431, accuracy=0.3090, 
[2025-09-11 23:56:33,353][__main__][INFO] - Test, Round 205: loss=5.7526, accuracy=0.3099, 
[2025-09-11 23:56:33,353][__main__][INFO] - Test, Round 206: loss=5.7640, accuracy=0.3094, 
[2025-09-11 23:56:33,353][__main__][INFO] - Test, Round 207: loss=5.7740, accuracy=0.3084, 
[2025-09-11 23:56:33,353][__main__][INFO] - Test, Round 208: loss=5.7830, accuracy=0.3089, 
[2025-09-11 23:56:33,353][__main__][INFO] - Test, Round 209: loss=5.7926, accuracy=0.3090, 
[2025-09-11 23:56:33,354][__main__][INFO] - Test, Round 210: loss=5.8026, accuracy=0.3092, 
[2025-09-11 23:56:33,354][__main__][INFO] - Test, Round 211: loss=5.8119, accuracy=0.3083, 
[2025-09-11 23:56:33,354][__main__][INFO] - Test, Round 212: loss=5.8212, accuracy=0.3087, 
[2025-09-11 23:56:33,354][__main__][INFO] - Test, Round 213: loss=5.8297, accuracy=0.3080, 
[2025-09-11 23:56:33,354][__main__][INFO] - Test, Round 214: loss=5.8394, accuracy=0.3089, 
[2025-09-11 23:56:33,354][__main__][INFO] - Test, Round 215: loss=5.8479, accuracy=0.3084, 
[2025-09-11 23:56:33,354][__main__][INFO] - Test, Round 216: loss=5.8581, accuracy=0.3076, 
[2025-09-11 23:56:33,354][__main__][INFO] - Test, Round 217: loss=5.8667, accuracy=0.3093, 
[2025-09-11 23:56:33,354][__main__][INFO] - Test, Round 218: loss=5.8742, accuracy=0.3084, 
[2025-09-11 23:56:33,354][__main__][INFO] - Test, Round 219: loss=5.8821, accuracy=0.3085, 
[2025-09-11 23:56:33,354][__main__][INFO] - Test, Round 220: loss=5.8909, accuracy=0.3079, 
[2025-09-11 23:56:33,354][__main__][INFO] - Test, Round 221: loss=5.8989, accuracy=0.3085, 
[2025-09-11 23:56:33,354][__main__][INFO] - Test, Round 222: loss=5.9067, accuracy=0.3077, 
[2025-09-11 23:56:33,354][__main__][INFO] - Test, Round 223: loss=5.9153, accuracy=0.3081, 
[2025-09-11 23:56:33,354][__main__][INFO] - Test, Round 224: loss=5.9237, accuracy=0.3075, 
[2025-09-11 23:56:33,354][__main__][INFO] - Test, Round 225: loss=5.9310, accuracy=0.3079, 
[2025-09-11 23:56:33,354][__main__][INFO] - Test, Round 226: loss=5.9387, accuracy=0.3081, 
[2025-09-11 23:56:33,354][__main__][INFO] - Test, Round 227: loss=5.9456, accuracy=0.3083, 
[2025-09-11 23:56:33,354][__main__][INFO] - Test, Round 228: loss=5.9538, accuracy=0.3080, 
[2025-09-11 23:56:33,354][__main__][INFO] - Test, Round 229: loss=5.9618, accuracy=0.3080, 
[2025-09-11 23:56:33,354][__main__][INFO] - Test, Round 230: loss=5.9683, accuracy=0.3079, 
[2025-09-11 23:56:33,354][__main__][INFO] - Test, Round 231: loss=5.9767, accuracy=0.3080, 
[2025-09-11 23:56:33,354][__main__][INFO] - Test, Round 232: loss=5.9826, accuracy=0.3078, 
[2025-09-11 23:56:33,354][__main__][INFO] - Test, Round 233: loss=5.9897, accuracy=0.3085, 
[2025-09-11 23:56:33,354][__main__][INFO] - Test, Round 234: loss=5.9973, accuracy=0.3077, 
[2025-09-11 23:56:33,354][__main__][INFO] - Test, Round 235: loss=6.0043, accuracy=0.3080, 
[2025-09-11 23:56:33,354][__main__][INFO] - Test, Round 236: loss=6.0110, accuracy=0.3074, 
[2025-09-11 23:56:33,354][__main__][INFO] - Test, Round 237: loss=6.0171, accuracy=0.3091, 
[2025-09-11 23:56:33,354][__main__][INFO] - Test, Round 238: loss=6.0237, accuracy=0.3080, 
[2025-09-11 23:56:33,354][__main__][INFO] - Test, Round 239: loss=6.0310, accuracy=0.3081, 
[2025-09-11 23:56:33,354][__main__][INFO] - Test, Round 240: loss=6.0380, accuracy=0.3084, 
[2025-09-11 23:56:33,354][__main__][INFO] - Test, Round 241: loss=6.0443, accuracy=0.3081, 
[2025-09-11 23:56:33,354][__main__][INFO] - Test, Round 242: loss=6.0510, accuracy=0.3085, 
[2025-09-11 23:56:33,354][__main__][INFO] - Test, Round 243: loss=6.0574, accuracy=0.3079, 
[2025-09-11 23:56:33,354][__main__][INFO] - Test, Round 244: loss=6.0629, accuracy=0.3085, 
[2025-09-11 23:56:33,354][__main__][INFO] - Test, Round 245: loss=6.0692, accuracy=0.3081, 
[2025-09-11 23:56:33,354][__main__][INFO] - Test, Round 246: loss=6.0746, accuracy=0.3079, 
[2025-09-11 23:56:33,354][__main__][INFO] - Test, Round 247: loss=6.0808, accuracy=0.3081, 
[2025-09-11 23:56:33,354][__main__][INFO] - Test, Round 248: loss=6.0879, accuracy=0.3079, 
[2025-09-11 23:56:33,354][__main__][INFO] - Test, Round 249: loss=6.0920, accuracy=0.3082, 
[2025-09-11 23:56:33,354][__main__][INFO] - Test, Round 250: loss=6.0990, accuracy=0.3086, 
[2025-09-11 23:56:33,354][__main__][INFO] - Test, Round 251: loss=6.1045, accuracy=0.3076, 
[2025-09-11 23:56:33,354][__main__][INFO] - Test, Round 252: loss=6.1100, accuracy=0.3079, 
[2025-09-11 23:56:33,354][__main__][INFO] - Test, Round 253: loss=6.1156, accuracy=0.3077, 
[2025-09-11 23:56:33,354][__main__][INFO] - Test, Round 254: loss=6.1209, accuracy=0.3078, 
[2025-09-11 23:56:33,354][__main__][INFO] - Test, Round 255: loss=6.1271, accuracy=0.3076, 
[2025-09-11 23:56:33,354][__main__][INFO] - Test, Round 256: loss=6.1329, accuracy=0.3078, 
[2025-09-11 23:56:33,354][__main__][INFO] - Test, Round 257: loss=6.1376, accuracy=0.3081, 
[2025-09-11 23:56:33,354][__main__][INFO] - Test, Round 258: loss=6.1429, accuracy=0.3079, 
[2025-09-11 23:56:33,354][__main__][INFO] - Test, Round 259: loss=6.1478, accuracy=0.3080, 
[2025-09-11 23:56:33,354][__main__][INFO] - Test, Round 260: loss=6.1537, accuracy=0.3082, 
[2025-09-11 23:56:33,354][__main__][INFO] - Test, Round 261: loss=6.1588, accuracy=0.3076, 
[2025-09-11 23:56:33,354][__main__][INFO] - Test, Round 262: loss=6.1642, accuracy=0.3079, 
[2025-09-11 23:56:33,354][__main__][INFO] - Test, Round 263: loss=6.1689, accuracy=0.3078, 
[2025-09-11 23:56:33,354][__main__][INFO] - Test, Round 264: loss=6.1747, accuracy=0.3073, 
[2025-09-11 23:56:33,354][__main__][INFO] - Test, Round 265: loss=6.1786, accuracy=0.3074, 
[2025-09-11 23:56:33,354][__main__][INFO] - Test, Round 266: loss=6.1839, accuracy=0.3081, 
[2025-09-11 23:56:33,354][__main__][INFO] - Test, Round 267: loss=6.1889, accuracy=0.3074, 
[2025-09-11 23:56:33,354][__main__][INFO] - Test, Round 268: loss=6.1942, accuracy=0.3074, 
[2025-09-11 23:56:33,354][__main__][INFO] - Test, Round 269: loss=6.1983, accuracy=0.3073, 
[2025-09-11 23:56:33,354][__main__][INFO] - Test, Round 270: loss=6.2036, accuracy=0.3085, 
[2025-09-11 23:56:33,354][__main__][INFO] - Test, Round 271: loss=6.2083, accuracy=0.3080, 
[2025-09-11 23:56:33,354][__main__][INFO] - Test, Round 272: loss=6.2121, accuracy=0.3083, 
[2025-09-11 23:56:33,354][__main__][INFO] - Test, Round 273: loss=6.2172, accuracy=0.3083, 
[2025-09-11 23:56:33,354][__main__][INFO] - Test, Round 274: loss=6.2215, accuracy=0.3080, 
[2025-09-11 23:56:33,354][__main__][INFO] - Test, Round 275: loss=6.2261, accuracy=0.3079, 
[2025-09-11 23:56:33,354][__main__][INFO] - Test, Round 276: loss=6.2310, accuracy=0.3080, 
[2025-09-11 23:56:33,354][__main__][INFO] - Test, Round 277: loss=6.2349, accuracy=0.3081, 
[2025-09-11 23:56:33,354][__main__][INFO] - Test, Round 278: loss=6.2390, accuracy=0.3080, 
[2025-09-11 23:56:33,354][__main__][INFO] - Test, Round 279: loss=6.2433, accuracy=0.3080, 
[2025-09-11 23:56:33,354][__main__][INFO] - Test, Round 280: loss=6.2475, accuracy=0.3079, 
[2025-09-11 23:56:33,354][__main__][INFO] - Test, Round 281: loss=6.2524, accuracy=0.3078, 
[2025-09-11 23:56:33,354][__main__][INFO] - Test, Round 282: loss=6.2563, accuracy=0.3079, 
[2025-09-11 23:56:33,354][__main__][INFO] - Test, Round 283: loss=6.2602, accuracy=0.3081, 
[2025-09-11 23:56:33,354][__main__][INFO] - Test, Round 284: loss=6.2650, accuracy=0.3079, 
[2025-09-11 23:56:33,354][__main__][INFO] - Test, Round 285: loss=6.2687, accuracy=0.3079, 
[2025-09-11 23:56:33,354][__main__][INFO] - Test, Round 286: loss=6.2730, accuracy=0.3082, 
[2025-09-11 23:56:33,354][__main__][INFO] - Test, Round 287: loss=6.2770, accuracy=0.3082, 
[2025-09-11 23:56:33,354][__main__][INFO] - Test, Round 288: loss=6.2810, accuracy=0.3076, 
[2025-09-11 23:56:33,354][__main__][INFO] - Test, Round 289: loss=6.2847, accuracy=0.3081, 
[2025-09-11 23:56:33,354][__main__][INFO] - Test, Round 290: loss=6.2883, accuracy=0.3077, 
[2025-09-11 23:56:33,354][__main__][INFO] - Test, Round 291: loss=6.2919, accuracy=0.3077, 
[2025-09-11 23:56:33,354][__main__][INFO] - Test, Round 292: loss=6.2963, accuracy=0.3073, 
[2025-09-11 23:56:33,354][__main__][INFO] - Test, Round 293: loss=6.3001, accuracy=0.3082, 
[2025-09-11 23:56:33,354][__main__][INFO] - Test, Round 294: loss=6.3039, accuracy=0.3082, 
[2025-09-11 23:56:33,354][__main__][INFO] - Test, Round 295: loss=6.3069, accuracy=0.3078, 
[2025-09-11 23:56:33,355][__main__][INFO] - Test, Round 296: loss=6.3113, accuracy=0.3086, 
[2025-09-11 23:56:33,355][__main__][INFO] - Test, Round 297: loss=6.3154, accuracy=0.3073, 
[2025-09-11 23:56:33,355][__main__][INFO] - Test, Round 298: loss=6.3181, accuracy=0.3078, 
[2025-09-11 23:56:33,355][__main__][INFO] - Test, Round 299: loss=6.3221, accuracy=0.3075, 
[2025-09-11 23:56:33,355][__main__][INFO] - Test, Round 300: loss=6.3258, accuracy=0.3077, 
[2025-09-11 23:56:33,355][__main__][INFO] - Test, Round 301: loss=6.3288, accuracy=0.3076, 
[2025-09-11 23:56:33,355][__main__][INFO] - Test, Round 302: loss=6.3325, accuracy=0.3081, 
[2025-09-11 23:56:33,355][__main__][INFO] - Test, Round 303: loss=6.3359, accuracy=0.3070, 
[2025-09-11 23:56:33,355][__main__][INFO] - Test, Round 304: loss=6.3392, accuracy=0.3074, 
[2025-09-11 23:56:33,355][__main__][INFO] - Test, Round 305: loss=6.3436, accuracy=0.3070, 
[2025-09-11 23:56:33,355][__main__][INFO] - Test, Round 306: loss=6.3462, accuracy=0.3075, 
[2025-09-11 23:56:33,355][__main__][INFO] - Test, Round 307: loss=6.3500, accuracy=0.3072, 
[2025-09-11 23:56:33,355][__main__][INFO] - Test, Round 308: loss=6.3533, accuracy=0.3075, 
[2025-09-11 23:56:33,355][__main__][INFO] - Test, Round 309: loss=6.3570, accuracy=0.3080, 
[2025-09-11 23:56:33,355][__main__][INFO] - Test, Round 310: loss=6.3595, accuracy=0.3079, 
[2025-09-11 23:56:33,355][__main__][INFO] - Test, Round 311: loss=6.3626, accuracy=0.3073, 
[2025-09-11 23:56:33,355][__main__][INFO] - Test, Round 312: loss=6.3655, accuracy=0.3076, 
[2025-09-11 23:56:33,355][__main__][INFO] - Test, Round 313: loss=6.3687, accuracy=0.3076, 
[2025-09-11 23:56:33,355][__main__][INFO] - Test, Round 314: loss=6.3721, accuracy=0.3072, 
[2025-09-11 23:56:33,355][__main__][INFO] - Test, Round 315: loss=6.3752, accuracy=0.3076, 
[2025-09-11 23:56:33,355][__main__][INFO] - Test, Round 316: loss=6.3791, accuracy=0.3071, 
[2025-09-11 23:56:33,355][__main__][INFO] - Test, Round 317: loss=6.3819, accuracy=0.3070, 
[2025-09-11 23:56:33,355][__main__][INFO] - Test, Round 318: loss=6.3849, accuracy=0.3070, 
[2025-09-11 23:56:33,355][__main__][INFO] - Test, Round 319: loss=6.3875, accuracy=0.3073, 
[2025-09-11 23:56:33,355][__main__][INFO] - Test, Round 320: loss=6.3906, accuracy=0.3072, 
[2025-09-11 23:56:33,355][__main__][INFO] - Test, Round 321: loss=6.3934, accuracy=0.3075, 
[2025-09-11 23:56:33,355][__main__][INFO] - Test, Round 322: loss=6.3970, accuracy=0.3075, 
[2025-09-11 23:56:33,355][__main__][INFO] - Test, Round 323: loss=6.3994, accuracy=0.3072, 
[2025-09-11 23:56:33,355][__main__][INFO] - Test, Round 324: loss=6.4021, accuracy=0.3071, 
[2025-09-11 23:56:33,355][__main__][INFO] - Test, Round 325: loss=6.4051, accuracy=0.3074, 
[2025-09-11 23:56:33,355][__main__][INFO] - Test, Round 326: loss=6.4074, accuracy=0.3070, 
[2025-09-11 23:56:33,355][__main__][INFO] - Test, Round 327: loss=6.4106, accuracy=0.3072, 
[2025-09-11 23:56:33,355][__main__][INFO] - Test, Round 328: loss=6.4137, accuracy=0.3070, 
[2025-09-11 23:56:33,355][__main__][INFO] - Test, Round 329: loss=6.4168, accuracy=0.3070, 
[2025-09-11 23:56:33,355][__main__][INFO] - Test, Round 330: loss=6.4193, accuracy=0.3068, 
[2025-09-11 23:56:33,355][__main__][INFO] - Test, Round 331: loss=6.4216, accuracy=0.3072, 
[2025-09-11 23:56:33,355][__main__][INFO] - Test, Round 332: loss=6.4246, accuracy=0.3067, 
[2025-09-11 23:56:33,355][__main__][INFO] - Test, Round 333: loss=6.4273, accuracy=0.3074, 
[2025-09-11 23:56:33,355][__main__][INFO] - Test, Round 334: loss=6.4296, accuracy=0.3074, 
[2025-09-11 23:56:33,355][__main__][INFO] - Test, Round 335: loss=6.4325, accuracy=0.3066, 
[2025-09-11 23:56:33,355][__main__][INFO] - Test, Round 336: loss=6.4347, accuracy=0.3070, 
[2025-09-11 23:56:33,355][__main__][INFO] - Test, Round 337: loss=6.4375, accuracy=0.3067, 
[2025-09-11 23:56:33,355][__main__][INFO] - Test, Round 338: loss=6.4403, accuracy=0.3068, 
[2025-09-11 23:56:33,355][__main__][INFO] - Test, Round 339: loss=6.4420, accuracy=0.3074, 
[2025-09-11 23:56:33,355][__main__][INFO] - Test, Round 340: loss=6.4450, accuracy=0.3070, 
[2025-09-11 23:56:33,355][__main__][INFO] - Test, Round 341: loss=6.4477, accuracy=0.3072, 
[2025-09-11 23:56:33,355][__main__][INFO] - Test, Round 342: loss=6.4500, accuracy=0.3065, 
[2025-09-11 23:56:33,355][__main__][INFO] - Test, Round 343: loss=6.4525, accuracy=0.3072, 
[2025-09-11 23:56:33,355][__main__][INFO] - Test, Round 344: loss=6.4552, accuracy=0.3069, 
[2025-09-11 23:56:33,355][__main__][INFO] - Test, Round 345: loss=6.4572, accuracy=0.3074, 
[2025-09-11 23:56:33,355][__main__][INFO] - Test, Round 346: loss=6.4599, accuracy=0.3073, 
[2025-09-11 23:56:33,355][__main__][INFO] - Test, Round 347: loss=6.4614, accuracy=0.3071, 
[2025-09-11 23:56:33,355][__main__][INFO] - Test, Round 348: loss=6.4642, accuracy=0.3069, 
[2025-09-11 23:56:33,355][__main__][INFO] - Test, Round 349: loss=6.4668, accuracy=0.3074, 
[2025-09-11 23:56:33,355][__main__][INFO] - Test, Round 350: loss=6.4694, accuracy=0.3069, 
[2025-09-11 23:56:33,355][__main__][INFO] - Test, Round 351: loss=6.4713, accuracy=0.3069, 
[2025-09-11 23:56:33,355][__main__][INFO] - Test, Round 352: loss=6.4738, accuracy=0.3066, 
[2025-09-11 23:56:33,355][__main__][INFO] - Test, Round 353: loss=6.4754, accuracy=0.3067, 
[2025-09-11 23:56:33,355][__main__][INFO] - Test, Round 354: loss=6.4781, accuracy=0.3067, 
[2025-09-11 23:56:33,355][__main__][INFO] - Test, Round 355: loss=6.4806, accuracy=0.3066, 
[2025-09-11 23:56:33,355][__main__][INFO] - Test, Round 356: loss=6.4828, accuracy=0.3067, 
[2025-09-11 23:56:33,355][__main__][INFO] - Test, Round 357: loss=6.4842, accuracy=0.3068, 
[2025-09-11 23:56:33,355][__main__][INFO] - Test, Round 358: loss=6.4868, accuracy=0.3068, 
[2025-09-11 23:56:33,355][__main__][INFO] - Test, Round 359: loss=6.4887, accuracy=0.3068, 
[2025-09-11 23:56:33,355][__main__][INFO] - Test, Round 360: loss=6.4913, accuracy=0.3069, 
[2025-09-11 23:56:33,355][__main__][INFO] - Test, Round 361: loss=6.4926, accuracy=0.3064, 
[2025-09-11 23:56:33,355][__main__][INFO] - Test, Round 362: loss=6.4951, accuracy=0.3067, 
[2025-09-11 23:56:33,355][__main__][INFO] - Test, Round 363: loss=6.4971, accuracy=0.3068, 
[2025-09-11 23:56:33,355][__main__][INFO] - Test, Round 364: loss=6.4992, accuracy=0.3069, 
[2025-09-11 23:56:33,355][__main__][INFO] - Test, Round 365: loss=6.5014, accuracy=0.3071, 
[2025-09-11 23:56:33,355][__main__][INFO] - Test, Round 366: loss=6.5033, accuracy=0.3069, 
[2025-09-11 23:56:33,355][__main__][INFO] - Test, Round 367: loss=6.5047, accuracy=0.3069, 
[2025-09-11 23:56:33,355][__main__][INFO] - Test, Round 368: loss=6.5077, accuracy=0.3068, 
[2025-09-11 23:56:33,355][__main__][INFO] - Test, Round 369: loss=6.5095, accuracy=0.3066, 
[2025-09-11 23:56:33,355][__main__][INFO] - Test, Round 370: loss=6.5112, accuracy=0.3066, 
[2025-09-11 23:56:33,355][__main__][INFO] - Test, Round 371: loss=6.5132, accuracy=0.3063, 
[2025-09-11 23:56:33,355][__main__][INFO] - Test, Round 372: loss=6.5148, accuracy=0.3067, 
[2025-09-11 23:56:33,355][__main__][INFO] - Test, Round 373: loss=6.5170, accuracy=0.3073, 
[2025-09-11 23:56:33,355][__main__][INFO] - Test, Round 374: loss=6.5188, accuracy=0.3072, 
[2025-09-11 23:56:33,355][__main__][INFO] - Test, Round 375: loss=6.5210, accuracy=0.3063, 
[2025-09-11 23:56:33,355][__main__][INFO] - Test, Round 376: loss=6.5226, accuracy=0.3066, 
[2025-09-11 23:56:33,355][__main__][INFO] - Test, Round 377: loss=6.5243, accuracy=0.3063, 
[2025-09-11 23:56:33,355][__main__][INFO] - Test, Round 378: loss=6.5263, accuracy=0.3065, 
[2025-09-11 23:56:33,355][__main__][INFO] - Test, Round 379: loss=6.5280, accuracy=0.3067, 
[2025-09-11 23:56:33,355][__main__][INFO] - Test, Round 380: loss=6.5297, accuracy=0.3066, 
[2025-09-11 23:56:33,356][__main__][INFO] - Test, Round 381: loss=6.5315, accuracy=0.3065, 
[2025-09-11 23:56:33,356][__main__][INFO] - Test, Round 382: loss=6.5333, accuracy=0.3062, 
[2025-09-11 23:56:33,356][__main__][INFO] - Test, Round 383: loss=6.5353, accuracy=0.3066, 
[2025-09-11 23:56:33,356][__main__][INFO] - Test, Round 384: loss=6.5371, accuracy=0.3063, 
[2025-09-11 23:56:33,356][__main__][INFO] - Test, Round 385: loss=6.5387, accuracy=0.3068, 
[2025-09-11 23:56:33,356][__main__][INFO] - Test, Round 386: loss=6.5401, accuracy=0.3066, 
[2025-09-11 23:56:33,356][__main__][INFO] - Test, Round 387: loss=6.5419, accuracy=0.3069, 
[2025-09-11 23:56:33,356][__main__][INFO] - Test, Round 388: loss=6.5437, accuracy=0.3065, 
[2025-09-11 23:56:33,356][__main__][INFO] - Test, Round 389: loss=6.5452, accuracy=0.3065, 
[2025-09-11 23:56:33,356][__main__][INFO] - Test, Round 390: loss=6.5471, accuracy=0.3069, 
[2025-09-11 23:56:33,356][__main__][INFO] - Test, Round 391: loss=6.5485, accuracy=0.3063, 
[2025-09-11 23:56:33,356][__main__][INFO] - Test, Round 392: loss=6.5503, accuracy=0.3068, 
[2025-09-11 23:56:33,356][__main__][INFO] - Test, Round 393: loss=6.5518, accuracy=0.3066, 
[2025-09-11 23:56:33,356][__main__][INFO] - Test, Round 394: loss=6.5533, accuracy=0.3066, 
[2025-09-11 23:56:33,356][__main__][INFO] - Test, Round 395: loss=6.5552, accuracy=0.3069, 
[2025-09-11 23:56:33,356][__main__][INFO] - Test, Round 396: loss=6.5564, accuracy=0.3068, 
[2025-09-11 23:56:33,356][__main__][INFO] - Test, Round 397: loss=6.5583, accuracy=0.3064, 
[2025-09-11 23:56:33,356][__main__][INFO] - Test, Round 398: loss=6.5596, accuracy=0.3067, 
[2025-09-11 23:56:33,356][__main__][INFO] - Test, Round 399: loss=6.5615, accuracy=0.3065, 
[2025-09-11 23:56:33,356][__main__][INFO] - Test, Round 400: loss=6.5631, accuracy=0.3063, 
[2025-09-11 23:56:33,356][__main__][INFO] - Test, Round 401: loss=6.5647, accuracy=0.3067, 
[2025-09-11 23:56:33,356][__main__][INFO] - Test, Round 402: loss=6.5655, accuracy=0.3067, 
[2025-09-11 23:56:33,356][__main__][INFO] - Test, Round 403: loss=6.5673, accuracy=0.3064, 
[2025-09-11 23:56:33,356][__main__][INFO] - Test, Round 404: loss=6.5692, accuracy=0.3067, 
[2025-09-11 23:56:33,356][__main__][INFO] - Test, Round 405: loss=6.5705, accuracy=0.3066, 
[2025-09-11 23:56:33,356][__main__][INFO] - Test, Round 406: loss=6.5714, accuracy=0.3067, 
[2025-09-11 23:56:33,356][__main__][INFO] - Test, Round 407: loss=6.5733, accuracy=0.3068, 
[2025-09-11 23:56:33,356][__main__][INFO] - Test, Round 408: loss=6.5745, accuracy=0.3065, 
[2025-09-11 23:56:33,356][__main__][INFO] - Test, Round 409: loss=6.5762, accuracy=0.3068, 
[2025-09-11 23:56:33,356][__main__][INFO] - Test, Round 410: loss=6.5774, accuracy=0.3067, 
[2025-09-11 23:56:33,356][__main__][INFO] - Test, Round 411: loss=6.5789, accuracy=0.3066, 
[2025-09-11 23:56:33,356][__main__][INFO] - Test, Round 412: loss=6.5804, accuracy=0.3065, 
[2025-09-11 23:56:33,356][__main__][INFO] - Test, Round 413: loss=6.5815, accuracy=0.3065, 
[2025-09-11 23:56:33,356][__main__][INFO] - Test, Round 414: loss=6.5831, accuracy=0.3064, 
[2025-09-11 23:56:33,356][__main__][INFO] - Test, Round 415: loss=6.5844, accuracy=0.3065, 
[2025-09-11 23:56:33,356][__main__][INFO] - Test, Round 416: loss=6.5858, accuracy=0.3065, 
[2025-09-11 23:56:33,356][__main__][INFO] - Test, Round 417: loss=6.5869, accuracy=0.3066, 
[2025-09-11 23:56:33,356][__main__][INFO] - Test, Round 418: loss=6.5883, accuracy=0.3067, 
[2025-09-11 23:56:33,356][__main__][INFO] - Test, Round 419: loss=6.5896, accuracy=0.3065, 
[2025-09-11 23:56:33,356][__main__][INFO] - Test, Round 420: loss=6.5911, accuracy=0.3064, 
[2025-09-11 23:56:33,356][__main__][INFO] - Test, Round 421: loss=6.5921, accuracy=0.3061, 
[2025-09-11 23:56:33,356][__main__][INFO] - Test, Round 422: loss=6.5936, accuracy=0.3065, 
[2025-09-11 23:56:33,356][__main__][INFO] - Test, Round 423: loss=6.5946, accuracy=0.3066, 
[2025-09-11 23:56:33,356][__main__][INFO] - Test, Round 424: loss=6.5959, accuracy=0.3066, 
[2025-09-11 23:56:33,356][__main__][INFO] - Test, Round 425: loss=6.5972, accuracy=0.3066, 
[2025-09-11 23:56:33,356][__main__][INFO] - Test, Round 426: loss=6.5984, accuracy=0.3064, 
[2025-09-11 23:56:33,356][__main__][INFO] - Test, Round 427: loss=6.5997, accuracy=0.3069, 
[2025-09-11 23:56:33,356][__main__][INFO] - Test, Round 428: loss=6.6014, accuracy=0.3065, 
[2025-09-11 23:56:33,356][__main__][INFO] - Test, Round 429: loss=6.6024, accuracy=0.3066, 
[2025-09-11 23:56:33,356][__main__][INFO] - Test, Round 430: loss=6.6036, accuracy=0.3067, 
[2025-09-11 23:56:33,356][__main__][INFO] - Test, Round 431: loss=6.6050, accuracy=0.3064, 
[2025-09-11 23:56:33,356][__main__][INFO] - Test, Round 432: loss=6.6058, accuracy=0.3065, 
[2025-09-11 23:56:33,356][__main__][INFO] - Test, Round 433: loss=6.6070, accuracy=0.3063, 
[2025-09-11 23:56:33,356][__main__][INFO] - Test, Round 434: loss=6.6082, accuracy=0.3064, 
[2025-09-11 23:56:33,356][__main__][INFO] - Test, Round 435: loss=6.6090, accuracy=0.3063, 
[2025-09-11 23:56:33,356][__main__][INFO] - Test, Round 436: loss=6.6105, accuracy=0.3064, 
[2025-09-11 23:56:33,356][__main__][INFO] - Test, Round 437: loss=6.6117, accuracy=0.3062, 
[2025-09-11 23:56:33,356][__main__][INFO] - Test, Round 438: loss=6.6127, accuracy=0.3063, 
[2025-09-11 23:56:33,356][__main__][INFO] - Test, Round 439: loss=6.6139, accuracy=0.3063, 
[2025-09-11 23:56:33,356][__main__][INFO] - Test, Round 440: loss=6.6149, accuracy=0.3065, 
[2025-09-11 23:56:33,356][__main__][INFO] - Test, Round 441: loss=6.6161, accuracy=0.3060, 
[2025-09-11 23:56:33,356][__main__][INFO] - Test, Round 442: loss=6.6170, accuracy=0.3068, 
[2025-09-11 23:56:33,356][__main__][INFO] - Test, Round 443: loss=6.6182, accuracy=0.3065, 
[2025-09-11 23:56:33,356][__main__][INFO] - Test, Round 444: loss=6.6196, accuracy=0.3064, 
[2025-09-11 23:56:33,356][__main__][INFO] - Test, Round 445: loss=6.6206, accuracy=0.3060, 
[2025-09-11 23:56:33,356][__main__][INFO] - Test, Round 446: loss=6.6216, accuracy=0.3066, 
[2025-09-11 23:56:33,356][__main__][INFO] - Test, Round 447: loss=6.6225, accuracy=0.3069, 
[2025-09-11 23:56:33,356][__main__][INFO] - Test, Round 448: loss=6.6230, accuracy=0.3059, 
[2025-09-11 23:56:33,356][__main__][INFO] - Test, Round 449: loss=6.6245, accuracy=0.3067, 
[2025-09-11 23:56:33,356][__main__][INFO] - Test, Round 450: loss=6.6255, accuracy=0.3064, 
[2025-09-11 23:56:33,356][__main__][INFO] - Test, Round 451: loss=6.6265, accuracy=0.3066, 
[2025-09-11 23:56:33,356][__main__][INFO] - Test, Round 452: loss=6.6279, accuracy=0.3067, 
[2025-09-11 23:56:33,356][__main__][INFO] - Test, Round 453: loss=6.6285, accuracy=0.3064, 
[2025-09-11 23:56:33,356][__main__][INFO] - Test, Round 454: loss=6.6294, accuracy=0.3065, 
[2025-09-11 23:56:33,356][__main__][INFO] - Test, Round 455: loss=6.6306, accuracy=0.3063, 
[2025-09-11 23:56:33,356][__main__][INFO] - Test, Round 456: loss=6.6314, accuracy=0.3065, 
[2025-09-11 23:56:33,356][__main__][INFO] - Test, Round 457: loss=6.6324, accuracy=0.3061, 
[2025-09-11 23:56:33,356][__main__][INFO] - Test, Round 458: loss=6.6332, accuracy=0.3063, 
[2025-09-11 23:56:33,356][__main__][INFO] - Test, Round 459: loss=6.6344, accuracy=0.3067, 
[2025-09-11 23:56:33,356][__main__][INFO] - Test, Round 460: loss=6.6353, accuracy=0.3064, 
[2025-09-11 23:56:33,356][__main__][INFO] - Test, Round 461: loss=6.6361, accuracy=0.3062, 
[2025-09-11 23:56:33,356][__main__][INFO] - Test, Round 462: loss=6.6370, accuracy=0.3066, 
[2025-09-11 23:56:33,356][__main__][INFO] - Test, Round 463: loss=6.6378, accuracy=0.3058, 
[2025-09-11 23:56:33,356][__main__][INFO] - Test, Round 464: loss=6.6391, accuracy=0.3066, 
[2025-09-11 23:56:33,356][__main__][INFO] - Test, Round 465: loss=6.6399, accuracy=0.3064, 
[2025-09-11 23:56:33,357][__main__][INFO] - Test, Round 466: loss=6.6408, accuracy=0.3066, 
[2025-09-11 23:56:33,357][__main__][INFO] - Test, Round 467: loss=6.6415, accuracy=0.3065, 
[2025-09-11 23:56:33,357][__main__][INFO] - Test, Round 468: loss=6.6426, accuracy=0.3063, 
[2025-09-11 23:56:33,357][__main__][INFO] - Test, Round 469: loss=6.6432, accuracy=0.3065, 
[2025-09-11 23:56:33,357][__main__][INFO] - Test, Round 470: loss=6.6441, accuracy=0.3063, 
[2025-09-11 23:56:33,357][__main__][INFO] - Test, Round 471: loss=6.6451, accuracy=0.3064, 
[2025-09-11 23:56:33,357][__main__][INFO] - Test, Round 472: loss=6.6459, accuracy=0.3066, 
[2025-09-11 23:56:33,357][__main__][INFO] - Test, Round 473: loss=6.6471, accuracy=0.3066, 
[2025-09-11 23:56:33,357][__main__][INFO] - Test, Round 474: loss=6.6481, accuracy=0.3067, 
[2025-09-11 23:56:33,357][__main__][INFO] - Test, Round 475: loss=6.6481, accuracy=0.3064, 
[2025-09-11 23:56:33,357][__main__][INFO] - Test, Round 476: loss=6.6490, accuracy=0.3069, 
[2025-09-11 23:56:33,357][__main__][INFO] - Test, Round 477: loss=6.6501, accuracy=0.3064, 
[2025-09-11 23:56:33,357][__main__][INFO] - Test, Round 478: loss=6.6511, accuracy=0.3068, 
[2025-09-11 23:56:33,357][__main__][INFO] - Test, Round 479: loss=6.6515, accuracy=0.3064, 
[2025-09-11 23:56:33,357][__main__][INFO] - Test, Round 480: loss=6.6524, accuracy=0.3067, 
[2025-09-11 23:56:33,357][__main__][INFO] - Test, Round 481: loss=6.6532, accuracy=0.3067, 
[2025-09-11 23:56:33,357][__main__][INFO] - Test, Round 482: loss=6.6538, accuracy=0.3066, 
[2025-09-11 23:56:33,357][__main__][INFO] - Test, Round 483: loss=6.6547, accuracy=0.3067, 
[2025-09-11 23:56:33,357][__main__][INFO] - Test, Round 484: loss=6.6556, accuracy=0.3067, 
[2025-09-11 23:56:33,357][__main__][INFO] - Test, Round 485: loss=6.6561, accuracy=0.3065, 
[2025-09-11 23:56:33,357][__main__][INFO] - Test, Round 486: loss=6.6569, accuracy=0.3064, 
[2025-09-11 23:56:33,357][__main__][INFO] - Test, Round 487: loss=6.6576, accuracy=0.3064, 
[2025-09-11 23:56:33,357][__main__][INFO] - Test, Round 488: loss=6.6584, accuracy=0.3067, 
[2025-09-11 23:56:33,357][__main__][INFO] - Test, Round 489: loss=6.6588, accuracy=0.3069, 
[2025-09-11 23:56:33,357][__main__][INFO] - Test, Round 490: loss=6.6598, accuracy=0.3067, 
[2025-09-11 23:56:33,357][__main__][INFO] - Test, Round 491: loss=6.6604, accuracy=0.3068, 
[2025-09-11 23:56:33,357][__main__][INFO] - Test, Round 492: loss=6.6613, accuracy=0.3066, 
[2025-09-11 23:56:33,357][__main__][INFO] - Test, Round 493: loss=6.6622, accuracy=0.3066, 
[2025-09-11 23:56:33,357][__main__][INFO] - Test, Round 494: loss=6.6629, accuracy=0.3063, 
[2025-09-11 23:56:33,357][__main__][INFO] - Test, Round 495: loss=6.6630, accuracy=0.3067, 
[2025-09-11 23:56:33,357][__main__][INFO] - Test, Round 496: loss=6.6639, accuracy=0.3065, 
[2025-09-11 23:56:33,357][__main__][INFO] - Test, Round 497: loss=6.6646, accuracy=0.3067, 
[2025-09-11 23:56:33,357][__main__][INFO] - Test, Round 498: loss=6.6651, accuracy=0.3068, 
[2025-09-11 23:56:33,357][__main__][INFO] - Test, Round 499: loss=6.6662, accuracy=0.3066, 
[2025-09-11 23:56:33,357][__main__][INFO] - Test, Round 500: loss=6.6665, accuracy=0.3065, 
