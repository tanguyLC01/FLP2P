[2025-09-18 13:42:48,948][flp2p.graph_runner][INFO] - Train, Round 0 : loss => 1.642421499296024,  accuracy: 0.4966060606060606, gradient_norm : 0.9501439683146775
[2025-09-18 13:42:56,249][flp2p.graph_runner][INFO] - Test, Round 0 : loss => 2.163609340487831,  accuracy: 0.1614
[2025-09-18 13:42:59,348][flp2p.graph_runner][INFO] - Train, Round 1 : loss => 1.2778094547750376,  accuracy: 0.5359393939393939, gradient_norm : 0.6512157763286347
[2025-09-18 13:43:06,592][flp2p.graph_runner][INFO] - Test, Round 1 : loss => 2.033358906384223,  accuracy: 0.1825
[2025-09-18 13:43:09,720][flp2p.graph_runner][INFO] - Train, Round 2 : loss => 1.23048712751674,  accuracy: 0.5484848484848486, gradient_norm : 0.6428112462292854
[2025-09-18 13:43:17,000][flp2p.graph_runner][INFO] - Test, Round 2 : loss => 1.9121052769934934,  accuracy: 0.2199
[2025-09-18 13:43:20,091][flp2p.graph_runner][INFO] - Train, Round 3 : loss => 1.2777283087807765,  accuracy: 0.5363030303030303, gradient_norm : 0.643945850844337
[2025-09-18 13:43:27,368][flp2p.graph_runner][INFO] - Test, Round 3 : loss => 1.796532084454148,  accuracy: 0.2558
[2025-09-18 13:43:30,178][flp2p.graph_runner][INFO] - Train, Round 4 : loss => 1.4152491932859739,  accuracy: 0.5054666666666667, gradient_norm : 0.7386512255212148
[2025-09-18 13:43:37,487][flp2p.graph_runner][INFO] - Test, Round 4 : loss => 1.6766811372515709,  accuracy: 0.2918
[2025-09-18 13:43:40,637][flp2p.graph_runner][INFO] - Train, Round 5 : loss => 0.9858074632351911,  accuracy: 0.6725454545454547, gradient_norm : 0.5884043794751864
[2025-09-18 13:43:47,975][flp2p.graph_runner][INFO] - Test, Round 5 : loss => 1.5797078280281542,  accuracy: 0.3228
[2025-09-18 13:43:51,089][flp2p.graph_runner][INFO] - Train, Round 6 : loss => 1.0605036349542085,  accuracy: 0.5952727272727273, gradient_norm : 0.5341118101449446
[2025-09-18 13:43:58,387][flp2p.graph_runner][INFO] - Test, Round 6 : loss => 1.499685739464712,  accuracy: 0.346
[2025-09-18 13:44:01,543][flp2p.graph_runner][INFO] - Train, Round 7 : loss => 1.060333464239807,  accuracy: 0.5706060606060606, gradient_norm : 0.520119648631174
[2025-09-18 13:44:08,938][flp2p.graph_runner][INFO] - Test, Round 7 : loss => 1.4223260467974528,  accuracy: 0.3683
[2025-09-18 13:44:12,101][flp2p.graph_runner][INFO] - Train, Round 8 : loss => 1.1107118392920228,  accuracy: 0.5936969696969696, gradient_norm : 0.5628299898350184
[2025-09-18 13:44:19,488][flp2p.graph_runner][INFO] - Test, Round 8 : loss => 1.3298019195963475,  accuracy: 0.3979
[2025-09-18 13:44:22,653][flp2p.graph_runner][INFO] - Train, Round 9 : loss => 0.836526252602833,  accuracy: 0.5776969696969697, gradient_norm : 0.32074137654058704
[2025-09-18 13:44:29,980][flp2p.graph_runner][INFO] - Test, Round 9 : loss => 1.2927351015050743,  accuracy: 0.4067
[2025-09-18 13:44:33,124][flp2p.graph_runner][INFO] - Train, Round 10 : loss => 1.2771563091714255,  accuracy: 0.5354545454545454, gradient_norm : 0.6485881680878677
[2025-09-18 13:44:40,468][flp2p.graph_runner][INFO] - Test, Round 10 : loss => 1.1803295730494323,  accuracy: 0.4425
[2025-09-18 13:44:43,614][flp2p.graph_runner][INFO] - Train, Round 11 : loss => 0.5789054575444106,  accuracy: 0.6958181818181819, gradient_norm : 0.23467862470661904
[2025-09-18 13:44:50,918][flp2p.graph_runner][INFO] - Test, Round 11 : loss => 1.1735481219705135,  accuracy: 0.4454
[2025-09-18 13:44:54,051][flp2p.graph_runner][INFO] - Train, Round 12 : loss => 0.6277584141778679,  accuracy: 0.6676363636363636, gradient_norm : 0.24003257344767148
[2025-09-18 13:45:01,434][flp2p.graph_runner][INFO] - Test, Round 12 : loss => 1.153446438297251,  accuracy: 0.454
[2025-09-18 13:45:04,582][flp2p.graph_runner][INFO] - Train, Round 13 : loss => 0.6800465403737854,  accuracy: 0.6881212121212121, gradient_norm : 0.315704621784308
[2025-09-18 13:45:11,978][flp2p.graph_runner][INFO] - Test, Round 13 : loss => 1.1185067274488179,  accuracy: 0.4716
[2025-09-18 13:45:15,105][flp2p.graph_runner][INFO] - Train, Round 14 : loss => 0.9833596628983713,  accuracy: 0.5672727272727272, gradient_norm : 0.44917880317756165
[2025-09-18 13:45:22,386][flp2p.graph_runner][INFO] - Test, Round 14 : loss => 1.0666720284783673,  accuracy: 0.4909
[2025-09-18 13:45:25,463][flp2p.graph_runner][INFO] - Train, Round 15 : loss => 0.6500956234955817,  accuracy: 0.692969696969697, gradient_norm : 0.3205502834511694
[2025-09-18 13:45:32,786][flp2p.graph_runner][INFO] - Test, Round 15 : loss => 1.0821730717311797,  accuracy: 0.5034
[2025-09-18 13:45:35,885][flp2p.graph_runner][INFO] - Train, Round 16 : loss => 1.045965622751192,  accuracy: 0.5943030303030302, gradient_norm : 0.5800048159407626
[2025-09-18 13:45:43,133][flp2p.graph_runner][INFO] - Test, Round 16 : loss => 0.994859442098322,  accuracy: 0.5285
[2025-09-18 13:45:45,976][flp2p.graph_runner][INFO] - Train, Round 17 : loss => 0.7657033609763044,  accuracy: 0.6796666666666666, gradient_norm : 0.3539087557170939
[2025-09-18 13:45:53,326][flp2p.graph_runner][INFO] - Test, Round 17 : loss => 0.9555916930169353,  accuracy: 0.5361
[2025-09-18 13:45:56,432][flp2p.graph_runner][INFO] - Train, Round 18 : loss => 0.6608480498913794,  accuracy: 0.7160606060606061, gradient_norm : 0.3550321800445891
[2025-09-18 13:46:03,748][flp2p.graph_runner][INFO] - Test, Round 18 : loss => 0.9164165210740916,  accuracy: 0.5482
[2025-09-18 13:46:06,865][flp2p.graph_runner][INFO] - Train, Round 19 : loss => 0.735691979563775,  accuracy: 0.6970909090909091, gradient_norm : 0.4348408452353631
[2025-09-18 13:46:14,218][flp2p.graph_runner][INFO] - Test, Round 19 : loss => 0.9029275268184296,  accuracy: 0.5526
[2025-09-18 13:46:17,376][flp2p.graph_runner][INFO] - Train, Round 20 : loss => 0.7610064669189254,  accuracy: 0.6504242424242423, gradient_norm : 0.3723647826630796
[2025-09-18 13:46:24,667][flp2p.graph_runner][INFO] - Test, Round 20 : loss => 0.8871712430322399,  accuracy: 0.5575
[2025-09-18 13:46:27,566][flp2p.graph_runner][INFO] - Train, Round 21 : loss => 0.6877548161159076,  accuracy: 0.6246666666666667, gradient_norm : 0.28480133409728
[2025-09-18 13:46:34,809][flp2p.graph_runner][INFO] - Test, Round 21 : loss => 0.8771146084216791,  accuracy: 0.5597
[2025-09-18 13:46:37,936][flp2p.graph_runner][INFO] - Train, Round 22 : loss => 0.6611130754815504,  accuracy: 0.7046666666666666, gradient_norm : 0.3663340049667622
[2025-09-18 13:46:45,212][flp2p.graph_runner][INFO] - Test, Round 22 : loss => 0.8450291449991423,  accuracy: 0.5662
[2025-09-18 13:46:48,348][flp2p.graph_runner][INFO] - Train, Round 23 : loss => 0.7723086740162117,  accuracy: 0.6123636363636363, gradient_norm : 0.3335208691301114
[2025-09-18 13:46:55,632][flp2p.graph_runner][INFO] - Test, Round 23 : loss => 0.8146453032746139,  accuracy: 0.5792
[2025-09-18 13:46:58,770][flp2p.graph_runner][INFO] - Train, Round 24 : loss => 0.5738530860993875,  accuracy: 0.6801212121212121, gradient_norm : 0.23885348945231355
[2025-09-18 13:47:06,090][flp2p.graph_runner][INFO] - Test, Round 24 : loss => 0.7960115801143199,  accuracy: 0.5881
[2025-09-18 13:47:09,230][flp2p.graph_runner][INFO] - Train, Round 25 : loss => 0.6303389975792335,  accuracy: 0.671939393939394, gradient_norm : 0.2526713766871778
[2025-09-18 13:47:16,568][flp2p.graph_runner][INFO] - Test, Round 25 : loss => 0.7703549411032811,  accuracy: 0.595
[2025-09-18 13:47:19,709][flp2p.graph_runner][INFO] - Train, Round 26 : loss => 0.5552218586891213,  accuracy: 0.7370303030303029, gradient_norm : 0.27505321986633374
[2025-09-18 13:47:27,029][flp2p.graph_runner][INFO] - Test, Round 26 : loss => 0.755020164682555,  accuracy: 0.6005
[2025-09-18 13:47:29,917][flp2p.graph_runner][INFO] - Train, Round 27 : loss => 0.5125063599831339,  accuracy: 0.7333999999999999, gradient_norm : 0.21012330250234296
[2025-09-18 13:47:37,255][flp2p.graph_runner][INFO] - Test, Round 27 : loss => 0.7322461896307809,  accuracy: 0.6071
[2025-09-18 13:47:40,152][flp2p.graph_runner][INFO] - Train, Round 28 : loss => 0.755767863329116,  accuracy: 0.6695333333333333, gradient_norm : 0.36930611100777405
[2025-09-18 13:47:47,468][flp2p.graph_runner][INFO] - Test, Round 28 : loss => 0.692302099218019,  accuracy: 0.6207
[2025-09-18 13:47:50,633][flp2p.graph_runner][INFO] - Train, Round 29 : loss => 0.4044468775918745,  accuracy: 0.8052121212121213, gradient_norm : 0.18089373152821223
[2025-09-18 13:47:57,933][flp2p.graph_runner][INFO] - Test, Round 29 : loss => 0.6857924855585776,  accuracy: 0.6235
[2025-09-18 13:48:01,051][flp2p.graph_runner][INFO] - Train, Round 30 : loss => 0.49479742259289955,  accuracy: 0.7318181818181818, gradient_norm : 0.18620624342303965
[2025-09-18 13:48:08,384][flp2p.graph_runner][INFO] - Test, Round 30 : loss => 0.6768311055298549,  accuracy: 0.6273
[2025-09-18 13:48:11,528][flp2p.graph_runner][INFO] - Train, Round 31 : loss => 0.47394846042473365,  accuracy: 0.728, gradient_norm : 0.15029219826512727
[2025-09-18 13:48:18,848][flp2p.graph_runner][INFO] - Test, Round 31 : loss => 0.6662645336650402,  accuracy: 0.634
[2025-09-18 13:48:21,982][flp2p.graph_runner][INFO] - Train, Round 32 : loss => 0.6237422372801297,  accuracy: 0.6523030303030304, gradient_norm : 0.2425997257076755
[2025-09-18 13:48:29,314][flp2p.graph_runner][INFO] - Test, Round 32 : loss => 0.6444838381958607,  accuracy: 0.6455
[2025-09-18 13:48:32,500][flp2p.graph_runner][INFO] - Train, Round 33 : loss => 0.5021942818447733,  accuracy: 0.6874545454545455, gradient_norm : 0.14156637408388864
[2025-09-18 13:48:39,822][flp2p.graph_runner][INFO] - Test, Round 33 : loss => 0.6401722974932139,  accuracy: 0.6481
[2025-09-18 13:48:42,680][flp2p.graph_runner][INFO] - Train, Round 34 : loss => 0.5188724873133438,  accuracy: 0.6871333333333334, gradient_norm : 0.1525552828597459
[2025-09-18 13:48:50,047][flp2p.graph_runner][INFO] - Test, Round 34 : loss => 0.6380506008091186,  accuracy: 0.6545
[2025-09-18 13:48:53,181][flp2p.graph_runner][INFO] - Train, Round 35 : loss => 0.49228447507786627,  accuracy: 0.7989090909090909, gradient_norm : 0.28266082068815773
[2025-09-18 13:49:00,527][flp2p.graph_runner][INFO] - Test, Round 35 : loss => 0.6057025643653031,  accuracy: 0.6659
[2025-09-18 13:49:03,706][flp2p.graph_runner][INFO] - Train, Round 36 : loss => 0.46082653132128076,  accuracy: 0.7568484848484848, gradient_norm : 0.18149327133967014
[2025-09-18 13:49:11,025][flp2p.graph_runner][INFO] - Test, Round 36 : loss => 0.6007139801180716,  accuracy: 0.6745
[2025-09-18 13:49:14,160][flp2p.graph_runner][INFO] - Train, Round 37 : loss => 0.4116613664577903,  accuracy: 0.7672727272727272, gradient_norm : 0.12247633766732383
[2025-09-18 13:49:21,515][flp2p.graph_runner][INFO] - Test, Round 37 : loss => 0.598176509982648,  accuracy: 0.6768
[2025-09-18 13:49:24,602][flp2p.graph_runner][INFO] - Train, Round 38 : loss => 0.5184275318796224,  accuracy: 0.7601212121212122, gradient_norm : 0.2093565562120766
[2025-09-18 13:49:31,815][flp2p.graph_runner][INFO] - Test, Round 38 : loss => 0.5811137779503759,  accuracy: 0.678
[2025-09-18 13:49:34,946][flp2p.graph_runner][INFO] - Train, Round 39 : loss => 0.5913165115054274,  accuracy: 0.7248484848484849, gradient_norm : 0.2862285384403267
[2025-09-18 13:49:42,348][flp2p.graph_runner][INFO] - Test, Round 39 : loss => 0.5618464413000176,  accuracy: 0.6814
[2025-09-18 13:49:45,446][flp2p.graph_runner][INFO] - Train, Round 40 : loss => 0.411905944528104,  accuracy: 0.8041818181818182, gradient_norm : 0.22405377500534163
[2025-09-18 13:49:52,750][flp2p.graph_runner][INFO] - Test, Round 40 : loss => 0.5507104080502891,  accuracy: 0.6885
[2025-09-18 13:49:55,864][flp2p.graph_runner][INFO] - Train, Round 41 : loss => 0.37891445493079473,  accuracy: 0.7949090909090909, gradient_norm : 0.1771793107897557
[2025-09-18 13:50:03,205][flp2p.graph_runner][INFO] - Test, Round 41 : loss => 0.5471921538954102,  accuracy: 0.6909
[2025-09-18 13:50:06,324][flp2p.graph_runner][INFO] - Train, Round 42 : loss => 0.4498735903922353,  accuracy: 0.7529090909090909, gradient_norm : 0.1604826435958622
[2025-09-18 13:50:13,622][flp2p.graph_runner][INFO] - Test, Round 42 : loss => 0.5433019620152109,  accuracy: 0.6971
[2025-09-18 13:50:16,713][flp2p.graph_runner][INFO] - Train, Round 43 : loss => 0.3499179835334273,  accuracy: 0.8193939393939395, gradient_norm : 0.1773806212700797
[2025-09-18 13:50:24,071][flp2p.graph_runner][INFO] - Test, Round 43 : loss => 0.540818570141507,  accuracy: 0.6975
[2025-09-18 13:50:27,223][flp2p.graph_runner][INFO] - Train, Round 44 : loss => 0.3463167954943647,  accuracy: 0.8030909090909091, gradient_norm : 0.1514322516966247
[2025-09-18 13:50:34,584][flp2p.graph_runner][INFO] - Test, Round 44 : loss => 0.5406471590264105,  accuracy: 0.6965
[2025-09-18 13:50:37,709][flp2p.graph_runner][INFO] - Train, Round 45 : loss => 0.3929171396370981,  accuracy: 0.784, gradient_norm : 0.16958688109918513
[2025-09-18 13:50:45,120][flp2p.graph_runner][INFO] - Test, Round 45 : loss => 0.5389561484972766,  accuracy: 0.6957
[2025-09-18 13:50:48,281][flp2p.graph_runner][INFO] - Train, Round 46 : loss => 0.44140025990078646,  accuracy: 0.7448484848484848, gradient_norm : 0.1594269401578881
[2025-09-18 13:50:55,575][flp2p.graph_runner][INFO] - Test, Round 46 : loss => 0.5348137031610777,  accuracy: 0.7009
[2025-09-18 13:50:58,739][flp2p.graph_runner][INFO] - Train, Round 47 : loss => 0.3978859279565953,  accuracy: 0.7644848484848485, gradient_norm : 0.1393434212558697
[2025-09-18 13:51:06,107][flp2p.graph_runner][INFO] - Test, Round 47 : loss => 0.5352133746780593,  accuracy: 0.7041
[2025-09-18 13:51:09,216][flp2p.graph_runner][INFO] - Train, Round 48 : loss => 0.28950267286436215,  accuracy: 0.8493939393939393, gradient_norm : 0.12371288471759743
[2025-09-18 13:51:16,546][flp2p.graph_runner][INFO] - Test, Round 48 : loss => 0.5347818187919069,  accuracy: 0.7046
[2025-09-18 13:51:19,673][flp2p.graph_runner][INFO] - Train, Round 49 : loss => 0.4096758776973429,  accuracy: 0.7655151515151516, gradient_norm : 0.13561436076552494
[2025-09-18 13:51:27,025][flp2p.graph_runner][INFO] - Test, Round 49 : loss => 0.5332746064673008,  accuracy: 0.705
[2025-09-18 13:51:29,925][flp2p.graph_runner][INFO] - Train, Round 50 : loss => 0.31067937032535986,  accuracy: 0.8255333333333333, gradient_norm : 0.13548457737560732
[2025-09-18 13:51:37,297][flp2p.graph_runner][INFO] - Test, Round 50 : loss => 0.5307268077186557,  accuracy: 0.707
[2025-09-18 13:51:40,131][flp2p.graph_runner][INFO] - Train, Round 51 : loss => 0.4661662988680065,  accuracy: 0.7450000000000001, gradient_norm : 0.18588478166064842
[2025-09-18 13:51:47,452][flp2p.graph_runner][INFO] - Test, Round 51 : loss => 0.5197319312339173,  accuracy: 0.7125
[2025-09-18 13:51:50,671][flp2p.graph_runner][INFO] - Train, Round 52 : loss => 0.32654126182263404,  accuracy: 0.8327272727272729, gradient_norm : 0.13293102451110286
[2025-09-18 13:51:57,966][flp2p.graph_runner][INFO] - Test, Round 52 : loss => 0.5174210657941117,  accuracy: 0.7137
[2025-09-18 13:52:01,138][flp2p.graph_runner][INFO] - Train, Round 53 : loss => 0.3658169293089764,  accuracy: 0.8172121212121211, gradient_norm : 0.15038024553264753
[2025-09-18 13:52:08,439][flp2p.graph_runner][INFO] - Test, Round 53 : loss => 0.5108920008825079,  accuracy: 0.721
[2025-09-18 13:52:11,525][flp2p.graph_runner][INFO] - Train, Round 54 : loss => 0.3707689101830918,  accuracy: 0.8024242424242425, gradient_norm : 0.12936066694998347
[2025-09-18 13:52:18,842][flp2p.graph_runner][INFO] - Test, Round 54 : loss => 0.5086663865722865,  accuracy: 0.7227
[2025-09-18 13:52:21,983][flp2p.graph_runner][INFO] - Train, Round 55 : loss => 0.3772145230989627,  accuracy: 0.7881818181818182, gradient_norm : 0.14539278228710686
[2025-09-18 13:52:29,313][flp2p.graph_runner][INFO] - Test, Round 55 : loss => 0.5063531708635539,  accuracy: 0.7248
[2025-09-18 13:52:32,433][flp2p.graph_runner][INFO] - Train, Round 56 : loss => 0.3936544511551082,  accuracy: 0.7914545454545454, gradient_norm : 0.15227137141518024
[2025-09-18 13:52:39,769][flp2p.graph_runner][INFO] - Test, Round 56 : loss => 0.5064898914836473,  accuracy: 0.7279
[2025-09-18 13:52:42,861][flp2p.graph_runner][INFO] - Train, Round 57 : loss => 0.4162424043042648,  accuracy: 0.7461212121212122, gradient_norm : 0.1270332222331085
[2025-09-18 13:52:50,240][flp2p.graph_runner][INFO] - Test, Round 57 : loss => 0.5090477009238592,  accuracy: 0.7284
[2025-09-18 13:52:53,342][flp2p.graph_runner][INFO] - Train, Round 58 : loss => 0.3365084573352097,  accuracy: 0.8136969696969697, gradient_norm : 0.11563614707443202
[2025-09-18 13:53:00,685][flp2p.graph_runner][INFO] - Test, Round 58 : loss => 0.5061132396699778,  accuracy: 0.7331
[2025-09-18 13:53:03,786][flp2p.graph_runner][INFO] - Train, Round 59 : loss => 0.3221265524483301,  accuracy: 0.8212121212121213, gradient_norm : 0.10385323508690983
[2025-09-18 13:53:11,141][flp2p.graph_runner][INFO] - Test, Round 59 : loss => 0.5043255377450458,  accuracy: 0.7361
[2025-09-18 13:53:14,219][flp2p.graph_runner][INFO] - Train, Round 60 : loss => 0.31395246841927543,  accuracy: 0.8296969696969698, gradient_norm : 0.11813656201426227
[2025-09-18 13:53:21,572][flp2p.graph_runner][INFO] - Test, Round 60 : loss => 0.5078276404964545,  accuracy: 0.7374
[2025-09-18 13:53:24,681][flp2p.graph_runner][INFO] - Train, Round 61 : loss => 0.4104023638844306,  accuracy: 0.7849696969696969, gradient_norm : 0.12705649363061203
[2025-09-18 13:53:32,015][flp2p.graph_runner][INFO] - Test, Round 61 : loss => 0.5069349773500645,  accuracy: 0.7389
[2025-09-18 13:53:35,142][flp2p.graph_runner][INFO] - Train, Round 62 : loss => 0.3681094703334126,  accuracy: 0.8108484848484849, gradient_norm : 0.17264564773202515
[2025-09-18 13:53:42,551][flp2p.graph_runner][INFO] - Test, Round 62 : loss => 0.4993082430294106,  accuracy: 0.7445
[2025-09-18 13:53:45,690][flp2p.graph_runner][INFO] - Train, Round 63 : loss => 0.2716548806631582,  accuracy: 0.8633333333333333, gradient_norm : 0.14462392254248277
[2025-09-18 13:53:53,042][flp2p.graph_runner][INFO] - Test, Round 63 : loss => 0.4908687443889793,  accuracy: 0.7489
[2025-09-18 13:53:56,139][flp2p.graph_runner][INFO] - Train, Round 64 : loss => 0.3459845405334236,  accuracy: 0.8277575757575758, gradient_norm : 0.14595131757438223
[2025-09-18 13:54:03,507][flp2p.graph_runner][INFO] - Test, Round 64 : loss => 0.48002463536263656,  accuracy: 0.7539
[2025-09-18 13:54:06,588][flp2p.graph_runner][INFO] - Train, Round 65 : loss => 0.35530962891223816,  accuracy: 0.8169090909090909, gradient_norm : 0.13927925876101532
[2025-09-18 13:54:13,890][flp2p.graph_runner][INFO] - Test, Round 65 : loss => 0.47945353998244955,  accuracy: 0.7553
[2025-09-18 13:54:16,977][flp2p.graph_runner][INFO] - Train, Round 66 : loss => 0.23327350246010992,  accuracy: 0.8993333333333333, gradient_norm : 0.13228761157775312
[2025-09-18 13:54:24,313][flp2p.graph_runner][INFO] - Test, Round 66 : loss => 0.47781837560014084,  accuracy: 0.7548
[2025-09-18 13:54:27,433][flp2p.graph_runner][INFO] - Train, Round 67 : loss => 0.3850933284768798,  accuracy: 0.8021212121212121, gradient_norm : 0.16663690011647317
[2025-09-18 13:54:34,781][flp2p.graph_runner][INFO] - Test, Round 67 : loss => 0.48008197026030375,  accuracy: 0.755
[2025-09-18 13:54:37,904][flp2p.graph_runner][INFO] - Train, Round 68 : loss => 0.3567493683217495,  accuracy: 0.8058181818181818, gradient_norm : 0.12635330738730155
[2025-09-18 13:54:45,284][flp2p.graph_runner][INFO] - Test, Round 68 : loss => 0.478204756018155,  accuracy: 0.761
[2025-09-18 13:54:48,131][flp2p.graph_runner][INFO] - Train, Round 69 : loss => 0.3102694922372706,  accuracy: 0.8424666666666666, gradient_norm : 0.12213056167832412
[2025-09-18 13:54:55,486][flp2p.graph_runner][INFO] - Test, Round 69 : loss => 0.47685276282894085,  accuracy: 0.7635
[2025-09-18 13:54:58,599][flp2p.graph_runner][INFO] - Train, Round 70 : loss => 0.3350255385301615,  accuracy: 0.8264242424242424, gradient_norm : 0.15819917282229656
[2025-09-18 13:55:05,953][flp2p.graph_runner][INFO] - Test, Round 70 : loss => 0.47777509209249536,  accuracy: 0.7627
[2025-09-18 13:55:09,101][flp2p.graph_runner][INFO] - Train, Round 71 : loss => 0.32545314654667507,  accuracy: 0.8313333333333335, gradient_norm : 0.12045833179355737
[2025-09-18 13:55:16,511][flp2p.graph_runner][INFO] - Test, Round 71 : loss => 0.47945975047549383,  accuracy: 0.7646
[2025-09-18 13:55:19,689][flp2p.graph_runner][INFO] - Train, Round 72 : loss => 0.37142455011897985,  accuracy: 0.8035151515151514, gradient_norm : 0.1385662978719268
[2025-09-18 13:55:27,026][flp2p.graph_runner][INFO] - Test, Round 72 : loss => 0.47357162607081166,  accuracy: 0.7681
[2025-09-18 13:55:30,183][flp2p.graph_runner][INFO] - Train, Round 73 : loss => 0.33534156395371034,  accuracy: 0.8337575757575758, gradient_norm : 0.13869936152327447
[2025-09-18 13:55:37,532][flp2p.graph_runner][INFO] - Test, Round 73 : loss => 0.4702765394876473,  accuracy: 0.7705
[2025-09-18 13:55:40,668][flp2p.graph_runner][INFO] - Train, Round 74 : loss => 0.3967837850448128,  accuracy: 0.7826666666666665, gradient_norm : 0.12171372497933962
[2025-09-18 13:55:47,990][flp2p.graph_runner][INFO] - Test, Round 74 : loss => 0.4701521677596772,  accuracy: 0.7685
[2025-09-18 13:55:51,118][flp2p.graph_runner][INFO] - Train, Round 75 : loss => 0.3153899965855565,  accuracy: 0.8155757575757576, gradient_norm : 0.09571834608969453
[2025-09-18 13:55:58,440][flp2p.graph_runner][INFO] - Test, Round 75 : loss => 0.47070617211163734,  accuracy: 0.7684
[2025-09-18 13:56:01,591][flp2p.graph_runner][INFO] - Train, Round 76 : loss => 0.3846086824812814,  accuracy: 0.8063030303030304, gradient_norm : 0.145317829289924
[2025-09-18 13:56:08,920][flp2p.graph_runner][INFO] - Test, Round 76 : loss => 0.4702963863888616,  accuracy: 0.7704
[2025-09-18 13:56:12,074][flp2p.graph_runner][INFO] - Train, Round 77 : loss => 0.26105581981061665,  accuracy: 0.8855151515151515, gradient_norm : 0.16466279974509562
[2025-09-18 13:56:19,384][flp2p.graph_runner][INFO] - Test, Round 77 : loss => 0.468375634635743,  accuracy: 0.7706
[2025-09-18 13:56:22,496][flp2p.graph_runner][INFO] - Train, Round 78 : loss => 0.3096362605553052,  accuracy: 0.8558181818181818, gradient_norm : 0.14687344914312964
[2025-09-18 13:56:29,827][flp2p.graph_runner][INFO] - Test, Round 78 : loss => 0.46856604071847896,  accuracy: 0.7724
[2025-09-18 13:56:32,985][flp2p.graph_runner][INFO] - Train, Round 79 : loss => 0.28534196945418927,  accuracy: 0.8613939393939395, gradient_norm : 0.1289603180298791
[2025-09-18 13:56:40,325][flp2p.graph_runner][INFO] - Test, Round 79 : loss => 0.46586269905443595,  accuracy: 0.7722
[2025-09-18 13:56:43,155][flp2p.graph_runner][INFO] - Train, Round 80 : loss => 0.15745706072015442,  accuracy: 0.9336, gradient_norm : 0.1251689500045423
[2025-09-18 13:56:50,533][flp2p.graph_runner][INFO] - Test, Round 80 : loss => 0.4647711673235784,  accuracy: 0.7725
[2025-09-18 13:56:53,338][flp2p.graph_runner][INFO] - Train, Round 81 : loss => 0.26197327070565235,  accuracy: 0.8648, gradient_norm : 0.11936796314191114
[2025-09-18 13:57:00,672][flp2p.graph_runner][INFO] - Test, Round 81 : loss => 0.46206193274947216,  accuracy: 0.7767
[2025-09-18 13:57:03,762][flp2p.graph_runner][INFO] - Train, Round 82 : loss => 0.3074452741961645,  accuracy: 0.8392727272727273, gradient_norm : 0.13656561071303505
[2025-09-18 13:57:11,043][flp2p.graph_runner][INFO] - Test, Round 82 : loss => 0.46489495382450186,  accuracy: 0.7737
[2025-09-18 13:57:14,174][flp2p.graph_runner][INFO] - Train, Round 83 : loss => 0.36348550139177477,  accuracy: 0.8102424242424242, gradient_norm : 0.13270756150455423
[2025-09-18 13:57:21,521][flp2p.graph_runner][INFO] - Test, Round 83 : loss => 0.462349315129308,  accuracy: 0.7748
[2025-09-18 13:57:24,339][flp2p.graph_runner][INFO] - Train, Round 84 : loss => 0.2690351328299252,  accuracy: 0.8675333333333333, gradient_norm : 0.12297741265303264
[2025-09-18 13:57:31,656][flp2p.graph_runner][INFO] - Test, Round 84 : loss => 0.45921432361197584,  accuracy: 0.7767
[2025-09-18 13:57:34,771][flp2p.graph_runner][INFO] - Train, Round 85 : loss => 0.24608208197654255,  accuracy: 0.884, gradient_norm : 0.1153799416292926
[2025-09-18 13:57:42,055][flp2p.graph_runner][INFO] - Test, Round 85 : loss => 0.4570987960003358,  accuracy: 0.7761
[2025-09-18 13:57:45,165][flp2p.graph_runner][INFO] - Train, Round 86 : loss => 0.23024707573547576,  accuracy: 0.8827272727272727, gradient_norm : 0.12058425501505914
[2025-09-18 13:57:52,455][flp2p.graph_runner][INFO] - Test, Round 86 : loss => 0.451446093630215,  accuracy: 0.7783
[2025-09-18 13:57:55,579][flp2p.graph_runner][INFO] - Train, Round 87 : loss => 0.38780891049769195,  accuracy: 0.7952121212121213, gradient_norm : 0.1361342827625921
[2025-09-18 13:58:02,927][flp2p.graph_runner][INFO] - Test, Round 87 : loss => 0.4469018277912537,  accuracy: 0.7816
[2025-09-18 13:58:06,048][flp2p.graph_runner][INFO] - Train, Round 88 : loss => 0.15680478266427963,  accuracy: 0.9312727272727275, gradient_norm : 0.11245810675138747
[2025-09-18 13:58:13,406][flp2p.graph_runner][INFO] - Test, Round 88 : loss => 0.4491496923642883,  accuracy: 0.7817
[2025-09-18 13:58:16,534][flp2p.graph_runner][INFO] - Train, Round 89 : loss => 0.31254921388952467,  accuracy: 0.8472121212121213, gradient_norm : 0.167906285125211
[2025-09-18 13:58:23,879][flp2p.graph_runner][INFO] - Test, Round 89 : loss => 0.4556356663827294,  accuracy: 0.7796
[2025-09-18 13:58:27,000][flp2p.graph_runner][INFO] - Train, Round 90 : loss => 0.23422451875628794,  accuracy: 0.8923636363636364, gradient_norm : 0.11314422463175608
[2025-09-18 13:58:34,252][flp2p.graph_runner][INFO] - Test, Round 90 : loss => 0.4505219611519763,  accuracy: 0.7818
[2025-09-18 13:58:37,101][flp2p.graph_runner][INFO] - Train, Round 91 : loss => 0.40762818302567755,  accuracy: 0.7842, gradient_norm : 0.13796257378125323
[2025-09-18 13:58:44,465][flp2p.graph_runner][INFO] - Test, Round 91 : loss => 0.4469792494933658,  accuracy: 0.7853
[2025-09-18 13:58:47,614][flp2p.graph_runner][INFO] - Train, Round 92 : loss => 0.3588325315899978,  accuracy: 0.8203636363636364, gradient_norm : 0.16052391983964515
[2025-09-18 13:58:54,978][flp2p.graph_runner][INFO] - Test, Round 92 : loss => 0.4477445722296023,  accuracy: 0.7863
[2025-09-18 13:58:58,134][flp2p.graph_runner][INFO] - Train, Round 93 : loss => 0.2505831876555536,  accuracy: 0.8654545454545453, gradient_norm : 0.1306182684293341
[2025-09-18 13:59:05,523][flp2p.graph_runner][INFO] - Test, Round 93 : loss => 0.4506201935706571,  accuracy: 0.7884
[2025-09-18 13:59:08,681][flp2p.graph_runner][INFO] - Train, Round 94 : loss => 0.23568638784390417,  accuracy: 0.8884242424242423, gradient_norm : 0.13004373255272575
[2025-09-18 13:59:16,057][flp2p.graph_runner][INFO] - Test, Round 94 : loss => 0.44790038201507926,  accuracy: 0.7907
[2025-09-18 13:59:19,200][flp2p.graph_runner][INFO] - Train, Round 95 : loss => 0.26629585778352227,  accuracy: 0.8582424242424244, gradient_norm : 0.10569975261664766
[2025-09-18 13:59:26,530][flp2p.graph_runner][INFO] - Test, Round 95 : loss => 0.44651196582738584,  accuracy: 0.7902
[2025-09-18 13:59:29,694][flp2p.graph_runner][INFO] - Train, Round 96 : loss => 0.17610002598979302,  accuracy: 0.9258181818181819, gradient_norm : 0.1453642487513336
[2025-09-18 13:59:37,067][flp2p.graph_runner][INFO] - Test, Round 96 : loss => 0.44328578667647955,  accuracy: 0.7918
[2025-09-18 13:59:40,186][flp2p.graph_runner][INFO] - Train, Round 97 : loss => 0.316353053217865,  accuracy: 0.852909090909091, gradient_norm : 0.1552150198833941
[2025-09-18 13:59:47,575][flp2p.graph_runner][INFO] - Test, Round 97 : loss => 0.4381986103954311,  accuracy: 0.7954
[2025-09-18 13:59:50,389][flp2p.graph_runner][INFO] - Train, Round 98 : loss => 0.25894938604126344,  accuracy: 0.8724666666666666, gradient_norm : 0.1393696220439716
[2025-09-18 13:59:57,702][flp2p.graph_runner][INFO] - Test, Round 98 : loss => 0.43776581984335144,  accuracy: 0.7961
[2025-09-18 14:00:00,831][flp2p.graph_runner][INFO] - Train, Round 99 : loss => 0.24805675679076697,  accuracy: 0.8641212121212122, gradient_norm : 0.09405194619065171
[2025-09-18 14:00:08,108][flp2p.graph_runner][INFO] - Test, Round 99 : loss => 0.43836727650669455,  accuracy: 0.7971
[2025-09-18 14:00:11,256][flp2p.graph_runner][INFO] - Train, Round 100 : loss => 0.23338148157039354,  accuracy: 0.8856363636363637, gradient_norm : 0.1173401825219143
[2025-09-18 14:00:18,640][flp2p.graph_runner][INFO] - Test, Round 100 : loss => 0.4330186948355145,  accuracy: 0.8009
[2025-09-18 14:00:21,801][flp2p.graph_runner][INFO] - Train, Round 101 : loss => 0.3173031789082608,  accuracy: 0.8453939393939395, gradient_norm : 0.13004952038470155
[2025-09-18 14:00:29,179][flp2p.graph_runner][INFO] - Test, Round 101 : loss => 0.43379135913810196,  accuracy: 0.8015
[2025-09-18 14:00:32,285][flp2p.graph_runner][INFO] - Train, Round 102 : loss => 0.3002920082304494,  accuracy: 0.8577575757575758, gradient_norm : 0.1399665374644817
[2025-09-18 14:00:39,645][flp2p.graph_runner][INFO] - Test, Round 102 : loss => 0.43271756153233554,  accuracy: 0.8013
[2025-09-18 14:00:42,748][flp2p.graph_runner][INFO] - Train, Round 103 : loss => 0.2430604596121522,  accuracy: 0.886121212121212, gradient_norm : 0.16567216581562308
[2025-09-18 14:00:50,072][flp2p.graph_runner][INFO] - Test, Round 103 : loss => 0.43703956844966596,  accuracy: 0.7998
[2025-09-18 14:00:53,209][flp2p.graph_runner][INFO] - Train, Round 104 : loss => 0.26052775436293973,  accuracy: 0.8738181818181818, gradient_norm : 0.13982346950237198
[2025-09-18 14:01:00,595][flp2p.graph_runner][INFO] - Test, Round 104 : loss => 0.4436552524069242,  accuracy: 0.8003
[2025-09-18 14:01:03,726][flp2p.graph_runner][INFO] - Train, Round 105 : loss => 0.19483381708914818,  accuracy: 0.9082424242424242, gradient_norm : 0.12563920048206847
[2025-09-18 14:01:11,103][flp2p.graph_runner][INFO] - Test, Round 105 : loss => 0.4457803148529713,  accuracy: 0.7993
[2025-09-18 14:01:14,247][flp2p.graph_runner][INFO] - Train, Round 106 : loss => 0.2653331923359077,  accuracy: 0.8775151515151515, gradient_norm : 0.14178001654011993
[2025-09-18 14:01:21,550][flp2p.graph_runner][INFO] - Test, Round 106 : loss => 0.44469600048591956,  accuracy: 0.7993
[2025-09-18 14:01:24,679][flp2p.graph_runner][INFO] - Train, Round 107 : loss => 0.2811552372170188,  accuracy: 0.8738787878787878, gradient_norm : 0.1569937240486969
[2025-09-18 14:01:32,010][flp2p.graph_runner][INFO] - Test, Round 107 : loss => 0.4360775780223833,  accuracy: 0.8059
[2025-09-18 14:01:35,133][flp2p.graph_runner][INFO] - Train, Round 108 : loss => 0.3463239983855309,  accuracy: 0.8287272727272728, gradient_norm : 0.16767685182499215
[2025-09-18 14:01:42,482][flp2p.graph_runner][INFO] - Test, Round 108 : loss => 0.43197668229007286,  accuracy: 0.8055
[2025-09-18 14:01:45,589][flp2p.graph_runner][INFO] - Train, Round 109 : loss => 0.36142927856541046,  accuracy: 0.8046060606060605, gradient_norm : 0.14004777118783346
[2025-09-18 14:01:52,934][flp2p.graph_runner][INFO] - Test, Round 109 : loss => 0.43243747197088434,  accuracy: 0.8069
[2025-09-18 14:01:56,069][flp2p.graph_runner][INFO] - Train, Round 110 : loss => 0.2678952383896489,  accuracy: 0.8738787878787878, gradient_norm : 0.14510018291546503
[2025-09-18 14:02:03,407][flp2p.graph_runner][INFO] - Test, Round 110 : loss => 0.4291484661123495,  accuracy: 0.8081
[2025-09-18 14:02:06,562][flp2p.graph_runner][INFO] - Train, Round 111 : loss => 0.1344957600509214,  accuracy: 0.9454545454545457, gradient_norm : 0.09324560293933436
[2025-09-18 14:02:13,835][flp2p.graph_runner][INFO] - Test, Round 111 : loss => 0.43103116482128734,  accuracy: 0.8067
[2025-09-18 14:02:16,711][flp2p.graph_runner][INFO] - Train, Round 112 : loss => 0.23116437328036968,  accuracy: 0.8897333333333334, gradient_norm : 0.14739013073442117
[2025-09-18 14:02:24,112][flp2p.graph_runner][INFO] - Test, Round 112 : loss => 0.4304800057267642,  accuracy: 0.8084
[2025-09-18 14:02:27,254][flp2p.graph_runner][INFO] - Train, Round 113 : loss => 0.29374839807031294,  accuracy: 0.8558787878787878, gradient_norm : 0.1409254275758856
[2025-09-18 14:02:34,657][flp2p.graph_runner][INFO] - Test, Round 113 : loss => 0.4339946422093934,  accuracy: 0.8084
[2025-09-18 14:02:37,600][flp2p.graph_runner][INFO] - Train, Round 114 : loss => 0.23401851036896915,  accuracy: 0.9039999999999998, gradient_norm : 0.15428120039950494
[2025-09-18 14:02:44,856][flp2p.graph_runner][INFO] - Test, Round 114 : loss => 0.4277130734348124,  accuracy: 0.8122
[2025-09-18 14:02:48,091][flp2p.graph_runner][INFO] - Train, Round 115 : loss => 0.27720791566022235,  accuracy: 0.8646060606060607, gradient_norm : 0.13261216138299353
[2025-09-18 14:02:55,487][flp2p.graph_runner][INFO] - Test, Round 115 : loss => 0.42570320655405175,  accuracy: 0.8127
[2025-09-18 14:02:58,706][flp2p.graph_runner][INFO] - Train, Round 116 : loss => 0.20970034997042528,  accuracy: 0.9109696969696969, gradient_norm : 0.1564273714326465
[2025-09-18 14:03:06,093][flp2p.graph_runner][INFO] - Test, Round 116 : loss => 0.4272212245348387,  accuracy: 0.8132
[2025-09-18 14:03:09,236][flp2p.graph_runner][INFO] - Train, Round 117 : loss => 0.2599024310487184,  accuracy: 0.8717575757575756, gradient_norm : 0.1263293442011501
[2025-09-18 14:03:16,631][flp2p.graph_runner][INFO] - Test, Round 117 : loss => 0.42822433975570245,  accuracy: 0.8144
[2025-09-18 14:03:19,762][flp2p.graph_runner][INFO] - Train, Round 118 : loss => 0.21424517525056563,  accuracy: 0.8968484848484849, gradient_norm : 0.11406018837629923
[2025-09-18 14:03:27,109][flp2p.graph_runner][INFO] - Test, Round 118 : loss => 0.4270706091998294,  accuracy: 0.8163
[2025-09-18 14:03:30,244][flp2p.graph_runner][INFO] - Train, Round 119 : loss => 0.2192826246529654,  accuracy: 0.8984848484848486, gradient_norm : 0.12841432776034287
[2025-09-18 14:03:37,613][flp2p.graph_runner][INFO] - Test, Round 119 : loss => 0.42675486383917577,  accuracy: 0.8176
[2025-09-18 14:03:40,796][flp2p.graph_runner][INFO] - Train, Round 120 : loss => 0.23524851515031858,  accuracy: 0.8927878787878789, gradient_norm : 0.1558628485386213
[2025-09-18 14:03:48,125][flp2p.graph_runner][INFO] - Test, Round 120 : loss => 0.43217197269825347,  accuracy: 0.8158
[2025-09-18 14:03:51,228][flp2p.graph_runner][INFO] - Train, Round 121 : loss => 0.15358558749116383,  accuracy: 0.9406666666666668, gradient_norm : 0.1639649468822191
[2025-09-18 14:03:58,598][flp2p.graph_runner][INFO] - Test, Round 121 : loss => 0.43616106143729394,  accuracy: 0.8148
[2025-09-18 14:04:01,713][flp2p.graph_runner][INFO] - Train, Round 122 : loss => 0.2300775379344389,  accuracy: 0.8948484848484848, gradient_norm : 0.13720659926069811
[2025-09-18 14:04:09,073][flp2p.graph_runner][INFO] - Test, Round 122 : loss => 0.43599663183303183,  accuracy: 0.814
[2025-09-18 14:04:12,172][flp2p.graph_runner][INFO] - Train, Round 123 : loss => 0.16669637374480034,  accuracy: 0.9286060606060605, gradient_norm : 0.13554933329296712
[2025-09-18 14:04:19,527][flp2p.graph_runner][INFO] - Test, Round 123 : loss => 0.43665178416777023,  accuracy: 0.8138
[2025-09-18 14:04:22,679][flp2p.graph_runner][INFO] - Train, Round 124 : loss => 0.12367736418080388,  accuracy: 0.9494545454545453, gradient_norm : 0.13506766649908133
[2025-09-18 14:04:30,051][flp2p.graph_runner][INFO] - Test, Round 124 : loss => 0.4328847552617391,  accuracy: 0.8158
[2025-09-18 14:04:33,179][flp2p.graph_runner][INFO] - Train, Round 125 : loss => 0.25985148807408254,  accuracy: 0.8774545454545456, gradient_norm : 0.1450652014067757
[2025-09-18 14:04:40,510][flp2p.graph_runner][INFO] - Test, Round 125 : loss => 0.4296781407564047,  accuracy: 0.8165
[2025-09-18 14:04:43,610][flp2p.graph_runner][INFO] - Train, Round 126 : loss => 0.19032070381119118,  accuracy: 0.9064242424242424, gradient_norm : 0.14025473731410254
[2025-09-18 14:04:50,941][flp2p.graph_runner][INFO] - Test, Round 126 : loss => 0.4297594667716141,  accuracy: 0.8171
[2025-09-18 14:04:54,079][flp2p.graph_runner][INFO] - Train, Round 127 : loss => 0.22005948760404748,  accuracy: 0.9008484848484849, gradient_norm : 0.1386980759173969
[2025-09-18 14:05:01,341][flp2p.graph_runner][INFO] - Test, Round 127 : loss => 0.42379944165199784,  accuracy: 0.8209
[2025-09-18 14:05:04,463][flp2p.graph_runner][INFO] - Train, Round 128 : loss => 0.1388534845282422,  accuracy: 0.9433333333333335, gradient_norm : 0.1340878768352568
[2025-09-18 14:05:11,824][flp2p.graph_runner][INFO] - Test, Round 128 : loss => 0.426320332411095,  accuracy: 0.8192
[2025-09-18 14:05:15,007][flp2p.graph_runner][INFO] - Train, Round 129 : loss => 0.2166644494707517,  accuracy: 0.8990909090909093, gradient_norm : 0.140982298392587
[2025-09-18 14:05:22,316][flp2p.graph_runner][INFO] - Test, Round 129 : loss => 0.4266996184711624,  accuracy: 0.8195
[2025-09-18 14:05:25,437][flp2p.graph_runner][INFO] - Train, Round 130 : loss => 0.13281802050678773,  accuracy: 0.9453939393939392, gradient_norm : 0.13618383407140924
[2025-09-18 14:05:32,767][flp2p.graph_runner][INFO] - Test, Round 130 : loss => 0.4261928649813729,  accuracy: 0.8202
[2025-09-18 14:05:35,895][flp2p.graph_runner][INFO] - Train, Round 131 : loss => 0.2719217646774022,  accuracy: 0.8729696969696971, gradient_norm : 0.1484408269051694
[2025-09-18 14:05:43,254][flp2p.graph_runner][INFO] - Test, Round 131 : loss => 0.4287078482872456,  accuracy: 0.8207
[2025-09-18 14:05:46,423][flp2p.graph_runner][INFO] - Train, Round 132 : loss => 0.2374140550627025,  accuracy: 0.8872121212121211, gradient_norm : 0.12932029243213716
[2025-09-18 14:05:53,738][flp2p.graph_runner][INFO] - Test, Round 132 : loss => 0.42494470042287485,  accuracy: 0.8223
[2025-09-18 14:05:56,824][flp2p.graph_runner][INFO] - Train, Round 133 : loss => 0.19183672145685204,  accuracy: 0.9163030303030302, gradient_norm : 0.13668071541009577
[2025-09-18 14:06:04,164][flp2p.graph_runner][INFO] - Test, Round 133 : loss => 0.4242752738051842,  accuracy: 0.8231
[2025-09-18 14:06:07,293][flp2p.graph_runner][INFO] - Train, Round 134 : loss => 0.19069201831465646,  accuracy: 0.9181212121212122, gradient_norm : 0.14496401451841254
[2025-09-18 14:06:14,575][flp2p.graph_runner][INFO] - Test, Round 134 : loss => 0.4269151484452035,  accuracy: 0.8231
[2025-09-18 14:06:17,761][flp2p.graph_runner][INFO] - Train, Round 135 : loss => 0.15765224724302906,  accuracy: 0.9395151515151514, gradient_norm : 0.16330933230740557
[2025-09-18 14:06:25,097][flp2p.graph_runner][INFO] - Test, Round 135 : loss => 0.42892969903894484,  accuracy: 0.8233
[2025-09-18 14:06:28,285][flp2p.graph_runner][INFO] - Train, Round 136 : loss => 0.1852669487790792,  accuracy: 0.9146666666666665, gradient_norm : 0.10306275120143743
[2025-09-18 14:06:35,649][flp2p.graph_runner][INFO] - Test, Round 136 : loss => 0.4249079457882156,  accuracy: 0.8258
[2025-09-18 14:06:38,854][flp2p.graph_runner][INFO] - Train, Round 137 : loss => 0.13765592375635402,  accuracy: 0.9361212121212121, gradient_norm : 0.12991444711628752
[2025-09-18 14:06:46,190][flp2p.graph_runner][INFO] - Test, Round 137 : loss => 0.42499193830322085,  accuracy: 0.8284
[2025-09-18 14:06:49,448][flp2p.graph_runner][INFO] - Train, Round 138 : loss => 0.17156397240910407,  accuracy: 0.9292121212121213, gradient_norm : 0.13799205566660866
[2025-09-18 14:06:56,818][flp2p.graph_runner][INFO] - Test, Round 138 : loss => 0.4227975500472451,  accuracy: 0.8299
[2025-09-18 14:07:00,060][flp2p.graph_runner][INFO] - Train, Round 139 : loss => 0.21887437366876503,  accuracy: 0.8932727272727272, gradient_norm : 0.12824561961342332
[2025-09-18 14:07:07,440][flp2p.graph_runner][INFO] - Test, Round 139 : loss => 0.42643319628450915,  accuracy: 0.8298
[2025-09-18 14:07:10,657][flp2p.graph_runner][INFO] - Train, Round 140 : loss => 0.08837971318771443,  accuracy: 0.9674545454545456, gradient_norm : 0.1279136961212197
[2025-09-18 14:07:18,015][flp2p.graph_runner][INFO] - Test, Round 140 : loss => 0.43038163549333824,  accuracy: 0.8285
[2025-09-18 14:07:21,236][flp2p.graph_runner][INFO] - Train, Round 141 : loss => 0.16443446578487264,  accuracy: 0.9284242424242424, gradient_norm : 0.13027965165299496
[2025-09-18 14:07:28,634][flp2p.graph_runner][INFO] - Test, Round 141 : loss => 0.429235996199943,  accuracy: 0.83
[2025-09-18 14:07:31,873][flp2p.graph_runner][INFO] - Train, Round 142 : loss => 0.22096090570765983,  accuracy: 0.897090909090909, gradient_norm : 0.1400074338398142
[2025-09-18 14:07:39,187][flp2p.graph_runner][INFO] - Test, Round 142 : loss => 0.42806022748281064,  accuracy: 0.8308
[2025-09-18 14:07:42,385][flp2p.graph_runner][INFO] - Train, Round 143 : loss => 0.23987671308425615,  accuracy: 0.8946666666666667, gradient_norm : 0.15307069166759682
[2025-09-18 14:07:49,737][flp2p.graph_runner][INFO] - Test, Round 143 : loss => 0.4286027863434199,  accuracy: 0.8317
[2025-09-18 14:07:52,886][flp2p.graph_runner][INFO] - Train, Round 144 : loss => 0.1755821199626671,  accuracy: 0.9221212121212122, gradient_norm : 0.13040842149517287
[2025-09-18 14:08:00,195][flp2p.graph_runner][INFO] - Test, Round 144 : loss => 0.4301779617102587,  accuracy: 0.8333
[2025-09-18 14:08:03,329][flp2p.graph_runner][INFO] - Train, Round 145 : loss => 0.13875705652613946,  accuracy: 0.9412121212121211, gradient_norm : 0.12824288171952844
[2025-09-18 14:08:10,618][flp2p.graph_runner][INFO] - Test, Round 145 : loss => 0.4351504947860365,  accuracy: 0.832
[2025-09-18 14:08:13,773][flp2p.graph_runner][INFO] - Train, Round 146 : loss => 0.12211593227984291,  accuracy: 0.9510303030303031, gradient_norm : 0.13316038925868054
[2025-09-18 14:08:21,036][flp2p.graph_runner][INFO] - Test, Round 146 : loss => 0.43402099365883057,  accuracy: 0.8325
[2025-09-18 14:08:24,195][flp2p.graph_runner][INFO] - Train, Round 147 : loss => 0.16999753336489667,  accuracy: 0.9295757575757575, gradient_norm : 0.1427535933920803
[2025-09-18 14:08:31,574][flp2p.graph_runner][INFO] - Test, Round 147 : loss => 0.4360739681442814,  accuracy: 0.8307
[2025-09-18 14:08:34,716][flp2p.graph_runner][INFO] - Train, Round 148 : loss => 0.2570062781467537,  accuracy: 0.887030303030303, gradient_norm : 0.17106994256580502
[2025-09-18 14:08:42,014][flp2p.graph_runner][INFO] - Test, Round 148 : loss => 0.43713199291529325,  accuracy: 0.8327
[2025-09-18 14:08:44,886][flp2p.graph_runner][INFO] - Train, Round 149 : loss => 0.06355054274740526,  accuracy: 0.9780666666666666, gradient_norm : 0.08341850926517949
[2025-09-18 14:08:52,245][flp2p.graph_runner][INFO] - Test, Round 149 : loss => 0.43559976943401874,  accuracy: 0.8345
[2025-09-18 14:08:55,392][flp2p.graph_runner][INFO] - Train, Round 150 : loss => 0.23894940143053947,  accuracy: 0.8954545454545455, gradient_norm : 0.17714887516889222
[2025-09-18 14:09:02,760][flp2p.graph_runner][INFO] - Test, Round 150 : loss => 0.43905308292331086,  accuracy: 0.8348
[2025-09-18 14:09:05,870][flp2p.graph_runner][INFO] - Train, Round 151 : loss => 0.23850685857483528,  accuracy: 0.8954545454545455, gradient_norm : 0.16967389522438814
[2025-09-18 14:09:13,181][flp2p.graph_runner][INFO] - Test, Round 151 : loss => 0.4361432182009485,  accuracy: 0.8352
[2025-09-18 14:09:16,327][flp2p.graph_runner][INFO] - Train, Round 152 : loss => 0.11533761155117182,  accuracy: 0.9506060606060607, gradient_norm : 0.09226512168461108
[2025-09-18 14:09:23,697][flp2p.graph_runner][INFO] - Test, Round 152 : loss => 0.4395594097565883,  accuracy: 0.8335
[2025-09-18 14:09:26,847][flp2p.graph_runner][INFO] - Train, Round 153 : loss => 0.13575688913992492,  accuracy: 0.9391515151515151, gradient_norm : 0.10648852396051202
[2025-09-18 14:09:34,211][flp2p.graph_runner][INFO] - Test, Round 153 : loss => 0.44265609606717526,  accuracy: 0.8331
[2025-09-18 14:09:37,375][flp2p.graph_runner][INFO] - Train, Round 154 : loss => 0.09842587567002656,  accuracy: 0.9613333333333335, gradient_norm : 0.11830415708734625
[2025-09-18 14:09:44,690][flp2p.graph_runner][INFO] - Test, Round 154 : loss => 0.4452177485225694,  accuracy: 0.8337
[2025-09-18 14:09:47,788][flp2p.graph_runner][INFO] - Train, Round 155 : loss => 0.07448368516966293,  accuracy: 0.9726060606060607, gradient_norm : 0.10353663817520711
[2025-09-18 14:09:54,963][flp2p.graph_runner][INFO] - Test, Round 155 : loss => 0.44755356860174905,  accuracy: 0.8348
[2025-09-18 14:09:58,078][flp2p.graph_runner][INFO] - Train, Round 156 : loss => 0.13297558628727502,  accuracy: 0.9488484848484847, gradient_norm : 0.14338524921909906
[2025-09-18 14:10:05,176][flp2p.graph_runner][INFO] - Test, Round 156 : loss => 0.448764328382305,  accuracy: 0.8344
[2025-09-18 14:10:08,180][flp2p.graph_runner][INFO] - Train, Round 157 : loss => 0.09994036469225123,  accuracy: 0.956848484848485, gradient_norm : 0.11949892726929168
[2025-09-18 14:10:14,078][flp2p.graph_runner][INFO] - Test, Round 157 : loss => 0.45070618657334816,  accuracy: 0.8343
[2025-09-18 14:10:16,674][flp2p.graph_runner][INFO] - Train, Round 158 : loss => 0.08376452374491845,  accuracy: 0.9663333333333334, gradient_norm : 0.0896616007288408
[2025-09-18 14:10:22,654][flp2p.graph_runner][INFO] - Test, Round 158 : loss => 0.45541888999539,  accuracy: 0.834
[2025-09-18 14:10:25,543][flp2p.graph_runner][INFO] - Train, Round 159 : loss => 0.1517600586490888,  accuracy: 0.9347272727272727, gradient_norm : 0.12125169482365189
[2025-09-18 14:10:32,082][flp2p.graph_runner][INFO] - Test, Round 159 : loss => 0.4514825051541703,  accuracy: 0.8367
[2025-09-18 14:10:35,007][flp2p.graph_runner][INFO] - Train, Round 160 : loss => 0.07521383835846952,  accuracy: 0.9738181818181818, gradient_norm : 0.11407958178754374
[2025-09-18 14:10:40,975][flp2p.graph_runner][INFO] - Test, Round 160 : loss => 0.4542352026249545,  accuracy: 0.8359
[2025-09-18 14:10:43,805][flp2p.graph_runner][INFO] - Train, Round 161 : loss => 0.1726496579725223,  accuracy: 0.9292727272727273, gradient_norm : 0.14595783669711268
[2025-09-18 14:10:49,828][flp2p.graph_runner][INFO] - Test, Round 161 : loss => 0.45397884224087764,  accuracy: 0.8381
[2025-09-18 14:10:52,710][flp2p.graph_runner][INFO] - Train, Round 162 : loss => 0.05175529258882482,  accuracy: 0.9824848484848484, gradient_norm : 0.0795639984646136
[2025-09-18 14:10:58,687][flp2p.graph_runner][INFO] - Test, Round 162 : loss => 0.45168256370960835,  accuracy: 0.8396
[2025-09-18 14:11:01,538][flp2p.graph_runner][INFO] - Train, Round 163 : loss => 0.12965467983221107,  accuracy: 0.9469696969696969, gradient_norm : 0.13421460089993015
[2025-09-18 14:11:07,400][flp2p.graph_runner][INFO] - Test, Round 163 : loss => 0.45468228911249997,  accuracy: 0.8397
[2025-09-18 14:11:10,223][flp2p.graph_runner][INFO] - Train, Round 164 : loss => 0.07023397667953606,  accuracy: 0.9751515151515151, gradient_norm : 0.11397936322497676
[2025-09-18 14:11:16,245][flp2p.graph_runner][INFO] - Test, Round 164 : loss => 0.4551195426146762,  accuracy: 0.8405
[2025-09-18 14:11:19,074][flp2p.graph_runner][INFO] - Train, Round 165 : loss => 0.1835678113282592,  accuracy: 0.9203030303030302, gradient_norm : 0.13448539652247837
[2025-09-18 14:11:25,096][flp2p.graph_runner][INFO] - Test, Round 165 : loss => 0.4501084391829607,  accuracy: 0.8427
[2025-09-18 14:11:27,982][flp2p.graph_runner][INFO] - Train, Round 166 : loss => 0.10279180342286655,  accuracy: 0.9587878787878787, gradient_norm : 0.12387615083747243
[2025-09-18 14:11:34,026][flp2p.graph_runner][INFO] - Test, Round 166 : loss => 0.44866129173571734,  accuracy: 0.8445
[2025-09-18 14:11:36,850][flp2p.graph_runner][INFO] - Train, Round 167 : loss => 0.09456569276417262,  accuracy: 0.962, gradient_norm : 0.08574396825802728
[2025-09-18 14:11:42,716][flp2p.graph_runner][INFO] - Test, Round 167 : loss => 0.4500619221800056,  accuracy: 0.8444
[2025-09-18 14:11:45,617][flp2p.graph_runner][INFO] - Train, Round 168 : loss => 0.12456990194600473,  accuracy: 0.943939393939394, gradient_norm : 0.10866100799145452
[2025-09-18 14:11:52,215][flp2p.graph_runner][INFO] - Test, Round 168 : loss => 0.4556601817558361,  accuracy: 0.8439
[2025-09-18 14:11:55,149][flp2p.graph_runner][INFO] - Train, Round 169 : loss => 0.08669588540711928,  accuracy: 0.9658181818181818, gradient_norm : 0.120502975838623
[2025-09-18 14:12:01,166][flp2p.graph_runner][INFO] - Test, Round 169 : loss => 0.4524775789315625,  accuracy: 0.844
[2025-09-18 14:12:04,039][flp2p.graph_runner][INFO] - Train, Round 170 : loss => 0.15085877333696218,  accuracy: 0.9389696969696969, gradient_norm : 0.12463654018798125
[2025-09-18 14:12:09,886][flp2p.graph_runner][INFO] - Test, Round 170 : loss => 0.4536904694961696,  accuracy: 0.8448
[2025-09-18 14:12:12,734][flp2p.graph_runner][INFO] - Train, Round 171 : loss => 0.12855679200730938,  accuracy: 0.9523030303030302, gradient_norm : 0.1411967821626154
[2025-09-18 14:12:18,728][flp2p.graph_runner][INFO] - Test, Round 171 : loss => 0.4525584577001309,  accuracy: 0.8458
[2025-09-18 14:12:21,633][flp2p.graph_runner][INFO] - Train, Round 172 : loss => 0.10118696300498212,  accuracy: 0.9610909090909092, gradient_norm : 0.13494644072216735
[2025-09-18 14:12:28,351][flp2p.graph_runner][INFO] - Test, Round 172 : loss => 0.45313406965141273,  accuracy: 0.8466
[2025-09-18 14:12:31,464][flp2p.graph_runner][INFO] - Train, Round 173 : loss => 0.06623169867094178,  accuracy: 0.978969696969697, gradient_norm : 0.13898227286740472
[2025-09-18 14:12:38,618][flp2p.graph_runner][INFO] - Test, Round 173 : loss => 0.46163522891307457,  accuracy: 0.8451
[2025-09-18 14:12:41,760][flp2p.graph_runner][INFO] - Train, Round 174 : loss => 0.10861223455598472,  accuracy: 0.9506060606060606, gradient_norm : 0.10042193329687045
[2025-09-18 14:12:48,925][flp2p.graph_runner][INFO] - Test, Round 174 : loss => 0.46452699080776366,  accuracy: 0.845
[2025-09-18 14:12:51,995][flp2p.graph_runner][INFO] - Train, Round 175 : loss => 0.08296110372529612,  accuracy: 0.9664848484848485, gradient_norm : 0.10277862864269025
[2025-09-18 14:12:59,183][flp2p.graph_runner][INFO] - Test, Round 175 : loss => 0.471456993017218,  accuracy: 0.8436
[2025-09-18 14:13:02,219][flp2p.graph_runner][INFO] - Train, Round 176 : loss => 0.09944021310391948,  accuracy: 0.9607272727272728, gradient_norm : 0.11815786850079452
[2025-09-18 14:13:09,368][flp2p.graph_runner][INFO] - Test, Round 176 : loss => 0.4688169240213876,  accuracy: 0.8445
[2025-09-18 14:13:12,155][flp2p.graph_runner][INFO] - Train, Round 177 : loss => 0.05474830887580346,  accuracy: 0.9788666666666668, gradient_norm : 0.07825043798244666
[2025-09-18 14:13:19,319][flp2p.graph_runner][INFO] - Test, Round 177 : loss => 0.47164237716507335,  accuracy: 0.8448
[2025-09-18 14:13:22,392][flp2p.graph_runner][INFO] - Train, Round 178 : loss => 0.05933643491832296,  accuracy: 0.9781818181818182, gradient_norm : 0.07698776347903637
[2025-09-18 14:13:29,568][flp2p.graph_runner][INFO] - Test, Round 178 : loss => 0.47273170644731816,  accuracy: 0.8448
[2025-09-18 14:13:32,682][flp2p.graph_runner][INFO] - Train, Round 179 : loss => 0.1402857342097194,  accuracy: 0.9404848484848487, gradient_norm : 0.1321897070531655
[2025-09-18 14:13:39,841][flp2p.graph_runner][INFO] - Test, Round 179 : loss => 0.4725210027662387,  accuracy: 0.8442
[2025-09-18 14:13:42,937][flp2p.graph_runner][INFO] - Train, Round 180 : loss => 0.033824628443319044,  accuracy: 0.9878787878787878, gradient_norm : 0.061888000919716
[2025-09-18 14:13:50,004][flp2p.graph_runner][INFO] - Test, Round 180 : loss => 0.4742330449213143,  accuracy: 0.8443
[2025-09-18 14:13:53,050][flp2p.graph_runner][INFO] - Train, Round 181 : loss => 0.11525909061828361,  accuracy: 0.9540606060606062, gradient_norm : 0.11915455240221888
[2025-09-18 14:14:00,200][flp2p.graph_runner][INFO] - Test, Round 181 : loss => 0.4762527881542381,  accuracy: 0.8449
[2025-09-18 14:14:03,297][flp2p.graph_runner][INFO] - Train, Round 182 : loss => 0.08557789447096165,  accuracy: 0.9690909090909091, gradient_norm : 0.12644494722526023
[2025-09-18 14:14:10,461][flp2p.graph_runner][INFO] - Test, Round 182 : loss => 0.4822820527043135,  accuracy: 0.8432
[2025-09-18 14:14:13,588][flp2p.graph_runner][INFO] - Train, Round 183 : loss => 0.15910177794744984,  accuracy: 0.9300000000000002, gradient_norm : 0.14103683038617468
[2025-09-18 14:14:20,680][flp2p.graph_runner][INFO] - Test, Round 183 : loss => 0.481852168746409,  accuracy: 0.8458
[2025-09-18 14:14:23,782][flp2p.graph_runner][INFO] - Train, Round 184 : loss => 0.11941918056716544,  accuracy: 0.954060606060606, gradient_norm : 0.13283747761436157
[2025-09-18 14:14:30,933][flp2p.graph_runner][INFO] - Test, Round 184 : loss => 0.48154570076232694,  accuracy: 0.8453
[2025-09-18 14:14:33,989][flp2p.graph_runner][INFO] - Train, Round 185 : loss => 0.11460433615291747,  accuracy: 0.9525454545454546, gradient_norm : 0.12782683771703843
[2025-09-18 14:14:41,154][flp2p.graph_runner][INFO] - Test, Round 185 : loss => 0.48668556032725857,  accuracy: 0.8457
[2025-09-18 14:14:44,258][flp2p.graph_runner][INFO] - Train, Round 186 : loss => 0.08194160259457745,  accuracy: 0.9675151515151514, gradient_norm : 0.08601702424183923
[2025-09-18 14:14:51,437][flp2p.graph_runner][INFO] - Test, Round 186 : loss => 0.48778003818539817,  accuracy: 0.846
[2025-09-18 14:14:54,561][flp2p.graph_runner][INFO] - Train, Round 187 : loss => 0.10004369968241814,  accuracy: 0.9596363636363636, gradient_norm : 0.10050462588193318
[2025-09-18 14:15:01,798][flp2p.graph_runner][INFO] - Test, Round 187 : loss => 0.4853721766816044,  accuracy: 0.8479
[2025-09-18 14:15:04,850][flp2p.graph_runner][INFO] - Train, Round 188 : loss => 0.10576139301469915,  accuracy: 0.9496969696969696, gradient_norm : 0.11158290092960997
[2025-09-18 14:15:12,049][flp2p.graph_runner][INFO] - Test, Round 188 : loss => 0.48289833590824854,  accuracy: 0.849
[2025-09-18 14:15:15,135][flp2p.graph_runner][INFO] - Train, Round 189 : loss => 0.05548301438864007,  accuracy: 0.9815151515151515, gradient_norm : 0.09590378856439953
[2025-09-18 14:15:22,252][flp2p.graph_runner][INFO] - Test, Round 189 : loss => 0.4849796478761748,  accuracy: 0.8497
[2025-09-18 14:15:25,331][flp2p.graph_runner][INFO] - Train, Round 190 : loss => 0.09072954575510582,  accuracy: 0.966181818181818, gradient_norm : 0.11877962678909305
[2025-09-18 14:15:32,426][flp2p.graph_runner][INFO] - Test, Round 190 : loss => 0.4886090448232323,  accuracy: 0.8505
[2025-09-18 14:15:35,534][flp2p.graph_runner][INFO] - Train, Round 191 : loss => 0.11514510327771899,  accuracy: 0.9543636363636364, gradient_norm : 0.1333670916507745
[2025-09-18 14:15:42,704][flp2p.graph_runner][INFO] - Test, Round 191 : loss => 0.4851864675604767,  accuracy: 0.852
[2025-09-18 14:15:45,743][flp2p.graph_runner][INFO] - Train, Round 192 : loss => 0.008813305240130716,  accuracy: 0.9984848484848485, gradient_norm : 0.03701895470302599
[2025-09-18 14:15:52,897][flp2p.graph_runner][INFO] - Test, Round 192 : loss => 0.4848299768000661,  accuracy: 0.8517
[2025-09-18 14:15:56,009][flp2p.graph_runner][INFO] - Train, Round 193 : loss => 0.0896241607193266,  accuracy: 0.9638181818181818, gradient_norm : 0.1134288147993331
[2025-09-18 14:16:03,223][flp2p.graph_runner][INFO] - Test, Round 193 : loss => 0.4868676615329066,  accuracy: 0.8513
[2025-09-18 14:16:06,346][flp2p.graph_runner][INFO] - Train, Round 194 : loss => 0.11123896044008173,  accuracy: 0.9572121212121213, gradient_norm : 0.1087407906365962
[2025-09-18 14:16:13,587][flp2p.graph_runner][INFO] - Test, Round 194 : loss => 0.4875969300865947,  accuracy: 0.8507
[2025-09-18 14:16:16,678][flp2p.graph_runner][INFO] - Train, Round 195 : loss => 0.0737884193458213,  accuracy: 0.9741212121212122, gradient_norm : 0.12186247800758991
[2025-09-18 14:16:23,799][flp2p.graph_runner][INFO] - Test, Round 195 : loss => 0.48733412379099117,  accuracy: 0.8508
[2025-09-18 14:16:26,865][flp2p.graph_runner][INFO] - Train, Round 196 : loss => 0.07667993837252565,  accuracy: 0.9644848484848484, gradient_norm : 0.07028904321708068
[2025-09-18 14:16:34,121][flp2p.graph_runner][INFO] - Test, Round 196 : loss => 0.48572954105435,  accuracy: 0.8518
[2025-09-18 14:16:37,220][flp2p.graph_runner][INFO] - Train, Round 197 : loss => 0.13939173678328234,  accuracy: 0.9427272727272727, gradient_norm : 0.10569669525428309
[2025-09-18 14:16:44,431][flp2p.graph_runner][INFO] - Test, Round 197 : loss => 0.49129030447265193,  accuracy: 0.8523
[2025-09-18 14:16:47,535][flp2p.graph_runner][INFO] - Train, Round 198 : loss => 0.10145934574288909,  accuracy: 0.9603030303030303, gradient_norm : 0.12347049082088414
[2025-09-18 14:16:54,748][flp2p.graph_runner][INFO] - Test, Round 198 : loss => 0.49305174649782274,  accuracy: 0.8525
[2025-09-18 14:16:57,835][flp2p.graph_runner][INFO] - Train, Round 199 : loss => 0.07349473940695425,  accuracy: 0.9736969696969696, gradient_norm : 0.10768379814013601
[2025-09-18 14:17:04,994][flp2p.graph_runner][INFO] - Test, Round 199 : loss => 0.4963526348275671,  accuracy: 0.8519
[2025-09-18 14:17:04,995][__main__][INFO] - Train, Round 001: loss=1.6424, accuracy=0.4966, gradient_norm=0.9501, 
[2025-09-18 14:17:04,995][__main__][INFO] - Train, Round 002: loss=1.2778, accuracy=0.5359, gradient_norm=0.6512, 
[2025-09-18 14:17:04,995][__main__][INFO] - Train, Round 003: loss=1.2305, accuracy=0.5485, gradient_norm=0.6428, 
[2025-09-18 14:17:04,995][__main__][INFO] - Train, Round 004: loss=1.2777, accuracy=0.5363, gradient_norm=0.6439, 
[2025-09-18 14:17:04,995][__main__][INFO] - Train, Round 005: loss=1.4152, accuracy=0.5055, gradient_norm=0.7387, 
[2025-09-18 14:17:04,995][__main__][INFO] - Train, Round 006: loss=0.9858, accuracy=0.6725, gradient_norm=0.5884, 
[2025-09-18 14:17:04,995][__main__][INFO] - Train, Round 007: loss=1.0605, accuracy=0.5953, gradient_norm=0.5341, 
[2025-09-18 14:17:04,995][__main__][INFO] - Train, Round 008: loss=1.0603, accuracy=0.5706, gradient_norm=0.5201, 
[2025-09-18 14:17:04,995][__main__][INFO] - Train, Round 009: loss=1.1107, accuracy=0.5937, gradient_norm=0.5628, 
[2025-09-18 14:17:04,995][__main__][INFO] - Train, Round 010: loss=0.8365, accuracy=0.5777, gradient_norm=0.3207, 
[2025-09-18 14:17:04,995][__main__][INFO] - Train, Round 011: loss=1.2772, accuracy=0.5355, gradient_norm=0.6486, 
[2025-09-18 14:17:04,995][__main__][INFO] - Train, Round 012: loss=0.5789, accuracy=0.6958, gradient_norm=0.2347, 
[2025-09-18 14:17:04,995][__main__][INFO] - Train, Round 013: loss=0.6278, accuracy=0.6676, gradient_norm=0.2400, 
[2025-09-18 14:17:04,995][__main__][INFO] - Train, Round 014: loss=0.6800, accuracy=0.6881, gradient_norm=0.3157, 
[2025-09-18 14:17:04,995][__main__][INFO] - Train, Round 015: loss=0.9834, accuracy=0.5673, gradient_norm=0.4492, 
[2025-09-18 14:17:04,995][__main__][INFO] - Train, Round 016: loss=0.6501, accuracy=0.6930, gradient_norm=0.3206, 
[2025-09-18 14:17:04,995][__main__][INFO] - Train, Round 017: loss=1.0460, accuracy=0.5943, gradient_norm=0.5800, 
[2025-09-18 14:17:04,995][__main__][INFO] - Train, Round 018: loss=0.7657, accuracy=0.6797, gradient_norm=0.3539, 
[2025-09-18 14:17:04,995][__main__][INFO] - Train, Round 019: loss=0.6608, accuracy=0.7161, gradient_norm=0.3550, 
[2025-09-18 14:17:04,995][__main__][INFO] - Train, Round 020: loss=0.7357, accuracy=0.6971, gradient_norm=0.4348, 
[2025-09-18 14:17:04,995][__main__][INFO] - Train, Round 021: loss=0.7610, accuracy=0.6504, gradient_norm=0.3724, 
[2025-09-18 14:17:04,995][__main__][INFO] - Train, Round 022: loss=0.6878, accuracy=0.6247, gradient_norm=0.2848, 
[2025-09-18 14:17:04,995][__main__][INFO] - Train, Round 023: loss=0.6611, accuracy=0.7047, gradient_norm=0.3663, 
[2025-09-18 14:17:04,995][__main__][INFO] - Train, Round 024: loss=0.7723, accuracy=0.6124, gradient_norm=0.3335, 
[2025-09-18 14:17:04,995][__main__][INFO] - Train, Round 025: loss=0.5739, accuracy=0.6801, gradient_norm=0.2389, 
[2025-09-18 14:17:04,995][__main__][INFO] - Train, Round 026: loss=0.6303, accuracy=0.6719, gradient_norm=0.2527, 
[2025-09-18 14:17:04,995][__main__][INFO] - Train, Round 027: loss=0.5552, accuracy=0.7370, gradient_norm=0.2751, 
[2025-09-18 14:17:04,995][__main__][INFO] - Train, Round 028: loss=0.5125, accuracy=0.7334, gradient_norm=0.2101, 
[2025-09-18 14:17:04,995][__main__][INFO] - Train, Round 029: loss=0.7558, accuracy=0.6695, gradient_norm=0.3693, 
[2025-09-18 14:17:04,995][__main__][INFO] - Train, Round 030: loss=0.4044, accuracy=0.8052, gradient_norm=0.1809, 
[2025-09-18 14:17:04,995][__main__][INFO] - Train, Round 031: loss=0.4948, accuracy=0.7318, gradient_norm=0.1862, 
[2025-09-18 14:17:04,995][__main__][INFO] - Train, Round 032: loss=0.4739, accuracy=0.7280, gradient_norm=0.1503, 
[2025-09-18 14:17:04,995][__main__][INFO] - Train, Round 033: loss=0.6237, accuracy=0.6523, gradient_norm=0.2426, 
[2025-09-18 14:17:04,995][__main__][INFO] - Train, Round 034: loss=0.5022, accuracy=0.6875, gradient_norm=0.1416, 
[2025-09-18 14:17:04,995][__main__][INFO] - Train, Round 035: loss=0.5189, accuracy=0.6871, gradient_norm=0.1526, 
[2025-09-18 14:17:04,995][__main__][INFO] - Train, Round 036: loss=0.4923, accuracy=0.7989, gradient_norm=0.2827, 
[2025-09-18 14:17:04,995][__main__][INFO] - Train, Round 037: loss=0.4608, accuracy=0.7568, gradient_norm=0.1815, 
[2025-09-18 14:17:04,995][__main__][INFO] - Train, Round 038: loss=0.4117, accuracy=0.7673, gradient_norm=0.1225, 
[2025-09-18 14:17:04,995][__main__][INFO] - Train, Round 039: loss=0.5184, accuracy=0.7601, gradient_norm=0.2094, 
[2025-09-18 14:17:04,995][__main__][INFO] - Train, Round 040: loss=0.5913, accuracy=0.7248, gradient_norm=0.2862, 
[2025-09-18 14:17:04,995][__main__][INFO] - Train, Round 041: loss=0.4119, accuracy=0.8042, gradient_norm=0.2241, 
[2025-09-18 14:17:04,995][__main__][INFO] - Train, Round 042: loss=0.3789, accuracy=0.7949, gradient_norm=0.1772, 
[2025-09-18 14:17:04,995][__main__][INFO] - Train, Round 043: loss=0.4499, accuracy=0.7529, gradient_norm=0.1605, 
[2025-09-18 14:17:04,995][__main__][INFO] - Train, Round 044: loss=0.3499, accuracy=0.8194, gradient_norm=0.1774, 
[2025-09-18 14:17:04,995][__main__][INFO] - Train, Round 045: loss=0.3463, accuracy=0.8031, gradient_norm=0.1514, 
[2025-09-18 14:17:04,995][__main__][INFO] - Train, Round 046: loss=0.3929, accuracy=0.7840, gradient_norm=0.1696, 
[2025-09-18 14:17:04,995][__main__][INFO] - Train, Round 047: loss=0.4414, accuracy=0.7448, gradient_norm=0.1594, 
[2025-09-18 14:17:04,995][__main__][INFO] - Train, Round 048: loss=0.3979, accuracy=0.7645, gradient_norm=0.1393, 
[2025-09-18 14:17:04,995][__main__][INFO] - Train, Round 049: loss=0.2895, accuracy=0.8494, gradient_norm=0.1237, 
[2025-09-18 14:17:04,995][__main__][INFO] - Train, Round 050: loss=0.4097, accuracy=0.7655, gradient_norm=0.1356, 
[2025-09-18 14:17:04,995][__main__][INFO] - Train, Round 051: loss=0.3107, accuracy=0.8255, gradient_norm=0.1355, 
[2025-09-18 14:17:04,995][__main__][INFO] - Train, Round 052: loss=0.4662, accuracy=0.7450, gradient_norm=0.1859, 
[2025-09-18 14:17:04,995][__main__][INFO] - Train, Round 053: loss=0.3265, accuracy=0.8327, gradient_norm=0.1329, 
[2025-09-18 14:17:04,995][__main__][INFO] - Train, Round 054: loss=0.3658, accuracy=0.8172, gradient_norm=0.1504, 
[2025-09-18 14:17:04,995][__main__][INFO] - Train, Round 055: loss=0.3708, accuracy=0.8024, gradient_norm=0.1294, 
[2025-09-18 14:17:04,995][__main__][INFO] - Train, Round 056: loss=0.3772, accuracy=0.7882, gradient_norm=0.1454, 
[2025-09-18 14:17:04,995][__main__][INFO] - Train, Round 057: loss=0.3937, accuracy=0.7915, gradient_norm=0.1523, 
[2025-09-18 14:17:04,995][__main__][INFO] - Train, Round 058: loss=0.4162, accuracy=0.7461, gradient_norm=0.1270, 
[2025-09-18 14:17:04,995][__main__][INFO] - Train, Round 059: loss=0.3365, accuracy=0.8137, gradient_norm=0.1156, 
[2025-09-18 14:17:04,996][__main__][INFO] - Train, Round 060: loss=0.3221, accuracy=0.8212, gradient_norm=0.1039, 
[2025-09-18 14:17:04,996][__main__][INFO] - Train, Round 061: loss=0.3140, accuracy=0.8297, gradient_norm=0.1181, 
[2025-09-18 14:17:04,996][__main__][INFO] - Train, Round 062: loss=0.4104, accuracy=0.7850, gradient_norm=0.1271, 
[2025-09-18 14:17:04,996][__main__][INFO] - Train, Round 063: loss=0.3681, accuracy=0.8108, gradient_norm=0.1726, 
[2025-09-18 14:17:04,996][__main__][INFO] - Train, Round 064: loss=0.2717, accuracy=0.8633, gradient_norm=0.1446, 
[2025-09-18 14:17:04,996][__main__][INFO] - Train, Round 065: loss=0.3460, accuracy=0.8278, gradient_norm=0.1460, 
[2025-09-18 14:17:04,996][__main__][INFO] - Train, Round 066: loss=0.3553, accuracy=0.8169, gradient_norm=0.1393, 
[2025-09-18 14:17:04,996][__main__][INFO] - Train, Round 067: loss=0.2333, accuracy=0.8993, gradient_norm=0.1323, 
[2025-09-18 14:17:04,996][__main__][INFO] - Train, Round 068: loss=0.3851, accuracy=0.8021, gradient_norm=0.1666, 
[2025-09-18 14:17:04,996][__main__][INFO] - Train, Round 069: loss=0.3567, accuracy=0.8058, gradient_norm=0.1264, 
[2025-09-18 14:17:04,996][__main__][INFO] - Train, Round 070: loss=0.3103, accuracy=0.8425, gradient_norm=0.1221, 
[2025-09-18 14:17:04,996][__main__][INFO] - Train, Round 071: loss=0.3350, accuracy=0.8264, gradient_norm=0.1582, 
[2025-09-18 14:17:04,996][__main__][INFO] - Train, Round 072: loss=0.3255, accuracy=0.8313, gradient_norm=0.1205, 
[2025-09-18 14:17:04,996][__main__][INFO] - Train, Round 073: loss=0.3714, accuracy=0.8035, gradient_norm=0.1386, 
[2025-09-18 14:17:04,996][__main__][INFO] - Train, Round 074: loss=0.3353, accuracy=0.8338, gradient_norm=0.1387, 
[2025-09-18 14:17:04,996][__main__][INFO] - Train, Round 075: loss=0.3968, accuracy=0.7827, gradient_norm=0.1217, 
[2025-09-18 14:17:04,996][__main__][INFO] - Train, Round 076: loss=0.3154, accuracy=0.8156, gradient_norm=0.0957, 
[2025-09-18 14:17:04,996][__main__][INFO] - Train, Round 077: loss=0.3846, accuracy=0.8063, gradient_norm=0.1453, 
[2025-09-18 14:17:04,996][__main__][INFO] - Train, Round 078: loss=0.2611, accuracy=0.8855, gradient_norm=0.1647, 
[2025-09-18 14:17:04,996][__main__][INFO] - Train, Round 079: loss=0.3096, accuracy=0.8558, gradient_norm=0.1469, 
[2025-09-18 14:17:04,996][__main__][INFO] - Train, Round 080: loss=0.2853, accuracy=0.8614, gradient_norm=0.1290, 
[2025-09-18 14:17:04,996][__main__][INFO] - Train, Round 081: loss=0.1575, accuracy=0.9336, gradient_norm=0.1252, 
[2025-09-18 14:17:04,996][__main__][INFO] - Train, Round 082: loss=0.2620, accuracy=0.8648, gradient_norm=0.1194, 
[2025-09-18 14:17:04,996][__main__][INFO] - Train, Round 083: loss=0.3074, accuracy=0.8393, gradient_norm=0.1366, 
[2025-09-18 14:17:04,996][__main__][INFO] - Train, Round 084: loss=0.3635, accuracy=0.8102, gradient_norm=0.1327, 
[2025-09-18 14:17:04,996][__main__][INFO] - Train, Round 085: loss=0.2690, accuracy=0.8675, gradient_norm=0.1230, 
[2025-09-18 14:17:04,996][__main__][INFO] - Train, Round 086: loss=0.2461, accuracy=0.8840, gradient_norm=0.1154, 
[2025-09-18 14:17:04,996][__main__][INFO] - Train, Round 087: loss=0.2302, accuracy=0.8827, gradient_norm=0.1206, 
[2025-09-18 14:17:04,996][__main__][INFO] - Train, Round 088: loss=0.3878, accuracy=0.7952, gradient_norm=0.1361, 
[2025-09-18 14:17:04,996][__main__][INFO] - Train, Round 089: loss=0.1568, accuracy=0.9313, gradient_norm=0.1125, 
[2025-09-18 14:17:04,996][__main__][INFO] - Train, Round 090: loss=0.3125, accuracy=0.8472, gradient_norm=0.1679, 
[2025-09-18 14:17:04,996][__main__][INFO] - Train, Round 091: loss=0.2342, accuracy=0.8924, gradient_norm=0.1131, 
[2025-09-18 14:17:04,996][__main__][INFO] - Train, Round 092: loss=0.4076, accuracy=0.7842, gradient_norm=0.1380, 
[2025-09-18 14:17:04,996][__main__][INFO] - Train, Round 093: loss=0.3588, accuracy=0.8204, gradient_norm=0.1605, 
[2025-09-18 14:17:04,996][__main__][INFO] - Train, Round 094: loss=0.2506, accuracy=0.8655, gradient_norm=0.1306, 
[2025-09-18 14:17:04,996][__main__][INFO] - Train, Round 095: loss=0.2357, accuracy=0.8884, gradient_norm=0.1300, 
[2025-09-18 14:17:04,996][__main__][INFO] - Train, Round 096: loss=0.2663, accuracy=0.8582, gradient_norm=0.1057, 
[2025-09-18 14:17:04,996][__main__][INFO] - Train, Round 097: loss=0.1761, accuracy=0.9258, gradient_norm=0.1454, 
[2025-09-18 14:17:04,996][__main__][INFO] - Train, Round 098: loss=0.3164, accuracy=0.8529, gradient_norm=0.1552, 
[2025-09-18 14:17:04,996][__main__][INFO] - Train, Round 099: loss=0.2589, accuracy=0.8725, gradient_norm=0.1394, 
[2025-09-18 14:17:04,996][__main__][INFO] - Train, Round 100: loss=0.2481, accuracy=0.8641, gradient_norm=0.0941, 
[2025-09-18 14:17:04,996][__main__][INFO] - Train, Round 101: loss=0.2334, accuracy=0.8856, gradient_norm=0.1173, 
[2025-09-18 14:17:04,996][__main__][INFO] - Train, Round 102: loss=0.3173, accuracy=0.8454, gradient_norm=0.1300, 
[2025-09-18 14:17:04,996][__main__][INFO] - Train, Round 103: loss=0.3003, accuracy=0.8578, gradient_norm=0.1400, 
[2025-09-18 14:17:04,996][__main__][INFO] - Train, Round 104: loss=0.2431, accuracy=0.8861, gradient_norm=0.1657, 
[2025-09-18 14:17:04,996][__main__][INFO] - Train, Round 105: loss=0.2605, accuracy=0.8738, gradient_norm=0.1398, 
[2025-09-18 14:17:04,996][__main__][INFO] - Train, Round 106: loss=0.1948, accuracy=0.9082, gradient_norm=0.1256, 
[2025-09-18 14:17:04,996][__main__][INFO] - Train, Round 107: loss=0.2653, accuracy=0.8775, gradient_norm=0.1418, 
[2025-09-18 14:17:04,996][__main__][INFO] - Train, Round 108: loss=0.2812, accuracy=0.8739, gradient_norm=0.1570, 
[2025-09-18 14:17:04,996][__main__][INFO] - Train, Round 109: loss=0.3463, accuracy=0.8287, gradient_norm=0.1677, 
[2025-09-18 14:17:04,996][__main__][INFO] - Train, Round 110: loss=0.3614, accuracy=0.8046, gradient_norm=0.1400, 
[2025-09-18 14:17:04,996][__main__][INFO] - Train, Round 111: loss=0.2679, accuracy=0.8739, gradient_norm=0.1451, 
[2025-09-18 14:17:04,996][__main__][INFO] - Train, Round 112: loss=0.1345, accuracy=0.9455, gradient_norm=0.0932, 
[2025-09-18 14:17:04,996][__main__][INFO] - Train, Round 113: loss=0.2312, accuracy=0.8897, gradient_norm=0.1474, 
[2025-09-18 14:17:04,996][__main__][INFO] - Train, Round 114: loss=0.2937, accuracy=0.8559, gradient_norm=0.1409, 
[2025-09-18 14:17:04,996][__main__][INFO] - Train, Round 115: loss=0.2340, accuracy=0.9040, gradient_norm=0.1543, 
[2025-09-18 14:17:04,996][__main__][INFO] - Train, Round 116: loss=0.2772, accuracy=0.8646, gradient_norm=0.1326, 
[2025-09-18 14:17:04,996][__main__][INFO] - Train, Round 117: loss=0.2097, accuracy=0.9110, gradient_norm=0.1564, 
[2025-09-18 14:17:04,996][__main__][INFO] - Train, Round 118: loss=0.2599, accuracy=0.8718, gradient_norm=0.1263, 
[2025-09-18 14:17:04,996][__main__][INFO] - Train, Round 119: loss=0.2142, accuracy=0.8968, gradient_norm=0.1141, 
[2025-09-18 14:17:04,996][__main__][INFO] - Train, Round 120: loss=0.2193, accuracy=0.8985, gradient_norm=0.1284, 
[2025-09-18 14:17:04,996][__main__][INFO] - Train, Round 121: loss=0.2352, accuracy=0.8928, gradient_norm=0.1559, 
[2025-09-18 14:17:04,996][__main__][INFO] - Train, Round 122: loss=0.1536, accuracy=0.9407, gradient_norm=0.1640, 
[2025-09-18 14:17:04,996][__main__][INFO] - Train, Round 123: loss=0.2301, accuracy=0.8948, gradient_norm=0.1372, 
[2025-09-18 14:17:04,996][__main__][INFO] - Train, Round 124: loss=0.1667, accuracy=0.9286, gradient_norm=0.1355, 
[2025-09-18 14:17:04,996][__main__][INFO] - Train, Round 125: loss=0.1237, accuracy=0.9495, gradient_norm=0.1351, 
[2025-09-18 14:17:04,996][__main__][INFO] - Train, Round 126: loss=0.2599, accuracy=0.8775, gradient_norm=0.1451, 
[2025-09-18 14:17:04,997][__main__][INFO] - Train, Round 127: loss=0.1903, accuracy=0.9064, gradient_norm=0.1403, 
[2025-09-18 14:17:04,997][__main__][INFO] - Train, Round 128: loss=0.2201, accuracy=0.9008, gradient_norm=0.1387, 
[2025-09-18 14:17:04,997][__main__][INFO] - Train, Round 129: loss=0.1389, accuracy=0.9433, gradient_norm=0.1341, 
[2025-09-18 14:17:04,997][__main__][INFO] - Train, Round 130: loss=0.2167, accuracy=0.8991, gradient_norm=0.1410, 
[2025-09-18 14:17:04,997][__main__][INFO] - Train, Round 131: loss=0.1328, accuracy=0.9454, gradient_norm=0.1362, 
[2025-09-18 14:17:04,997][__main__][INFO] - Train, Round 132: loss=0.2719, accuracy=0.8730, gradient_norm=0.1484, 
[2025-09-18 14:17:04,997][__main__][INFO] - Train, Round 133: loss=0.2374, accuracy=0.8872, gradient_norm=0.1293, 
[2025-09-18 14:17:04,997][__main__][INFO] - Train, Round 134: loss=0.1918, accuracy=0.9163, gradient_norm=0.1367, 
[2025-09-18 14:17:04,997][__main__][INFO] - Train, Round 135: loss=0.1907, accuracy=0.9181, gradient_norm=0.1450, 
[2025-09-18 14:17:04,997][__main__][INFO] - Train, Round 136: loss=0.1577, accuracy=0.9395, gradient_norm=0.1633, 
[2025-09-18 14:17:04,997][__main__][INFO] - Train, Round 137: loss=0.1853, accuracy=0.9147, gradient_norm=0.1031, 
[2025-09-18 14:17:04,997][__main__][INFO] - Train, Round 138: loss=0.1377, accuracy=0.9361, gradient_norm=0.1299, 
[2025-09-18 14:17:04,997][__main__][INFO] - Train, Round 139: loss=0.1716, accuracy=0.9292, gradient_norm=0.1380, 
[2025-09-18 14:17:04,997][__main__][INFO] - Train, Round 140: loss=0.2189, accuracy=0.8933, gradient_norm=0.1282, 
[2025-09-18 14:17:04,997][__main__][INFO] - Train, Round 141: loss=0.0884, accuracy=0.9675, gradient_norm=0.1279, 
[2025-09-18 14:17:04,997][__main__][INFO] - Train, Round 142: loss=0.1644, accuracy=0.9284, gradient_norm=0.1303, 
[2025-09-18 14:17:04,997][__main__][INFO] - Train, Round 143: loss=0.2210, accuracy=0.8971, gradient_norm=0.1400, 
[2025-09-18 14:17:04,997][__main__][INFO] - Train, Round 144: loss=0.2399, accuracy=0.8947, gradient_norm=0.1531, 
[2025-09-18 14:17:04,997][__main__][INFO] - Train, Round 145: loss=0.1756, accuracy=0.9221, gradient_norm=0.1304, 
[2025-09-18 14:17:04,997][__main__][INFO] - Train, Round 146: loss=0.1388, accuracy=0.9412, gradient_norm=0.1282, 
[2025-09-18 14:17:04,997][__main__][INFO] - Train, Round 147: loss=0.1221, accuracy=0.9510, gradient_norm=0.1332, 
[2025-09-18 14:17:04,997][__main__][INFO] - Train, Round 148: loss=0.1700, accuracy=0.9296, gradient_norm=0.1428, 
[2025-09-18 14:17:04,997][__main__][INFO] - Train, Round 149: loss=0.2570, accuracy=0.8870, gradient_norm=0.1711, 
[2025-09-18 14:17:04,997][__main__][INFO] - Train, Round 150: loss=0.0636, accuracy=0.9781, gradient_norm=0.0834, 
[2025-09-18 14:17:04,997][__main__][INFO] - Train, Round 151: loss=0.2389, accuracy=0.8955, gradient_norm=0.1771, 
[2025-09-18 14:17:04,997][__main__][INFO] - Train, Round 152: loss=0.2385, accuracy=0.8955, gradient_norm=0.1697, 
[2025-09-18 14:17:04,997][__main__][INFO] - Train, Round 153: loss=0.1153, accuracy=0.9506, gradient_norm=0.0923, 
[2025-09-18 14:17:04,997][__main__][INFO] - Train, Round 154: loss=0.1358, accuracy=0.9392, gradient_norm=0.1065, 
[2025-09-18 14:17:04,997][__main__][INFO] - Train, Round 155: loss=0.0984, accuracy=0.9613, gradient_norm=0.1183, 
[2025-09-18 14:17:04,997][__main__][INFO] - Train, Round 156: loss=0.0745, accuracy=0.9726, gradient_norm=0.1035, 
[2025-09-18 14:17:04,997][__main__][INFO] - Train, Round 157: loss=0.1330, accuracy=0.9488, gradient_norm=0.1434, 
[2025-09-18 14:17:04,997][__main__][INFO] - Train, Round 158: loss=0.0999, accuracy=0.9568, gradient_norm=0.1195, 
[2025-09-18 14:17:04,997][__main__][INFO] - Train, Round 159: loss=0.0838, accuracy=0.9663, gradient_norm=0.0897, 
[2025-09-18 14:17:04,997][__main__][INFO] - Train, Round 160: loss=0.1518, accuracy=0.9347, gradient_norm=0.1213, 
[2025-09-18 14:17:04,997][__main__][INFO] - Train, Round 161: loss=0.0752, accuracy=0.9738, gradient_norm=0.1141, 
[2025-09-18 14:17:04,997][__main__][INFO] - Train, Round 162: loss=0.1726, accuracy=0.9293, gradient_norm=0.1460, 
[2025-09-18 14:17:04,997][__main__][INFO] - Train, Round 163: loss=0.0518, accuracy=0.9825, gradient_norm=0.0796, 
[2025-09-18 14:17:04,997][__main__][INFO] - Train, Round 164: loss=0.1297, accuracy=0.9470, gradient_norm=0.1342, 
[2025-09-18 14:17:04,997][__main__][INFO] - Train, Round 165: loss=0.0702, accuracy=0.9752, gradient_norm=0.1140, 
[2025-09-18 14:17:04,997][__main__][INFO] - Train, Round 166: loss=0.1836, accuracy=0.9203, gradient_norm=0.1345, 
[2025-09-18 14:17:04,997][__main__][INFO] - Train, Round 167: loss=0.1028, accuracy=0.9588, gradient_norm=0.1239, 
[2025-09-18 14:17:04,997][__main__][INFO] - Train, Round 168: loss=0.0946, accuracy=0.9620, gradient_norm=0.0857, 
[2025-09-18 14:17:04,997][__main__][INFO] - Train, Round 169: loss=0.1246, accuracy=0.9439, gradient_norm=0.1087, 
[2025-09-18 14:17:04,997][__main__][INFO] - Train, Round 170: loss=0.0867, accuracy=0.9658, gradient_norm=0.1205, 
[2025-09-18 14:17:04,997][__main__][INFO] - Train, Round 171: loss=0.1509, accuracy=0.9390, gradient_norm=0.1246, 
[2025-09-18 14:17:04,997][__main__][INFO] - Train, Round 172: loss=0.1286, accuracy=0.9523, gradient_norm=0.1412, 
[2025-09-18 14:17:04,997][__main__][INFO] - Train, Round 173: loss=0.1012, accuracy=0.9611, gradient_norm=0.1349, 
[2025-09-18 14:17:04,997][__main__][INFO] - Train, Round 174: loss=0.0662, accuracy=0.9790, gradient_norm=0.1390, 
[2025-09-18 14:17:04,997][__main__][INFO] - Train, Round 175: loss=0.1086, accuracy=0.9506, gradient_norm=0.1004, 
[2025-09-18 14:17:04,997][__main__][INFO] - Train, Round 176: loss=0.0830, accuracy=0.9665, gradient_norm=0.1028, 
[2025-09-18 14:17:04,997][__main__][INFO] - Train, Round 177: loss=0.0994, accuracy=0.9607, gradient_norm=0.1182, 
[2025-09-18 14:17:04,997][__main__][INFO] - Train, Round 178: loss=0.0547, accuracy=0.9789, gradient_norm=0.0783, 
[2025-09-18 14:17:04,997][__main__][INFO] - Train, Round 179: loss=0.0593, accuracy=0.9782, gradient_norm=0.0770, 
[2025-09-18 14:17:04,997][__main__][INFO] - Train, Round 180: loss=0.1403, accuracy=0.9405, gradient_norm=0.1322, 
[2025-09-18 14:17:04,997][__main__][INFO] - Train, Round 181: loss=0.0338, accuracy=0.9879, gradient_norm=0.0619, 
[2025-09-18 14:17:04,997][__main__][INFO] - Train, Round 182: loss=0.1153, accuracy=0.9541, gradient_norm=0.1192, 
[2025-09-18 14:17:04,997][__main__][INFO] - Train, Round 183: loss=0.0856, accuracy=0.9691, gradient_norm=0.1264, 
[2025-09-18 14:17:04,997][__main__][INFO] - Train, Round 184: loss=0.1591, accuracy=0.9300, gradient_norm=0.1410, 
[2025-09-18 14:17:04,997][__main__][INFO] - Train, Round 185: loss=0.1194, accuracy=0.9541, gradient_norm=0.1328, 
[2025-09-18 14:17:04,997][__main__][INFO] - Train, Round 186: loss=0.1146, accuracy=0.9525, gradient_norm=0.1278, 
[2025-09-18 14:17:04,997][__main__][INFO] - Train, Round 187: loss=0.0819, accuracy=0.9675, gradient_norm=0.0860, 
[2025-09-18 14:17:04,997][__main__][INFO] - Train, Round 188: loss=0.1000, accuracy=0.9596, gradient_norm=0.1005, 
[2025-09-18 14:17:04,997][__main__][INFO] - Train, Round 189: loss=0.1058, accuracy=0.9497, gradient_norm=0.1116, 
[2025-09-18 14:17:04,997][__main__][INFO] - Train, Round 190: loss=0.0555, accuracy=0.9815, gradient_norm=0.0959, 
[2025-09-18 14:17:04,997][__main__][INFO] - Train, Round 191: loss=0.0907, accuracy=0.9662, gradient_norm=0.1188, 
[2025-09-18 14:17:04,997][__main__][INFO] - Train, Round 192: loss=0.1151, accuracy=0.9544, gradient_norm=0.1334, 
[2025-09-18 14:17:04,997][__main__][INFO] - Train, Round 193: loss=0.0088, accuracy=0.9985, gradient_norm=0.0370, 
[2025-09-18 14:17:04,997][__main__][INFO] - Train, Round 194: loss=0.0896, accuracy=0.9638, gradient_norm=0.1134, 
[2025-09-18 14:17:04,998][__main__][INFO] - Train, Round 195: loss=0.1112, accuracy=0.9572, gradient_norm=0.1087, 
[2025-09-18 14:17:04,998][__main__][INFO] - Train, Round 196: loss=0.0738, accuracy=0.9741, gradient_norm=0.1219, 
[2025-09-18 14:17:04,998][__main__][INFO] - Train, Round 197: loss=0.0767, accuracy=0.9645, gradient_norm=0.0703, 
[2025-09-18 14:17:04,998][__main__][INFO] - Train, Round 198: loss=0.1394, accuracy=0.9427, gradient_norm=0.1057, 
[2025-09-18 14:17:04,998][__main__][INFO] - Train, Round 199: loss=0.1015, accuracy=0.9603, gradient_norm=0.1235, 
[2025-09-18 14:17:04,998][__main__][INFO] - Train, Round 200: loss=0.0735, accuracy=0.9737, gradient_norm=0.1077, 
[2025-09-18 14:17:04,998][__main__][INFO] - Test, Round 001: loss=2.1636, accuracy=0.1614, 
[2025-09-18 14:17:04,998][__main__][INFO] - Test, Round 002: loss=2.0334, accuracy=0.1825, 
[2025-09-18 14:17:04,998][__main__][INFO] - Test, Round 003: loss=1.9121, accuracy=0.2199, 
[2025-09-18 14:17:04,998][__main__][INFO] - Test, Round 004: loss=1.7965, accuracy=0.2558, 
[2025-09-18 14:17:04,998][__main__][INFO] - Test, Round 005: loss=1.6767, accuracy=0.2918, 
[2025-09-18 14:17:04,998][__main__][INFO] - Test, Round 006: loss=1.5797, accuracy=0.3228, 
[2025-09-18 14:17:04,998][__main__][INFO] - Test, Round 007: loss=1.4997, accuracy=0.3460, 
[2025-09-18 14:17:04,998][__main__][INFO] - Test, Round 008: loss=1.4223, accuracy=0.3683, 
[2025-09-18 14:17:04,998][__main__][INFO] - Test, Round 009: loss=1.3298, accuracy=0.3979, 
[2025-09-18 14:17:04,998][__main__][INFO] - Test, Round 010: loss=1.2927, accuracy=0.4067, 
[2025-09-18 14:17:04,998][__main__][INFO] - Test, Round 011: loss=1.1803, accuracy=0.4425, 
[2025-09-18 14:17:04,998][__main__][INFO] - Test, Round 012: loss=1.1735, accuracy=0.4454, 
[2025-09-18 14:17:04,998][__main__][INFO] - Test, Round 013: loss=1.1534, accuracy=0.4540, 
[2025-09-18 14:17:04,998][__main__][INFO] - Test, Round 014: loss=1.1185, accuracy=0.4716, 
[2025-09-18 14:17:04,998][__main__][INFO] - Test, Round 015: loss=1.0667, accuracy=0.4909, 
[2025-09-18 14:17:04,998][__main__][INFO] - Test, Round 016: loss=1.0822, accuracy=0.5034, 
[2025-09-18 14:17:04,998][__main__][INFO] - Test, Round 017: loss=0.9949, accuracy=0.5285, 
[2025-09-18 14:17:04,998][__main__][INFO] - Test, Round 018: loss=0.9556, accuracy=0.5361, 
[2025-09-18 14:17:04,998][__main__][INFO] - Test, Round 019: loss=0.9164, accuracy=0.5482, 
[2025-09-18 14:17:04,998][__main__][INFO] - Test, Round 020: loss=0.9029, accuracy=0.5526, 
[2025-09-18 14:17:04,998][__main__][INFO] - Test, Round 021: loss=0.8872, accuracy=0.5575, 
[2025-09-18 14:17:04,998][__main__][INFO] - Test, Round 022: loss=0.8771, accuracy=0.5597, 
[2025-09-18 14:17:04,998][__main__][INFO] - Test, Round 023: loss=0.8450, accuracy=0.5662, 
[2025-09-18 14:17:04,998][__main__][INFO] - Test, Round 024: loss=0.8146, accuracy=0.5792, 
[2025-09-18 14:17:04,998][__main__][INFO] - Test, Round 025: loss=0.7960, accuracy=0.5881, 
[2025-09-18 14:17:04,998][__main__][INFO] - Test, Round 026: loss=0.7704, accuracy=0.5950, 
[2025-09-18 14:17:04,998][__main__][INFO] - Test, Round 027: loss=0.7550, accuracy=0.6005, 
[2025-09-18 14:17:04,998][__main__][INFO] - Test, Round 028: loss=0.7322, accuracy=0.6071, 
[2025-09-18 14:17:04,998][__main__][INFO] - Test, Round 029: loss=0.6923, accuracy=0.6207, 
[2025-09-18 14:17:04,998][__main__][INFO] - Test, Round 030: loss=0.6858, accuracy=0.6235, 
[2025-09-18 14:17:04,998][__main__][INFO] - Test, Round 031: loss=0.6768, accuracy=0.6273, 
[2025-09-18 14:17:04,998][__main__][INFO] - Test, Round 032: loss=0.6663, accuracy=0.6340, 
[2025-09-18 14:17:04,998][__main__][INFO] - Test, Round 033: loss=0.6445, accuracy=0.6455, 
[2025-09-18 14:17:04,998][__main__][INFO] - Test, Round 034: loss=0.6402, accuracy=0.6481, 
[2025-09-18 14:17:04,998][__main__][INFO] - Test, Round 035: loss=0.6381, accuracy=0.6545, 
[2025-09-18 14:17:04,998][__main__][INFO] - Test, Round 036: loss=0.6057, accuracy=0.6659, 
[2025-09-18 14:17:04,998][__main__][INFO] - Test, Round 037: loss=0.6007, accuracy=0.6745, 
[2025-09-18 14:17:04,998][__main__][INFO] - Test, Round 038: loss=0.5982, accuracy=0.6768, 
[2025-09-18 14:17:04,998][__main__][INFO] - Test, Round 039: loss=0.5811, accuracy=0.6780, 
[2025-09-18 14:17:04,998][__main__][INFO] - Test, Round 040: loss=0.5618, accuracy=0.6814, 
[2025-09-18 14:17:04,998][__main__][INFO] - Test, Round 041: loss=0.5507, accuracy=0.6885, 
[2025-09-18 14:17:04,998][__main__][INFO] - Test, Round 042: loss=0.5472, accuracy=0.6909, 
[2025-09-18 14:17:04,998][__main__][INFO] - Test, Round 043: loss=0.5433, accuracy=0.6971, 
[2025-09-18 14:17:04,998][__main__][INFO] - Test, Round 044: loss=0.5408, accuracy=0.6975, 
[2025-09-18 14:17:04,998][__main__][INFO] - Test, Round 045: loss=0.5406, accuracy=0.6965, 
[2025-09-18 14:17:04,998][__main__][INFO] - Test, Round 046: loss=0.5390, accuracy=0.6957, 
[2025-09-18 14:17:04,998][__main__][INFO] - Test, Round 047: loss=0.5348, accuracy=0.7009, 
[2025-09-18 14:17:04,998][__main__][INFO] - Test, Round 048: loss=0.5352, accuracy=0.7041, 
[2025-09-18 14:17:04,998][__main__][INFO] - Test, Round 049: loss=0.5348, accuracy=0.7046, 
[2025-09-18 14:17:04,998][__main__][INFO] - Test, Round 050: loss=0.5333, accuracy=0.7050, 
[2025-09-18 14:17:04,998][__main__][INFO] - Test, Round 051: loss=0.5307, accuracy=0.7070, 
[2025-09-18 14:17:04,998][__main__][INFO] - Test, Round 052: loss=0.5197, accuracy=0.7125, 
[2025-09-18 14:17:04,998][__main__][INFO] - Test, Round 053: loss=0.5174, accuracy=0.7137, 
[2025-09-18 14:17:04,998][__main__][INFO] - Test, Round 054: loss=0.5109, accuracy=0.7210, 
[2025-09-18 14:17:04,998][__main__][INFO] - Test, Round 055: loss=0.5087, accuracy=0.7227, 
[2025-09-18 14:17:04,998][__main__][INFO] - Test, Round 056: loss=0.5064, accuracy=0.7248, 
[2025-09-18 14:17:04,998][__main__][INFO] - Test, Round 057: loss=0.5065, accuracy=0.7279, 
[2025-09-18 14:17:04,998][__main__][INFO] - Test, Round 058: loss=0.5090, accuracy=0.7284, 
[2025-09-18 14:17:04,998][__main__][INFO] - Test, Round 059: loss=0.5061, accuracy=0.7331, 
[2025-09-18 14:17:04,998][__main__][INFO] - Test, Round 060: loss=0.5043, accuracy=0.7361, 
[2025-09-18 14:17:04,998][__main__][INFO] - Test, Round 061: loss=0.5078, accuracy=0.7374, 
[2025-09-18 14:17:04,998][__main__][INFO] - Test, Round 062: loss=0.5069, accuracy=0.7389, 
[2025-09-18 14:17:04,998][__main__][INFO] - Test, Round 063: loss=0.4993, accuracy=0.7445, 
[2025-09-18 14:17:04,999][__main__][INFO] - Test, Round 064: loss=0.4909, accuracy=0.7489, 
[2025-09-18 14:17:04,999][__main__][INFO] - Test, Round 065: loss=0.4800, accuracy=0.7539, 
[2025-09-18 14:17:04,999][__main__][INFO] - Test, Round 066: loss=0.4795, accuracy=0.7553, 
[2025-09-18 14:17:04,999][__main__][INFO] - Test, Round 067: loss=0.4778, accuracy=0.7548, 
[2025-09-18 14:17:04,999][__main__][INFO] - Test, Round 068: loss=0.4801, accuracy=0.7550, 
[2025-09-18 14:17:04,999][__main__][INFO] - Test, Round 069: loss=0.4782, accuracy=0.7610, 
[2025-09-18 14:17:04,999][__main__][INFO] - Test, Round 070: loss=0.4769, accuracy=0.7635, 
[2025-09-18 14:17:04,999][__main__][INFO] - Test, Round 071: loss=0.4778, accuracy=0.7627, 
[2025-09-18 14:17:04,999][__main__][INFO] - Test, Round 072: loss=0.4795, accuracy=0.7646, 
[2025-09-18 14:17:04,999][__main__][INFO] - Test, Round 073: loss=0.4736, accuracy=0.7681, 
[2025-09-18 14:17:04,999][__main__][INFO] - Test, Round 074: loss=0.4703, accuracy=0.7705, 
[2025-09-18 14:17:04,999][__main__][INFO] - Test, Round 075: loss=0.4702, accuracy=0.7685, 
[2025-09-18 14:17:04,999][__main__][INFO] - Test, Round 076: loss=0.4707, accuracy=0.7684, 
[2025-09-18 14:17:04,999][__main__][INFO] - Test, Round 077: loss=0.4703, accuracy=0.7704, 
[2025-09-18 14:17:04,999][__main__][INFO] - Test, Round 078: loss=0.4684, accuracy=0.7706, 
[2025-09-18 14:17:04,999][__main__][INFO] - Test, Round 079: loss=0.4686, accuracy=0.7724, 
[2025-09-18 14:17:04,999][__main__][INFO] - Test, Round 080: loss=0.4659, accuracy=0.7722, 
[2025-09-18 14:17:04,999][__main__][INFO] - Test, Round 081: loss=0.4648, accuracy=0.7725, 
[2025-09-18 14:17:04,999][__main__][INFO] - Test, Round 082: loss=0.4621, accuracy=0.7767, 
[2025-09-18 14:17:04,999][__main__][INFO] - Test, Round 083: loss=0.4649, accuracy=0.7737, 
[2025-09-18 14:17:04,999][__main__][INFO] - Test, Round 084: loss=0.4623, accuracy=0.7748, 
[2025-09-18 14:17:04,999][__main__][INFO] - Test, Round 085: loss=0.4592, accuracy=0.7767, 
[2025-09-18 14:17:04,999][__main__][INFO] - Test, Round 086: loss=0.4571, accuracy=0.7761, 
[2025-09-18 14:17:04,999][__main__][INFO] - Test, Round 087: loss=0.4514, accuracy=0.7783, 
[2025-09-18 14:17:04,999][__main__][INFO] - Test, Round 088: loss=0.4469, accuracy=0.7816, 
[2025-09-18 14:17:04,999][__main__][INFO] - Test, Round 089: loss=0.4491, accuracy=0.7817, 
[2025-09-18 14:17:04,999][__main__][INFO] - Test, Round 090: loss=0.4556, accuracy=0.7796, 
[2025-09-18 14:17:04,999][__main__][INFO] - Test, Round 091: loss=0.4505, accuracy=0.7818, 
[2025-09-18 14:17:04,999][__main__][INFO] - Test, Round 092: loss=0.4470, accuracy=0.7853, 
[2025-09-18 14:17:04,999][__main__][INFO] - Test, Round 093: loss=0.4477, accuracy=0.7863, 
[2025-09-18 14:17:04,999][__main__][INFO] - Test, Round 094: loss=0.4506, accuracy=0.7884, 
[2025-09-18 14:17:04,999][__main__][INFO] - Test, Round 095: loss=0.4479, accuracy=0.7907, 
[2025-09-18 14:17:04,999][__main__][INFO] - Test, Round 096: loss=0.4465, accuracy=0.7902, 
[2025-09-18 14:17:04,999][__main__][INFO] - Test, Round 097: loss=0.4433, accuracy=0.7918, 
[2025-09-18 14:17:04,999][__main__][INFO] - Test, Round 098: loss=0.4382, accuracy=0.7954, 
[2025-09-18 14:17:04,999][__main__][INFO] - Test, Round 099: loss=0.4378, accuracy=0.7961, 
[2025-09-18 14:17:04,999][__main__][INFO] - Test, Round 100: loss=0.4384, accuracy=0.7971, 
[2025-09-18 14:17:04,999][__main__][INFO] - Test, Round 101: loss=0.4330, accuracy=0.8009, 
[2025-09-18 14:17:04,999][__main__][INFO] - Test, Round 102: loss=0.4338, accuracy=0.8015, 
[2025-09-18 14:17:04,999][__main__][INFO] - Test, Round 103: loss=0.4327, accuracy=0.8013, 
[2025-09-18 14:17:04,999][__main__][INFO] - Test, Round 104: loss=0.4370, accuracy=0.7998, 
[2025-09-18 14:17:04,999][__main__][INFO] - Test, Round 105: loss=0.4437, accuracy=0.8003, 
[2025-09-18 14:17:04,999][__main__][INFO] - Test, Round 106: loss=0.4458, accuracy=0.7993, 
[2025-09-18 14:17:04,999][__main__][INFO] - Test, Round 107: loss=0.4447, accuracy=0.7993, 
[2025-09-18 14:17:04,999][__main__][INFO] - Test, Round 108: loss=0.4361, accuracy=0.8059, 
[2025-09-18 14:17:04,999][__main__][INFO] - Test, Round 109: loss=0.4320, accuracy=0.8055, 
[2025-09-18 14:17:04,999][__main__][INFO] - Test, Round 110: loss=0.4324, accuracy=0.8069, 
[2025-09-18 14:17:04,999][__main__][INFO] - Test, Round 111: loss=0.4291, accuracy=0.8081, 
[2025-09-18 14:17:04,999][__main__][INFO] - Test, Round 112: loss=0.4310, accuracy=0.8067, 
[2025-09-18 14:17:04,999][__main__][INFO] - Test, Round 113: loss=0.4305, accuracy=0.8084, 
[2025-09-18 14:17:04,999][__main__][INFO] - Test, Round 114: loss=0.4340, accuracy=0.8084, 
[2025-09-18 14:17:04,999][__main__][INFO] - Test, Round 115: loss=0.4277, accuracy=0.8122, 
[2025-09-18 14:17:04,999][__main__][INFO] - Test, Round 116: loss=0.4257, accuracy=0.8127, 
[2025-09-18 14:17:04,999][__main__][INFO] - Test, Round 117: loss=0.4272, accuracy=0.8132, 
[2025-09-18 14:17:04,999][__main__][INFO] - Test, Round 118: loss=0.4282, accuracy=0.8144, 
[2025-09-18 14:17:04,999][__main__][INFO] - Test, Round 119: loss=0.4271, accuracy=0.8163, 
[2025-09-18 14:17:04,999][__main__][INFO] - Test, Round 120: loss=0.4268, accuracy=0.8176, 
[2025-09-18 14:17:04,999][__main__][INFO] - Test, Round 121: loss=0.4322, accuracy=0.8158, 
[2025-09-18 14:17:04,999][__main__][INFO] - Test, Round 122: loss=0.4362, accuracy=0.8148, 
[2025-09-18 14:17:04,999][__main__][INFO] - Test, Round 123: loss=0.4360, accuracy=0.8140, 
[2025-09-18 14:17:04,999][__main__][INFO] - Test, Round 124: loss=0.4367, accuracy=0.8138, 
[2025-09-18 14:17:04,999][__main__][INFO] - Test, Round 125: loss=0.4329, accuracy=0.8158, 
[2025-09-18 14:17:04,999][__main__][INFO] - Test, Round 126: loss=0.4297, accuracy=0.8165, 
[2025-09-18 14:17:04,999][__main__][INFO] - Test, Round 127: loss=0.4298, accuracy=0.8171, 
[2025-09-18 14:17:04,999][__main__][INFO] - Test, Round 128: loss=0.4238, accuracy=0.8209, 
[2025-09-18 14:17:04,999][__main__][INFO] - Test, Round 129: loss=0.4263, accuracy=0.8192, 
[2025-09-18 14:17:04,999][__main__][INFO] - Test, Round 130: loss=0.4267, accuracy=0.8195, 
[2025-09-18 14:17:04,999][__main__][INFO] - Test, Round 131: loss=0.4262, accuracy=0.8202, 
[2025-09-18 14:17:04,999][__main__][INFO] - Test, Round 132: loss=0.4287, accuracy=0.8207, 
[2025-09-18 14:17:04,999][__main__][INFO] - Test, Round 133: loss=0.4249, accuracy=0.8223, 
[2025-09-18 14:17:05,000][__main__][INFO] - Test, Round 134: loss=0.4243, accuracy=0.8231, 
[2025-09-18 14:17:05,000][__main__][INFO] - Test, Round 135: loss=0.4269, accuracy=0.8231, 
[2025-09-18 14:17:05,000][__main__][INFO] - Test, Round 136: loss=0.4289, accuracy=0.8233, 
[2025-09-18 14:17:05,000][__main__][INFO] - Test, Round 137: loss=0.4249, accuracy=0.8258, 
[2025-09-18 14:17:05,000][__main__][INFO] - Test, Round 138: loss=0.4250, accuracy=0.8284, 
[2025-09-18 14:17:05,000][__main__][INFO] - Test, Round 139: loss=0.4228, accuracy=0.8299, 
[2025-09-18 14:17:05,000][__main__][INFO] - Test, Round 140: loss=0.4264, accuracy=0.8298, 
[2025-09-18 14:17:05,000][__main__][INFO] - Test, Round 141: loss=0.4304, accuracy=0.8285, 
[2025-09-18 14:17:05,000][__main__][INFO] - Test, Round 142: loss=0.4292, accuracy=0.8300, 
[2025-09-18 14:17:05,000][__main__][INFO] - Test, Round 143: loss=0.4281, accuracy=0.8308, 
[2025-09-18 14:17:05,000][__main__][INFO] - Test, Round 144: loss=0.4286, accuracy=0.8317, 
[2025-09-18 14:17:05,000][__main__][INFO] - Test, Round 145: loss=0.4302, accuracy=0.8333, 
[2025-09-18 14:17:05,000][__main__][INFO] - Test, Round 146: loss=0.4352, accuracy=0.8320, 
[2025-09-18 14:17:05,000][__main__][INFO] - Test, Round 147: loss=0.4340, accuracy=0.8325, 
[2025-09-18 14:17:05,000][__main__][INFO] - Test, Round 148: loss=0.4361, accuracy=0.8307, 
[2025-09-18 14:17:05,000][__main__][INFO] - Test, Round 149: loss=0.4371, accuracy=0.8327, 
[2025-09-18 14:17:05,000][__main__][INFO] - Test, Round 150: loss=0.4356, accuracy=0.8345, 
[2025-09-18 14:17:05,000][__main__][INFO] - Test, Round 151: loss=0.4391, accuracy=0.8348, 
[2025-09-18 14:17:05,000][__main__][INFO] - Test, Round 152: loss=0.4361, accuracy=0.8352, 
[2025-09-18 14:17:05,000][__main__][INFO] - Test, Round 153: loss=0.4396, accuracy=0.8335, 
[2025-09-18 14:17:05,000][__main__][INFO] - Test, Round 154: loss=0.4427, accuracy=0.8331, 
[2025-09-18 14:17:05,000][__main__][INFO] - Test, Round 155: loss=0.4452, accuracy=0.8337, 
[2025-09-18 14:17:05,000][__main__][INFO] - Test, Round 156: loss=0.4476, accuracy=0.8348, 
[2025-09-18 14:17:05,000][__main__][INFO] - Test, Round 157: loss=0.4488, accuracy=0.8344, 
[2025-09-18 14:17:05,000][__main__][INFO] - Test, Round 158: loss=0.4507, accuracy=0.8343, 
[2025-09-18 14:17:05,000][__main__][INFO] - Test, Round 159: loss=0.4554, accuracy=0.8340, 
[2025-09-18 14:17:05,000][__main__][INFO] - Test, Round 160: loss=0.4515, accuracy=0.8367, 
[2025-09-18 14:17:05,000][__main__][INFO] - Test, Round 161: loss=0.4542, accuracy=0.8359, 
[2025-09-18 14:17:05,000][__main__][INFO] - Test, Round 162: loss=0.4540, accuracy=0.8381, 
[2025-09-18 14:17:05,000][__main__][INFO] - Test, Round 163: loss=0.4517, accuracy=0.8396, 
[2025-09-18 14:17:05,000][__main__][INFO] - Test, Round 164: loss=0.4547, accuracy=0.8397, 
[2025-09-18 14:17:05,000][__main__][INFO] - Test, Round 165: loss=0.4551, accuracy=0.8405, 
[2025-09-18 14:17:05,000][__main__][INFO] - Test, Round 166: loss=0.4501, accuracy=0.8427, 
[2025-09-18 14:17:05,000][__main__][INFO] - Test, Round 167: loss=0.4487, accuracy=0.8445, 
[2025-09-18 14:17:05,000][__main__][INFO] - Test, Round 168: loss=0.4501, accuracy=0.8444, 
[2025-09-18 14:17:05,000][__main__][INFO] - Test, Round 169: loss=0.4557, accuracy=0.8439, 
[2025-09-18 14:17:05,000][__main__][INFO] - Test, Round 170: loss=0.4525, accuracy=0.8440, 
[2025-09-18 14:17:05,000][__main__][INFO] - Test, Round 171: loss=0.4537, accuracy=0.8448, 
[2025-09-18 14:17:05,000][__main__][INFO] - Test, Round 172: loss=0.4526, accuracy=0.8458, 
[2025-09-18 14:17:05,000][__main__][INFO] - Test, Round 173: loss=0.4531, accuracy=0.8466, 
[2025-09-18 14:17:05,000][__main__][INFO] - Test, Round 174: loss=0.4616, accuracy=0.8451, 
[2025-09-18 14:17:05,000][__main__][INFO] - Test, Round 175: loss=0.4645, accuracy=0.8450, 
[2025-09-18 14:17:05,000][__main__][INFO] - Test, Round 176: loss=0.4715, accuracy=0.8436, 
[2025-09-18 14:17:05,000][__main__][INFO] - Test, Round 177: loss=0.4688, accuracy=0.8445, 
[2025-09-18 14:17:05,000][__main__][INFO] - Test, Round 178: loss=0.4716, accuracy=0.8448, 
[2025-09-18 14:17:05,000][__main__][INFO] - Test, Round 179: loss=0.4727, accuracy=0.8448, 
[2025-09-18 14:17:05,000][__main__][INFO] - Test, Round 180: loss=0.4725, accuracy=0.8442, 
[2025-09-18 14:17:05,000][__main__][INFO] - Test, Round 181: loss=0.4742, accuracy=0.8443, 
[2025-09-18 14:17:05,000][__main__][INFO] - Test, Round 182: loss=0.4763, accuracy=0.8449, 
[2025-09-18 14:17:05,000][__main__][INFO] - Test, Round 183: loss=0.4823, accuracy=0.8432, 
[2025-09-18 14:17:05,000][__main__][INFO] - Test, Round 184: loss=0.4819, accuracy=0.8458, 
[2025-09-18 14:17:05,000][__main__][INFO] - Test, Round 185: loss=0.4815, accuracy=0.8453, 
[2025-09-18 14:17:05,000][__main__][INFO] - Test, Round 186: loss=0.4867, accuracy=0.8457, 
[2025-09-18 14:17:05,000][__main__][INFO] - Test, Round 187: loss=0.4878, accuracy=0.8460, 
[2025-09-18 14:17:05,000][__main__][INFO] - Test, Round 188: loss=0.4854, accuracy=0.8479, 
[2025-09-18 14:17:05,000][__main__][INFO] - Test, Round 189: loss=0.4829, accuracy=0.8490, 
[2025-09-18 14:17:05,000][__main__][INFO] - Test, Round 190: loss=0.4850, accuracy=0.8497, 
[2025-09-18 14:17:05,000][__main__][INFO] - Test, Round 191: loss=0.4886, accuracy=0.8505, 
[2025-09-18 14:17:05,000][__main__][INFO] - Test, Round 192: loss=0.4852, accuracy=0.8520, 
[2025-09-18 14:17:05,000][__main__][INFO] - Test, Round 193: loss=0.4848, accuracy=0.8517, 
[2025-09-18 14:17:05,000][__main__][INFO] - Test, Round 194: loss=0.4869, accuracy=0.8513, 
[2025-09-18 14:17:05,000][__main__][INFO] - Test, Round 195: loss=0.4876, accuracy=0.8507, 
[2025-09-18 14:17:05,000][__main__][INFO] - Test, Round 196: loss=0.4873, accuracy=0.8508, 
[2025-09-18 14:17:05,000][__main__][INFO] - Test, Round 197: loss=0.4857, accuracy=0.8518, 
[2025-09-18 14:17:05,000][__main__][INFO] - Test, Round 198: loss=0.4913, accuracy=0.8523, 
[2025-09-18 14:17:05,000][__main__][INFO] - Test, Round 199: loss=0.4931, accuracy=0.8525, 
[2025-09-18 14:17:05,000][__main__][INFO] - Test, Round 200: loss=0.4964, accuracy=0.8519, 
