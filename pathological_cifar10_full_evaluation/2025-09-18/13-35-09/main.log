[2025-09-18 13:35:16,260][flp2p.graph_runner][INFO] - Train, Round 0 : loss => 1.6424215346298383,  accuracy: 0.4966060606060606, gradient_norm : 0.950143952344714
[2025-09-18 13:35:23,310][flp2p.graph_runner][INFO] - Test, Round 0 : loss => 2.1636093532134226,  accuracy: 0.1614
[2025-09-18 13:35:26,443][flp2p.graph_runner][INFO] - Train, Round 1 : loss => 1.277809324954315,  accuracy: 0.5359393939393939, gradient_norm : 0.6512153348435444
[2025-09-18 13:35:33,531][flp2p.graph_runner][INFO] - Test, Round 1 : loss => 2.033358853541725,  accuracy: 0.1825
[2025-09-18 13:35:36,618][flp2p.graph_runner][INFO] - Train, Round 2 : loss => 1.230487223875113,  accuracy: 0.5484848484848486, gradient_norm : 0.6428118292914129
[2025-09-18 13:35:43,730][flp2p.graph_runner][INFO] - Test, Round 2 : loss => 1.9121054078793327,  accuracy: 0.2199
[2025-09-18 13:35:46,809][flp2p.graph_runner][INFO] - Train, Round 3 : loss => 1.2777279520560088,  accuracy: 0.5363030303030303, gradient_norm : 0.6439354103936906
[2025-09-18 13:35:53,895][flp2p.graph_runner][INFO] - Test, Round 3 : loss => 1.7965320945392538,  accuracy: 0.2558
[2025-09-18 13:35:56,700][flp2p.graph_runner][INFO] - Train, Round 4 : loss => 1.4152533027778784,  accuracy: 0.5056666666666667, gradient_norm : 0.7386931377038597
[2025-09-18 13:36:03,752][flp2p.graph_runner][INFO] - Test, Round 4 : loss => 1.676683808579758,  accuracy: 0.2918
[2025-09-18 13:36:06,783][flp2p.graph_runner][INFO] - Train, Round 5 : loss => 0.9858143566167873,  accuracy: 0.6726666666666667, gradient_norm : 0.5884900109408275
[2025-09-18 13:36:13,814][flp2p.graph_runner][INFO] - Test, Round 5 : loss => 1.5797082286488533,  accuracy: 0.3228
[2025-09-18 13:36:16,887][flp2p.graph_runner][INFO] - Train, Round 6 : loss => 1.0605397989717722,  accuracy: 0.5952121212121212, gradient_norm : 0.5341875215085984
[2025-09-18 13:36:23,946][flp2p.graph_runner][INFO] - Test, Round 6 : loss => 1.4996682307552813,  accuracy: 0.3461
[2025-09-18 13:36:26,987][flp2p.graph_runner][INFO] - Train, Round 7 : loss => 1.0603334587083153,  accuracy: 0.5703030303030303, gradient_norm : 0.5199286488773365
[2025-09-18 13:36:34,099][flp2p.graph_runner][INFO] - Test, Round 7 : loss => 1.422324363481818,  accuracy: 0.3683
[2025-09-18 13:36:37,200][flp2p.graph_runner][INFO] - Train, Round 8 : loss => 1.1107327900988861,  accuracy: 0.5935151515151513, gradient_norm : 0.5627955245632643
[2025-09-18 13:36:44,307][flp2p.graph_runner][INFO] - Test, Round 8 : loss => 1.3298020710636709,  accuracy: 0.398
[2025-09-18 13:36:47,351][flp2p.graph_runner][INFO] - Train, Round 9 : loss => 0.8365092417984544,  accuracy: 0.5778787878787879, gradient_norm : 0.3205900020931761
[2025-09-18 13:36:54,457][flp2p.graph_runner][INFO] - Test, Round 9 : loss => 1.2927651643116806,  accuracy: 0.4067
[2025-09-18 13:36:57,543][flp2p.graph_runner][INFO] - Train, Round 10 : loss => 1.2771442983839532,  accuracy: 0.5355151515151515, gradient_norm : 0.6484997424323047
[2025-09-18 13:37:04,660][flp2p.graph_runner][INFO] - Test, Round 10 : loss => 1.180353540238029,  accuracy: 0.4426
[2025-09-18 13:37:07,736][flp2p.graph_runner][INFO] - Train, Round 11 : loss => 0.5789855751051078,  accuracy: 0.6958181818181819, gradient_norm : 0.234674265359203
[2025-09-18 13:37:14,836][flp2p.graph_runner][INFO] - Test, Round 11 : loss => 1.173596172535041,  accuracy: 0.4455
[2025-09-18 13:37:17,857][flp2p.graph_runner][INFO] - Train, Round 12 : loss => 0.6277676208377341,  accuracy: 0.6674545454545455, gradient_norm : 0.2400985880588598
[2025-09-18 13:37:24,930][flp2p.graph_runner][INFO] - Test, Round 12 : loss => 1.1534981431152613,  accuracy: 0.4539
[2025-09-18 13:37:28,025][flp2p.graph_runner][INFO] - Train, Round 13 : loss => 0.6800408194497076,  accuracy: 0.6883030303030303, gradient_norm : 0.31537905743669087
[2025-09-18 13:37:35,147][flp2p.graph_runner][INFO] - Test, Round 13 : loss => 1.1185669886327951,  accuracy: 0.4715
[2025-09-18 13:37:38,240][flp2p.graph_runner][INFO] - Train, Round 14 : loss => 0.9833807337047374,  accuracy: 0.5670303030303029, gradient_norm : 0.44928293506124406
[2025-09-18 13:37:45,337][flp2p.graph_runner][INFO] - Test, Round 14 : loss => 1.0667374753171037,  accuracy: 0.4909
[2025-09-18 13:37:48,411][flp2p.graph_runner][INFO] - Train, Round 15 : loss => 0.64998008547962,  accuracy: 0.6930303030303031, gradient_norm : 0.32051625789943655
[2025-09-18 13:37:55,548][flp2p.graph_runner][INFO] - Test, Round 15 : loss => 1.082297101436108,  accuracy: 0.5036
[2025-09-18 13:37:58,603][flp2p.graph_runner][INFO] - Train, Round 16 : loss => 1.0459628523769735,  accuracy: 0.5943030303030302, gradient_norm : 0.5799164860082131
[2025-09-18 13:38:05,755][flp2p.graph_runner][INFO] - Test, Round 16 : loss => 0.9948720415665532,  accuracy: 0.5286
[2025-09-18 13:38:08,600][flp2p.graph_runner][INFO] - Train, Round 17 : loss => 0.765755757060176,  accuracy: 0.6793333333333333, gradient_norm : 0.3540847681097046
[2025-09-18 13:38:15,846][flp2p.graph_runner][INFO] - Test, Round 17 : loss => 0.955726294341732,  accuracy: 0.5362
[2025-09-18 13:38:18,926][flp2p.graph_runner][INFO] - Train, Round 18 : loss => 0.6607498897337784,  accuracy: 0.7157575757575758, gradient_norm : 0.354054176122535
[2025-09-18 13:38:26,091][flp2p.graph_runner][INFO] - Test, Round 18 : loss => 0.9163886389958016,  accuracy: 0.5484
[2025-09-18 13:38:29,176][flp2p.graph_runner][INFO] - Train, Round 19 : loss => 0.7354483239038391,  accuracy: 0.6972727272727273, gradient_norm : 0.4353937987852648
[2025-09-18 13:38:36,287][flp2p.graph_runner][INFO] - Test, Round 19 : loss => 0.9030587549345842,  accuracy: 0.5529
[2025-09-18 13:38:39,374][flp2p.graph_runner][INFO] - Train, Round 20 : loss => 0.7607467147876041,  accuracy: 0.6504848484848486, gradient_norm : 0.37213405311735864
[2025-09-18 13:38:46,534][flp2p.graph_runner][INFO] - Test, Round 20 : loss => 0.8872738781386843,  accuracy: 0.5577
[2025-09-18 13:38:49,424][flp2p.graph_runner][INFO] - Train, Round 21 : loss => 0.6880871251421787,  accuracy: 0.6243333333333333, gradient_norm : 0.285141076401482
[2025-09-18 13:38:56,559][flp2p.graph_runner][INFO] - Test, Round 21 : loss => 0.8771531110522705,  accuracy: 0.5596
[2025-09-18 13:38:59,686][flp2p.graph_runner][INFO] - Train, Round 22 : loss => 0.6614268549336323,  accuracy: 0.7046060606060606, gradient_norm : 0.36639815101314566
[2025-09-18 13:39:06,872][flp2p.graph_runner][INFO] - Test, Round 22 : loss => 0.8452582513464051,  accuracy: 0.5661
[2025-09-18 13:39:10,076][flp2p.graph_runner][INFO] - Train, Round 23 : loss => 0.7724025775780964,  accuracy: 0.6121818181818182, gradient_norm : 0.33368734300809816
[2025-09-18 13:39:17,172][flp2p.graph_runner][INFO] - Test, Round 23 : loss => 0.8147859115421,  accuracy: 0.5794
[2025-09-18 13:39:20,262][flp2p.graph_runner][INFO] - Train, Round 24 : loss => 0.5738848124142713,  accuracy: 0.6803030303030304, gradient_norm : 0.2386661465476747
[2025-09-18 13:39:27,339][flp2p.graph_runner][INFO] - Test, Round 24 : loss => 0.7961151955398471,  accuracy: 0.5882
[2025-09-18 13:39:30,465][flp2p.graph_runner][INFO] - Train, Round 25 : loss => 0.630254014620075,  accuracy: 0.672, gradient_norm : 0.25195719431117086
[2025-09-18 13:39:37,708][flp2p.graph_runner][INFO] - Test, Round 25 : loss => 0.7701889196832195,  accuracy: 0.5949
[2025-09-18 13:39:40,778][flp2p.graph_runner][INFO] - Train, Round 26 : loss => 0.5556870190201328,  accuracy: 0.7366666666666667, gradient_norm : 0.2749079334645321
[2025-09-18 13:39:47,974][flp2p.graph_runner][INFO] - Test, Round 26 : loss => 0.7550403458124277,  accuracy: 0.6006
[2025-09-18 13:39:50,834][flp2p.graph_runner][INFO] - Train, Round 27 : loss => 0.5126093339239938,  accuracy: 0.7330666666666665, gradient_norm : 0.21021606714056196
[2025-09-18 13:39:57,980][flp2p.graph_runner][INFO] - Test, Round 27 : loss => 0.7322282339684708,  accuracy: 0.6072
[2025-09-18 13:40:00,793][flp2p.graph_runner][INFO] - Train, Round 28 : loss => 0.7557487511123804,  accuracy: 0.6694, gradient_norm : 0.3695440919488728
[2025-09-18 13:40:07,938][flp2p.graph_runner][INFO] - Test, Round 28 : loss => 0.6923759710635613,  accuracy: 0.6209
[2025-09-18 13:40:11,028][flp2p.graph_runner][INFO] - Train, Round 29 : loss => 0.4045371230700987,  accuracy: 0.8050909090909091, gradient_norm : 0.18072016219935286
[2025-09-18 13:40:18,250][flp2p.graph_runner][INFO] - Test, Round 29 : loss => 0.6858737534226231,  accuracy: 0.6236
[2025-09-18 13:40:21,318][flp2p.graph_runner][INFO] - Train, Round 30 : loss => 0.4947021694616859,  accuracy: 0.7316969696969697, gradient_norm : 0.18665950429132872
[2025-09-18 13:40:28,670][flp2p.graph_runner][INFO] - Test, Round 30 : loss => 0.6769058839487947,  accuracy: 0.6273
[2025-09-18 13:40:31,849][flp2p.graph_runner][INFO] - Train, Round 31 : loss => 0.47394754798312266,  accuracy: 0.7281212121212122, gradient_norm : 0.15036398185682076
[2025-09-18 13:40:39,138][flp2p.graph_runner][INFO] - Test, Round 31 : loss => 0.6663375952993412,  accuracy: 0.634
[2025-09-18 13:40:42,216][flp2p.graph_runner][INFO] - Train, Round 32 : loss => 0.6237287489464743,  accuracy: 0.6526060606060606, gradient_norm : 0.24262279106200707
[2025-09-18 13:40:49,432][flp2p.graph_runner][INFO] - Test, Round 32 : loss => 0.6446057317486645,  accuracy: 0.6457
[2025-09-18 13:40:52,497][flp2p.graph_runner][INFO] - Train, Round 33 : loss => 0.5022096882414319,  accuracy: 0.686969696969697, gradient_norm : 0.1410757573157286
[2025-09-18 13:40:59,670][flp2p.graph_runner][INFO] - Test, Round 33 : loss => 0.6403811272049518,  accuracy: 0.6483
[2025-09-18 13:41:02,455][flp2p.graph_runner][INFO] - Train, Round 34 : loss => 0.5189201047870808,  accuracy: 0.6872, gradient_norm : 0.15256838026641314
[2025-09-18 13:41:09,652][flp2p.graph_runner][INFO] - Test, Round 34 : loss => 0.6382783740144207,  accuracy: 0.6542
[2025-09-18 13:41:12,731][flp2p.graph_runner][INFO] - Train, Round 35 : loss => 0.49235745910626294,  accuracy: 0.799878787878788, gradient_norm : 0.28262756214898793
[2025-09-18 13:41:19,937][flp2p.graph_runner][INFO] - Test, Round 35 : loss => 0.6059430132543016,  accuracy: 0.6655
[2025-09-18 13:41:23,019][flp2p.graph_runner][INFO] - Train, Round 36 : loss => 0.4608909534054936,  accuracy: 0.7569090909090909, gradient_norm : 0.18109645170017905
[2025-09-18 13:41:30,340][flp2p.graph_runner][INFO] - Test, Round 36 : loss => 0.6009447211896347,  accuracy: 0.6739
[2025-09-18 13:41:33,497][flp2p.graph_runner][INFO] - Train, Round 37 : loss => 0.411717856239246,  accuracy: 0.768, gradient_norm : 0.12243228753664036
[2025-09-18 13:41:40,775][flp2p.graph_runner][INFO] - Test, Round 37 : loss => 0.5984052743174928,  accuracy: 0.6766
[2025-09-18 13:41:43,849][flp2p.graph_runner][INFO] - Train, Round 38 : loss => 0.5184713747451094,  accuracy: 0.7603030303030304, gradient_norm : 0.2094064825521487
[2025-09-18 13:41:51,100][flp2p.graph_runner][INFO] - Test, Round 38 : loss => 0.5812338547373768,  accuracy: 0.6781
[2025-09-18 13:41:54,226][flp2p.graph_runner][INFO] - Train, Round 39 : loss => 0.5913321653752636,  accuracy: 0.7248484848484849, gradient_norm : 0.28620353045074476
[2025-09-18 13:42:01,413][flp2p.graph_runner][INFO] - Test, Round 39 : loss => 0.5619400141638142,  accuracy: 0.6816
[2025-09-18 13:42:04,455][flp2p.graph_runner][INFO] - Train, Round 40 : loss => 0.41186796362457606,  accuracy: 0.8041818181818182, gradient_norm : 0.22401003948558482
[2025-09-18 13:42:11,606][flp2p.graph_runner][INFO] - Test, Round 40 : loss => 0.5507320726528713,  accuracy: 0.6887
[2025-09-18 13:42:14,688][flp2p.graph_runner][INFO] - Train, Round 41 : loss => 0.3789287794472006,  accuracy: 0.7947878787878789, gradient_norm : 0.1770602267883579
[2025-09-18 13:42:21,926][flp2p.graph_runner][INFO] - Test, Round 41 : loss => 0.547201217880053,  accuracy: 0.691
[2025-09-18 13:42:25,025][flp2p.graph_runner][INFO] - Train, Round 42 : loss => 0.44992859591373097,  accuracy: 0.7528484848484849, gradient_norm : 0.16051743962860923
[2025-09-18 13:42:32,193][flp2p.graph_runner][INFO] - Test, Round 42 : loss => 0.5433003382885149,  accuracy: 0.6971
[2025-09-18 13:42:35,322][flp2p.graph_runner][INFO] - Train, Round 43 : loss => 0.34950961111460255,  accuracy: 0.8196363636363636, gradient_norm : 0.1777792295325405
[2025-09-18 13:42:42,585][flp2p.graph_runner][INFO] - Test, Round 43 : loss => 0.5409437408530404,  accuracy: 0.6976
[2025-09-18 13:42:45,746][flp2p.graph_runner][INFO] - Train, Round 44 : loss => 0.34651909078969545,  accuracy: 0.8032121212121212, gradient_norm : 0.15103414198312046
[2025-09-18 13:42:53,202][flp2p.graph_runner][INFO] - Test, Round 44 : loss => 0.5407870126991157,  accuracy: 0.6963
[2025-09-18 13:42:56,364][flp2p.graph_runner][INFO] - Train, Round 45 : loss => 0.392945110768687,  accuracy: 0.784121212121212, gradient_norm : 0.16935640260950874
[2025-09-18 13:43:03,857][flp2p.graph_runner][INFO] - Test, Round 45 : loss => 0.5390635905251249,  accuracy: 0.6959
[2025-09-18 13:43:07,086][flp2p.graph_runner][INFO] - Train, Round 46 : loss => 0.4415534168334143,  accuracy: 0.7446060606060606, gradient_norm : 0.15934581800458963
[2025-09-18 13:43:14,469][flp2p.graph_runner][INFO] - Test, Round 46 : loss => 0.5350113524364438,  accuracy: 0.7011
[2025-09-18 13:43:17,639][flp2p.graph_runner][INFO] - Train, Round 47 : loss => 0.3982409245151687,  accuracy: 0.7640606060606062, gradient_norm : 0.1396355503547767
[2025-09-18 13:43:25,027][flp2p.graph_runner][INFO] - Test, Round 47 : loss => 0.5354254552949144,  accuracy: 0.7045
[2025-09-18 13:43:28,237][flp2p.graph_runner][INFO] - Train, Round 48 : loss => 0.2893950038978672,  accuracy: 0.8493333333333333, gradient_norm : 0.12329633427699395
[2025-09-18 13:43:35,602][flp2p.graph_runner][INFO] - Test, Round 48 : loss => 0.534897509399917,  accuracy: 0.7049
[2025-09-18 13:43:38,852][flp2p.graph_runner][INFO] - Train, Round 49 : loss => 0.40960051668782665,  accuracy: 0.765939393939394, gradient_norm : 0.1354683087384986
[2025-09-18 13:43:46,271][flp2p.graph_runner][INFO] - Test, Round 49 : loss => 0.5334581935758715,  accuracy: 0.7052
[2025-09-18 13:43:49,215][flp2p.graph_runner][INFO] - Train, Round 50 : loss => 0.3109561118579978,  accuracy: 0.8254666666666668, gradient_norm : 0.13556769958809686
[2025-09-18 13:43:56,659][flp2p.graph_runner][INFO] - Test, Round 50 : loss => 0.5308853733372676,  accuracy: 0.7072
[2025-09-18 13:43:59,560][flp2p.graph_runner][INFO] - Train, Round 51 : loss => 0.46579102362722397,  accuracy: 0.7454666666666668, gradient_norm : 0.18526394305203678
[2025-09-18 13:44:06,937][flp2p.graph_runner][INFO] - Test, Round 51 : loss => 0.5198467447355702,  accuracy: 0.7126
[2025-09-18 13:44:10,122][flp2p.graph_runner][INFO] - Train, Round 52 : loss => 0.32631583132420094,  accuracy: 0.8325454545454546, gradient_norm : 0.13262052111491632
[2025-09-18 13:44:17,525][flp2p.graph_runner][INFO] - Test, Round 52 : loss => 0.5175925782780987,  accuracy: 0.7136
[2025-09-18 13:44:20,671][flp2p.graph_runner][INFO] - Train, Round 53 : loss => 0.3656057744600689,  accuracy: 0.818, gradient_norm : 0.15065546798595342
[2025-09-18 13:44:28,074][flp2p.graph_runner][INFO] - Test, Round 53 : loss => 0.5113993195202094,  accuracy: 0.7208
[2025-09-18 13:44:31,212][flp2p.graph_runner][INFO] - Train, Round 54 : loss => 0.37109298519291994,  accuracy: 0.801939393939394, gradient_norm : 0.130696692466214
[2025-09-18 13:44:38,654][flp2p.graph_runner][INFO] - Test, Round 54 : loss => 0.5089399774258638,  accuracy: 0.7227
[2025-09-18 13:44:41,813][flp2p.graph_runner][INFO] - Train, Round 55 : loss => 0.37705567883996116,  accuracy: 0.7884848484848483, gradient_norm : 0.14488467526544957
[2025-09-18 13:44:49,217][flp2p.graph_runner][INFO] - Test, Round 55 : loss => 0.5066414813497295,  accuracy: 0.7249
[2025-09-18 13:44:52,414][flp2p.graph_runner][INFO] - Train, Round 56 : loss => 0.3932088490501237,  accuracy: 0.7918787878787878, gradient_norm : 0.15206583610681623
[2025-09-18 13:44:59,822][flp2p.graph_runner][INFO] - Test, Round 56 : loss => 0.5073527364250956,  accuracy: 0.7275
[2025-09-18 13:45:03,045][flp2p.graph_runner][INFO] - Train, Round 57 : loss => 0.4159727336358682,  accuracy: 0.7464242424242425, gradient_norm : 0.1263625589459892
[2025-09-18 13:45:10,432][flp2p.graph_runner][INFO] - Test, Round 57 : loss => 0.5095815117335416,  accuracy: 0.7282
[2025-09-18 13:45:13,577][flp2p.graph_runner][INFO] - Train, Round 58 : loss => 0.3366130376083866,  accuracy: 0.8140606060606062, gradient_norm : 0.11585070330295145
[2025-09-18 13:45:21,029][flp2p.graph_runner][INFO] - Test, Round 58 : loss => 0.5066863836789556,  accuracy: 0.7332
[2025-09-18 13:45:24,164][flp2p.graph_runner][INFO] - Train, Round 59 : loss => 0.3220183573143809,  accuracy: 0.8208484848484849, gradient_norm : 0.10333816467733632
[2025-09-18 13:45:31,456][flp2p.graph_runner][INFO] - Test, Round 59 : loss => 0.5048795646966996,  accuracy: 0.7362
[2025-09-18 13:45:34,636][flp2p.graph_runner][INFO] - Train, Round 60 : loss => 0.3140938563943839,  accuracy: 0.8295757575757576, gradient_norm : 0.11833199844724178
[2025-09-18 13:45:41,989][flp2p.graph_runner][INFO] - Test, Round 60 : loss => 0.5084278631112886,  accuracy: 0.7377
[2025-09-18 13:45:45,146][flp2p.graph_runner][INFO] - Train, Round 61 : loss => 0.4103120424529517,  accuracy: 0.7852121212121211, gradient_norm : 0.12673244162776579
[2025-09-18 13:45:52,530][flp2p.graph_runner][INFO] - Test, Round 61 : loss => 0.5075003677601856,  accuracy: 0.7389
[2025-09-18 13:45:55,676][flp2p.graph_runner][INFO] - Train, Round 62 : loss => 0.36810569008640653,  accuracy: 0.8104848484848486, gradient_norm : 0.17238865908499784
[2025-09-18 13:46:03,058][flp2p.graph_runner][INFO] - Test, Round 62 : loss => 0.49988050319670657,  accuracy: 0.7445
[2025-09-18 13:46:06,218][flp2p.graph_runner][INFO] - Train, Round 63 : loss => 0.27166699185250787,  accuracy: 0.8633333333333333, gradient_norm : 0.14473311303385117
[2025-09-18 13:46:13,552][flp2p.graph_runner][INFO] - Test, Round 63 : loss => 0.49147353387477277,  accuracy: 0.7489
[2025-09-18 13:46:16,694][flp2p.graph_runner][INFO] - Train, Round 64 : loss => 0.3449535269514485,  accuracy: 0.8284242424242424, gradient_norm : 0.14417989537479203
[2025-09-18 13:46:24,085][flp2p.graph_runner][INFO] - Test, Round 64 : loss => 0.48065251676384524,  accuracy: 0.7538
[2025-09-18 13:46:27,288][flp2p.graph_runner][INFO] - Train, Round 65 : loss => 0.35541838012215105,  accuracy: 0.8170303030303029, gradient_norm : 0.13898221365442678
[2025-09-18 13:46:34,729][flp2p.graph_runner][INFO] - Test, Round 65 : loss => 0.4800829415825722,  accuracy: 0.7551
[2025-09-18 13:46:37,848][flp2p.graph_runner][INFO] - Train, Round 66 : loss => 0.2330846592015979,  accuracy: 0.8991515151515153, gradient_norm : 0.1345639570392296
[2025-09-18 13:46:45,220][flp2p.graph_runner][INFO] - Test, Round 66 : loss => 0.4783942004711473,  accuracy: 0.7546
[2025-09-18 13:46:48,355][flp2p.graph_runner][INFO] - Train, Round 67 : loss => 0.3849533897983334,  accuracy: 0.8021212121212121, gradient_norm : 0.16523686186488407
[2025-09-18 13:46:55,786][flp2p.graph_runner][INFO] - Test, Round 67 : loss => 0.4807730332719986,  accuracy: 0.7545
[2025-09-18 13:46:58,958][flp2p.graph_runner][INFO] - Train, Round 68 : loss => 0.356724899755775,  accuracy: 0.805939393939394, gradient_norm : 0.12534212093459615
[2025-09-18 13:47:06,319][flp2p.graph_runner][INFO] - Test, Round 68 : loss => 0.47871417570078467,  accuracy: 0.7604
[2025-09-18 13:47:09,224][flp2p.graph_runner][INFO] - Train, Round 69 : loss => 0.3123977258992025,  accuracy: 0.8417333333333332, gradient_norm : 0.12164482219857742
[2025-09-18 13:47:16,633][flp2p.graph_runner][INFO] - Test, Round 69 : loss => 0.4767434959443234,  accuracy: 0.7641
[2025-09-18 13:47:19,781][flp2p.graph_runner][INFO] - Train, Round 70 : loss => 0.3351412232194068,  accuracy: 0.825939393939394, gradient_norm : 0.15834098139444705
[2025-09-18 13:47:27,182][flp2p.graph_runner][INFO] - Test, Round 70 : loss => 0.4776701404844869,  accuracy: 0.7632
[2025-09-18 13:47:30,379][flp2p.graph_runner][INFO] - Train, Round 71 : loss => 0.32544816056217823,  accuracy: 0.8313939393939396, gradient_norm : 0.12025674125540203
[2025-09-18 13:47:37,729][flp2p.graph_runner][INFO] - Test, Round 71 : loss => 0.47933935872698546,  accuracy: 0.7653
[2025-09-18 13:47:40,912][flp2p.graph_runner][INFO] - Train, Round 72 : loss => 0.3713426738840421,  accuracy: 0.8035757575757576, gradient_norm : 0.13856470185479597
[2025-09-18 13:47:48,277][flp2p.graph_runner][INFO] - Test, Round 72 : loss => 0.4734752565796314,  accuracy: 0.7688
[2025-09-18 13:47:51,479][flp2p.graph_runner][INFO] - Train, Round 73 : loss => 0.3348166760131155,  accuracy: 0.8337575757575757, gradient_norm : 0.13903691451678837
[2025-09-18 13:47:58,909][flp2p.graph_runner][INFO] - Test, Round 73 : loss => 0.4707398507593537,  accuracy: 0.7706
[2025-09-18 13:48:02,128][flp2p.graph_runner][INFO] - Train, Round 74 : loss => 0.3968798204940334,  accuracy: 0.7826666666666667, gradient_norm : 0.12170836417264498
[2025-09-18 13:48:09,471][flp2p.graph_runner][INFO] - Test, Round 74 : loss => 0.4706555880041704,  accuracy: 0.7679
[2025-09-18 13:48:12,690][flp2p.graph_runner][INFO] - Train, Round 75 : loss => 0.315366196002559,  accuracy: 0.8162424242424242, gradient_norm : 0.09557167404353678
[2025-09-18 13:48:20,125][flp2p.graph_runner][INFO] - Test, Round 75 : loss => 0.47123602972511963,  accuracy: 0.7677
[2025-09-18 13:48:23,332][flp2p.graph_runner][INFO] - Train, Round 76 : loss => 0.38309343081523195,  accuracy: 0.8083030303030304, gradient_norm : 0.14141474640168153
[2025-09-18 13:48:30,751][flp2p.graph_runner][INFO] - Test, Round 76 : loss => 0.4705080268605826,  accuracy: 0.77
[2025-09-18 13:48:33,968][flp2p.graph_runner][INFO] - Train, Round 77 : loss => 0.26100467874281413,  accuracy: 0.8855757575757575, gradient_norm : 0.16450014481788122
[2025-09-18 13:48:41,307][flp2p.graph_runner][INFO] - Test, Round 77 : loss => 0.4684565413658087,  accuracy: 0.7702
[2025-09-18 13:48:44,491][flp2p.graph_runner][INFO] - Train, Round 78 : loss => 0.3093812830788387,  accuracy: 0.8564242424242425, gradient_norm : 0.14519390736926283
[2025-09-18 13:48:51,872][flp2p.graph_runner][INFO] - Test, Round 78 : loss => 0.4685407990372106,  accuracy: 0.7718
[2025-09-18 13:48:55,057][flp2p.graph_runner][INFO] - Train, Round 79 : loss => 0.28507186860342176,  accuracy: 0.8616969696969699, gradient_norm : 0.1287001165027005
[2025-09-18 13:49:02,449][flp2p.graph_runner][INFO] - Test, Round 79 : loss => 0.46576284598518525,  accuracy: 0.7717
[2025-09-18 13:49:05,342][flp2p.graph_runner][INFO] - Train, Round 80 : loss => 0.1574580144562686,  accuracy: 0.9332666666666666, gradient_norm : 0.12569169482160206
[2025-09-18 13:49:12,714][flp2p.graph_runner][INFO] - Test, Round 80 : loss => 0.46467633122628677,  accuracy: 0.7723
[2025-09-18 13:49:15,558][flp2p.graph_runner][INFO] - Train, Round 81 : loss => 0.2622197480640578,  accuracy: 0.864, gradient_norm : 0.1187952349571646
[2025-09-18 13:49:22,950][flp2p.graph_runner][INFO] - Test, Round 81 : loss => 0.46193954046659047,  accuracy: 0.776
[2025-09-18 13:49:26,112][flp2p.graph_runner][INFO] - Train, Round 82 : loss => 0.30746228823545957,  accuracy: 0.8386666666666667, gradient_norm : 0.13751378767555023
[2025-09-18 13:49:33,490][flp2p.graph_runner][INFO] - Test, Round 82 : loss => 0.4648018055115366,  accuracy: 0.7734
[2025-09-18 13:49:36,653][flp2p.graph_runner][INFO] - Train, Round 83 : loss => 0.3634067103766922,  accuracy: 0.8102424242424242, gradient_norm : 0.13238778036738494
[2025-09-18 13:49:43,977][flp2p.graph_runner][INFO] - Test, Round 83 : loss => 0.4621324964903342,  accuracy: 0.7753
[2025-09-18 13:49:46,880][flp2p.graph_runner][INFO] - Train, Round 84 : loss => 0.2690911587064207,  accuracy: 0.8676666666666668, gradient_norm : 0.12346000586551127
[2025-09-18 13:49:54,246][flp2p.graph_runner][INFO] - Test, Round 84 : loss => 0.4589788679415197,  accuracy: 0.7771
[2025-09-18 13:49:57,379][flp2p.graph_runner][INFO] - Train, Round 85 : loss => 0.24571317092448064,  accuracy: 0.8836363636363638, gradient_norm : 0.11562193485477965
[2025-09-18 13:50:04,778][flp2p.graph_runner][INFO] - Test, Round 85 : loss => 0.45688757914147754,  accuracy: 0.7766
[2025-09-18 13:50:07,977][flp2p.graph_runner][INFO] - Train, Round 86 : loss => 0.22983143713711943,  accuracy: 0.8829090909090909, gradient_norm : 0.11995455831720098
[2025-09-18 13:50:15,381][flp2p.graph_runner][INFO] - Test, Round 86 : loss => 0.4513805347294468,  accuracy: 0.7784
[2025-09-18 13:50:18,532][flp2p.graph_runner][INFO] - Train, Round 87 : loss => 0.3885829960725587,  accuracy: 0.7948484848484849, gradient_norm : 0.1353471654641449
[2025-09-18 13:50:25,953][flp2p.graph_runner][INFO] - Test, Round 87 : loss => 0.4466555037174805,  accuracy: 0.7815
[2025-09-18 13:50:29,137][flp2p.graph_runner][INFO] - Train, Round 88 : loss => 0.1569559424699489,  accuracy: 0.9314545454545454, gradient_norm : 0.11234240301158799
[2025-09-18 13:50:36,545][flp2p.graph_runner][INFO] - Test, Round 88 : loss => 0.44899511124433783,  accuracy: 0.7817
[2025-09-18 13:50:39,713][flp2p.graph_runner][INFO] - Train, Round 89 : loss => 0.31235346912956485,  accuracy: 0.8477575757575756, gradient_norm : 0.16643049202384452
[2025-09-18 13:50:47,128][flp2p.graph_runner][INFO] - Test, Round 89 : loss => 0.45510099505889756,  accuracy: 0.7798
[2025-09-18 13:50:50,275][flp2p.graph_runner][INFO] - Train, Round 90 : loss => 0.23363096187630406,  accuracy: 0.8923636363636362, gradient_norm : 0.11329610825857861
[2025-09-18 13:50:57,684][flp2p.graph_runner][INFO] - Test, Round 90 : loss => 0.4495038504614589,  accuracy: 0.7827
[2025-09-18 13:51:00,540][flp2p.graph_runner][INFO] - Train, Round 91 : loss => 0.4062004617923393,  accuracy: 0.7848666666666667, gradient_norm : 0.13808142888511385
[2025-09-18 13:51:07,884][flp2p.graph_runner][INFO] - Test, Round 91 : loss => 0.4464632585066041,  accuracy: 0.7864
[2025-09-18 13:51:11,018][flp2p.graph_runner][INFO] - Train, Round 92 : loss => 0.35644763686358355,  accuracy: 0.8212121212121211, gradient_norm : 0.15846605094898908
[2025-09-18 13:51:18,454][flp2p.graph_runner][INFO] - Test, Round 92 : loss => 0.44706520327983346,  accuracy: 0.7876
[2025-09-18 13:51:21,583][flp2p.graph_runner][INFO] - Train, Round 93 : loss => 0.25097719777633126,  accuracy: 0.8647878787878788, gradient_norm : 0.13076267244532305
[2025-09-18 13:51:28,939][flp2p.graph_runner][INFO] - Test, Round 93 : loss => 0.4500982796332553,  accuracy: 0.79
[2025-09-18 13:51:32,113][flp2p.graph_runner][INFO] - Train, Round 94 : loss => 0.23569071977061895,  accuracy: 0.888060606060606, gradient_norm : 0.12914523337126774
[2025-09-18 13:51:39,530][flp2p.graph_runner][INFO] - Test, Round 94 : loss => 0.44679124153054145,  accuracy: 0.7921
[2025-09-18 13:51:42,709][flp2p.graph_runner][INFO] - Train, Round 95 : loss => 0.2649316148015771,  accuracy: 0.8589090909090907, gradient_norm : 0.10216332300498214
[2025-09-18 13:51:50,116][flp2p.graph_runner][INFO] - Test, Round 95 : loss => 0.44564830909088243,  accuracy: 0.7921
[2025-09-18 13:51:53,268][flp2p.graph_runner][INFO] - Train, Round 96 : loss => 0.17566044145182347,  accuracy: 0.926, gradient_norm : 0.1449727459898881
[2025-09-18 13:52:00,626][flp2p.graph_runner][INFO] - Test, Round 96 : loss => 0.442668013161735,  accuracy: 0.7935
[2025-09-18 13:52:03,754][flp2p.graph_runner][INFO] - Train, Round 97 : loss => 0.31552668759613783,  accuracy: 0.8532727272727273, gradient_norm : 0.1588261593680989
[2025-09-18 13:52:11,197][flp2p.graph_runner][INFO] - Test, Round 97 : loss => 0.43847343881939704,  accuracy: 0.7959
[2025-09-18 13:52:14,112][flp2p.graph_runner][INFO] - Train, Round 98 : loss => 0.25894490644115736,  accuracy: 0.8728666666666668, gradient_norm : 0.139242121015401
[2025-09-18 13:52:21,524][flp2p.graph_runner][INFO] - Test, Round 98 : loss => 0.4379093383158057,  accuracy: 0.797
[2025-09-18 13:52:24,726][flp2p.graph_runner][INFO] - Train, Round 99 : loss => 0.24818286660417482,  accuracy: 0.864121212121212, gradient_norm : 0.09391237540797605
[2025-09-18 13:52:32,184][flp2p.graph_runner][INFO] - Test, Round 99 : loss => 0.4386965832277927,  accuracy: 0.7975
[2025-09-18 13:52:35,358][flp2p.graph_runner][INFO] - Train, Round 100 : loss => 0.2337268761478161,  accuracy: 0.8847272727272727, gradient_norm : 0.11534607355293783
[2025-09-18 13:52:42,826][flp2p.graph_runner][INFO] - Test, Round 100 : loss => 0.43341689768501446,  accuracy: 0.8013
[2025-09-18 13:52:46,045][flp2p.graph_runner][INFO] - Train, Round 101 : loss => 0.3166340115499733,  accuracy: 0.8459999999999999, gradient_norm : 0.12925576329017027
[2025-09-18 13:52:53,426][flp2p.graph_runner][INFO] - Test, Round 101 : loss => 0.43408030656081,  accuracy: 0.8016
[2025-09-18 13:52:56,690][flp2p.graph_runner][INFO] - Train, Round 102 : loss => 0.30026244176550826,  accuracy: 0.8573333333333334, gradient_norm : 0.140184756341443
[2025-09-18 13:53:04,108][flp2p.graph_runner][INFO] - Test, Round 102 : loss => 0.4331579173073311,  accuracy: 0.8014
[2025-09-18 13:53:07,283][flp2p.graph_runner][INFO] - Train, Round 103 : loss => 0.24375184822794824,  accuracy: 0.8863030303030304, gradient_norm : 0.17154055999010362
[2025-09-18 13:53:14,722][flp2p.graph_runner][INFO] - Test, Round 103 : loss => 0.4421982295567098,  accuracy: 0.7976
[2025-09-18 13:53:17,900][flp2p.graph_runner][INFO] - Train, Round 104 : loss => 0.2603807008410039,  accuracy: 0.8737575757575758, gradient_norm : 0.1395266894018869
[2025-09-18 13:53:25,266][flp2p.graph_runner][INFO] - Test, Round 104 : loss => 0.44913464126777164,  accuracy: 0.7982
[2025-09-18 13:53:28,435][flp2p.graph_runner][INFO] - Train, Round 105 : loss => 0.20675825011205953,  accuracy: 0.9041818181818183, gradient_norm : 0.13431800853073061
[2025-09-18 13:53:35,896][flp2p.graph_runner][INFO] - Test, Round 105 : loss => 0.44719947552771744,  accuracy: 0.7981
[2025-09-18 13:53:39,063][flp2p.graph_runner][INFO] - Train, Round 106 : loss => 0.2660128772904343,  accuracy: 0.8771515151515152, gradient_norm : 0.1423910450944357
[2025-09-18 13:53:46,496][flp2p.graph_runner][INFO] - Test, Round 106 : loss => 0.4455159046278607,  accuracy: 0.7986
[2025-09-18 13:53:49,654][flp2p.graph_runner][INFO] - Train, Round 107 : loss => 0.2815673191331041,  accuracy: 0.8731515151515152, gradient_norm : 0.15736011636952185
[2025-09-18 13:53:57,071][flp2p.graph_runner][INFO] - Test, Round 107 : loss => 0.43769084314206685,  accuracy: 0.8041
[2025-09-18 13:54:00,209][flp2p.graph_runner][INFO] - Train, Round 108 : loss => 0.3473768933289399,  accuracy: 0.8278181818181818, gradient_norm : 0.16837410208642167
[2025-09-18 13:54:07,597][flp2p.graph_runner][INFO] - Test, Round 108 : loss => 0.4332520341735686,  accuracy: 0.8041
[2025-09-18 13:54:10,716][flp2p.graph_runner][INFO] - Train, Round 109 : loss => 0.36360069314933546,  accuracy: 0.8027272727272727, gradient_norm : 0.13982612386915003
[2025-09-18 13:54:18,037][flp2p.graph_runner][INFO] - Test, Round 109 : loss => 0.4320172008162077,  accuracy: 0.8055
[2025-09-18 13:54:21,153][flp2p.graph_runner][INFO] - Train, Round 110 : loss => 0.26761042984854655,  accuracy: 0.8732121212121211, gradient_norm : 0.14486320027290878
[2025-09-18 13:54:28,619][flp2p.graph_runner][INFO] - Test, Round 110 : loss => 0.42841543152641853,  accuracy: 0.8074
[2025-09-18 13:54:31,765][flp2p.graph_runner][INFO] - Train, Round 111 : loss => 0.1351842020284996,  accuracy: 0.9458181818181818, gradient_norm : 0.09398857314388936
[2025-09-18 13:54:39,181][flp2p.graph_runner][INFO] - Test, Round 111 : loss => 0.43032592536123027,  accuracy: 0.8063
[2025-09-18 13:54:42,031][flp2p.graph_runner][INFO] - Train, Round 112 : loss => 0.23312285541829633,  accuracy: 0.89, gradient_norm : 0.1404755527130478
[2025-09-18 13:54:49,374][flp2p.graph_runner][INFO] - Test, Round 112 : loss => 0.4297005866089927,  accuracy: 0.8081
[2025-09-18 13:54:52,511][flp2p.graph_runner][INFO] - Train, Round 113 : loss => 0.2932484369213512,  accuracy: 0.8566666666666666, gradient_norm : 0.1417136725051491
[2025-09-18 13:54:59,915][flp2p.graph_runner][INFO] - Test, Round 113 : loss => 0.43325457649523497,  accuracy: 0.8082
[2025-09-18 13:55:02,764][flp2p.graph_runner][INFO] - Train, Round 114 : loss => 0.23366191628004646,  accuracy: 0.9040666666666668, gradient_norm : 0.15386881828302673
[2025-09-18 13:55:10,147][flp2p.graph_runner][INFO] - Test, Round 114 : loss => 0.42680828654462577,  accuracy: 0.8118
[2025-09-18 13:55:13,295][flp2p.graph_runner][INFO] - Train, Round 115 : loss => 0.27528839318512677,  accuracy: 0.8663636363636363, gradient_norm : 0.1434148415029439
[2025-09-18 13:55:20,676][flp2p.graph_runner][INFO] - Test, Round 115 : loss => 0.429129834476668,  accuracy: 0.8102
[2025-09-18 13:55:23,785][flp2p.graph_runner][INFO] - Train, Round 116 : loss => 0.20950950207557398,  accuracy: 0.9113939393939394, gradient_norm : 0.15683382662659892
[2025-09-18 13:55:31,213][flp2p.graph_runner][INFO] - Test, Round 116 : loss => 0.43073353408232967,  accuracy: 0.8105
[2025-09-18 13:55:34,341][flp2p.graph_runner][INFO] - Train, Round 117 : loss => 0.25984234706906556,  accuracy: 0.8723636363636363, gradient_norm : 0.12583723147059656
[2025-09-18 13:55:41,694][flp2p.graph_runner][INFO] - Test, Round 117 : loss => 0.43175883564176465,  accuracy: 0.8114
[2025-09-18 13:55:44,853][flp2p.graph_runner][INFO] - Train, Round 118 : loss => 0.21432359575330082,  accuracy: 0.896121212121212, gradient_norm : 0.1139834009298816
[2025-09-18 13:55:52,274][flp2p.graph_runner][INFO] - Test, Round 118 : loss => 0.43062079692057825,  accuracy: 0.8136
[2025-09-18 13:55:55,405][flp2p.graph_runner][INFO] - Train, Round 119 : loss => 0.21933032576845451,  accuracy: 0.8989696969696969, gradient_norm : 0.12830406743036146
[2025-09-18 13:56:02,856][flp2p.graph_runner][INFO] - Test, Round 119 : loss => 0.4301266855021847,  accuracy: 0.8149
[2025-09-18 13:56:06,060][flp2p.graph_runner][INFO] - Train, Round 120 : loss => 0.23394563748324804,  accuracy: 0.8926666666666667, gradient_norm : 0.15596438604166554
[2025-09-18 13:56:13,442][flp2p.graph_runner][INFO] - Test, Round 120 : loss => 0.4345768419764181,  accuracy: 0.8139
[2025-09-18 13:56:16,593][flp2p.graph_runner][INFO] - Train, Round 121 : loss => 0.1529223730133362,  accuracy: 0.9407878787878787, gradient_norm : 0.16830240071769323
[2025-09-18 13:56:23,976][flp2p.graph_runner][INFO] - Test, Round 121 : loss => 0.44087717668290544,  accuracy: 0.8117
[2025-09-18 13:56:27,160][flp2p.graph_runner][INFO] - Train, Round 122 : loss => 0.2301480117006811,  accuracy: 0.8947878787878789, gradient_norm : 0.13685291603803507
[2025-09-18 13:56:34,557][flp2p.graph_runner][INFO] - Test, Round 122 : loss => 0.4406911446146714,  accuracy: 0.8112
[2025-09-18 13:56:37,718][flp2p.graph_runner][INFO] - Train, Round 123 : loss => 0.16695892695093625,  accuracy: 0.9285454545454543, gradient_norm : 0.13721988464913812
[2025-09-18 13:56:45,095][flp2p.graph_runner][INFO] - Test, Round 123 : loss => 0.441460998852771,  accuracy: 0.8107
[2025-09-18 13:56:48,215][flp2p.graph_runner][INFO] - Train, Round 124 : loss => 0.12566194543586015,  accuracy: 0.9484242424242424, gradient_norm : 0.13666557786101974
[2025-09-18 13:56:55,635][flp2p.graph_runner][INFO] - Test, Round 124 : loss => 0.43778739486615215,  accuracy: 0.8124
[2025-09-18 13:56:58,820][flp2p.graph_runner][INFO] - Train, Round 125 : loss => 0.2594027174906426,  accuracy: 0.8775151515151516, gradient_norm : 0.14394959954884481
[2025-09-18 13:57:06,159][flp2p.graph_runner][INFO] - Test, Round 125 : loss => 0.43407190691337155,  accuracy: 0.8137
[2025-09-18 13:57:09,389][flp2p.graph_runner][INFO] - Train, Round 126 : loss => 0.19074051208222412,  accuracy: 0.9067878787878789, gradient_norm : 0.1416977894901346
[2025-09-18 13:57:16,782][flp2p.graph_runner][INFO] - Test, Round 126 : loss => 0.43319355443240964,  accuracy: 0.8148
[2025-09-18 13:57:19,961][flp2p.graph_runner][INFO] - Train, Round 127 : loss => 0.22049941424831357,  accuracy: 0.9003030303030304, gradient_norm : 0.13952634051071974
[2025-09-18 13:57:27,359][flp2p.graph_runner][INFO] - Test, Round 127 : loss => 0.4270797811616279,  accuracy: 0.8194
[2025-09-18 13:57:30,514][flp2p.graph_runner][INFO] - Train, Round 128 : loss => 0.1381835106858848,  accuracy: 0.943878787878788, gradient_norm : 0.13103048512108526
[2025-09-18 13:57:37,980][flp2p.graph_runner][INFO] - Test, Round 128 : loss => 0.43000927779314907,  accuracy: 0.8178
[2025-09-18 13:57:41,168][flp2p.graph_runner][INFO] - Train, Round 129 : loss => 0.21566495063984015,  accuracy: 0.8992121212121212, gradient_norm : 0.13922712740992144
[2025-09-18 13:57:48,552][flp2p.graph_runner][INFO] - Test, Round 129 : loss => 0.4305781887588018,  accuracy: 0.8175
[2025-09-18 13:57:51,721][flp2p.graph_runner][INFO] - Train, Round 130 : loss => 0.13293018393731826,  accuracy: 0.945878787878788, gradient_norm : 0.13715517443556913
[2025-09-18 13:57:59,136][flp2p.graph_runner][INFO] - Test, Round 130 : loss => 0.4292828083018065,  accuracy: 0.8189
[2025-09-18 13:58:02,306][flp2p.graph_runner][INFO] - Train, Round 131 : loss => 0.2817786912058072,  accuracy: 0.8681818181818182, gradient_norm : 0.14542898765542278
[2025-09-18 13:58:09,731][flp2p.graph_runner][INFO] - Test, Round 131 : loss => 0.42337920250925726,  accuracy: 0.8227
[2025-09-18 13:58:12,864][flp2p.graph_runner][INFO] - Train, Round 132 : loss => 0.23675804896022315,  accuracy: 0.8876363636363637, gradient_norm : 0.12798256565643562
[2025-09-18 13:58:20,231][flp2p.graph_runner][INFO] - Test, Round 132 : loss => 0.42022822294934203,  accuracy: 0.8237
[2025-09-18 13:58:23,403][flp2p.graph_runner][INFO] - Train, Round 133 : loss => 0.19172631234328158,  accuracy: 0.9161212121212122, gradient_norm : 0.13699732158873199
[2025-09-18 13:58:30,776][flp2p.graph_runner][INFO] - Test, Round 133 : loss => 0.4199557742074471,  accuracy: 0.8258
[2025-09-18 13:58:33,897][flp2p.graph_runner][INFO] - Train, Round 134 : loss => 0.18973943602946614,  accuracy: 0.9186060606060605, gradient_norm : 0.14611741970171355
[2025-09-18 13:58:41,293][flp2p.graph_runner][INFO] - Test, Round 134 : loss => 0.42248625558592756,  accuracy: 0.8259
[2025-09-18 13:58:44,400][flp2p.graph_runner][INFO] - Train, Round 135 : loss => 0.15580723836110644,  accuracy: 0.9398181818181818, gradient_norm : 0.1724058765291063
[2025-09-18 13:58:51,793][flp2p.graph_runner][INFO] - Test, Round 135 : loss => 0.42611205568043087,  accuracy: 0.826
[2025-09-18 13:58:54,948][flp2p.graph_runner][INFO] - Train, Round 136 : loss => 0.18463822577331424,  accuracy: 0.9147878787878789, gradient_norm : 0.1042639338624288
[2025-09-18 13:59:02,271][flp2p.graph_runner][INFO] - Test, Round 136 : loss => 0.42210488350637804,  accuracy: 0.8281
[2025-09-18 13:59:05,440][flp2p.graph_runner][INFO] - Train, Round 137 : loss => 0.1378039738043974,  accuracy: 0.9356969696969697, gradient_norm : 0.12731667016479065
[2025-09-18 13:59:12,835][flp2p.graph_runner][INFO] - Test, Round 137 : loss => 0.42155290236172943,  accuracy: 0.8309
[2025-09-18 13:59:15,965][flp2p.graph_runner][INFO] - Train, Round 138 : loss => 0.17193774272899895,  accuracy: 0.9289696969696969, gradient_norm : 0.13801401768297303
[2025-09-18 13:59:23,423][flp2p.graph_runner][INFO] - Test, Round 138 : loss => 0.4195042637904254,  accuracy: 0.8324
[2025-09-18 13:59:26,560][flp2p.graph_runner][INFO] - Train, Round 139 : loss => 0.21831268223743455,  accuracy: 0.8929090909090908, gradient_norm : 0.1300631104927275
[2025-09-18 13:59:33,936][flp2p.graph_runner][INFO] - Test, Round 139 : loss => 0.42375552733637323,  accuracy: 0.8322
[2025-09-18 13:59:37,090][flp2p.graph_runner][INFO] - Train, Round 140 : loss => 0.09153920238274796,  accuracy: 0.9653333333333333, gradient_norm : 0.13613688615202602
[2025-09-18 13:59:44,447][flp2p.graph_runner][INFO] - Test, Round 140 : loss => 0.42877820008096335,  accuracy: 0.8302
[2025-09-18 13:59:47,600][flp2p.graph_runner][INFO] - Train, Round 141 : loss => 0.1637915567051092,  accuracy: 0.9292121212121213, gradient_norm : 0.1320199557702697
[2025-09-18 13:59:54,910][flp2p.graph_runner][INFO] - Test, Round 141 : loss => 0.42731355987722164,  accuracy: 0.8319
[2025-09-18 13:59:58,076][flp2p.graph_runner][INFO] - Train, Round 142 : loss => 0.22399960150735795,  accuracy: 0.8954545454545455, gradient_norm : 0.14227129156938476
[2025-09-18 14:00:05,458][flp2p.graph_runner][INFO] - Test, Round 142 : loss => 0.42561155109373483,  accuracy: 0.8339
[2025-09-18 14:00:08,577][flp2p.graph_runner][INFO] - Train, Round 143 : loss => 0.23686055201119283,  accuracy: 0.8955151515151515, gradient_norm : 0.14545451440534854
[2025-09-18 14:00:16,087][flp2p.graph_runner][INFO] - Test, Round 143 : loss => 0.42694962244365714,  accuracy: 0.834
[2025-09-18 14:00:19,206][flp2p.graph_runner][INFO] - Train, Round 144 : loss => 0.175195246524039,  accuracy: 0.9228484848484849, gradient_norm : 0.12898063772507504
[2025-09-18 14:00:26,620][flp2p.graph_runner][INFO] - Test, Round 144 : loss => 0.42784289853080865,  accuracy: 0.8347
[2025-09-18 14:00:29,776][flp2p.graph_runner][INFO] - Train, Round 145 : loss => 0.14031340277922816,  accuracy: 0.9409696969696969, gradient_norm : 0.1290520503942176
[2025-09-18 14:00:37,163][flp2p.graph_runner][INFO] - Test, Round 145 : loss => 0.4323241448875756,  accuracy: 0.8333
[2025-09-18 14:00:40,313][flp2p.graph_runner][INFO] - Train, Round 146 : loss => 0.1220777344574563,  accuracy: 0.9508484848484849, gradient_norm : 0.13067709890383167
[2025-09-18 14:00:47,737][flp2p.graph_runner][INFO] - Test, Round 146 : loss => 0.43092072106167334,  accuracy: 0.8343
[2025-09-18 14:00:50,898][flp2p.graph_runner][INFO] - Train, Round 147 : loss => 0.170507098077399,  accuracy: 0.9299393939393938, gradient_norm : 0.14296347016463443
[2025-09-18 14:00:58,227][flp2p.graph_runner][INFO] - Test, Round 147 : loss => 0.43306926980771193,  accuracy: 0.8324
[2025-09-18 14:01:01,407][flp2p.graph_runner][INFO] - Train, Round 148 : loss => 0.2567984091641646,  accuracy: 0.886969696969697, gradient_norm : 0.16904370443460104
[2025-09-18 14:01:08,854][flp2p.graph_runner][INFO] - Test, Round 148 : loss => 0.43403827712518295,  accuracy: 0.8341
[2025-09-18 14:01:11,716][flp2p.graph_runner][INFO] - Train, Round 149 : loss => 0.06405910646975191,  accuracy: 0.9774, gradient_norm : 0.0823488563939914
[2025-09-18 14:01:19,141][flp2p.graph_runner][INFO] - Test, Round 149 : loss => 0.43222037803413543,  accuracy: 0.8362
[2025-09-18 14:01:22,307][flp2p.graph_runner][INFO] - Train, Round 150 : loss => 0.23673091183465458,  accuracy: 0.8967878787878789, gradient_norm : 0.17230828719524488
[2025-09-18 14:01:29,623][flp2p.graph_runner][INFO] - Test, Round 150 : loss => 0.4355886249049072,  accuracy: 0.8368
[2025-09-18 14:01:32,836][flp2p.graph_runner][INFO] - Train, Round 151 : loss => 0.2398091243803244,  accuracy: 0.8941212121212122, gradient_norm : 0.17112408744070665
[2025-09-18 14:01:40,260][flp2p.graph_runner][INFO] - Test, Round 151 : loss => 0.4312979181859433,  accuracy: 0.8379
[2025-09-18 14:01:43,419][flp2p.graph_runner][INFO] - Train, Round 152 : loss => 0.11603340840563438,  accuracy: 0.9506666666666665, gradient_norm : 0.09094758735767053
[2025-09-18 14:01:50,738][flp2p.graph_runner][INFO] - Test, Round 152 : loss => 0.4350013211623842,  accuracy: 0.8363
[2025-09-18 14:01:54,002][flp2p.graph_runner][INFO] - Train, Round 153 : loss => 0.1390072920312636,  accuracy: 0.9370909090909091, gradient_norm : 0.11021793610634839
[2025-09-18 14:02:01,340][flp2p.graph_runner][INFO] - Test, Round 153 : loss => 0.43731419815871814,  accuracy: 0.8365
[2025-09-18 14:02:04,564][flp2p.graph_runner][INFO] - Train, Round 154 : loss => 0.09831175277939341,  accuracy: 0.9613939393939395, gradient_norm : 0.1195866468743826
[2025-09-18 14:02:11,965][flp2p.graph_runner][INFO] - Test, Round 154 : loss => 0.43994911781581797,  accuracy: 0.8372
[2025-09-18 14:02:15,196][flp2p.graph_runner][INFO] - Train, Round 155 : loss => 0.0763249748270818,  accuracy: 0.9712121212121213, gradient_norm : 0.1084495754007524
[2025-09-18 14:02:22,587][flp2p.graph_runner][INFO] - Test, Round 155 : loss => 0.44332798860510947,  accuracy: 0.8377
[2025-09-18 14:02:25,831][flp2p.graph_runner][INFO] - Train, Round 156 : loss => 0.13519872634722654,  accuracy: 0.9478787878787879, gradient_norm : 0.14656278411095294
[2025-09-18 14:02:33,263][flp2p.graph_runner][INFO] - Test, Round 156 : loss => 0.4434411330474621,  accuracy: 0.8376
[2025-09-18 14:02:36,456][flp2p.graph_runner][INFO] - Train, Round 157 : loss => 0.09985164278615846,  accuracy: 0.9567272727272728, gradient_norm : 0.11883513793886182
[2025-09-18 14:02:43,827][flp2p.graph_runner][INFO] - Test, Round 157 : loss => 0.44583719617830586,  accuracy: 0.8374
[2025-09-18 14:02:46,746][flp2p.graph_runner][INFO] - Train, Round 158 : loss => 0.08099725105765863,  accuracy: 0.9667333333333332, gradient_norm : 0.08190375363253077
[2025-09-18 14:02:54,160][flp2p.graph_runner][INFO] - Test, Round 158 : loss => 0.4499578478083667,  accuracy: 0.8371
[2025-09-18 14:02:57,359][flp2p.graph_runner][INFO] - Train, Round 159 : loss => 0.14815582571149508,  accuracy: 0.9370909090909091, gradient_norm : 0.12401794587766463
[2025-09-18 14:03:04,784][flp2p.graph_runner][INFO] - Test, Round 159 : loss => 0.44891578827945305,  accuracy: 0.8388
[2025-09-18 14:03:07,976][flp2p.graph_runner][INFO] - Train, Round 160 : loss => 0.07866166314872769,  accuracy: 0.9716363636363636, gradient_norm : 0.108450039990333
[2025-09-18 14:03:15,340][flp2p.graph_runner][INFO] - Test, Round 160 : loss => 0.45402594957159026,  accuracy: 0.8365
[2025-09-18 14:03:18,523][flp2p.graph_runner][INFO] - Train, Round 161 : loss => 0.16957063781692247,  accuracy: 0.93, gradient_norm : 0.14544126934335724
[2025-09-18 14:03:25,875][flp2p.graph_runner][INFO] - Test, Round 161 : loss => 0.45246639350227935,  accuracy: 0.838
[2025-09-18 14:03:29,019][flp2p.graph_runner][INFO] - Train, Round 162 : loss => 0.05235602452846578,  accuracy: 0.9818181818181818, gradient_norm : 0.07997468686637887
[2025-09-18 14:03:36,351][flp2p.graph_runner][INFO] - Test, Round 162 : loss => 0.449711468581082,  accuracy: 0.8393
[2025-09-18 14:03:39,500][flp2p.graph_runner][INFO] - Train, Round 163 : loss => 0.1282783491965698,  accuracy: 0.9475757575757575, gradient_norm : 0.13134079114578448
[2025-09-18 14:03:46,941][flp2p.graph_runner][INFO] - Test, Round 163 : loss => 0.4532601892589377,  accuracy: 0.8395
[2025-09-18 14:03:50,093][flp2p.graph_runner][INFO] - Train, Round 164 : loss => 0.0689531698267351,  accuracy: 0.9756363636363636, gradient_norm : 0.11112009408391578
[2025-09-18 14:03:57,485][flp2p.graph_runner][INFO] - Test, Round 164 : loss => 0.4545958700572935,  accuracy: 0.8401
[2025-09-18 14:04:00,693][flp2p.graph_runner][INFO] - Train, Round 165 : loss => 0.18450731117473146,  accuracy: 0.9208484848484847, gradient_norm : 0.13276300895841422
[2025-09-18 14:04:08,029][flp2p.graph_runner][INFO] - Test, Round 165 : loss => 0.4495230326826828,  accuracy: 0.8416
[2025-09-18 14:04:11,178][flp2p.graph_runner][INFO] - Train, Round 166 : loss => 0.10990995374500481,  accuracy: 0.9555151515151517, gradient_norm : 0.12075637909826117
[2025-09-18 14:04:18,575][flp2p.graph_runner][INFO] - Test, Round 166 : loss => 0.446401516451729,  accuracy: 0.8434
[2025-09-18 14:04:21,728][flp2p.graph_runner][INFO] - Train, Round 167 : loss => 0.09347876117326366,  accuracy: 0.9617575757575757, gradient_norm : 0.08903959339037154
[2025-09-18 14:04:29,166][flp2p.graph_runner][INFO] - Test, Round 167 : loss => 0.44855648494656564,  accuracy: 0.8427
[2025-09-18 14:04:32,322][flp2p.graph_runner][INFO] - Train, Round 168 : loss => 0.12347319116203252,  accuracy: 0.9449090909090909, gradient_norm : 0.10883411854056392
[2025-09-18 14:04:39,718][flp2p.graph_runner][INFO] - Test, Round 168 : loss => 0.45397428582802785,  accuracy: 0.8428
[2025-09-18 14:04:42,871][flp2p.graph_runner][INFO] - Train, Round 169 : loss => 0.08572386711060219,  accuracy: 0.9668484848484847, gradient_norm : 0.12226954183681743
[2025-09-18 14:04:50,219][flp2p.graph_runner][INFO] - Test, Round 169 : loss => 0.451988170589288,  accuracy: 0.842
[2025-09-18 14:04:53,421][flp2p.graph_runner][INFO] - Train, Round 170 : loss => 0.15078957227554018,  accuracy: 0.9391515151515151, gradient_norm : 0.12638989181515778
[2025-09-18 14:05:00,764][flp2p.graph_runner][INFO] - Test, Round 170 : loss => 0.4528964870290074,  accuracy: 0.8426
[2025-09-18 14:05:03,918][flp2p.graph_runner][INFO] - Train, Round 171 : loss => 0.12870614383500537,  accuracy: 0.9518181818181818, gradient_norm : 0.14313768276109487
[2025-09-18 14:05:11,288][flp2p.graph_runner][INFO] - Test, Round 171 : loss => 0.45173292969095147,  accuracy: 0.8439
[2025-09-18 14:05:14,446][flp2p.graph_runner][INFO] - Train, Round 172 : loss => 0.10569187683109842,  accuracy: 0.9586060606060607, gradient_norm : 0.1362965407472341
[2025-09-18 14:05:21,885][flp2p.graph_runner][INFO] - Test, Round 172 : loss => 0.45522526227229565,  accuracy: 0.8444
[2025-09-18 14:05:25,063][flp2p.graph_runner][INFO] - Train, Round 173 : loss => 0.06550273312834287,  accuracy: 0.9794545454545455, gradient_norm : 0.1379107105738079
[2025-09-18 14:05:32,474][flp2p.graph_runner][INFO] - Test, Round 173 : loss => 0.4624485596644574,  accuracy: 0.8433
[2025-09-18 14:05:35,619][flp2p.graph_runner][INFO] - Train, Round 174 : loss => 0.10828887718858891,  accuracy: 0.9505454545454546, gradient_norm : 0.09934333023772136
[2025-09-18 14:05:42,979][flp2p.graph_runner][INFO] - Test, Round 174 : loss => 0.4644398333262382,  accuracy: 0.8431
[2025-09-18 14:05:46,147][flp2p.graph_runner][INFO] - Train, Round 175 : loss => 0.08093299995340776,  accuracy: 0.9670909090909092, gradient_norm : 0.10472049675973799
[2025-09-18 14:05:53,516][flp2p.graph_runner][INFO] - Test, Round 175 : loss => 0.47358215343307075,  accuracy: 0.841
[2025-09-18 14:05:56,629][flp2p.graph_runner][INFO] - Train, Round 176 : loss => 0.10270226042953397,  accuracy: 0.9594545454545453, gradient_norm : 0.12079023967271797
[2025-09-18 14:06:04,019][flp2p.graph_runner][INFO] - Test, Round 176 : loss => 0.46870108701996815,  accuracy: 0.8418
[2025-09-18 14:06:06,917][flp2p.graph_runner][INFO] - Train, Round 177 : loss => 0.0550182163775636,  accuracy: 0.9784, gradient_norm : 0.07648542740129738
[2025-09-18 14:06:14,348][flp2p.graph_runner][INFO] - Test, Round 177 : loss => 0.4721207508049038,  accuracy: 0.8416
[2025-09-18 14:06:17,572][flp2p.graph_runner][INFO] - Train, Round 178 : loss => 0.05912813894991053,  accuracy: 0.977818181818182, gradient_norm : 0.07518205645641261
[2025-09-18 14:06:24,969][flp2p.graph_runner][INFO] - Test, Round 178 : loss => 0.47316851410000965,  accuracy: 0.842
[2025-09-18 14:06:28,196][flp2p.graph_runner][INFO] - Train, Round 179 : loss => 0.14218437003795323,  accuracy: 0.9401818181818182, gradient_norm : 0.13592050702579428
[2025-09-18 14:06:35,568][flp2p.graph_runner][INFO] - Test, Round 179 : loss => 0.4708216504685992,  accuracy: 0.8427
[2025-09-18 14:06:38,758][flp2p.graph_runner][INFO] - Train, Round 180 : loss => 0.03388897329927444,  accuracy: 0.987878787878788, gradient_norm : 0.060924984568751055
[2025-09-18 14:06:46,109][flp2p.graph_runner][INFO] - Test, Round 180 : loss => 0.4725337623819696,  accuracy: 0.8424
[2025-09-18 14:06:49,365][flp2p.graph_runner][INFO] - Train, Round 181 : loss => 0.11455471579310586,  accuracy: 0.9537575757575758, gradient_norm : 0.11565987545026161
[2025-09-18 14:06:56,753][flp2p.graph_runner][INFO] - Test, Round 181 : loss => 0.4753531216298497,  accuracy: 0.8431
[2025-09-18 14:06:59,967][flp2p.graph_runner][INFO] - Train, Round 182 : loss => 0.08768551537219463,  accuracy: 0.967939393939394, gradient_norm : 0.13034949647882008
[2025-09-18 14:07:07,360][flp2p.graph_runner][INFO] - Test, Round 182 : loss => 0.4813938980635585,  accuracy: 0.8414
[2025-09-18 14:07:10,571][flp2p.graph_runner][INFO] - Train, Round 183 : loss => 0.15793677509588533,  accuracy: 0.931090909090909, gradient_norm : 0.14039829936540887
[2025-09-18 14:07:17,945][flp2p.graph_runner][INFO] - Test, Round 183 : loss => 0.48176527801933,  accuracy: 0.8436
[2025-09-18 14:07:21,140][flp2p.graph_runner][INFO] - Train, Round 184 : loss => 0.11895846527572863,  accuracy: 0.9543636363636364, gradient_norm : 0.13355381882047365
[2025-09-18 14:07:28,562][flp2p.graph_runner][INFO] - Test, Round 184 : loss => 0.48104320176974497,  accuracy: 0.8432
[2025-09-18 14:07:31,776][flp2p.graph_runner][INFO] - Train, Round 185 : loss => 0.11463441572647433,  accuracy: 0.9533333333333335, gradient_norm : 0.12886198505289911
[2025-09-18 14:07:39,200][flp2p.graph_runner][INFO] - Test, Round 185 : loss => 0.48660242244636276,  accuracy: 0.844
[2025-09-18 14:07:42,389][flp2p.graph_runner][INFO] - Train, Round 186 : loss => 0.08254630971616321,  accuracy: 0.966848484848485, gradient_norm : 0.08434607631698916
[2025-09-18 14:07:49,805][flp2p.graph_runner][INFO] - Test, Round 186 : loss => 0.48668336204092805,  accuracy: 0.8446
[2025-09-18 14:07:52,992][flp2p.graph_runner][INFO] - Train, Round 187 : loss => 0.09992555664556586,  accuracy: 0.9592727272727273, gradient_norm : 0.09998919476192568
[2025-09-18 14:08:00,345][flp2p.graph_runner][INFO] - Test, Round 187 : loss => 0.48441708020899577,  accuracy: 0.8466
[2025-09-18 14:08:03,522][flp2p.graph_runner][INFO] - Train, Round 188 : loss => 0.1079378127840411,  accuracy: 0.9498181818181818, gradient_norm : 0.10959511714426866
[2025-09-18 14:08:10,911][flp2p.graph_runner][INFO] - Test, Round 188 : loss => 0.47939173131535634,  accuracy: 0.8489
[2025-09-18 14:08:14,058][flp2p.graph_runner][INFO] - Train, Round 189 : loss => 0.05487584563888506,  accuracy: 0.9813333333333334, gradient_norm : 0.09643404210216829
[2025-09-18 14:08:21,408][flp2p.graph_runner][INFO] - Test, Round 189 : loss => 0.4821536106688671,  accuracy: 0.849
[2025-09-18 14:08:24,561][flp2p.graph_runner][INFO] - Train, Round 190 : loss => 0.09164463427644116,  accuracy: 0.9661212121212122, gradient_norm : 0.1178993713742016
[2025-09-18 14:08:31,947][flp2p.graph_runner][INFO] - Test, Round 190 : loss => 0.4850781286868316,  accuracy: 0.8498
[2025-09-18 14:08:35,112][flp2p.graph_runner][INFO] - Train, Round 191 : loss => 0.11404862240647212,  accuracy: 0.9550303030303031, gradient_norm : 0.1388786382719996
[2025-09-18 14:08:42,500][flp2p.graph_runner][INFO] - Test, Round 191 : loss => 0.48292876798919304,  accuracy: 0.8517
[2025-09-18 14:08:45,654][flp2p.graph_runner][INFO] - Train, Round 192 : loss => 0.010083123576528637,  accuracy: 0.9984848484848485, gradient_norm : 0.040364729567220224
[2025-09-18 14:08:53,041][flp2p.graph_runner][INFO] - Test, Round 192 : loss => 0.48449347072429527,  accuracy: 0.8515
[2025-09-18 14:08:56,154][flp2p.graph_runner][INFO] - Train, Round 193 : loss => 0.08955225229485841,  accuracy: 0.963939393939394, gradient_norm : 0.11310425729313246
[2025-09-18 14:09:03,552][flp2p.graph_runner][INFO] - Test, Round 193 : loss => 0.485970859315527,  accuracy: 0.8511
[2025-09-18 14:09:06,728][flp2p.graph_runner][INFO] - Train, Round 194 : loss => 0.11218786124982105,  accuracy: 0.9563636363636364, gradient_norm : 0.10567426438031306
[2025-09-18 14:09:14,107][flp2p.graph_runner][INFO] - Test, Round 194 : loss => 0.4862185740608577,  accuracy: 0.8505
[2025-09-18 14:09:17,279][flp2p.graph_runner][INFO] - Train, Round 195 : loss => 0.06813803307272713,  accuracy: 0.9763030303030302, gradient_norm : 0.11264956341800404
[2025-09-18 14:09:24,643][flp2p.graph_runner][INFO] - Test, Round 195 : loss => 0.4875082508067522,  accuracy: 0.8501
[2025-09-18 14:09:27,805][flp2p.graph_runner][INFO] - Train, Round 196 : loss => 0.07954044954720535,  accuracy: 0.9635151515151514, gradient_norm : 0.07918681047994369
[2025-09-18 14:09:35,170][flp2p.graph_runner][INFO] - Test, Round 196 : loss => 0.4854640802701918,  accuracy: 0.8512
[2025-09-18 14:09:38,342][flp2p.graph_runner][INFO] - Train, Round 197 : loss => 0.13994963785537348,  accuracy: 0.9418787878787879, gradient_norm : 0.11514525537898312
[2025-09-18 14:09:45,617][flp2p.graph_runner][INFO] - Test, Round 197 : loss => 0.49130248299942075,  accuracy: 0.8514
[2025-09-18 14:09:48,709][flp2p.graph_runner][INFO] - Train, Round 198 : loss => 0.09822825920585251,  accuracy: 0.9630909090909091, gradient_norm : 0.11755692608302697
[2025-09-18 14:09:55,912][flp2p.graph_runner][INFO] - Test, Round 198 : loss => 0.49365101262518263,  accuracy: 0.8519
[2025-09-18 14:09:59,028][flp2p.graph_runner][INFO] - Train, Round 199 : loss => 0.07290836267368107,  accuracy: 0.9736363636363636, gradient_norm : 0.10569119699126142
[2025-09-18 14:10:06,198][flp2p.graph_runner][INFO] - Test, Round 199 : loss => 0.4972521219582233,  accuracy: 0.8511
[2025-09-18 14:10:06,198][__main__][INFO] - Train, Round 001: loss=1.6424, accuracy=0.4966, gradient_norm=0.9501, 
[2025-09-18 14:10:06,198][__main__][INFO] - Train, Round 002: loss=1.2778, accuracy=0.5359, gradient_norm=0.6512, 
[2025-09-18 14:10:06,199][__main__][INFO] - Train, Round 003: loss=1.2305, accuracy=0.5485, gradient_norm=0.6428, 
[2025-09-18 14:10:06,199][__main__][INFO] - Train, Round 004: loss=1.2777, accuracy=0.5363, gradient_norm=0.6439, 
[2025-09-18 14:10:06,199][__main__][INFO] - Train, Round 005: loss=1.4153, accuracy=0.5057, gradient_norm=0.7387, 
[2025-09-18 14:10:06,199][__main__][INFO] - Train, Round 006: loss=0.9858, accuracy=0.6727, gradient_norm=0.5885, 
[2025-09-18 14:10:06,199][__main__][INFO] - Train, Round 007: loss=1.0605, accuracy=0.5952, gradient_norm=0.5342, 
[2025-09-18 14:10:06,199][__main__][INFO] - Train, Round 008: loss=1.0603, accuracy=0.5703, gradient_norm=0.5199, 
[2025-09-18 14:10:06,199][__main__][INFO] - Train, Round 009: loss=1.1107, accuracy=0.5935, gradient_norm=0.5628, 
[2025-09-18 14:10:06,199][__main__][INFO] - Train, Round 010: loss=0.8365, accuracy=0.5779, gradient_norm=0.3206, 
[2025-09-18 14:10:06,199][__main__][INFO] - Train, Round 011: loss=1.2771, accuracy=0.5355, gradient_norm=0.6485, 
[2025-09-18 14:10:06,199][__main__][INFO] - Train, Round 012: loss=0.5790, accuracy=0.6958, gradient_norm=0.2347, 
[2025-09-18 14:10:06,199][__main__][INFO] - Train, Round 013: loss=0.6278, accuracy=0.6675, gradient_norm=0.2401, 
[2025-09-18 14:10:06,199][__main__][INFO] - Train, Round 014: loss=0.6800, accuracy=0.6883, gradient_norm=0.3154, 
[2025-09-18 14:10:06,199][__main__][INFO] - Train, Round 015: loss=0.9834, accuracy=0.5670, gradient_norm=0.4493, 
[2025-09-18 14:10:06,199][__main__][INFO] - Train, Round 016: loss=0.6500, accuracy=0.6930, gradient_norm=0.3205, 
[2025-09-18 14:10:06,199][__main__][INFO] - Train, Round 017: loss=1.0460, accuracy=0.5943, gradient_norm=0.5799, 
[2025-09-18 14:10:06,199][__main__][INFO] - Train, Round 018: loss=0.7658, accuracy=0.6793, gradient_norm=0.3541, 
[2025-09-18 14:10:06,199][__main__][INFO] - Train, Round 019: loss=0.6607, accuracy=0.7158, gradient_norm=0.3541, 
[2025-09-18 14:10:06,199][__main__][INFO] - Train, Round 020: loss=0.7354, accuracy=0.6973, gradient_norm=0.4354, 
[2025-09-18 14:10:06,199][__main__][INFO] - Train, Round 021: loss=0.7607, accuracy=0.6505, gradient_norm=0.3721, 
[2025-09-18 14:10:06,199][__main__][INFO] - Train, Round 022: loss=0.6881, accuracy=0.6243, gradient_norm=0.2851, 
[2025-09-18 14:10:06,199][__main__][INFO] - Train, Round 023: loss=0.6614, accuracy=0.7046, gradient_norm=0.3664, 
[2025-09-18 14:10:06,199][__main__][INFO] - Train, Round 024: loss=0.7724, accuracy=0.6122, gradient_norm=0.3337, 
[2025-09-18 14:10:06,199][__main__][INFO] - Train, Round 025: loss=0.5739, accuracy=0.6803, gradient_norm=0.2387, 
[2025-09-18 14:10:06,199][__main__][INFO] - Train, Round 026: loss=0.6303, accuracy=0.6720, gradient_norm=0.2520, 
[2025-09-18 14:10:06,199][__main__][INFO] - Train, Round 027: loss=0.5557, accuracy=0.7367, gradient_norm=0.2749, 
[2025-09-18 14:10:06,199][__main__][INFO] - Train, Round 028: loss=0.5126, accuracy=0.7331, gradient_norm=0.2102, 
[2025-09-18 14:10:06,199][__main__][INFO] - Train, Round 029: loss=0.7557, accuracy=0.6694, gradient_norm=0.3695, 
[2025-09-18 14:10:06,199][__main__][INFO] - Train, Round 030: loss=0.4045, accuracy=0.8051, gradient_norm=0.1807, 
[2025-09-18 14:10:06,199][__main__][INFO] - Train, Round 031: loss=0.4947, accuracy=0.7317, gradient_norm=0.1867, 
[2025-09-18 14:10:06,199][__main__][INFO] - Train, Round 032: loss=0.4739, accuracy=0.7281, gradient_norm=0.1504, 
[2025-09-18 14:10:06,199][__main__][INFO] - Train, Round 033: loss=0.6237, accuracy=0.6526, gradient_norm=0.2426, 
[2025-09-18 14:10:06,199][__main__][INFO] - Train, Round 034: loss=0.5022, accuracy=0.6870, gradient_norm=0.1411, 
[2025-09-18 14:10:06,199][__main__][INFO] - Train, Round 035: loss=0.5189, accuracy=0.6872, gradient_norm=0.1526, 
[2025-09-18 14:10:06,199][__main__][INFO] - Train, Round 036: loss=0.4924, accuracy=0.7999, gradient_norm=0.2826, 
[2025-09-18 14:10:06,199][__main__][INFO] - Train, Round 037: loss=0.4609, accuracy=0.7569, gradient_norm=0.1811, 
[2025-09-18 14:10:06,199][__main__][INFO] - Train, Round 038: loss=0.4117, accuracy=0.7680, gradient_norm=0.1224, 
[2025-09-18 14:10:06,199][__main__][INFO] - Train, Round 039: loss=0.5185, accuracy=0.7603, gradient_norm=0.2094, 
[2025-09-18 14:10:06,199][__main__][INFO] - Train, Round 040: loss=0.5913, accuracy=0.7248, gradient_norm=0.2862, 
[2025-09-18 14:10:06,199][__main__][INFO] - Train, Round 041: loss=0.4119, accuracy=0.8042, gradient_norm=0.2240, 
[2025-09-18 14:10:06,199][__main__][INFO] - Train, Round 042: loss=0.3789, accuracy=0.7948, gradient_norm=0.1771, 
[2025-09-18 14:10:06,199][__main__][INFO] - Train, Round 043: loss=0.4499, accuracy=0.7528, gradient_norm=0.1605, 
[2025-09-18 14:10:06,199][__main__][INFO] - Train, Round 044: loss=0.3495, accuracy=0.8196, gradient_norm=0.1778, 
[2025-09-18 14:10:06,199][__main__][INFO] - Train, Round 045: loss=0.3465, accuracy=0.8032, gradient_norm=0.1510, 
[2025-09-18 14:10:06,199][__main__][INFO] - Train, Round 046: loss=0.3929, accuracy=0.7841, gradient_norm=0.1694, 
[2025-09-18 14:10:06,200][__main__][INFO] - Train, Round 047: loss=0.4416, accuracy=0.7446, gradient_norm=0.1593, 
[2025-09-18 14:10:06,200][__main__][INFO] - Train, Round 048: loss=0.3982, accuracy=0.7641, gradient_norm=0.1396, 
[2025-09-18 14:10:06,200][__main__][INFO] - Train, Round 049: loss=0.2894, accuracy=0.8493, gradient_norm=0.1233, 
[2025-09-18 14:10:06,200][__main__][INFO] - Train, Round 050: loss=0.4096, accuracy=0.7659, gradient_norm=0.1355, 
[2025-09-18 14:10:06,200][__main__][INFO] - Train, Round 051: loss=0.3110, accuracy=0.8255, gradient_norm=0.1356, 
[2025-09-18 14:10:06,200][__main__][INFO] - Train, Round 052: loss=0.4658, accuracy=0.7455, gradient_norm=0.1853, 
[2025-09-18 14:10:06,200][__main__][INFO] - Train, Round 053: loss=0.3263, accuracy=0.8325, gradient_norm=0.1326, 
[2025-09-18 14:10:06,200][__main__][INFO] - Train, Round 054: loss=0.3656, accuracy=0.8180, gradient_norm=0.1507, 
[2025-09-18 14:10:06,200][__main__][INFO] - Train, Round 055: loss=0.3711, accuracy=0.8019, gradient_norm=0.1307, 
[2025-09-18 14:10:06,200][__main__][INFO] - Train, Round 056: loss=0.3771, accuracy=0.7885, gradient_norm=0.1449, 
[2025-09-18 14:10:06,200][__main__][INFO] - Train, Round 057: loss=0.3932, accuracy=0.7919, gradient_norm=0.1521, 
[2025-09-18 14:10:06,200][__main__][INFO] - Train, Round 058: loss=0.4160, accuracy=0.7464, gradient_norm=0.1264, 
[2025-09-18 14:10:06,200][__main__][INFO] - Train, Round 059: loss=0.3366, accuracy=0.8141, gradient_norm=0.1159, 
[2025-09-18 14:10:06,200][__main__][INFO] - Train, Round 060: loss=0.3220, accuracy=0.8208, gradient_norm=0.1033, 
[2025-09-18 14:10:06,200][__main__][INFO] - Train, Round 061: loss=0.3141, accuracy=0.8296, gradient_norm=0.1183, 
[2025-09-18 14:10:06,200][__main__][INFO] - Train, Round 062: loss=0.4103, accuracy=0.7852, gradient_norm=0.1267, 
[2025-09-18 14:10:06,200][__main__][INFO] - Train, Round 063: loss=0.3681, accuracy=0.8105, gradient_norm=0.1724, 
[2025-09-18 14:10:06,200][__main__][INFO] - Train, Round 064: loss=0.2717, accuracy=0.8633, gradient_norm=0.1447, 
[2025-09-18 14:10:06,200][__main__][INFO] - Train, Round 065: loss=0.3450, accuracy=0.8284, gradient_norm=0.1442, 
[2025-09-18 14:10:06,200][__main__][INFO] - Train, Round 066: loss=0.3554, accuracy=0.8170, gradient_norm=0.1390, 
[2025-09-18 14:10:06,200][__main__][INFO] - Train, Round 067: loss=0.2331, accuracy=0.8992, gradient_norm=0.1346, 
[2025-09-18 14:10:06,200][__main__][INFO] - Train, Round 068: loss=0.3850, accuracy=0.8021, gradient_norm=0.1652, 
[2025-09-18 14:10:06,200][__main__][INFO] - Train, Round 069: loss=0.3567, accuracy=0.8059, gradient_norm=0.1253, 
[2025-09-18 14:10:06,200][__main__][INFO] - Train, Round 070: loss=0.3124, accuracy=0.8417, gradient_norm=0.1216, 
[2025-09-18 14:10:06,200][__main__][INFO] - Train, Round 071: loss=0.3351, accuracy=0.8259, gradient_norm=0.1583, 
[2025-09-18 14:10:06,200][__main__][INFO] - Train, Round 072: loss=0.3254, accuracy=0.8314, gradient_norm=0.1203, 
[2025-09-18 14:10:06,200][__main__][INFO] - Train, Round 073: loss=0.3713, accuracy=0.8036, gradient_norm=0.1386, 
[2025-09-18 14:10:06,200][__main__][INFO] - Train, Round 074: loss=0.3348, accuracy=0.8338, gradient_norm=0.1390, 
[2025-09-18 14:10:06,200][__main__][INFO] - Train, Round 075: loss=0.3969, accuracy=0.7827, gradient_norm=0.1217, 
[2025-09-18 14:10:06,200][__main__][INFO] - Train, Round 076: loss=0.3154, accuracy=0.8162, gradient_norm=0.0956, 
[2025-09-18 14:10:06,200][__main__][INFO] - Train, Round 077: loss=0.3831, accuracy=0.8083, gradient_norm=0.1414, 
[2025-09-18 14:10:06,200][__main__][INFO] - Train, Round 078: loss=0.2610, accuracy=0.8856, gradient_norm=0.1645, 
[2025-09-18 14:10:06,200][__main__][INFO] - Train, Round 079: loss=0.3094, accuracy=0.8564, gradient_norm=0.1452, 
[2025-09-18 14:10:06,200][__main__][INFO] - Train, Round 080: loss=0.2851, accuracy=0.8617, gradient_norm=0.1287, 
[2025-09-18 14:10:06,200][__main__][INFO] - Train, Round 081: loss=0.1575, accuracy=0.9333, gradient_norm=0.1257, 
[2025-09-18 14:10:06,200][__main__][INFO] - Train, Round 082: loss=0.2622, accuracy=0.8640, gradient_norm=0.1188, 
[2025-09-18 14:10:06,200][__main__][INFO] - Train, Round 083: loss=0.3075, accuracy=0.8387, gradient_norm=0.1375, 
[2025-09-18 14:10:06,200][__main__][INFO] - Train, Round 084: loss=0.3634, accuracy=0.8102, gradient_norm=0.1324, 
[2025-09-18 14:10:06,200][__main__][INFO] - Train, Round 085: loss=0.2691, accuracy=0.8677, gradient_norm=0.1235, 
[2025-09-18 14:10:06,200][__main__][INFO] - Train, Round 086: loss=0.2457, accuracy=0.8836, gradient_norm=0.1156, 
[2025-09-18 14:10:06,200][__main__][INFO] - Train, Round 087: loss=0.2298, accuracy=0.8829, gradient_norm=0.1200, 
[2025-09-18 14:10:06,200][__main__][INFO] - Train, Round 088: loss=0.3886, accuracy=0.7948, gradient_norm=0.1353, 
[2025-09-18 14:10:06,200][__main__][INFO] - Train, Round 089: loss=0.1570, accuracy=0.9315, gradient_norm=0.1123, 
[2025-09-18 14:10:06,200][__main__][INFO] - Train, Round 090: loss=0.3124, accuracy=0.8478, gradient_norm=0.1664, 
[2025-09-18 14:10:06,200][__main__][INFO] - Train, Round 091: loss=0.2336, accuracy=0.8924, gradient_norm=0.1133, 
[2025-09-18 14:10:06,200][__main__][INFO] - Train, Round 092: loss=0.4062, accuracy=0.7849, gradient_norm=0.1381, 
[2025-09-18 14:10:06,200][__main__][INFO] - Train, Round 093: loss=0.3564, accuracy=0.8212, gradient_norm=0.1585, 
[2025-09-18 14:10:06,200][__main__][INFO] - Train, Round 094: loss=0.2510, accuracy=0.8648, gradient_norm=0.1308, 
[2025-09-18 14:10:06,201][__main__][INFO] - Train, Round 095: loss=0.2357, accuracy=0.8881, gradient_norm=0.1291, 
[2025-09-18 14:10:06,201][__main__][INFO] - Train, Round 096: loss=0.2649, accuracy=0.8589, gradient_norm=0.1022, 
[2025-09-18 14:10:06,201][__main__][INFO] - Train, Round 097: loss=0.1757, accuracy=0.9260, gradient_norm=0.1450, 
[2025-09-18 14:10:06,201][__main__][INFO] - Train, Round 098: loss=0.3155, accuracy=0.8533, gradient_norm=0.1588, 
[2025-09-18 14:10:06,201][__main__][INFO] - Train, Round 099: loss=0.2589, accuracy=0.8729, gradient_norm=0.1392, 
[2025-09-18 14:10:06,201][__main__][INFO] - Train, Round 100: loss=0.2482, accuracy=0.8641, gradient_norm=0.0939, 
[2025-09-18 14:10:06,201][__main__][INFO] - Train, Round 101: loss=0.2337, accuracy=0.8847, gradient_norm=0.1153, 
[2025-09-18 14:10:06,201][__main__][INFO] - Train, Round 102: loss=0.3166, accuracy=0.8460, gradient_norm=0.1293, 
[2025-09-18 14:10:06,201][__main__][INFO] - Train, Round 103: loss=0.3003, accuracy=0.8573, gradient_norm=0.1402, 
[2025-09-18 14:10:06,201][__main__][INFO] - Train, Round 104: loss=0.2438, accuracy=0.8863, gradient_norm=0.1715, 
[2025-09-18 14:10:06,201][__main__][INFO] - Train, Round 105: loss=0.2604, accuracy=0.8738, gradient_norm=0.1395, 
[2025-09-18 14:10:06,201][__main__][INFO] - Train, Round 106: loss=0.2068, accuracy=0.9042, gradient_norm=0.1343, 
[2025-09-18 14:10:06,201][__main__][INFO] - Train, Round 107: loss=0.2660, accuracy=0.8772, gradient_norm=0.1424, 
[2025-09-18 14:10:06,201][__main__][INFO] - Train, Round 108: loss=0.2816, accuracy=0.8732, gradient_norm=0.1574, 
[2025-09-18 14:10:06,201][__main__][INFO] - Train, Round 109: loss=0.3474, accuracy=0.8278, gradient_norm=0.1684, 
[2025-09-18 14:10:06,201][__main__][INFO] - Train, Round 110: loss=0.3636, accuracy=0.8027, gradient_norm=0.1398, 
[2025-09-18 14:10:06,201][__main__][INFO] - Train, Round 111: loss=0.2676, accuracy=0.8732, gradient_norm=0.1449, 
[2025-09-18 14:10:06,201][__main__][INFO] - Train, Round 112: loss=0.1352, accuracy=0.9458, gradient_norm=0.0940, 
[2025-09-18 14:10:06,201][__main__][INFO] - Train, Round 113: loss=0.2331, accuracy=0.8900, gradient_norm=0.1405, 
[2025-09-18 14:10:06,201][__main__][INFO] - Train, Round 114: loss=0.2932, accuracy=0.8567, gradient_norm=0.1417, 
[2025-09-18 14:10:06,201][__main__][INFO] - Train, Round 115: loss=0.2337, accuracy=0.9041, gradient_norm=0.1539, 
[2025-09-18 14:10:06,201][__main__][INFO] - Train, Round 116: loss=0.2753, accuracy=0.8664, gradient_norm=0.1434, 
[2025-09-18 14:10:06,201][__main__][INFO] - Train, Round 117: loss=0.2095, accuracy=0.9114, gradient_norm=0.1568, 
[2025-09-18 14:10:06,201][__main__][INFO] - Train, Round 118: loss=0.2598, accuracy=0.8724, gradient_norm=0.1258, 
[2025-09-18 14:10:06,201][__main__][INFO] - Train, Round 119: loss=0.2143, accuracy=0.8961, gradient_norm=0.1140, 
[2025-09-18 14:10:06,201][__main__][INFO] - Train, Round 120: loss=0.2193, accuracy=0.8990, gradient_norm=0.1283, 
[2025-09-18 14:10:06,201][__main__][INFO] - Train, Round 121: loss=0.2339, accuracy=0.8927, gradient_norm=0.1560, 
[2025-09-18 14:10:06,201][__main__][INFO] - Train, Round 122: loss=0.1529, accuracy=0.9408, gradient_norm=0.1683, 
[2025-09-18 14:10:06,201][__main__][INFO] - Train, Round 123: loss=0.2301, accuracy=0.8948, gradient_norm=0.1369, 
[2025-09-18 14:10:06,201][__main__][INFO] - Train, Round 124: loss=0.1670, accuracy=0.9285, gradient_norm=0.1372, 
[2025-09-18 14:10:06,201][__main__][INFO] - Train, Round 125: loss=0.1257, accuracy=0.9484, gradient_norm=0.1367, 
[2025-09-18 14:10:06,201][__main__][INFO] - Train, Round 126: loss=0.2594, accuracy=0.8775, gradient_norm=0.1439, 
[2025-09-18 14:10:06,201][__main__][INFO] - Train, Round 127: loss=0.1907, accuracy=0.9068, gradient_norm=0.1417, 
[2025-09-18 14:10:06,201][__main__][INFO] - Train, Round 128: loss=0.2205, accuracy=0.9003, gradient_norm=0.1395, 
[2025-09-18 14:10:06,201][__main__][INFO] - Train, Round 129: loss=0.1382, accuracy=0.9439, gradient_norm=0.1310, 
[2025-09-18 14:10:06,201][__main__][INFO] - Train, Round 130: loss=0.2157, accuracy=0.8992, gradient_norm=0.1392, 
[2025-09-18 14:10:06,201][__main__][INFO] - Train, Round 131: loss=0.1329, accuracy=0.9459, gradient_norm=0.1372, 
[2025-09-18 14:10:06,201][__main__][INFO] - Train, Round 132: loss=0.2818, accuracy=0.8682, gradient_norm=0.1454, 
[2025-09-18 14:10:06,201][__main__][INFO] - Train, Round 133: loss=0.2368, accuracy=0.8876, gradient_norm=0.1280, 
[2025-09-18 14:10:06,201][__main__][INFO] - Train, Round 134: loss=0.1917, accuracy=0.9161, gradient_norm=0.1370, 
[2025-09-18 14:10:06,201][__main__][INFO] - Train, Round 135: loss=0.1897, accuracy=0.9186, gradient_norm=0.1461, 
[2025-09-18 14:10:06,201][__main__][INFO] - Train, Round 136: loss=0.1558, accuracy=0.9398, gradient_norm=0.1724, 
[2025-09-18 14:10:06,201][__main__][INFO] - Train, Round 137: loss=0.1846, accuracy=0.9148, gradient_norm=0.1043, 
[2025-09-18 14:10:06,201][__main__][INFO] - Train, Round 138: loss=0.1378, accuracy=0.9357, gradient_norm=0.1273, 
[2025-09-18 14:10:06,201][__main__][INFO] - Train, Round 139: loss=0.1719, accuracy=0.9290, gradient_norm=0.1380, 
[2025-09-18 14:10:06,201][__main__][INFO] - Train, Round 140: loss=0.2183, accuracy=0.8929, gradient_norm=0.1301, 
[2025-09-18 14:10:06,201][__main__][INFO] - Train, Round 141: loss=0.0915, accuracy=0.9653, gradient_norm=0.1361, 
[2025-09-18 14:10:06,201][__main__][INFO] - Train, Round 142: loss=0.1638, accuracy=0.9292, gradient_norm=0.1320, 
[2025-09-18 14:10:06,201][__main__][INFO] - Train, Round 143: loss=0.2240, accuracy=0.8955, gradient_norm=0.1423, 
[2025-09-18 14:10:06,201][__main__][INFO] - Train, Round 144: loss=0.2369, accuracy=0.8955, gradient_norm=0.1455, 
[2025-09-18 14:10:06,201][__main__][INFO] - Train, Round 145: loss=0.1752, accuracy=0.9228, gradient_norm=0.1290, 
[2025-09-18 14:10:06,201][__main__][INFO] - Train, Round 146: loss=0.1403, accuracy=0.9410, gradient_norm=0.1291, 
[2025-09-18 14:10:06,201][__main__][INFO] - Train, Round 147: loss=0.1221, accuracy=0.9508, gradient_norm=0.1307, 
[2025-09-18 14:10:06,201][__main__][INFO] - Train, Round 148: loss=0.1705, accuracy=0.9299, gradient_norm=0.1430, 
[2025-09-18 14:10:06,201][__main__][INFO] - Train, Round 149: loss=0.2568, accuracy=0.8870, gradient_norm=0.1690, 
[2025-09-18 14:10:06,201][__main__][INFO] - Train, Round 150: loss=0.0641, accuracy=0.9774, gradient_norm=0.0823, 
[2025-09-18 14:10:06,201][__main__][INFO] - Train, Round 151: loss=0.2367, accuracy=0.8968, gradient_norm=0.1723, 
[2025-09-18 14:10:06,202][__main__][INFO] - Train, Round 152: loss=0.2398, accuracy=0.8941, gradient_norm=0.1711, 
[2025-09-18 14:10:06,202][__main__][INFO] - Train, Round 153: loss=0.1160, accuracy=0.9507, gradient_norm=0.0909, 
[2025-09-18 14:10:06,202][__main__][INFO] - Train, Round 154: loss=0.1390, accuracy=0.9371, gradient_norm=0.1102, 
[2025-09-18 14:10:06,202][__main__][INFO] - Train, Round 155: loss=0.0983, accuracy=0.9614, gradient_norm=0.1196, 
[2025-09-18 14:10:06,202][__main__][INFO] - Train, Round 156: loss=0.0763, accuracy=0.9712, gradient_norm=0.1084, 
[2025-09-18 14:10:06,202][__main__][INFO] - Train, Round 157: loss=0.1352, accuracy=0.9479, gradient_norm=0.1466, 
[2025-09-18 14:10:06,202][__main__][INFO] - Train, Round 158: loss=0.0999, accuracy=0.9567, gradient_norm=0.1188, 
[2025-09-18 14:10:06,202][__main__][INFO] - Train, Round 159: loss=0.0810, accuracy=0.9667, gradient_norm=0.0819, 
[2025-09-18 14:10:06,202][__main__][INFO] - Train, Round 160: loss=0.1482, accuracy=0.9371, gradient_norm=0.1240, 
[2025-09-18 14:10:06,202][__main__][INFO] - Train, Round 161: loss=0.0787, accuracy=0.9716, gradient_norm=0.1085, 
[2025-09-18 14:10:06,202][__main__][INFO] - Train, Round 162: loss=0.1696, accuracy=0.9300, gradient_norm=0.1454, 
[2025-09-18 14:10:06,202][__main__][INFO] - Train, Round 163: loss=0.0524, accuracy=0.9818, gradient_norm=0.0800, 
[2025-09-18 14:10:06,202][__main__][INFO] - Train, Round 164: loss=0.1283, accuracy=0.9476, gradient_norm=0.1313, 
[2025-09-18 14:10:06,202][__main__][INFO] - Train, Round 165: loss=0.0690, accuracy=0.9756, gradient_norm=0.1111, 
[2025-09-18 14:10:06,202][__main__][INFO] - Train, Round 166: loss=0.1845, accuracy=0.9208, gradient_norm=0.1328, 
[2025-09-18 14:10:06,202][__main__][INFO] - Train, Round 167: loss=0.1099, accuracy=0.9555, gradient_norm=0.1208, 
[2025-09-18 14:10:06,202][__main__][INFO] - Train, Round 168: loss=0.0935, accuracy=0.9618, gradient_norm=0.0890, 
[2025-09-18 14:10:06,202][__main__][INFO] - Train, Round 169: loss=0.1235, accuracy=0.9449, gradient_norm=0.1088, 
[2025-09-18 14:10:06,202][__main__][INFO] - Train, Round 170: loss=0.0857, accuracy=0.9668, gradient_norm=0.1223, 
[2025-09-18 14:10:06,202][__main__][INFO] - Train, Round 171: loss=0.1508, accuracy=0.9392, gradient_norm=0.1264, 
[2025-09-18 14:10:06,202][__main__][INFO] - Train, Round 172: loss=0.1287, accuracy=0.9518, gradient_norm=0.1431, 
[2025-09-18 14:10:06,202][__main__][INFO] - Train, Round 173: loss=0.1057, accuracy=0.9586, gradient_norm=0.1363, 
[2025-09-18 14:10:06,202][__main__][INFO] - Train, Round 174: loss=0.0655, accuracy=0.9795, gradient_norm=0.1379, 
[2025-09-18 14:10:06,202][__main__][INFO] - Train, Round 175: loss=0.1083, accuracy=0.9505, gradient_norm=0.0993, 
[2025-09-18 14:10:06,202][__main__][INFO] - Train, Round 176: loss=0.0809, accuracy=0.9671, gradient_norm=0.1047, 
[2025-09-18 14:10:06,202][__main__][INFO] - Train, Round 177: loss=0.1027, accuracy=0.9595, gradient_norm=0.1208, 
[2025-09-18 14:10:06,202][__main__][INFO] - Train, Round 178: loss=0.0550, accuracy=0.9784, gradient_norm=0.0765, 
[2025-09-18 14:10:06,202][__main__][INFO] - Train, Round 179: loss=0.0591, accuracy=0.9778, gradient_norm=0.0752, 
[2025-09-18 14:10:06,202][__main__][INFO] - Train, Round 180: loss=0.1422, accuracy=0.9402, gradient_norm=0.1359, 
[2025-09-18 14:10:06,202][__main__][INFO] - Train, Round 181: loss=0.0339, accuracy=0.9879, gradient_norm=0.0609, 
[2025-09-18 14:10:06,202][__main__][INFO] - Train, Round 182: loss=0.1146, accuracy=0.9538, gradient_norm=0.1157, 
[2025-09-18 14:10:06,202][__main__][INFO] - Train, Round 183: loss=0.0877, accuracy=0.9679, gradient_norm=0.1303, 
[2025-09-18 14:10:06,202][__main__][INFO] - Train, Round 184: loss=0.1579, accuracy=0.9311, gradient_norm=0.1404, 
[2025-09-18 14:10:06,202][__main__][INFO] - Train, Round 185: loss=0.1190, accuracy=0.9544, gradient_norm=0.1336, 
[2025-09-18 14:10:06,202][__main__][INFO] - Train, Round 186: loss=0.1146, accuracy=0.9533, gradient_norm=0.1289, 
[2025-09-18 14:10:06,202][__main__][INFO] - Train, Round 187: loss=0.0825, accuracy=0.9668, gradient_norm=0.0843, 
[2025-09-18 14:10:06,202][__main__][INFO] - Train, Round 188: loss=0.0999, accuracy=0.9593, gradient_norm=0.1000, 
[2025-09-18 14:10:06,202][__main__][INFO] - Train, Round 189: loss=0.1079, accuracy=0.9498, gradient_norm=0.1096, 
[2025-09-18 14:10:06,202][__main__][INFO] - Train, Round 190: loss=0.0549, accuracy=0.9813, gradient_norm=0.0964, 
[2025-09-18 14:10:06,202][__main__][INFO] - Train, Round 191: loss=0.0916, accuracy=0.9661, gradient_norm=0.1179, 
[2025-09-18 14:10:06,202][__main__][INFO] - Train, Round 192: loss=0.1140, accuracy=0.9550, gradient_norm=0.1389, 
[2025-09-18 14:10:06,202][__main__][INFO] - Train, Round 193: loss=0.0101, accuracy=0.9985, gradient_norm=0.0404, 
[2025-09-18 14:10:06,202][__main__][INFO] - Train, Round 194: loss=0.0896, accuracy=0.9639, gradient_norm=0.1131, 
[2025-09-18 14:10:06,202][__main__][INFO] - Train, Round 195: loss=0.1122, accuracy=0.9564, gradient_norm=0.1057, 
[2025-09-18 14:10:06,202][__main__][INFO] - Train, Round 196: loss=0.0681, accuracy=0.9763, gradient_norm=0.1126, 
[2025-09-18 14:10:06,202][__main__][INFO] - Train, Round 197: loss=0.0795, accuracy=0.9635, gradient_norm=0.0792, 
[2025-09-18 14:10:06,202][__main__][INFO] - Train, Round 198: loss=0.1399, accuracy=0.9419, gradient_norm=0.1151, 
[2025-09-18 14:10:06,202][__main__][INFO] - Train, Round 199: loss=0.0982, accuracy=0.9631, gradient_norm=0.1176, 
[2025-09-18 14:10:06,202][__main__][INFO] - Train, Round 200: loss=0.0729, accuracy=0.9736, gradient_norm=0.1057, 
[2025-09-18 14:10:06,202][__main__][INFO] - Test, Round 001: loss=2.1636, accuracy=0.1614, 
[2025-09-18 14:10:06,202][__main__][INFO] - Test, Round 002: loss=2.0334, accuracy=0.1825, 
[2025-09-18 14:10:06,202][__main__][INFO] - Test, Round 003: loss=1.9121, accuracy=0.2199, 
[2025-09-18 14:10:06,202][__main__][INFO] - Test, Round 004: loss=1.7965, accuracy=0.2558, 
[2025-09-18 14:10:06,202][__main__][INFO] - Test, Round 005: loss=1.6767, accuracy=0.2918, 
[2025-09-18 14:10:06,202][__main__][INFO] - Test, Round 006: loss=1.5797, accuracy=0.3228, 
[2025-09-18 14:10:06,202][__main__][INFO] - Test, Round 007: loss=1.4997, accuracy=0.3461, 
[2025-09-18 14:10:06,202][__main__][INFO] - Test, Round 008: loss=1.4223, accuracy=0.3683, 
[2025-09-18 14:10:06,202][__main__][INFO] - Test, Round 009: loss=1.3298, accuracy=0.3980, 
[2025-09-18 14:10:06,202][__main__][INFO] - Test, Round 010: loss=1.2928, accuracy=0.4067, 
[2025-09-18 14:10:06,202][__main__][INFO] - Test, Round 011: loss=1.1804, accuracy=0.4426, 
[2025-09-18 14:10:06,202][__main__][INFO] - Test, Round 012: loss=1.1736, accuracy=0.4455, 
[2025-09-18 14:10:06,202][__main__][INFO] - Test, Round 013: loss=1.1535, accuracy=0.4539, 
[2025-09-18 14:10:06,202][__main__][INFO] - Test, Round 014: loss=1.1186, accuracy=0.4715, 
[2025-09-18 14:10:06,202][__main__][INFO] - Test, Round 015: loss=1.0667, accuracy=0.4909, 
[2025-09-18 14:10:06,202][__main__][INFO] - Test, Round 016: loss=1.0823, accuracy=0.5036, 
[2025-09-18 14:10:06,202][__main__][INFO] - Test, Round 017: loss=0.9949, accuracy=0.5286, 
[2025-09-18 14:10:06,202][__main__][INFO] - Test, Round 018: loss=0.9557, accuracy=0.5362, 
[2025-09-18 14:10:06,202][__main__][INFO] - Test, Round 019: loss=0.9164, accuracy=0.5484, 
[2025-09-18 14:10:06,202][__main__][INFO] - Test, Round 020: loss=0.9031, accuracy=0.5529, 
[2025-09-18 14:10:06,202][__main__][INFO] - Test, Round 021: loss=0.8873, accuracy=0.5577, 
[2025-09-18 14:10:06,202][__main__][INFO] - Test, Round 022: loss=0.8772, accuracy=0.5596, 
[2025-09-18 14:10:06,202][__main__][INFO] - Test, Round 023: loss=0.8453, accuracy=0.5661, 
[2025-09-18 14:10:06,203][__main__][INFO] - Test, Round 024: loss=0.8148, accuracy=0.5794, 
[2025-09-18 14:10:06,203][__main__][INFO] - Test, Round 025: loss=0.7961, accuracy=0.5882, 
[2025-09-18 14:10:06,203][__main__][INFO] - Test, Round 026: loss=0.7702, accuracy=0.5949, 
[2025-09-18 14:10:06,203][__main__][INFO] - Test, Round 027: loss=0.7550, accuracy=0.6006, 
[2025-09-18 14:10:06,203][__main__][INFO] - Test, Round 028: loss=0.7322, accuracy=0.6072, 
[2025-09-18 14:10:06,203][__main__][INFO] - Test, Round 029: loss=0.6924, accuracy=0.6209, 
[2025-09-18 14:10:06,203][__main__][INFO] - Test, Round 030: loss=0.6859, accuracy=0.6236, 
[2025-09-18 14:10:06,203][__main__][INFO] - Test, Round 031: loss=0.6769, accuracy=0.6273, 
[2025-09-18 14:10:06,203][__main__][INFO] - Test, Round 032: loss=0.6663, accuracy=0.6340, 
[2025-09-18 14:10:06,203][__main__][INFO] - Test, Round 033: loss=0.6446, accuracy=0.6457, 
[2025-09-18 14:10:06,203][__main__][INFO] - Test, Round 034: loss=0.6404, accuracy=0.6483, 
[2025-09-18 14:10:06,203][__main__][INFO] - Test, Round 035: loss=0.6383, accuracy=0.6542, 
[2025-09-18 14:10:06,203][__main__][INFO] - Test, Round 036: loss=0.6059, accuracy=0.6655, 
[2025-09-18 14:10:06,203][__main__][INFO] - Test, Round 037: loss=0.6009, accuracy=0.6739, 
[2025-09-18 14:10:06,203][__main__][INFO] - Test, Round 038: loss=0.5984, accuracy=0.6766, 
[2025-09-18 14:10:06,203][__main__][INFO] - Test, Round 039: loss=0.5812, accuracy=0.6781, 
[2025-09-18 14:10:06,203][__main__][INFO] - Test, Round 040: loss=0.5619, accuracy=0.6816, 
[2025-09-18 14:10:06,203][__main__][INFO] - Test, Round 041: loss=0.5507, accuracy=0.6887, 
[2025-09-18 14:10:06,203][__main__][INFO] - Test, Round 042: loss=0.5472, accuracy=0.6910, 
[2025-09-18 14:10:06,203][__main__][INFO] - Test, Round 043: loss=0.5433, accuracy=0.6971, 
[2025-09-18 14:10:06,203][__main__][INFO] - Test, Round 044: loss=0.5409, accuracy=0.6976, 
[2025-09-18 14:10:06,203][__main__][INFO] - Test, Round 045: loss=0.5408, accuracy=0.6963, 
[2025-09-18 14:10:06,203][__main__][INFO] - Test, Round 046: loss=0.5391, accuracy=0.6959, 
[2025-09-18 14:10:06,203][__main__][INFO] - Test, Round 047: loss=0.5350, accuracy=0.7011, 
[2025-09-18 14:10:06,203][__main__][INFO] - Test, Round 048: loss=0.5354, accuracy=0.7045, 
[2025-09-18 14:10:06,203][__main__][INFO] - Test, Round 049: loss=0.5349, accuracy=0.7049, 
[2025-09-18 14:10:06,203][__main__][INFO] - Test, Round 050: loss=0.5335, accuracy=0.7052, 
[2025-09-18 14:10:06,203][__main__][INFO] - Test, Round 051: loss=0.5309, accuracy=0.7072, 
[2025-09-18 14:10:06,203][__main__][INFO] - Test, Round 052: loss=0.5198, accuracy=0.7126, 
[2025-09-18 14:10:06,203][__main__][INFO] - Test, Round 053: loss=0.5176, accuracy=0.7136, 
[2025-09-18 14:10:06,203][__main__][INFO] - Test, Round 054: loss=0.5114, accuracy=0.7208, 
[2025-09-18 14:10:06,203][__main__][INFO] - Test, Round 055: loss=0.5089, accuracy=0.7227, 
[2025-09-18 14:10:06,203][__main__][INFO] - Test, Round 056: loss=0.5066, accuracy=0.7249, 
[2025-09-18 14:10:06,203][__main__][INFO] - Test, Round 057: loss=0.5074, accuracy=0.7275, 
[2025-09-18 14:10:06,203][__main__][INFO] - Test, Round 058: loss=0.5096, accuracy=0.7282, 
[2025-09-18 14:10:06,203][__main__][INFO] - Test, Round 059: loss=0.5067, accuracy=0.7332, 
[2025-09-18 14:10:06,203][__main__][INFO] - Test, Round 060: loss=0.5049, accuracy=0.7362, 
[2025-09-18 14:10:06,203][__main__][INFO] - Test, Round 061: loss=0.5084, accuracy=0.7377, 
[2025-09-18 14:10:06,203][__main__][INFO] - Test, Round 062: loss=0.5075, accuracy=0.7389, 
[2025-09-18 14:10:06,203][__main__][INFO] - Test, Round 063: loss=0.4999, accuracy=0.7445, 
[2025-09-18 14:10:06,203][__main__][INFO] - Test, Round 064: loss=0.4915, accuracy=0.7489, 
[2025-09-18 14:10:06,203][__main__][INFO] - Test, Round 065: loss=0.4807, accuracy=0.7538, 
[2025-09-18 14:10:06,203][__main__][INFO] - Test, Round 066: loss=0.4801, accuracy=0.7551, 
[2025-09-18 14:10:06,203][__main__][INFO] - Test, Round 067: loss=0.4784, accuracy=0.7546, 
[2025-09-18 14:10:06,203][__main__][INFO] - Test, Round 068: loss=0.4808, accuracy=0.7545, 
[2025-09-18 14:10:06,203][__main__][INFO] - Test, Round 069: loss=0.4787, accuracy=0.7604, 
[2025-09-18 14:10:06,203][__main__][INFO] - Test, Round 070: loss=0.4767, accuracy=0.7641, 
[2025-09-18 14:10:06,203][__main__][INFO] - Test, Round 071: loss=0.4777, accuracy=0.7632, 
[2025-09-18 14:10:06,203][__main__][INFO] - Test, Round 072: loss=0.4793, accuracy=0.7653, 
[2025-09-18 14:10:06,203][__main__][INFO] - Test, Round 073: loss=0.4735, accuracy=0.7688, 
[2025-09-18 14:10:06,203][__main__][INFO] - Test, Round 074: loss=0.4707, accuracy=0.7706, 
[2025-09-18 14:10:06,203][__main__][INFO] - Test, Round 075: loss=0.4707, accuracy=0.7679, 
[2025-09-18 14:10:06,203][__main__][INFO] - Test, Round 076: loss=0.4712, accuracy=0.7677, 
[2025-09-18 14:10:06,203][__main__][INFO] - Test, Round 077: loss=0.4705, accuracy=0.7700, 
[2025-09-18 14:10:06,203][__main__][INFO] - Test, Round 078: loss=0.4685, accuracy=0.7702, 
[2025-09-18 14:10:06,203][__main__][INFO] - Test, Round 079: loss=0.4685, accuracy=0.7718, 
[2025-09-18 14:10:06,203][__main__][INFO] - Test, Round 080: loss=0.4658, accuracy=0.7717, 
[2025-09-18 14:10:06,203][__main__][INFO] - Test, Round 081: loss=0.4647, accuracy=0.7723, 
[2025-09-18 14:10:06,203][__main__][INFO] - Test, Round 082: loss=0.4619, accuracy=0.7760, 
[2025-09-18 14:10:06,203][__main__][INFO] - Test, Round 083: loss=0.4648, accuracy=0.7734, 
[2025-09-18 14:10:06,203][__main__][INFO] - Test, Round 084: loss=0.4621, accuracy=0.7753, 
[2025-09-18 14:10:06,203][__main__][INFO] - Test, Round 085: loss=0.4590, accuracy=0.7771, 
[2025-09-18 14:10:06,203][__main__][INFO] - Test, Round 086: loss=0.4569, accuracy=0.7766, 
[2025-09-18 14:10:06,203][__main__][INFO] - Test, Round 087: loss=0.4514, accuracy=0.7784, 
[2025-09-18 14:10:06,203][__main__][INFO] - Test, Round 088: loss=0.4467, accuracy=0.7815, 
[2025-09-18 14:10:06,203][__main__][INFO] - Test, Round 089: loss=0.4490, accuracy=0.7817, 
[2025-09-18 14:10:06,203][__main__][INFO] - Test, Round 090: loss=0.4551, accuracy=0.7798, 
[2025-09-18 14:10:06,203][__main__][INFO] - Test, Round 091: loss=0.4495, accuracy=0.7827, 
[2025-09-18 14:10:06,203][__main__][INFO] - Test, Round 092: loss=0.4465, accuracy=0.7864, 
[2025-09-18 14:10:06,203][__main__][INFO] - Test, Round 093: loss=0.4471, accuracy=0.7876, 
[2025-09-18 14:10:06,203][__main__][INFO] - Test, Round 094: loss=0.4501, accuracy=0.7900, 
[2025-09-18 14:10:06,203][__main__][INFO] - Test, Round 095: loss=0.4468, accuracy=0.7921, 
[2025-09-18 14:10:06,203][__main__][INFO] - Test, Round 096: loss=0.4456, accuracy=0.7921, 
[2025-09-18 14:10:06,204][__main__][INFO] - Test, Round 097: loss=0.4427, accuracy=0.7935, 
[2025-09-18 14:10:06,204][__main__][INFO] - Test, Round 098: loss=0.4385, accuracy=0.7959, 
[2025-09-18 14:10:06,204][__main__][INFO] - Test, Round 099: loss=0.4379, accuracy=0.7970, 
[2025-09-18 14:10:06,204][__main__][INFO] - Test, Round 100: loss=0.4387, accuracy=0.7975, 
[2025-09-18 14:10:06,204][__main__][INFO] - Test, Round 101: loss=0.4334, accuracy=0.8013, 
[2025-09-18 14:10:06,204][__main__][INFO] - Test, Round 102: loss=0.4341, accuracy=0.8016, 
[2025-09-18 14:10:06,204][__main__][INFO] - Test, Round 103: loss=0.4332, accuracy=0.8014, 
[2025-09-18 14:10:06,204][__main__][INFO] - Test, Round 104: loss=0.4422, accuracy=0.7976, 
[2025-09-18 14:10:06,204][__main__][INFO] - Test, Round 105: loss=0.4491, accuracy=0.7982, 
[2025-09-18 14:10:06,204][__main__][INFO] - Test, Round 106: loss=0.4472, accuracy=0.7981, 
[2025-09-18 14:10:06,204][__main__][INFO] - Test, Round 107: loss=0.4455, accuracy=0.7986, 
[2025-09-18 14:10:06,204][__main__][INFO] - Test, Round 108: loss=0.4377, accuracy=0.8041, 
[2025-09-18 14:10:06,204][__main__][INFO] - Test, Round 109: loss=0.4333, accuracy=0.8041, 
[2025-09-18 14:10:06,204][__main__][INFO] - Test, Round 110: loss=0.4320, accuracy=0.8055, 
[2025-09-18 14:10:06,204][__main__][INFO] - Test, Round 111: loss=0.4284, accuracy=0.8074, 
[2025-09-18 14:10:06,204][__main__][INFO] - Test, Round 112: loss=0.4303, accuracy=0.8063, 
[2025-09-18 14:10:06,204][__main__][INFO] - Test, Round 113: loss=0.4297, accuracy=0.8081, 
[2025-09-18 14:10:06,204][__main__][INFO] - Test, Round 114: loss=0.4333, accuracy=0.8082, 
[2025-09-18 14:10:06,204][__main__][INFO] - Test, Round 115: loss=0.4268, accuracy=0.8118, 
[2025-09-18 14:10:06,204][__main__][INFO] - Test, Round 116: loss=0.4291, accuracy=0.8102, 
[2025-09-18 14:10:06,204][__main__][INFO] - Test, Round 117: loss=0.4307, accuracy=0.8105, 
[2025-09-18 14:10:06,204][__main__][INFO] - Test, Round 118: loss=0.4318, accuracy=0.8114, 
[2025-09-18 14:10:06,204][__main__][INFO] - Test, Round 119: loss=0.4306, accuracy=0.8136, 
[2025-09-18 14:10:06,204][__main__][INFO] - Test, Round 120: loss=0.4301, accuracy=0.8149, 
[2025-09-18 14:10:06,204][__main__][INFO] - Test, Round 121: loss=0.4346, accuracy=0.8139, 
[2025-09-18 14:10:06,204][__main__][INFO] - Test, Round 122: loss=0.4409, accuracy=0.8117, 
[2025-09-18 14:10:06,204][__main__][INFO] - Test, Round 123: loss=0.4407, accuracy=0.8112, 
[2025-09-18 14:10:06,204][__main__][INFO] - Test, Round 124: loss=0.4415, accuracy=0.8107, 
[2025-09-18 14:10:06,204][__main__][INFO] - Test, Round 125: loss=0.4378, accuracy=0.8124, 
[2025-09-18 14:10:06,204][__main__][INFO] - Test, Round 126: loss=0.4341, accuracy=0.8137, 
[2025-09-18 14:10:06,204][__main__][INFO] - Test, Round 127: loss=0.4332, accuracy=0.8148, 
[2025-09-18 14:10:06,204][__main__][INFO] - Test, Round 128: loss=0.4271, accuracy=0.8194, 
[2025-09-18 14:10:06,204][__main__][INFO] - Test, Round 129: loss=0.4300, accuracy=0.8178, 
[2025-09-18 14:10:06,204][__main__][INFO] - Test, Round 130: loss=0.4306, accuracy=0.8175, 
[2025-09-18 14:10:06,204][__main__][INFO] - Test, Round 131: loss=0.4293, accuracy=0.8189, 
[2025-09-18 14:10:06,204][__main__][INFO] - Test, Round 132: loss=0.4234, accuracy=0.8227, 
[2025-09-18 14:10:06,204][__main__][INFO] - Test, Round 133: loss=0.4202, accuracy=0.8237, 
[2025-09-18 14:10:06,204][__main__][INFO] - Test, Round 134: loss=0.4200, accuracy=0.8258, 
[2025-09-18 14:10:06,204][__main__][INFO] - Test, Round 135: loss=0.4225, accuracy=0.8259, 
[2025-09-18 14:10:06,204][__main__][INFO] - Test, Round 136: loss=0.4261, accuracy=0.8260, 
[2025-09-18 14:10:06,204][__main__][INFO] - Test, Round 137: loss=0.4221, accuracy=0.8281, 
[2025-09-18 14:10:06,204][__main__][INFO] - Test, Round 138: loss=0.4216, accuracy=0.8309, 
[2025-09-18 14:10:06,204][__main__][INFO] - Test, Round 139: loss=0.4195, accuracy=0.8324, 
[2025-09-18 14:10:06,204][__main__][INFO] - Test, Round 140: loss=0.4238, accuracy=0.8322, 
[2025-09-18 14:10:06,204][__main__][INFO] - Test, Round 141: loss=0.4288, accuracy=0.8302, 
[2025-09-18 14:10:06,204][__main__][INFO] - Test, Round 142: loss=0.4273, accuracy=0.8319, 
[2025-09-18 14:10:06,204][__main__][INFO] - Test, Round 143: loss=0.4256, accuracy=0.8339, 
[2025-09-18 14:10:06,204][__main__][INFO] - Test, Round 144: loss=0.4269, accuracy=0.8340, 
[2025-09-18 14:10:06,204][__main__][INFO] - Test, Round 145: loss=0.4278, accuracy=0.8347, 
[2025-09-18 14:10:06,204][__main__][INFO] - Test, Round 146: loss=0.4323, accuracy=0.8333, 
[2025-09-18 14:10:06,204][__main__][INFO] - Test, Round 147: loss=0.4309, accuracy=0.8343, 
[2025-09-18 14:10:06,204][__main__][INFO] - Test, Round 148: loss=0.4331, accuracy=0.8324, 
[2025-09-18 14:10:06,204][__main__][INFO] - Test, Round 149: loss=0.4340, accuracy=0.8341, 
[2025-09-18 14:10:06,204][__main__][INFO] - Test, Round 150: loss=0.4322, accuracy=0.8362, 
[2025-09-18 14:10:06,204][__main__][INFO] - Test, Round 151: loss=0.4356, accuracy=0.8368, 
[2025-09-18 14:10:06,204][__main__][INFO] - Test, Round 152: loss=0.4313, accuracy=0.8379, 
[2025-09-18 14:10:06,204][__main__][INFO] - Test, Round 153: loss=0.4350, accuracy=0.8363, 
[2025-09-18 14:10:06,204][__main__][INFO] - Test, Round 154: loss=0.4373, accuracy=0.8365, 
[2025-09-18 14:10:06,204][__main__][INFO] - Test, Round 155: loss=0.4399, accuracy=0.8372, 
[2025-09-18 14:10:06,204][__main__][INFO] - Test, Round 156: loss=0.4433, accuracy=0.8377, 
[2025-09-18 14:10:06,204][__main__][INFO] - Test, Round 157: loss=0.4434, accuracy=0.8376, 
[2025-09-18 14:10:06,204][__main__][INFO] - Test, Round 158: loss=0.4458, accuracy=0.8374, 
[2025-09-18 14:10:06,204][__main__][INFO] - Test, Round 159: loss=0.4500, accuracy=0.8371, 
[2025-09-18 14:10:06,204][__main__][INFO] - Test, Round 160: loss=0.4489, accuracy=0.8388, 
[2025-09-18 14:10:06,204][__main__][INFO] - Test, Round 161: loss=0.4540, accuracy=0.8365, 
[2025-09-18 14:10:06,204][__main__][INFO] - Test, Round 162: loss=0.4525, accuracy=0.8380, 
[2025-09-18 14:10:06,204][__main__][INFO] - Test, Round 163: loss=0.4497, accuracy=0.8393, 
[2025-09-18 14:10:06,204][__main__][INFO] - Test, Round 164: loss=0.4533, accuracy=0.8395, 
[2025-09-18 14:10:06,204][__main__][INFO] - Test, Round 165: loss=0.4546, accuracy=0.8401, 
[2025-09-18 14:10:06,204][__main__][INFO] - Test, Round 166: loss=0.4495, accuracy=0.8416, 
[2025-09-18 14:10:06,204][__main__][INFO] - Test, Round 167: loss=0.4464, accuracy=0.8434, 
[2025-09-18 14:10:06,204][__main__][INFO] - Test, Round 168: loss=0.4486, accuracy=0.8427, 
[2025-09-18 14:10:06,204][__main__][INFO] - Test, Round 169: loss=0.4540, accuracy=0.8428, 
[2025-09-18 14:10:06,204][__main__][INFO] - Test, Round 170: loss=0.4520, accuracy=0.8420, 
[2025-09-18 14:10:06,205][__main__][INFO] - Test, Round 171: loss=0.4529, accuracy=0.8426, 
[2025-09-18 14:10:06,205][__main__][INFO] - Test, Round 172: loss=0.4517, accuracy=0.8439, 
[2025-09-18 14:10:06,205][__main__][INFO] - Test, Round 173: loss=0.4552, accuracy=0.8444, 
[2025-09-18 14:10:06,205][__main__][INFO] - Test, Round 174: loss=0.4624, accuracy=0.8433, 
[2025-09-18 14:10:06,205][__main__][INFO] - Test, Round 175: loss=0.4644, accuracy=0.8431, 
[2025-09-18 14:10:06,205][__main__][INFO] - Test, Round 176: loss=0.4736, accuracy=0.8410, 
[2025-09-18 14:10:06,205][__main__][INFO] - Test, Round 177: loss=0.4687, accuracy=0.8418, 
[2025-09-18 14:10:06,205][__main__][INFO] - Test, Round 178: loss=0.4721, accuracy=0.8416, 
[2025-09-18 14:10:06,205][__main__][INFO] - Test, Round 179: loss=0.4732, accuracy=0.8420, 
[2025-09-18 14:10:06,205][__main__][INFO] - Test, Round 180: loss=0.4708, accuracy=0.8427, 
[2025-09-18 14:10:06,205][__main__][INFO] - Test, Round 181: loss=0.4725, accuracy=0.8424, 
[2025-09-18 14:10:06,205][__main__][INFO] - Test, Round 182: loss=0.4754, accuracy=0.8431, 
[2025-09-18 14:10:06,205][__main__][INFO] - Test, Round 183: loss=0.4814, accuracy=0.8414, 
[2025-09-18 14:10:06,205][__main__][INFO] - Test, Round 184: loss=0.4818, accuracy=0.8436, 
[2025-09-18 14:10:06,205][__main__][INFO] - Test, Round 185: loss=0.4810, accuracy=0.8432, 
[2025-09-18 14:10:06,205][__main__][INFO] - Test, Round 186: loss=0.4866, accuracy=0.8440, 
[2025-09-18 14:10:06,205][__main__][INFO] - Test, Round 187: loss=0.4867, accuracy=0.8446, 
[2025-09-18 14:10:06,205][__main__][INFO] - Test, Round 188: loss=0.4844, accuracy=0.8466, 
[2025-09-18 14:10:06,205][__main__][INFO] - Test, Round 189: loss=0.4794, accuracy=0.8489, 
[2025-09-18 14:10:06,205][__main__][INFO] - Test, Round 190: loss=0.4822, accuracy=0.8490, 
[2025-09-18 14:10:06,205][__main__][INFO] - Test, Round 191: loss=0.4851, accuracy=0.8498, 
[2025-09-18 14:10:06,205][__main__][INFO] - Test, Round 192: loss=0.4829, accuracy=0.8517, 
[2025-09-18 14:10:06,205][__main__][INFO] - Test, Round 193: loss=0.4845, accuracy=0.8515, 
[2025-09-18 14:10:06,205][__main__][INFO] - Test, Round 194: loss=0.4860, accuracy=0.8511, 
[2025-09-18 14:10:06,205][__main__][INFO] - Test, Round 195: loss=0.4862, accuracy=0.8505, 
[2025-09-18 14:10:06,205][__main__][INFO] - Test, Round 196: loss=0.4875, accuracy=0.8501, 
[2025-09-18 14:10:06,205][__main__][INFO] - Test, Round 197: loss=0.4855, accuracy=0.8512, 
[2025-09-18 14:10:06,205][__main__][INFO] - Test, Round 198: loss=0.4913, accuracy=0.8514, 
[2025-09-18 14:10:06,205][__main__][INFO] - Test, Round 199: loss=0.4937, accuracy=0.8519, 
[2025-09-18 14:10:06,205][__main__][INFO] - Test, Round 200: loss=0.4973, accuracy=0.8511, 
