[2025-09-16 17:54:53,843][flp2p.graph_runner][INFO] - Train, Round 0 : loss => 2.3029749479293824,  accuracy: 0.1, gradient_norm : 0.4745889325057479
[2025-09-16 17:55:04,486][flp2p.graph_runner][INFO] - Test, Round 0 : loss => 2.2945152040958403,  accuracy: 0.117
[2025-09-16 17:55:17,210][flp2p.graph_runner][INFO] - Train, Round 1 : loss => 2.291454794883728,  accuracy: 0.12825, gradient_norm : 0.48699972746074904
[2025-09-16 17:55:28,875][flp2p.graph_runner][INFO] - Test, Round 1 : loss => 2.283470797228813,  accuracy: 0.1735
[2025-09-16 17:55:42,512][flp2p.graph_runner][INFO] - Train, Round 2 : loss => 2.27926722407341,  accuracy: 0.19205, gradient_norm : 0.5478471982043359
[2025-09-16 17:55:53,689][flp2p.graph_runner][INFO] - Test, Round 2 : loss => 2.2685174567222597,  accuracy: 0.1918
[2025-09-16 17:56:07,228][flp2p.graph_runner][INFO] - Train, Round 3 : loss => 2.2613444130420683,  accuracy: 0.209, gradient_norm : 0.6591636196183629
[2025-09-16 17:56:18,552][flp2p.graph_runner][INFO] - Test, Round 3 : loss => 2.2469596603155138,  accuracy: 0.2239
[2025-09-16 17:56:32,167][flp2p.graph_runner][INFO] - Train, Round 4 : loss => 2.2317194043397905,  accuracy: 0.2372, gradient_norm : 0.8021448261825945
[2025-09-16 17:56:43,434][flp2p.graph_runner][INFO] - Test, Round 4 : loss => 2.238293720138073,  accuracy: 0.2778
[2025-09-16 17:56:57,112][flp2p.graph_runner][INFO] - Train, Round 5 : loss => 2.195311275243759,  accuracy: 0.29385, gradient_norm : 1.0166448365423963
[2025-09-16 17:57:08,282][flp2p.graph_runner][INFO] - Test, Round 5 : loss => 2.2709210258722305,  accuracy: 0.2054
[2025-09-16 17:57:21,754][flp2p.graph_runner][INFO] - Train, Round 6 : loss => 2.2068856168985365,  accuracy: 0.2371, gradient_norm : 1.1679767349264245
[2025-09-16 17:57:33,041][flp2p.graph_runner][INFO] - Test, Round 6 : loss => 2.3246298660993574,  accuracy: 0.2213
[2025-09-16 17:57:46,529][flp2p.graph_runner][INFO] - Train, Round 7 : loss => 2.2000221626758574,  accuracy: 0.24375, gradient_norm : 1.1682747472352708
[2025-09-16 17:57:57,890][flp2p.graph_runner][INFO] - Test, Round 7 : loss => 2.2909687698721886,  accuracy: 0.2428
[2025-09-16 17:58:11,362][flp2p.graph_runner][INFO] - Train, Round 8 : loss => 2.1930892461538316,  accuracy: 0.26075, gradient_norm : 1.2471388485132404
[2025-09-16 17:58:22,598][flp2p.graph_runner][INFO] - Test, Round 8 : loss => 2.3349464081406595,  accuracy: 0.1922
[2025-09-16 17:58:36,028][flp2p.graph_runner][INFO] - Train, Round 9 : loss => 2.1511539326906206,  accuracy: 0.22375, gradient_norm : 1.2761208426681465
[2025-09-16 17:58:47,300][flp2p.graph_runner][INFO] - Test, Round 9 : loss => 2.2100215462207795,  accuracy: 0.284
[2025-09-16 17:59:00,870][flp2p.graph_runner][INFO] - Train, Round 10 : loss => 2.099851369857788,  accuracy: 0.29646666666666666, gradient_norm : 1.4409868528030745
[2025-09-16 17:59:12,013][flp2p.graph_runner][INFO] - Test, Round 10 : loss => 2.2444267765164376,  accuracy: 0.2843
[2025-09-16 17:59:25,516][flp2p.graph_runner][INFO] - Train, Round 11 : loss => 2.1027955970168115,  accuracy: 0.30625, gradient_norm : 1.5694359921852732
[2025-09-16 17:59:36,828][flp2p.graph_runner][INFO] - Test, Round 11 : loss => 2.2342986971795558,  accuracy: 0.2831
[2025-09-16 17:59:50,446][flp2p.graph_runner][INFO] - Train, Round 12 : loss => 2.05824409866333,  accuracy: 0.29578333333333334, gradient_norm : 1.4350866134218585
[2025-09-16 18:00:01,710][flp2p.graph_runner][INFO] - Test, Round 12 : loss => 2.0396397408246996,  accuracy: 0.2917
[2025-09-16 18:00:15,269][flp2p.graph_runner][INFO] - Train, Round 13 : loss => 1.971029284775257,  accuracy: 0.3068166666666667, gradient_norm : 1.6225139254147254
[2025-09-16 18:00:26,503][flp2p.graph_runner][INFO] - Test, Round 13 : loss => 2.223551494818926,  accuracy: 0.297
[2025-09-16 18:00:39,931][flp2p.graph_runner][INFO] - Train, Round 14 : loss => 1.9210915281772614,  accuracy: 0.31753333333333333, gradient_norm : 1.8304747520925397
[2025-09-16 18:00:51,061][flp2p.graph_runner][INFO] - Test, Round 14 : loss => 1.9662010984241962,  accuracy: 0.3169
[2025-09-16 18:01:04,622][flp2p.graph_runner][INFO] - Train, Round 15 : loss => 1.8449310867786408,  accuracy: 0.35008333333333336, gradient_norm : 2.442235459029014
[2025-09-16 18:01:15,785][flp2p.graph_runner][INFO] - Test, Round 15 : loss => 4.761540906769037,  accuracy: 0.1941
[2025-09-16 18:01:29,334][flp2p.graph_runner][INFO] - Train, Round 16 : loss => 2.9040792939662934,  accuracy: 0.3128666666666667, gradient_norm : 2.506876242736055
[2025-09-16 18:01:40,601][flp2p.graph_runner][INFO] - Test, Round 16 : loss => 1.9951524247288703,  accuracy: 0.2909
[2025-09-16 18:01:54,137][flp2p.graph_runner][INFO] - Train, Round 17 : loss => 1.791663077354431,  accuracy: 0.36106666666666665, gradient_norm : 2.364428172153241
[2025-09-16 18:02:05,371][flp2p.graph_runner][INFO] - Test, Round 17 : loss => 1.9528406528830529,  accuracy: 0.3154
[2025-09-16 18:02:18,909][flp2p.graph_runner][INFO] - Train, Round 18 : loss => 1.726704394698143,  accuracy: 0.39826666666666666, gradient_norm : 2.4995343441926816
[2025-09-16 18:02:30,041][flp2p.graph_runner][INFO] - Test, Round 18 : loss => 1.9695550094544887,  accuracy: 0.3333
[2025-09-16 18:02:43,415][flp2p.graph_runner][INFO] - Train, Round 19 : loss => 1.6513395110964775,  accuracy: 0.4209833333333333, gradient_norm : 2.875744290291524
[2025-09-16 18:02:54,854][flp2p.graph_runner][INFO] - Test, Round 19 : loss => 2.2606426799595356,  accuracy: 0.2998
[2025-09-16 18:03:08,605][flp2p.graph_runner][INFO] - Train, Round 20 : loss => 1.6216197271347046,  accuracy: 0.4376333333333333, gradient_norm : 3.011110510993112
[2025-09-16 18:03:19,793][flp2p.graph_runner][INFO] - Test, Round 20 : loss => 2.109180593979359,  accuracy: 0.3384
[2025-09-16 18:03:33,353][flp2p.graph_runner][INFO] - Train, Round 21 : loss => 1.5322223774790764,  accuracy: 0.4638333333333333, gradient_norm : 3.1535084780770486
[2025-09-16 18:03:44,559][flp2p.graph_runner][INFO] - Test, Round 21 : loss => 2.1888331126689913,  accuracy: 0.3616
[2025-09-16 18:03:58,129][flp2p.graph_runner][INFO] - Train, Round 22 : loss => 1.4641092460155487,  accuracy: 0.49398333333333333, gradient_norm : 3.0448389829717297
[2025-09-16 18:04:09,385][flp2p.graph_runner][INFO] - Test, Round 22 : loss => 1.8317304207086562,  accuracy: 0.39
[2025-09-16 18:04:22,822][flp2p.graph_runner][INFO] - Train, Round 23 : loss => 1.3471354355216025,  accuracy: 0.5259, gradient_norm : 3.573463433377414
[2025-09-16 18:04:34,105][flp2p.graph_runner][INFO] - Test, Round 23 : loss => 1.944664893424511,  accuracy: 0.3652
[2025-09-16 18:04:47,713][flp2p.graph_runner][INFO] - Train, Round 24 : loss => 1.359501666545868,  accuracy: 0.5167166666666667, gradient_norm : 3.5153935043899316
[2025-09-16 18:04:58,899][flp2p.graph_runner][INFO] - Test, Round 24 : loss => 2.4955934107482434,  accuracy: 0.3751
[2025-09-16 18:05:12,599][flp2p.graph_runner][INFO] - Train, Round 25 : loss => 2.2572626247406005,  accuracy: 0.5457833333333333, gradient_norm : 3.331388334007219
[2025-09-16 18:05:23,856][flp2p.graph_runner][INFO] - Test, Round 25 : loss => 1.8622079540491103,  accuracy: 0.389
[2025-09-16 18:05:37,460][flp2p.graph_runner][INFO] - Train, Round 26 : loss => 1.228687027513981,  accuracy: 0.5639166666666666, gradient_norm : 3.3598546861253236
[2025-09-16 18:05:48,433][flp2p.graph_runner][INFO] - Test, Round 26 : loss => 1.8598605059206486,  accuracy: 0.3934
[2025-09-16 18:06:01,893][flp2p.graph_runner][INFO] - Train, Round 27 : loss => 1.1839407383203506,  accuracy: 0.5768, gradient_norm : 3.5993621590315854
[2025-09-16 18:06:12,992][flp2p.graph_runner][INFO] - Test, Round 27 : loss => 1.9310505643725395,  accuracy: 0.4027
[2025-09-16 18:06:26,461][flp2p.graph_runner][INFO] - Train, Round 28 : loss => 1.1711327103972435,  accuracy: 0.5823833333333334, gradient_norm : 3.4466725487908376
[2025-09-16 18:06:37,933][flp2p.graph_runner][INFO] - Test, Round 28 : loss => 1.8184777220726014,  accuracy: 0.4122
[2025-09-16 18:06:51,505][flp2p.graph_runner][INFO] - Train, Round 29 : loss => 1.1264009745121002,  accuracy: 0.5961833333333333, gradient_norm : 3.459786618057471
[2025-09-16 18:07:02,758][flp2p.graph_runner][INFO] - Test, Round 29 : loss => 1.8151719299674034,  accuracy: 0.4158
[2025-09-16 18:07:16,217][flp2p.graph_runner][INFO] - Train, Round 30 : loss => 1.09707539665699,  accuracy: 0.6032166666666666, gradient_norm : 3.4462438751031788
[2025-09-16 18:07:27,405][flp2p.graph_runner][INFO] - Test, Round 30 : loss => 1.7579964564979076,  accuracy: 0.422
[2025-09-16 18:07:40,830][flp2p.graph_runner][INFO] - Train, Round 31 : loss => 1.0653140426278114,  accuracy: 0.606, gradient_norm : 3.3912172282148494
[2025-09-16 18:07:52,052][flp2p.graph_runner][INFO] - Test, Round 31 : loss => 1.6931271861076356,  accuracy: 0.4547
[2025-09-16 18:08:05,633][flp2p.graph_runner][INFO] - Train, Round 32 : loss => 1.0462159010767937,  accuracy: 0.6189666666666667, gradient_norm : 3.437972675850007
[2025-09-16 18:08:16,819][flp2p.graph_runner][INFO] - Test, Round 32 : loss => 1.8044725682616234,  accuracy: 0.4378
[2025-09-16 18:08:30,223][flp2p.graph_runner][INFO] - Train, Round 33 : loss => 1.035967767059803,  accuracy: 0.6240666666666667, gradient_norm : 3.489078703302344
[2025-09-16 18:08:41,457][flp2p.graph_runner][INFO] - Test, Round 33 : loss => 1.7252387381196022,  accuracy: 0.4669
[2025-09-16 18:08:54,846][flp2p.graph_runner][INFO] - Train, Round 34 : loss => 1.017000849187374,  accuracy: 0.6268666666666667, gradient_norm : 3.425946312980945
[2025-09-16 18:09:05,943][flp2p.graph_runner][INFO] - Test, Round 34 : loss => 1.627569301944971,  accuracy: 0.4622
[2025-09-16 18:09:19,553][flp2p.graph_runner][INFO] - Train, Round 35 : loss => 0.9903771652579307,  accuracy: 0.6350333333333333, gradient_norm : 3.3798259332099643
[2025-09-16 18:09:30,760][flp2p.graph_runner][INFO] - Test, Round 35 : loss => 1.6095959198623895,  accuracy: 0.4708
[2025-09-16 18:09:44,130][flp2p.graph_runner][INFO] - Train, Round 36 : loss => 1.321605033904314,  accuracy: 0.64015, gradient_norm : 3.444397275020306
[2025-09-16 18:09:55,335][flp2p.graph_runner][INFO] - Test, Round 36 : loss => 1.6955811982840299,  accuracy: 0.4577
[2025-09-16 18:10:08,819][flp2p.graph_runner][INFO] - Train, Round 37 : loss => 0.9758165163695812,  accuracy: 0.64085, gradient_norm : 3.339185398414922
[2025-09-16 18:10:19,946][flp2p.graph_runner][INFO] - Test, Round 37 : loss => 1.6256700844734908,  accuracy: 0.4761
[2025-09-16 18:10:33,291][flp2p.graph_runner][INFO] - Train, Round 38 : loss => 0.9475482433140278,  accuracy: 0.6460333333333333, gradient_norm : 3.3839246203341276
[2025-09-16 18:10:44,674][flp2p.graph_runner][INFO] - Test, Round 38 : loss => 1.5874073226630687,  accuracy: 0.477
[2025-09-16 18:10:58,200][flp2p.graph_runner][INFO] - Train, Round 39 : loss => 0.9397900000214576,  accuracy: 0.6507833333333334, gradient_norm : 3.398900129941988
[2025-09-16 18:11:09,458][flp2p.graph_runner][INFO] - Test, Round 39 : loss => 1.5878590299725532,  accuracy: 0.4822
[2025-09-16 18:11:22,946][flp2p.graph_runner][INFO] - Train, Round 40 : loss => 0.9320865503549576,  accuracy: 0.6539166666666667, gradient_norm : 3.154992054878702
[2025-09-16 18:11:34,253][flp2p.graph_runner][INFO] - Test, Round 40 : loss => 1.4486917316794397,  accuracy: 0.5058
[2025-09-16 18:11:47,659][flp2p.graph_runner][INFO] - Train, Round 41 : loss => 0.9053881356716156,  accuracy: 0.6610666666666667, gradient_norm : 3.3052815414160754
[2025-09-16 18:11:58,910][flp2p.graph_runner][INFO] - Test, Round 41 : loss => 1.4940010167211295,  accuracy: 0.492
[2025-09-16 18:12:12,415][flp2p.graph_runner][INFO] - Train, Round 42 : loss => 0.901261990815401,  accuracy: 0.6625166666666666, gradient_norm : 3.261239296727536
[2025-09-16 18:12:23,501][flp2p.graph_runner][INFO] - Test, Round 42 : loss => 1.6258303947448731,  accuracy: 0.5
[2025-09-16 18:12:36,824][flp2p.graph_runner][INFO] - Train, Round 43 : loss => 0.9026925939619541,  accuracy: 0.6668333333333333, gradient_norm : 3.3614858387442554
[2025-09-16 18:12:48,001][flp2p.graph_runner][INFO] - Test, Round 43 : loss => 1.5117955571562052,  accuracy: 0.5059
[2025-09-16 18:13:01,466][flp2p.graph_runner][INFO] - Train, Round 44 : loss => 0.8885603130757809,  accuracy: 0.6669833333333334, gradient_norm : 3.142829835288457
[2025-09-16 18:13:12,670][flp2p.graph_runner][INFO] - Test, Round 44 : loss => 1.4073148121088743,  accuracy: 0.5244
[2025-09-16 18:13:26,028][flp2p.graph_runner][INFO] - Train, Round 45 : loss => 0.8627687053978443,  accuracy: 0.6732833333333333, gradient_norm : 3.1357460292015094
[2025-09-16 18:13:37,409][flp2p.graph_runner][INFO] - Test, Round 45 : loss => 1.4266682227700949,  accuracy: 0.521
[2025-09-16 18:13:50,767][flp2p.graph_runner][INFO] - Train, Round 46 : loss => 0.857234206855297,  accuracy: 0.6788666666666666, gradient_norm : 3.422665633038891
[2025-09-16 18:14:01,924][flp2p.graph_runner][INFO] - Test, Round 46 : loss => 1.520940095001459,  accuracy: 0.4984
[2025-09-16 18:14:15,502][flp2p.graph_runner][INFO] - Train, Round 47 : loss => 0.8683830690681934,  accuracy: 0.6759333333333334, gradient_norm : 3.3669868088832233
[2025-09-16 18:14:26,699][flp2p.graph_runner][INFO] - Test, Round 47 : loss => 1.755070421525836,  accuracy: 0.5185
[2025-09-16 18:14:40,222][flp2p.graph_runner][INFO] - Train, Round 48 : loss => 0.9625599930584431,  accuracy: 0.67835, gradient_norm : 3.2710086733493586
[2025-09-16 18:14:51,403][flp2p.graph_runner][INFO] - Test, Round 48 : loss => 1.4167515359014273,  accuracy: 0.5302
[2025-09-16 18:15:04,931][flp2p.graph_runner][INFO] - Train, Round 49 : loss => 0.8491540486514568,  accuracy: 0.6807333333333333, gradient_norm : 3.0696429661779403
[2025-09-16 18:15:16,180][flp2p.graph_runner][INFO] - Test, Round 49 : loss => 1.3443388625085353,  accuracy: 0.538
[2025-09-16 18:15:29,584][flp2p.graph_runner][INFO] - Train, Round 50 : loss => 0.8215458266735077,  accuracy: 0.6896666666666667, gradient_norm : 3.2630825671875554
[2025-09-16 18:15:40,808][flp2p.graph_runner][INFO] - Test, Round 50 : loss => 1.399078980472684,  accuracy: 0.5326
[2025-09-16 18:15:54,295][flp2p.graph_runner][INFO] - Train, Round 51 : loss => 0.9292482362091541,  accuracy: 0.68815, gradient_norm : 3.0841653257569823
[2025-09-16 18:16:05,677][flp2p.graph_runner][INFO] - Test, Round 51 : loss => 1.309868171042204,  accuracy: 0.5547
[2025-09-16 18:16:19,307][flp2p.graph_runner][INFO] - Train, Round 52 : loss => 0.8051883219778537,  accuracy: 0.6962, gradient_norm : 3.0849912803424853
[2025-09-16 18:16:30,580][flp2p.graph_runner][INFO] - Test, Round 52 : loss => 1.3527739357769488,  accuracy: 0.5393
[2025-09-16 18:16:44,110][flp2p.graph_runner][INFO] - Train, Round 53 : loss => 0.8056964744329452,  accuracy: 0.6939166666666666, gradient_norm : 3.144411893709442
[2025-09-16 18:16:55,396][flp2p.graph_runner][INFO] - Test, Round 53 : loss => 1.3081596049666404,  accuracy: 0.5528
[2025-09-16 18:17:08,959][flp2p.graph_runner][INFO] - Train, Round 54 : loss => 0.7986678120791912,  accuracy: 0.6946166666666667, gradient_norm : 3.1707761701810147
[2025-09-16 18:17:20,001][flp2p.graph_runner][INFO] - Test, Round 54 : loss => 1.3536789255440236,  accuracy: 0.5418
[2025-09-16 18:17:33,530][flp2p.graph_runner][INFO] - Train, Round 55 : loss => 0.7980686092078686,  accuracy: 0.6989166666666666, gradient_norm : 3.2089520851127635
[2025-09-16 18:17:44,814][flp2p.graph_runner][INFO] - Test, Round 55 : loss => 1.3993386102855205,  accuracy: 0.5323
[2025-09-16 18:17:58,206][flp2p.graph_runner][INFO] - Train, Round 56 : loss => 0.798696647644043,  accuracy: 0.6981333333333334, gradient_norm : 3.1659238857738656
[2025-09-16 18:18:09,326][flp2p.graph_runner][INFO] - Test, Round 56 : loss => 1.3259445394814013,  accuracy: 0.5544
[2025-09-16 18:18:22,804][flp2p.graph_runner][INFO] - Train, Round 57 : loss => 0.782759776622057,  accuracy: 0.7007166666666667, gradient_norm : 3.178087276660409
[2025-09-16 18:18:33,963][flp2p.graph_runner][INFO] - Test, Round 57 : loss => 1.3263540678739547,  accuracy: 0.553
[2025-09-16 18:18:47,394][flp2p.graph_runner][INFO] - Train, Round 58 : loss => 0.7822112720608712,  accuracy: 0.7023666666666667, gradient_norm : 3.137125914518506
[2025-09-16 18:18:58,635][flp2p.graph_runner][INFO] - Test, Round 58 : loss => 1.3237853259384633,  accuracy: 0.558
[2025-09-16 18:19:12,169][flp2p.graph_runner][INFO] - Train, Round 59 : loss => 1.4247139754593372,  accuracy: 0.7054, gradient_norm : 3.1900237143536265
[2025-09-16 18:19:23,324][flp2p.graph_runner][INFO] - Test, Round 59 : loss => 1.3340731206536294,  accuracy: 0.5544
[2025-09-16 18:19:36,692][flp2p.graph_runner][INFO] - Train, Round 60 : loss => 0.7726173191666603,  accuracy: 0.7057666666666667, gradient_norm : 3.1670911866624567
[2025-09-16 18:19:47,946][flp2p.graph_runner][INFO] - Test, Round 60 : loss => 1.3165582641720772,  accuracy: 0.5639
[2025-09-16 18:20:01,517][flp2p.graph_runner][INFO] - Train, Round 61 : loss => 0.7677615782767534,  accuracy: 0.7091666666666666, gradient_norm : 3.1860343885943707
[2025-09-16 18:20:12,749][flp2p.graph_runner][INFO] - Test, Round 61 : loss => 1.5310340070158244,  accuracy: 0.5493
[2025-09-16 18:20:26,003][flp2p.graph_runner][INFO] - Train, Round 62 : loss => 0.7777117009758949,  accuracy: 0.70825, gradient_norm : 2.922018028628049
[2025-09-16 18:20:37,267][flp2p.graph_runner][INFO] - Test, Round 62 : loss => 1.2395274848014117,  accuracy: 0.5703
[2025-09-16 18:20:50,780][flp2p.graph_runner][INFO] - Train, Round 63 : loss => 0.737330081820488,  accuracy: 0.7159, gradient_norm : 2.9751591679071474
[2025-09-16 18:21:01,870][flp2p.graph_runner][INFO] - Test, Round 63 : loss => 1.2254997338205575,  accuracy: 0.5786
[2025-09-16 18:21:15,247][flp2p.graph_runner][INFO] - Train, Round 64 : loss => 17.386157678693532,  accuracy: 0.7166666666666667, gradient_norm : 3.0547900532866534
[2025-09-16 18:21:26,479][flp2p.graph_runner][INFO] - Test, Round 64 : loss => 1.2290116873979569,  accuracy: 0.5773
[2025-09-16 18:21:39,985][flp2p.graph_runner][INFO] - Train, Round 65 : loss => 0.7377112341523171,  accuracy: 0.7195166666666667, gradient_norm : 3.0855294641809308
[2025-09-16 18:21:51,183][flp2p.graph_runner][INFO] - Test, Round 65 : loss => 1.2893877843856811,  accuracy: 0.564
[2025-09-16 18:22:04,736][flp2p.graph_runner][INFO] - Train, Round 66 : loss => 0.7349542919397354,  accuracy: 0.7179, gradient_norm : 3.0711852549018177
[2025-09-16 18:22:15,946][flp2p.graph_runner][INFO] - Test, Round 66 : loss => 1.2367138896763326,  accuracy: 0.5802
[2025-09-16 18:22:29,311][flp2p.graph_runner][INFO] - Train, Round 67 : loss => 0.7264475802779198,  accuracy: 0.7232333333333333, gradient_norm : 3.1094020642822073
[2025-09-16 18:22:40,720][flp2p.graph_runner][INFO] - Test, Round 67 : loss => 1.2080857243806125,  accuracy: 0.5841
[2025-09-16 18:22:54,263][flp2p.graph_runner][INFO] - Train, Round 68 : loss => 0.7179847908914089,  accuracy: 0.7233833333333334, gradient_norm : 2.9297911740794786
[2025-09-16 18:23:05,294][flp2p.graph_runner][INFO] - Test, Round 68 : loss => 1.183675414431095,  accuracy: 0.5895
[2025-09-16 18:23:18,747][flp2p.graph_runner][INFO] - Train, Round 69 : loss => 0.7057560632377863,  accuracy: 0.7263, gradient_norm : 3.2171944788100966
[2025-09-16 18:23:30,046][flp2p.graph_runner][INFO] - Test, Round 69 : loss => 1.2855710990220308,  accuracy: 0.5674
[2025-09-16 18:23:43,547][flp2p.graph_runner][INFO] - Train, Round 70 : loss => 0.7206698769927025,  accuracy: 0.72275, gradient_norm : 3.2931410471009297
[2025-09-16 18:23:54,743][flp2p.graph_runner][INFO] - Test, Round 70 : loss => 1.3113937175512314,  accuracy: 0.5692
[2025-09-16 18:24:08,293][flp2p.graph_runner][INFO] - Train, Round 71 : loss => 0.7254199796617031,  accuracy: 0.7255333333333334, gradient_norm : 3.049454048320714
[2025-09-16 18:24:19,393][flp2p.graph_runner][INFO] - Test, Round 71 : loss => 1.2030786016821862,  accuracy: 0.5958
[2025-09-16 18:24:32,823][flp2p.graph_runner][INFO] - Train, Round 72 : loss => 0.7030872150361538,  accuracy: 0.7276, gradient_norm : 3.0590769384908003
[2025-09-16 18:24:44,069][flp2p.graph_runner][INFO] - Test, Round 72 : loss => 1.2330066538125277,  accuracy: 0.5783
[2025-09-16 18:24:57,445][flp2p.graph_runner][INFO] - Train, Round 73 : loss => 0.7048825903087854,  accuracy: 0.7287833333333333, gradient_norm : 3.06535717133876
[2025-09-16 18:25:08,647][flp2p.graph_runner][INFO] - Test, Round 73 : loss => 1.2380626588493586,  accuracy: 0.5954
[2025-09-16 18:25:22,114][flp2p.graph_runner][INFO] - Train, Round 74 : loss => 0.700300851508975,  accuracy: 0.7322666666666666, gradient_norm : 2.983597775246007
[2025-09-16 18:25:33,348][flp2p.graph_runner][INFO] - Test, Round 74 : loss => 1.1781204267710448,  accuracy: 0.5911
[2025-09-16 18:25:46,888][flp2p.graph_runner][INFO] - Train, Round 75 : loss => 0.6885246815383435,  accuracy: 0.736, gradient_norm : 3.158506377783275
[2025-09-16 18:25:58,241][flp2p.graph_runner][INFO] - Test, Round 75 : loss => 1.1642127065628767,  accuracy: 0.6001
[2025-09-16 18:26:11,721][flp2p.graph_runner][INFO] - Train, Round 76 : loss => 0.6893074607551097,  accuracy: 0.734, gradient_norm : 3.0046385899868593
[2025-09-16 18:26:22,993][flp2p.graph_runner][INFO] - Test, Round 76 : loss => 1.2007959369391203,  accuracy: 0.5893
[2025-09-16 18:26:36,411][flp2p.graph_runner][INFO] - Train, Round 77 : loss => 0.6870182174742222,  accuracy: 0.7364833333333334, gradient_norm : 2.7813932834945048
[2025-09-16 18:26:47,769][flp2p.graph_runner][INFO] - Test, Round 77 : loss => 1.090706463831663,  accuracy: 0.6197
[2025-09-16 18:27:01,222][flp2p.graph_runner][INFO] - Train, Round 78 : loss => 0.6662435205429792,  accuracy: 0.7439166666666667, gradient_norm : 2.903331336713103
[2025-09-16 18:27:12,411][flp2p.graph_runner][INFO] - Test, Round 78 : loss => 1.1424099355906248,  accuracy: 0.5981
[2025-09-16 18:27:25,773][flp2p.graph_runner][INFO] - Train, Round 79 : loss => 0.6684126632362604,  accuracy: 0.73995, gradient_norm : 3.061302365834602
[2025-09-16 18:27:36,890][flp2p.graph_runner][INFO] - Test, Round 79 : loss => 1.196852735760808,  accuracy: 0.5962
[2025-09-16 18:27:50,374][flp2p.graph_runner][INFO] - Train, Round 80 : loss => 0.681331772506237,  accuracy: 0.7374666666666667, gradient_norm : 3.075748810817629
[2025-09-16 18:28:01,637][flp2p.graph_runner][INFO] - Test, Round 80 : loss => 1.2219028543651105,  accuracy: 0.5953
[2025-09-16 18:28:15,147][flp2p.graph_runner][INFO] - Train, Round 81 : loss => 0.6768485752344131,  accuracy: 0.7402, gradient_norm : 2.8611504836840482
[2025-09-16 18:28:26,297][flp2p.graph_runner][INFO] - Test, Round 81 : loss => 2.953672505477071,  accuracy: 0.6078
[2025-09-16 18:28:39,815][flp2p.graph_runner][INFO] - Train, Round 82 : loss => 0.8613858783245086,  accuracy: 0.74525, gradient_norm : 2.9871892335976065
[2025-09-16 18:28:51,125][flp2p.graph_runner][INFO] - Test, Round 82 : loss => 1.1362258893311024,  accuracy: 0.6144
[2025-09-16 18:29:04,531][flp2p.graph_runner][INFO] - Train, Round 83 : loss => 0.6593566923737526,  accuracy: 0.7466666666666667, gradient_norm : 2.9393467904894606
[2025-09-16 18:29:15,675][flp2p.graph_runner][INFO] - Test, Round 83 : loss => 1.144697861969471,  accuracy: 0.6092
[2025-09-16 18:29:29,196][flp2p.graph_runner][INFO] - Train, Round 84 : loss => 0.6542549727261067,  accuracy: 0.74975, gradient_norm : 3.1375513530817374
[2025-09-16 18:29:40,439][flp2p.graph_runner][INFO] - Test, Round 84 : loss => 1.2242442690074444,  accuracy: 0.5947
[2025-09-16 18:29:54,022][flp2p.graph_runner][INFO] - Train, Round 85 : loss => 0.6656545119583607,  accuracy: 0.74365, gradient_norm : 2.9529244203939116
[2025-09-16 18:30:05,142][flp2p.graph_runner][INFO] - Test, Round 85 : loss => 1.150631316536665,  accuracy: 0.6116
[2025-09-16 18:30:18,857][flp2p.graph_runner][INFO] - Train, Round 86 : loss => 0.6487546997815371,  accuracy: 0.7512666666666666, gradient_norm : 2.9707450050480886
[2025-09-16 18:30:30,007][flp2p.graph_runner][INFO] - Test, Round 86 : loss => 1.1368326764762402,  accuracy: 0.611
[2025-09-16 18:30:43,291][flp2p.graph_runner][INFO] - Train, Round 87 : loss => 0.6483768357485533,  accuracy: 0.75035, gradient_norm : 2.9921656552624487
[2025-09-16 18:30:54,634][flp2p.graph_runner][INFO] - Test, Round 87 : loss => 1.158353688621521,  accuracy: 0.608
[2025-09-16 18:31:08,371][flp2p.graph_runner][INFO] - Train, Round 88 : loss => 0.6457399487495422,  accuracy: 0.7515333333333334, gradient_norm : 2.847768621121109
[2025-09-16 18:31:19,584][flp2p.graph_runner][INFO] - Test, Round 88 : loss => 1.1273824157953263,  accuracy: 0.6191
[2025-09-16 18:31:33,112][flp2p.graph_runner][INFO] - Train, Round 89 : loss => 0.6325888315439224,  accuracy: 0.7567166666666667, gradient_norm : 3.1251453190148517
[2025-09-16 18:31:44,395][flp2p.graph_runner][INFO] - Test, Round 89 : loss => 1.154258140181005,  accuracy: 0.6074
[2025-09-16 18:31:57,983][flp2p.graph_runner][INFO] - Train, Round 90 : loss => 0.6375164605379104,  accuracy: 0.7516833333333334, gradient_norm : 3.0216040726329085
[2025-09-16 18:32:09,113][flp2p.graph_runner][INFO] - Test, Round 90 : loss => 1.15043348390162,  accuracy: 0.5996
[2025-09-16 18:32:22,665][flp2p.graph_runner][INFO] - Train, Round 91 : loss => 0.6454887930005789,  accuracy: 0.7506166666666667, gradient_norm : 2.779546240876256
[2025-09-16 18:32:33,893][flp2p.graph_runner][INFO] - Test, Round 91 : loss => 1.0722727730453014,  accuracy: 0.6278
[2025-09-16 18:32:47,444][flp2p.graph_runner][INFO] - Train, Round 92 : loss => 0.6356475880891085,  accuracy: 0.7589666666666667, gradient_norm : 2.8663002154954693
[2025-09-16 18:32:58,653][flp2p.graph_runner][INFO] - Test, Round 92 : loss => 1.1399948929667474,  accuracy: 0.6194
[2025-09-16 18:33:12,180][flp2p.graph_runner][INFO] - Train, Round 93 : loss => 0.6346846531331539,  accuracy: 0.7563, gradient_norm : 3.025981290277354
[2025-09-16 18:33:23,322][flp2p.graph_runner][INFO] - Test, Round 93 : loss => 1.1858128561004997,  accuracy: 0.6121
[2025-09-16 18:33:36,718][flp2p.graph_runner][INFO] - Train, Round 94 : loss => 0.6345327004790307,  accuracy: 0.75765, gradient_norm : 2.8388447947275903
[2025-09-16 18:33:48,100][flp2p.graph_runner][INFO] - Test, Round 94 : loss => 1.1105506258055569,  accuracy: 0.6287
[2025-09-16 18:34:01,611][flp2p.graph_runner][INFO] - Train, Round 95 : loss => 0.6170991961061955,  accuracy: 0.7625833333333333, gradient_norm : 2.857500975037925
[2025-09-16 18:34:12,848][flp2p.graph_runner][INFO] - Test, Round 95 : loss => 1.109629745155573,  accuracy: 0.6199
[2025-09-16 18:34:26,371][flp2p.graph_runner][INFO] - Train, Round 96 : loss => 0.6454012381881475,  accuracy: 0.7622666666666666, gradient_norm : 2.846148714091125
[2025-09-16 18:34:37,417][flp2p.graph_runner][INFO] - Test, Round 96 : loss => 1.1025800882697105,  accuracy: 0.6274
[2025-09-16 18:34:50,751][flp2p.graph_runner][INFO] - Train, Round 97 : loss => 0.6112901109457016,  accuracy: 0.7640666666666667, gradient_norm : 2.9065796129705244
[2025-09-16 18:35:01,883][flp2p.graph_runner][INFO] - Test, Round 97 : loss => 1.1269889268934727,  accuracy: 0.6265
[2025-09-16 18:35:15,449][flp2p.graph_runner][INFO] - Train, Round 98 : loss => 0.6146490125954152,  accuracy: 0.7638833333333334, gradient_norm : 4.519860112823206
[2025-09-16 18:35:26,730][flp2p.graph_runner][INFO] - Test, Round 98 : loss => 1.2256868340522051,  accuracy: 0.6082
[2025-09-16 18:35:40,147][flp2p.graph_runner][INFO] - Train, Round 99 : loss => 0.6345764235407114,  accuracy: 0.7577333333333334, gradient_norm : 3.0361072822224497
[2025-09-16 18:35:51,203][flp2p.graph_runner][INFO] - Test, Round 99 : loss => 1.2075803767174482,  accuracy: 0.6087
[2025-09-16 18:36:04,638][flp2p.graph_runner][INFO] - Train, Round 100 : loss => 0.6284283321648836,  accuracy: 0.7596666666666667, gradient_norm : 2.9529709206534016
[2025-09-16 18:36:15,886][flp2p.graph_runner][INFO] - Test, Round 100 : loss => 1.12948336070776,  accuracy: 0.6224
[2025-09-16 18:36:29,223][flp2p.graph_runner][INFO] - Train, Round 101 : loss => 0.6158709843605756,  accuracy: 0.7650666666666667, gradient_norm : 2.9326288412217965
[2025-09-16 18:36:40,415][flp2p.graph_runner][INFO] - Test, Round 101 : loss => 1.1608943590030074,  accuracy: 0.6185
[2025-09-16 18:36:53,901][flp2p.graph_runner][INFO] - Train, Round 102 : loss => 0.6265318118184805,  accuracy: 0.7649833333333333, gradient_norm : 2.975517162681361
[2025-09-16 18:37:05,014][flp2p.graph_runner][INFO] - Test, Round 102 : loss => 1.156684664583206,  accuracy: 0.6268
[2025-09-16 18:37:18,541][flp2p.graph_runner][INFO] - Train, Round 103 : loss => 0.6110928067713975,  accuracy: 0.76665, gradient_norm : 2.8499085870672327
[2025-09-16 18:37:29,720][flp2p.graph_runner][INFO] - Test, Round 103 : loss => 1.1179802566021682,  accuracy: 0.6253
[2025-09-16 18:37:43,041][flp2p.graph_runner][INFO] - Train, Round 104 : loss => 0.5973078498393297,  accuracy: 0.7691, gradient_norm : 2.9602020759799434
[2025-09-16 18:37:54,539][flp2p.graph_runner][INFO] - Test, Round 104 : loss => 1.1777850359857083,  accuracy: 0.6183
[2025-09-16 18:38:08,035][flp2p.graph_runner][INFO] - Train, Round 105 : loss => 0.603867424890399,  accuracy: 0.7685833333333333, gradient_norm : 2.817781278262246
[2025-09-16 18:38:19,176][flp2p.graph_runner][INFO] - Test, Round 105 : loss => 1.1633122956112028,  accuracy: 0.6287
[2025-09-16 18:38:32,635][flp2p.graph_runner][INFO] - Train, Round 106 : loss => 0.5880810443609953,  accuracy: 0.7736, gradient_norm : 2.801437469345119
[2025-09-16 18:38:43,976][flp2p.graph_runner][INFO] - Test, Round 106 : loss => 1.0801746152728795,  accuracy: 0.6326
[2025-09-16 18:38:57,385][flp2p.graph_runner][INFO] - Train, Round 107 : loss => 0.6017894212454558,  accuracy: 0.7744833333333333, gradient_norm : 2.9931771390537616
[2025-09-16 18:39:08,628][flp2p.graph_runner][INFO] - Test, Round 107 : loss => 1.1762725096255542,  accuracy: 0.6196
[2025-09-16 18:39:22,230][flp2p.graph_runner][INFO] - Train, Round 108 : loss => 0.5968806580901146,  accuracy: 0.7712, gradient_norm : 2.7935929670959343
[2025-09-16 18:39:33,410][flp2p.graph_runner][INFO] - Test, Round 108 : loss => 1.0623948791310192,  accuracy: 0.6432
[2025-09-16 18:39:46,903][flp2p.graph_runner][INFO] - Train, Round 109 : loss => 0.5743833697736264,  accuracy: 0.7783833333333333, gradient_norm : 2.962285343271974
[2025-09-16 18:39:58,188][flp2p.graph_runner][INFO] - Test, Round 109 : loss => 1.1198083579301834,  accuracy: 0.6324
[2025-09-16 18:40:11,823][flp2p.graph_runner][INFO] - Train, Round 110 : loss => 0.5843359509408474,  accuracy: 0.7763666666666666, gradient_norm : 3.065333709231199
[2025-09-16 18:40:23,160][flp2p.graph_runner][INFO] - Test, Round 110 : loss => 1.2271130926907063,  accuracy: 0.6219
[2025-09-16 18:40:36,628][flp2p.graph_runner][INFO] - Train, Round 111 : loss => 0.5896711443066597,  accuracy: 0.7754166666666666, gradient_norm : 2.8702432061427046
[2025-09-16 18:40:48,107][flp2p.graph_runner][INFO] - Test, Round 111 : loss => 1.0889541869282722,  accuracy: 0.6313
[2025-09-16 18:41:01,668][flp2p.graph_runner][INFO] - Train, Round 112 : loss => 0.5706042397022247,  accuracy: 0.7790333333333334, gradient_norm : 2.8827503459627626
[2025-09-16 18:41:13,062][flp2p.graph_runner][INFO] - Test, Round 112 : loss => 1.0748107008725405,  accuracy: 0.6477
[2025-09-16 18:41:26,822][flp2p.graph_runner][INFO] - Train, Round 113 : loss => 0.5693291130214929,  accuracy: 0.782, gradient_norm : 3.0228638231136493
[2025-09-16 18:41:38,055][flp2p.graph_runner][INFO] - Test, Round 113 : loss => 1.1823708465367555,  accuracy: 0.6299
[2025-09-16 18:41:51,630][flp2p.graph_runner][INFO] - Train, Round 114 : loss => 0.5828447360396385,  accuracy: 0.7779333333333334, gradient_norm : 2.9037757593303457
[2025-09-16 18:42:02,867][flp2p.graph_runner][INFO] - Test, Round 114 : loss => 1.1189523611724377,  accuracy: 0.6336
[2025-09-16 18:42:16,465][flp2p.graph_runner][INFO] - Train, Round 115 : loss => 0.5680035067051649,  accuracy: 0.7827, gradient_norm : 2.7675239933479965
[2025-09-16 18:42:27,675][flp2p.graph_runner][INFO] - Test, Round 115 : loss => 1.0640608871519566,  accuracy: 0.644
[2025-09-16 18:42:41,279][flp2p.graph_runner][INFO] - Train, Round 116 : loss => 0.552822064653039,  accuracy: 0.7851666666666667, gradient_norm : 2.948871295052626
[2025-09-16 18:42:52,711][flp2p.graph_runner][INFO] - Test, Round 116 : loss => 1.1216901914089918,  accuracy: 0.6373
[2025-09-16 18:43:06,311][flp2p.graph_runner][INFO] - Train, Round 117 : loss => 0.5616671229153871,  accuracy: 0.7832333333333333, gradient_norm : 2.700816833556532
[2025-09-16 18:43:17,616][flp2p.graph_runner][INFO] - Test, Round 117 : loss => 1.0696774589851499,  accuracy: 0.6406
[2025-09-16 18:43:31,282][flp2p.graph_runner][INFO] - Train, Round 118 : loss => 0.5445279447883368,  accuracy: 0.7888333333333334, gradient_norm : 2.837999238154112
[2025-09-16 18:43:42,615][flp2p.graph_runner][INFO] - Test, Round 118 : loss => 1.0952338431715964,  accuracy: 0.6347
[2025-09-16 18:43:56,113][flp2p.graph_runner][INFO] - Train, Round 119 : loss => 0.5545333078354597,  accuracy: 0.7881833333333333, gradient_norm : 2.887910324007403
[2025-09-16 18:44:07,312][flp2p.graph_runner][INFO] - Test, Round 119 : loss => 1.1943229010805487,  accuracy: 0.6293
[2025-09-16 18:44:21,015][flp2p.graph_runner][INFO] - Train, Round 120 : loss => 0.5563380238413811,  accuracy: 0.78945, gradient_norm : 2.93119044250553
[2025-09-16 18:44:32,232][flp2p.graph_runner][INFO] - Test, Round 120 : loss => 1.112876947927475,  accuracy: 0.6395
[2025-09-16 18:44:45,723][flp2p.graph_runner][INFO] - Train, Round 121 : loss => 0.5599996813237668,  accuracy: 0.78845, gradient_norm : 2.809523954282511
[2025-09-16 18:44:57,145][flp2p.graph_runner][INFO] - Test, Round 121 : loss => 1.0717896832838654,  accuracy: 0.6462
[2025-09-16 18:45:10,676][flp2p.graph_runner][INFO] - Train, Round 122 : loss => 0.54081624622643,  accuracy: 0.7926, gradient_norm : 2.926281970230267
[2025-09-16 18:45:21,796][flp2p.graph_runner][INFO] - Test, Round 122 : loss => 1.1078273401141168,  accuracy: 0.6402
[2025-09-16 18:45:35,342][flp2p.graph_runner][INFO] - Train, Round 123 : loss => 0.547775608792901,  accuracy: 0.7914166666666667, gradient_norm : 2.9397117878196615
[2025-09-16 18:45:46,431][flp2p.graph_runner][INFO] - Test, Round 123 : loss => 1.1272063614338637,  accuracy: 0.6377
[2025-09-16 18:45:59,911][flp2p.graph_runner][INFO] - Train, Round 124 : loss => 0.5449436353519559,  accuracy: 0.7915333333333333, gradient_norm : 2.7766364621821777
[2025-09-16 18:46:11,075][flp2p.graph_runner][INFO] - Test, Round 124 : loss => 1.097329178224504,  accuracy: 0.6457
[2025-09-16 18:46:24,678][flp2p.graph_runner][INFO] - Train, Round 125 : loss => 0.5366938564032316,  accuracy: 0.7962166666666667, gradient_norm : 2.8169152587059023
[2025-09-16 18:46:36,017][flp2p.graph_runner][INFO] - Test, Round 125 : loss => 1.0938337699472904,  accuracy: 0.6441
[2025-09-16 18:46:49,586][flp2p.graph_runner][INFO] - Train, Round 126 : loss => 0.5349192866533995,  accuracy: 0.79515, gradient_norm : 2.7773500438129015
[2025-09-16 18:47:00,868][flp2p.graph_runner][INFO] - Test, Round 126 : loss => 1.060422195264697,  accuracy: 0.6489
[2025-09-16 18:47:14,439][flp2p.graph_runner][INFO] - Train, Round 127 : loss => 0.5243473232090473,  accuracy: 0.79975, gradient_norm : 2.9187229977690556
[2025-09-16 18:47:25,579][flp2p.graph_runner][INFO] - Test, Round 127 : loss => 1.1123865753412248,  accuracy: 0.6442
[2025-09-16 18:47:39,098][flp2p.graph_runner][INFO] - Train, Round 128 : loss => 0.5330766807645559,  accuracy: 0.7965333333333333, gradient_norm : 2.7544564205077995
[2025-09-16 18:47:50,418][flp2p.graph_runner][INFO] - Test, Round 128 : loss => 1.0919125096917153,  accuracy: 0.652
[2025-09-16 18:48:03,876][flp2p.graph_runner][INFO] - Train, Round 129 : loss => 0.5245234285891056,  accuracy: 0.8003666666666667, gradient_norm : 2.830142614301288
[2025-09-16 18:48:15,140][flp2p.graph_runner][INFO] - Test, Round 129 : loss => 1.092168091930449,  accuracy: 0.6455
[2025-09-16 18:48:28,633][flp2p.graph_runner][INFO] - Train, Round 130 : loss => 0.5723150035142899,  accuracy: 0.7991333333333334, gradient_norm : 2.9325891273015707
[2025-09-16 18:48:39,748][flp2p.graph_runner][INFO] - Test, Round 130 : loss => 1.1219214856415987,  accuracy: 0.6466
[2025-09-16 18:48:53,218][flp2p.graph_runner][INFO] - Train, Round 131 : loss => 0.5300241265594959,  accuracy: 0.7980333333333334, gradient_norm : 2.712863684818116
[2025-09-16 18:49:04,428][flp2p.graph_runner][INFO] - Test, Round 131 : loss => 1.1702663416713477,  accuracy: 0.65
[2025-09-16 18:49:17,964][flp2p.graph_runner][INFO] - Train, Round 132 : loss => 0.5105834911465645,  accuracy: 0.80405, gradient_norm : 2.680354576815985
[2025-09-16 18:49:29,260][flp2p.graph_runner][INFO] - Test, Round 132 : loss => 1.0470675872072577,  accuracy: 0.6526
[2025-09-16 18:49:42,688][flp2p.graph_runner][INFO] - Train, Round 133 : loss => 0.5072956888489425,  accuracy: 0.8066, gradient_norm : 2.803062665087349
[2025-09-16 18:49:54,101][flp2p.graph_runner][INFO] - Test, Round 133 : loss => 1.0964323534056544,  accuracy: 0.6495
[2025-09-16 18:50:07,761][flp2p.graph_runner][INFO] - Train, Round 134 : loss => 0.5135800559073687,  accuracy: 0.8045666666666667, gradient_norm : 2.8527754962982654
[2025-09-16 18:50:18,886][flp2p.graph_runner][INFO] - Test, Round 134 : loss => 1.181749777932465,  accuracy: 0.6417
[2025-09-16 18:50:32,618][flp2p.graph_runner][INFO] - Train, Round 135 : loss => 0.5152355875521898,  accuracy: 0.8048333333333333, gradient_norm : 2.766341892020871
[2025-09-16 18:50:43,962][flp2p.graph_runner][INFO] - Test, Round 135 : loss => 1.141171083831787,  accuracy: 0.6555
[2025-09-16 18:50:57,502][flp2p.graph_runner][INFO] - Train, Round 136 : loss => 0.5036620546281337,  accuracy: 0.8082333333333334, gradient_norm : 2.9297838649957684
[2025-09-16 18:51:08,660][flp2p.graph_runner][INFO] - Test, Round 136 : loss => 1.15971895891428,  accuracy: 0.6466
[2025-09-16 18:51:22,367][flp2p.graph_runner][INFO] - Train, Round 137 : loss => 0.5104988412410021,  accuracy: 0.8057666666666666, gradient_norm : 2.7609529877588597
[2025-09-16 18:51:33,529][flp2p.graph_runner][INFO] - Test, Round 137 : loss => 1.1237030538856982,  accuracy: 0.661
[2025-09-16 18:51:46,959][flp2p.graph_runner][INFO] - Train, Round 138 : loss => 0.49649000211060046,  accuracy: 0.8098333333333333, gradient_norm : 2.7799189716682737
[2025-09-16 18:51:58,301][flp2p.graph_runner][INFO] - Test, Round 138 : loss => 1.0980167978107929,  accuracy: 0.6531
[2025-09-16 18:52:11,833][flp2p.graph_runner][INFO] - Train, Round 139 : loss => 0.49866321782767775,  accuracy: 0.8098, gradient_norm : 2.7501791724313343
[2025-09-16 18:52:22,913][flp2p.graph_runner][INFO] - Test, Round 139 : loss => 1.082264083147049,  accuracy: 0.6558
[2025-09-16 18:52:36,463][flp2p.graph_runner][INFO] - Train, Round 140 : loss => 0.49574947875738146,  accuracy: 0.8132833333333334, gradient_norm : 2.771099736196598
[2025-09-16 18:52:47,686][flp2p.graph_runner][INFO] - Test, Round 140 : loss => 1.1424848255544902,  accuracy: 0.6581
[2025-09-16 18:53:01,180][flp2p.graph_runner][INFO] - Train, Round 141 : loss => 0.49031482180953023,  accuracy: 0.812, gradient_norm : 2.9065062961359813
[2025-09-16 18:53:12,368][flp2p.graph_runner][INFO] - Test, Round 141 : loss => 1.3056258438304067,  accuracy: 0.6465
[2025-09-16 18:53:25,966][flp2p.graph_runner][INFO] - Train, Round 142 : loss => 0.4993964747041464,  accuracy: 0.81065, gradient_norm : 2.8401335703882715
[2025-09-16 18:53:37,128][flp2p.graph_runner][INFO] - Test, Round 142 : loss => 1.2934520394280553,  accuracy: 0.6537
[2025-09-16 18:53:50,797][flp2p.graph_runner][INFO] - Train, Round 143 : loss => 0.4935488491505384,  accuracy: 0.81325, gradient_norm : 2.8979681211241033
[2025-09-16 18:54:02,235][flp2p.graph_runner][INFO] - Test, Round 143 : loss => 1.1461677309304477,  accuracy: 0.6464
[2025-09-16 18:54:15,778][flp2p.graph_runner][INFO] - Train, Round 144 : loss => 0.4953400777578354,  accuracy: 0.8133333333333334, gradient_norm : 2.766111260021429
[2025-09-16 18:54:27,106][flp2p.graph_runner][INFO] - Test, Round 144 : loss => 1.0873941314920783,  accuracy: 0.6595
[2025-09-16 18:54:40,690][flp2p.graph_runner][INFO] - Train, Round 145 : loss => 0.48642313408851623,  accuracy: 0.81615, gradient_norm : 2.655269360210775
[2025-09-16 18:54:51,873][flp2p.graph_runner][INFO] - Test, Round 145 : loss => 1.117794574803114,  accuracy: 0.6716
[2025-09-16 18:55:05,361][flp2p.graph_runner][INFO] - Train, Round 146 : loss => 0.4741496978998184,  accuracy: 0.8187833333333333, gradient_norm : 2.8355238792486164
[2025-09-16 18:55:16,398][flp2p.graph_runner][INFO] - Test, Round 146 : loss => 1.349443763446808,  accuracy: 0.6526
[2025-09-16 18:55:29,977][flp2p.graph_runner][INFO] - Train, Round 147 : loss => 0.4816798745691776,  accuracy: 0.8155333333333333, gradient_norm : 2.859738453335347
[2025-09-16 18:55:41,167][flp2p.graph_runner][INFO] - Test, Round 147 : loss => 1.143738226109743,  accuracy: 0.6491
[2025-09-16 18:55:54,699][flp2p.graph_runner][INFO] - Train, Round 148 : loss => 0.48803476682305336,  accuracy: 0.81805, gradient_norm : 2.856804315711818
[2025-09-16 18:56:05,838][flp2p.graph_runner][INFO] - Test, Round 148 : loss => 1.1097795947641134,  accuracy: 0.6533
[2025-09-16 18:56:19,370][flp2p.graph_runner][INFO] - Train, Round 149 : loss => 0.4818512789160013,  accuracy: 0.8196833333333333, gradient_norm : 2.881205941945897
[2025-09-16 18:56:30,555][flp2p.graph_runner][INFO] - Test, Round 149 : loss => 1.192981826390326,  accuracy: 0.653
[2025-09-16 18:56:44,067][flp2p.graph_runner][INFO] - Train, Round 150 : loss => 2.4197648539692165,  accuracy: 0.81815, gradient_norm : 2.720148949820807
[2025-09-16 18:56:55,417][flp2p.graph_runner][INFO] - Test, Round 150 : loss => 1.0400127190291881,  accuracy: 0.6649
[2025-09-16 18:57:08,807][flp2p.graph_runner][INFO] - Train, Round 151 : loss => 0.4655956793501973,  accuracy: 0.8227333333333333, gradient_norm : 2.7101019394239554
[2025-09-16 18:57:19,937][flp2p.graph_runner][INFO] - Test, Round 151 : loss => 1.0358009167894722,  accuracy: 0.6647
[2025-09-16 18:57:33,412][flp2p.graph_runner][INFO] - Train, Round 152 : loss => 0.46911084712296724,  accuracy: 0.8245333333333333, gradient_norm : 2.8580891227228697
[2025-09-16 18:57:44,671][flp2p.graph_runner][INFO] - Test, Round 152 : loss => 1.075647551678121,  accuracy: 0.6616
[2025-09-16 18:57:58,299][flp2p.graph_runner][INFO] - Train, Round 153 : loss => 0.46915245178341863,  accuracy: 0.8224, gradient_norm : 2.939190035815221
[2025-09-16 18:58:09,493][flp2p.graph_runner][INFO] - Test, Round 153 : loss => 1.1119197002351284,  accuracy: 0.6599
[2025-09-16 18:58:22,664][flp2p.graph_runner][INFO] - Train, Round 154 : loss => 0.4725586183592677,  accuracy: 0.8218666666666666, gradient_norm : 2.9492182256360064
[2025-09-16 18:58:32,179][flp2p.graph_runner][INFO] - Test, Round 154 : loss => 1.1357283746629954,  accuracy: 0.6489
[2025-09-16 18:58:44,434][flp2p.graph_runner][INFO] - Train, Round 155 : loss => 0.4710706874355674,  accuracy: 0.823, gradient_norm : 2.6923717256479294
[2025-09-16 18:58:54,041][flp2p.graph_runner][INFO] - Test, Round 155 : loss => 1.0270833648443223,  accuracy: 0.6648
[2025-09-16 18:59:06,321][flp2p.graph_runner][INFO] - Train, Round 156 : loss => 0.44987603978812696,  accuracy: 0.8284, gradient_norm : 2.656287273254749
[2025-09-16 18:59:15,814][flp2p.graph_runner][INFO] - Test, Round 156 : loss => 1.0301145241603256,  accuracy: 0.6714
[2025-09-16 18:59:28,134][flp2p.graph_runner][INFO] - Train, Round 157 : loss => 0.4471140985712409,  accuracy: 0.8307166666666667, gradient_norm : 2.506268702675709
[2025-09-16 18:59:37,594][flp2p.graph_runner][INFO] - Test, Round 157 : loss => 0.9872795645862817,  accuracy: 0.6737
[2025-09-16 18:59:49,922][flp2p.graph_runner][INFO] - Train, Round 158 : loss => 0.43606346636265514,  accuracy: 0.8333166666666667, gradient_norm : 2.688863106596967
[2025-09-16 18:59:59,474][flp2p.graph_runner][INFO] - Test, Round 158 : loss => 1.0627582819372416,  accuracy: 0.6659
[2025-09-16 19:00:11,837][flp2p.graph_runner][INFO] - Train, Round 159 : loss => 0.44689910992980003,  accuracy: 0.83055, gradient_norm : 2.765649867455873
[2025-09-16 19:00:21,319][flp2p.graph_runner][INFO] - Test, Round 159 : loss => 1.064245427314937,  accuracy: 0.6705
[2025-09-16 19:00:33,532][flp2p.graph_runner][INFO] - Train, Round 160 : loss => 0.4486649104356766,  accuracy: 0.8309833333333333, gradient_norm : 2.732674207335543
[2025-09-16 19:00:43,090][flp2p.graph_runner][INFO] - Test, Round 160 : loss => 1.0272326476275921,  accuracy: 0.679
[2025-09-16 19:00:55,339][flp2p.graph_runner][INFO] - Train, Round 161 : loss => 0.4435228152424097,  accuracy: 0.8329, gradient_norm : 2.7074388936504734
[2025-09-16 19:01:04,943][flp2p.graph_runner][INFO] - Test, Round 161 : loss => 1.3605662614017724,  accuracy: 0.6769
[2025-09-16 19:01:17,237][flp2p.graph_runner][INFO] - Train, Round 162 : loss => 0.49097386419773104,  accuracy: 0.8350833333333333, gradient_norm : 2.613854854564359
[2025-09-16 19:01:26,860][flp2p.graph_runner][INFO] - Test, Round 162 : loss => 1.0148427710264922,  accuracy: 0.6748
[2025-09-16 19:01:39,262][flp2p.graph_runner][INFO] - Train, Round 163 : loss => 0.4323862786144018,  accuracy: 0.8379333333333333, gradient_norm : 2.971248495516244
[2025-09-16 19:01:48,827][flp2p.graph_runner][INFO] - Test, Round 163 : loss => 1.1517383246868849,  accuracy: 0.647
[2025-09-16 19:02:01,125][flp2p.graph_runner][INFO] - Train, Round 164 : loss => 0.4504809900522232,  accuracy: 0.83055, gradient_norm : 2.709993968665913
[2025-09-16 19:02:10,676][flp2p.graph_runner][INFO] - Test, Round 164 : loss => 1.0591218197852372,  accuracy: 0.6741
[2025-09-16 19:02:23,081][flp2p.graph_runner][INFO] - Train, Round 165 : loss => 0.43256640167161825,  accuracy: 0.8372166666666667, gradient_norm : 2.6136928445738272
[2025-09-16 19:02:32,660][flp2p.graph_runner][INFO] - Test, Round 165 : loss => 1.0380192319139838,  accuracy: 0.672
[2025-09-16 19:02:45,034][flp2p.graph_runner][INFO] - Train, Round 166 : loss => 0.42549619427323343,  accuracy: 0.8393833333333334, gradient_norm : 2.5965269677024287
[2025-09-16 19:02:54,649][flp2p.graph_runner][INFO] - Test, Round 166 : loss => 1.0290002432748675,  accuracy: 0.6794
[2025-09-16 19:03:06,992][flp2p.graph_runner][INFO] - Train, Round 167 : loss => 0.42371853987127545,  accuracy: 0.8409833333333333, gradient_norm : 2.7469732374130533
[2025-09-16 19:03:16,733][flp2p.graph_runner][INFO] - Test, Round 167 : loss => 1.0708044768720866,  accuracy: 0.6704
[2025-09-16 19:03:28,965][flp2p.graph_runner][INFO] - Train, Round 168 : loss => 0.43163401520997285,  accuracy: 0.8382833333333334, gradient_norm : 2.7164137383805507
[2025-09-16 19:03:38,603][flp2p.graph_runner][INFO] - Test, Round 168 : loss => 1.0593099161222577,  accuracy: 0.6676
[2025-09-16 19:03:50,980][flp2p.graph_runner][INFO] - Train, Round 169 : loss => 0.4235200151279569,  accuracy: 0.8406833333333333, gradient_norm : 2.81429234247755
[2025-09-16 19:04:00,636][flp2p.graph_runner][INFO] - Test, Round 169 : loss => 1.0952681382611393,  accuracy: 0.666
[2025-09-16 19:04:13,078][flp2p.graph_runner][INFO] - Train, Round 170 : loss => 0.4291460616514087,  accuracy: 0.8397666666666667, gradient_norm : 2.7465023809985887
[2025-09-16 19:04:22,560][flp2p.graph_runner][INFO] - Test, Round 170 : loss => 1.0584363078162073,  accuracy: 0.671
[2025-09-16 19:04:34,739][flp2p.graph_runner][INFO] - Train, Round 171 : loss => 0.4190974292755127,  accuracy: 0.8435166666666667, gradient_norm : 2.7738586833873238
[2025-09-16 19:04:44,271][flp2p.graph_runner][INFO] - Test, Round 171 : loss => 1.1137739327654244,  accuracy: 0.6635
[2025-09-16 19:04:56,599][flp2p.graph_runner][INFO] - Train, Round 172 : loss => 0.42646501933783293,  accuracy: 0.8414666666666667, gradient_norm : 2.710928788391623
[2025-09-16 19:05:06,043][flp2p.graph_runner][INFO] - Test, Round 172 : loss => 1.0664249277204274,  accuracy: 0.6697
[2025-09-16 19:05:18,420][flp2p.graph_runner][INFO] - Train, Round 173 : loss => 0.41649088662862777,  accuracy: 0.8437, gradient_norm : 2.704450282072058
[2025-09-16 19:05:27,977][flp2p.graph_runner][INFO] - Test, Round 173 : loss => 5.516436048571766,  accuracy: 0.6692
[2025-09-16 19:05:40,144][flp2p.graph_runner][INFO] - Train, Round 174 : loss => 33.011252727098764,  accuracy: 0.8466, gradient_norm : 2.7489624936421784
[2025-09-16 19:05:49,673][flp2p.graph_runner][INFO] - Test, Round 174 : loss => 1.0976281986474992,  accuracy: 0.6747
[2025-09-16 19:06:01,952][flp2p.graph_runner][INFO] - Train, Round 175 : loss => 0.4160953776910901,  accuracy: 0.8454, gradient_norm : 2.8381820627119208
[2025-09-16 19:06:11,539][flp2p.graph_runner][INFO] - Test, Round 175 : loss => 1.1264789211839437,  accuracy: 0.6727
[2025-09-16 19:06:23,925][flp2p.graph_runner][INFO] - Train, Round 176 : loss => 0.41807580371946096,  accuracy: 0.8439333333333333, gradient_norm : 2.7390363213803517
[2025-09-16 19:06:33,379][flp2p.graph_runner][INFO] - Test, Round 176 : loss => 1.0963589621394874,  accuracy: 0.6741
[2025-09-16 19:06:45,743][flp2p.graph_runner][INFO] - Train, Round 177 : loss => 0.4095797574967146,  accuracy: 0.8471833333333333, gradient_norm : 2.613399267833988
[2025-09-16 19:06:55,472][flp2p.graph_runner][INFO] - Test, Round 177 : loss => 1.040249093312025,  accuracy: 0.6833
[2025-09-16 19:07:07,739][flp2p.graph_runner][INFO] - Train, Round 178 : loss => 0.4008813031464815,  accuracy: 0.8509, gradient_norm : 2.660538293546111
[2025-09-16 19:07:17,359][flp2p.graph_runner][INFO] - Test, Round 178 : loss => 1.0560424096822738,  accuracy: 0.6732
[2025-09-16 19:07:29,609][flp2p.graph_runner][INFO] - Train, Round 179 : loss => 0.401861025840044,  accuracy: 0.8502666666666666, gradient_norm : 2.82267070722565
[2025-09-16 19:07:39,292][flp2p.graph_runner][INFO] - Test, Round 179 : loss => 1.120249571311474,  accuracy: 0.6668
[2025-09-16 19:07:51,679][flp2p.graph_runner][INFO] - Train, Round 180 : loss => 0.40957170858234165,  accuracy: 0.8466833333333333, gradient_norm : 2.5799637948484735
[2025-09-16 19:08:01,199][flp2p.graph_runner][INFO] - Test, Round 180 : loss => 1.07460840228647,  accuracy: 0.6714
[2025-09-16 19:08:13,425][flp2p.graph_runner][INFO] - Train, Round 181 : loss => 0.3989774718210101,  accuracy: 0.85295, gradient_norm : 2.669908199155243
[2025-09-16 19:08:23,066][flp2p.graph_runner][INFO] - Test, Round 181 : loss => 1.045863445201516,  accuracy: 0.6862
[2025-09-16 19:08:35,357][flp2p.graph_runner][INFO] - Train, Round 182 : loss => 0.3951429254189134,  accuracy: 0.8529, gradient_norm : 2.591154419430964
[2025-09-16 19:08:45,079][flp2p.graph_runner][INFO] - Test, Round 182 : loss => 1.0525407700493932,  accuracy: 0.6774
[2025-09-16 19:08:57,361][flp2p.graph_runner][INFO] - Train, Round 183 : loss => 0.38735099536925555,  accuracy: 0.8557, gradient_norm : 2.4856667060517297
[2025-09-16 19:09:06,937][flp2p.graph_runner][INFO] - Test, Round 183 : loss => 1.015173113925755,  accuracy: 0.6817
[2025-09-16 19:09:19,283][flp2p.graph_runner][INFO] - Train, Round 184 : loss => 0.38208299899473785,  accuracy: 0.8581, gradient_norm : 2.5952215136779193
[2025-09-16 19:09:28,752][flp2p.graph_runner][INFO] - Test, Round 184 : loss => 1.0526944137766958,  accuracy: 0.6841
[2025-09-16 19:09:41,043][flp2p.graph_runner][INFO] - Train, Round 185 : loss => 0.3885575027242303,  accuracy: 0.8566333333333334, gradient_norm : 2.6368848748399603
[2025-09-16 19:09:50,612][flp2p.graph_runner][INFO] - Test, Round 185 : loss => 1.0803341415286065,  accuracy: 0.6812
[2025-09-16 19:10:02,911][flp2p.graph_runner][INFO] - Train, Round 186 : loss => 0.3923159121796489,  accuracy: 0.8557333333333333, gradient_norm : 2.6856928611022988
[2025-09-16 19:10:12,455][flp2p.graph_runner][INFO] - Test, Round 186 : loss => 1.0656013211429118,  accuracy: 0.6806
[2025-09-16 19:10:24,803][flp2p.graph_runner][INFO] - Train, Round 187 : loss => 0.38804178772866726,  accuracy: 0.8579833333333333, gradient_norm : 2.6071556670666216
[2025-09-16 19:10:34,486][flp2p.graph_runner][INFO] - Test, Round 187 : loss => 1.0374769406929611,  accuracy: 0.6841
[2025-09-16 19:10:46,933][flp2p.graph_runner][INFO] - Train, Round 188 : loss => 0.382903940834105,  accuracy: 0.8582833333333333, gradient_norm : 2.723415479450446
[2025-09-16 19:10:56,588][flp2p.graph_runner][INFO] - Test, Round 188 : loss => 1.0802348227724432,  accuracy: 0.6775
[2025-09-16 19:11:08,933][flp2p.graph_runner][INFO] - Train, Round 189 : loss => 0.38405137515813115,  accuracy: 0.8582666666666666, gradient_norm : 2.6891339431054972
[2025-09-16 19:11:18,487][flp2p.graph_runner][INFO] - Test, Round 189 : loss => 1.056484523883462,  accuracy: 0.6787
[2025-09-16 19:11:30,903][flp2p.graph_runner][INFO] - Train, Round 190 : loss => 0.377515498906374,  accuracy: 0.8598833333333333, gradient_norm : 2.7300013385668795
[2025-09-16 19:11:40,516][flp2p.graph_runner][INFO] - Test, Round 190 : loss => 1.0901928198456765,  accuracy: 0.6777
[2025-09-16 19:11:52,865][flp2p.graph_runner][INFO] - Train, Round 191 : loss => 0.37816077321767805,  accuracy: 0.8597, gradient_norm : 2.568446271265297
[2025-09-16 19:12:02,478][flp2p.graph_runner][INFO] - Test, Round 191 : loss => 1.041095358863473,  accuracy: 0.6906
[2025-09-16 19:12:14,722][flp2p.graph_runner][INFO] - Train, Round 192 : loss => 0.36902679137140515,  accuracy: 0.8627333333333334, gradient_norm : 2.7156736572230162
[2025-09-16 19:12:24,238][flp2p.graph_runner][INFO] - Test, Round 192 : loss => 1.0911641500279308,  accuracy: 0.6753
[2025-09-16 19:12:36,623][flp2p.graph_runner][INFO] - Train, Round 193 : loss => 0.3773875405341387,  accuracy: 0.8626, gradient_norm : 2.7742991804034154
[2025-09-16 19:12:45,978][flp2p.graph_runner][INFO] - Test, Round 193 : loss => 1.1038541442349552,  accuracy: 0.6797
[2025-09-16 19:12:58,431][flp2p.graph_runner][INFO] - Train, Round 194 : loss => 0.37658960274606945,  accuracy: 0.8619666666666667, gradient_norm : 2.543573165760095
[2025-09-16 19:13:08,139][flp2p.graph_runner][INFO] - Test, Round 194 : loss => 1.0789390224277973,  accuracy: 0.6775
[2025-09-16 19:13:20,344][flp2p.graph_runner][INFO] - Train, Round 195 : loss => 0.36555357607454064,  accuracy: 0.8656833333333334, gradient_norm : 2.6571104547580897
[2025-09-16 19:13:30,093][flp2p.graph_runner][INFO] - Test, Round 195 : loss => 1.0719535202607513,  accuracy: 0.6843
[2025-09-16 19:13:42,377][flp2p.graph_runner][INFO] - Train, Round 196 : loss => 0.36737888646125794,  accuracy: 0.865, gradient_norm : 2.7096255943315866
[2025-09-16 19:13:51,998][flp2p.graph_runner][INFO] - Test, Round 196 : loss => 1.1053652914315462,  accuracy: 0.6798
[2025-09-16 19:14:04,323][flp2p.graph_runner][INFO] - Train, Round 197 : loss => 0.36990061689168213,  accuracy: 0.8655833333333334, gradient_norm : 2.7953802466904443
[2025-09-16 19:14:13,754][flp2p.graph_runner][INFO] - Test, Round 197 : loss => 1.154803503882885,  accuracy: 0.673
[2025-09-16 19:14:26,030][flp2p.graph_runner][INFO] - Train, Round 198 : loss => 0.3767195186689496,  accuracy: 0.8632833333333333, gradient_norm : 2.5292963389806484
[2025-09-16 19:14:35,610][flp2p.graph_runner][INFO] - Test, Round 198 : loss => 1.0381814568907022,  accuracy: 0.6927
[2025-09-16 19:14:47,952][flp2p.graph_runner][INFO] - Train, Round 199 : loss => 0.3562187308780849,  accuracy: 0.8698333333333333, gradient_norm : 2.677987274251219
[2025-09-16 19:14:57,452][flp2p.graph_runner][INFO] - Test, Round 199 : loss => 1.1158917830139399,  accuracy: 0.6787
[2025-09-16 19:14:57,460][__main__][INFO] - Train, Round 001: loss=2.3030, accuracy=0.1000, gradient_norm=0.4746, 
[2025-09-16 19:14:57,460][__main__][INFO] - Train, Round 002: loss=2.2915, accuracy=0.1283, gradient_norm=0.4870, 
[2025-09-16 19:14:57,460][__main__][INFO] - Train, Round 003: loss=2.2793, accuracy=0.1920, gradient_norm=0.5478, 
[2025-09-16 19:14:57,460][__main__][INFO] - Train, Round 004: loss=2.2613, accuracy=0.2090, gradient_norm=0.6592, 
[2025-09-16 19:14:57,460][__main__][INFO] - Train, Round 005: loss=2.2317, accuracy=0.2372, gradient_norm=0.8021, 
[2025-09-16 19:14:57,460][__main__][INFO] - Train, Round 006: loss=2.1953, accuracy=0.2939, gradient_norm=1.0166, 
[2025-09-16 19:14:57,460][__main__][INFO] - Train, Round 007: loss=2.2069, accuracy=0.2371, gradient_norm=1.1680, 
[2025-09-16 19:14:57,460][__main__][INFO] - Train, Round 008: loss=2.2000, accuracy=0.2437, gradient_norm=1.1683, 
[2025-09-16 19:14:57,460][__main__][INFO] - Train, Round 009: loss=2.1931, accuracy=0.2607, gradient_norm=1.2471, 
[2025-09-16 19:14:57,460][__main__][INFO] - Train, Round 010: loss=2.1512, accuracy=0.2238, gradient_norm=1.2761, 
[2025-09-16 19:14:57,460][__main__][INFO] - Train, Round 011: loss=2.0999, accuracy=0.2965, gradient_norm=1.4410, 
[2025-09-16 19:14:57,460][__main__][INFO] - Train, Round 012: loss=2.1028, accuracy=0.3063, gradient_norm=1.5694, 
[2025-09-16 19:14:57,460][__main__][INFO] - Train, Round 013: loss=2.0582, accuracy=0.2958, gradient_norm=1.4351, 
[2025-09-16 19:14:57,460][__main__][INFO] - Train, Round 014: loss=1.9710, accuracy=0.3068, gradient_norm=1.6225, 
[2025-09-16 19:14:57,460][__main__][INFO] - Train, Round 015: loss=1.9211, accuracy=0.3175, gradient_norm=1.8305, 
[2025-09-16 19:14:57,460][__main__][INFO] - Train, Round 016: loss=1.8449, accuracy=0.3501, gradient_norm=2.4422, 
[2025-09-16 19:14:57,460][__main__][INFO] - Train, Round 017: loss=2.9041, accuracy=0.3129, gradient_norm=2.5069, 
[2025-09-16 19:14:57,460][__main__][INFO] - Train, Round 018: loss=1.7917, accuracy=0.3611, gradient_norm=2.3644, 
[2025-09-16 19:14:57,460][__main__][INFO] - Train, Round 019: loss=1.7267, accuracy=0.3983, gradient_norm=2.4995, 
[2025-09-16 19:14:57,460][__main__][INFO] - Train, Round 020: loss=1.6513, accuracy=0.4210, gradient_norm=2.8757, 
[2025-09-16 19:14:57,460][__main__][INFO] - Train, Round 021: loss=1.6216, accuracy=0.4376, gradient_norm=3.0111, 
[2025-09-16 19:14:57,460][__main__][INFO] - Train, Round 022: loss=1.5322, accuracy=0.4638, gradient_norm=3.1535, 
[2025-09-16 19:14:57,460][__main__][INFO] - Train, Round 023: loss=1.4641, accuracy=0.4940, gradient_norm=3.0448, 
[2025-09-16 19:14:57,460][__main__][INFO] - Train, Round 024: loss=1.3471, accuracy=0.5259, gradient_norm=3.5735, 
[2025-09-16 19:14:57,460][__main__][INFO] - Train, Round 025: loss=1.3595, accuracy=0.5167, gradient_norm=3.5154, 
[2025-09-16 19:14:57,460][__main__][INFO] - Train, Round 026: loss=2.2573, accuracy=0.5458, gradient_norm=3.3314, 
[2025-09-16 19:14:57,461][__main__][INFO] - Train, Round 027: loss=1.2287, accuracy=0.5639, gradient_norm=3.3599, 
[2025-09-16 19:14:57,461][__main__][INFO] - Train, Round 028: loss=1.1839, accuracy=0.5768, gradient_norm=3.5994, 
[2025-09-16 19:14:57,461][__main__][INFO] - Train, Round 029: loss=1.1711, accuracy=0.5824, gradient_norm=3.4467, 
[2025-09-16 19:14:57,461][__main__][INFO] - Train, Round 030: loss=1.1264, accuracy=0.5962, gradient_norm=3.4598, 
[2025-09-16 19:14:57,461][__main__][INFO] - Train, Round 031: loss=1.0971, accuracy=0.6032, gradient_norm=3.4462, 
[2025-09-16 19:14:57,461][__main__][INFO] - Train, Round 032: loss=1.0653, accuracy=0.6060, gradient_norm=3.3912, 
[2025-09-16 19:14:57,461][__main__][INFO] - Train, Round 033: loss=1.0462, accuracy=0.6190, gradient_norm=3.4380, 
[2025-09-16 19:14:57,461][__main__][INFO] - Train, Round 034: loss=1.0360, accuracy=0.6241, gradient_norm=3.4891, 
[2025-09-16 19:14:57,461][__main__][INFO] - Train, Round 035: loss=1.0170, accuracy=0.6269, gradient_norm=3.4259, 
[2025-09-16 19:14:57,461][__main__][INFO] - Train, Round 036: loss=0.9904, accuracy=0.6350, gradient_norm=3.3798, 
[2025-09-16 19:14:57,461][__main__][INFO] - Train, Round 037: loss=1.3216, accuracy=0.6401, gradient_norm=3.4444, 
[2025-09-16 19:14:57,461][__main__][INFO] - Train, Round 038: loss=0.9758, accuracy=0.6409, gradient_norm=3.3392, 
[2025-09-16 19:14:57,461][__main__][INFO] - Train, Round 039: loss=0.9475, accuracy=0.6460, gradient_norm=3.3839, 
[2025-09-16 19:14:57,461][__main__][INFO] - Train, Round 040: loss=0.9398, accuracy=0.6508, gradient_norm=3.3989, 
[2025-09-16 19:14:57,461][__main__][INFO] - Train, Round 041: loss=0.9321, accuracy=0.6539, gradient_norm=3.1550, 
[2025-09-16 19:14:57,461][__main__][INFO] - Train, Round 042: loss=0.9054, accuracy=0.6611, gradient_norm=3.3053, 
[2025-09-16 19:14:57,461][__main__][INFO] - Train, Round 043: loss=0.9013, accuracy=0.6625, gradient_norm=3.2612, 
[2025-09-16 19:14:57,461][__main__][INFO] - Train, Round 044: loss=0.9027, accuracy=0.6668, gradient_norm=3.3615, 
[2025-09-16 19:14:57,461][__main__][INFO] - Train, Round 045: loss=0.8886, accuracy=0.6670, gradient_norm=3.1428, 
[2025-09-16 19:14:57,461][__main__][INFO] - Train, Round 046: loss=0.8628, accuracy=0.6733, gradient_norm=3.1357, 
[2025-09-16 19:14:57,461][__main__][INFO] - Train, Round 047: loss=0.8572, accuracy=0.6789, gradient_norm=3.4227, 
[2025-09-16 19:14:57,461][__main__][INFO] - Train, Round 048: loss=0.8684, accuracy=0.6759, gradient_norm=3.3670, 
[2025-09-16 19:14:57,461][__main__][INFO] - Train, Round 049: loss=0.9626, accuracy=0.6784, gradient_norm=3.2710, 
[2025-09-16 19:14:57,461][__main__][INFO] - Train, Round 050: loss=0.8492, accuracy=0.6807, gradient_norm=3.0696, 
[2025-09-16 19:14:57,461][__main__][INFO] - Train, Round 051: loss=0.8215, accuracy=0.6897, gradient_norm=3.2631, 
[2025-09-16 19:14:57,461][__main__][INFO] - Train, Round 052: loss=0.9292, accuracy=0.6882, gradient_norm=3.0842, 
[2025-09-16 19:14:57,461][__main__][INFO] - Train, Round 053: loss=0.8052, accuracy=0.6962, gradient_norm=3.0850, 
[2025-09-16 19:14:57,461][__main__][INFO] - Train, Round 054: loss=0.8057, accuracy=0.6939, gradient_norm=3.1444, 
[2025-09-16 19:14:57,461][__main__][INFO] - Train, Round 055: loss=0.7987, accuracy=0.6946, gradient_norm=3.1708, 
[2025-09-16 19:14:57,461][__main__][INFO] - Train, Round 056: loss=0.7981, accuracy=0.6989, gradient_norm=3.2090, 
[2025-09-16 19:14:57,461][__main__][INFO] - Train, Round 057: loss=0.7987, accuracy=0.6981, gradient_norm=3.1659, 
[2025-09-16 19:14:57,461][__main__][INFO] - Train, Round 058: loss=0.7828, accuracy=0.7007, gradient_norm=3.1781, 
[2025-09-16 19:14:57,461][__main__][INFO] - Train, Round 059: loss=0.7822, accuracy=0.7024, gradient_norm=3.1371, 
[2025-09-16 19:14:57,461][__main__][INFO] - Train, Round 060: loss=1.4247, accuracy=0.7054, gradient_norm=3.1900, 
[2025-09-16 19:14:57,461][__main__][INFO] - Train, Round 061: loss=0.7726, accuracy=0.7058, gradient_norm=3.1671, 
[2025-09-16 19:14:57,461][__main__][INFO] - Train, Round 062: loss=0.7678, accuracy=0.7092, gradient_norm=3.1860, 
[2025-09-16 19:14:57,461][__main__][INFO] - Train, Round 063: loss=0.7777, accuracy=0.7083, gradient_norm=2.9220, 
[2025-09-16 19:14:57,461][__main__][INFO] - Train, Round 064: loss=0.7373, accuracy=0.7159, gradient_norm=2.9752, 
[2025-09-16 19:14:57,461][__main__][INFO] - Train, Round 065: loss=17.3862, accuracy=0.7167, gradient_norm=3.0548, 
[2025-09-16 19:14:57,461][__main__][INFO] - Train, Round 066: loss=0.7377, accuracy=0.7195, gradient_norm=3.0855, 
[2025-09-16 19:14:57,461][__main__][INFO] - Train, Round 067: loss=0.7350, accuracy=0.7179, gradient_norm=3.0712, 
[2025-09-16 19:14:57,461][__main__][INFO] - Train, Round 068: loss=0.7264, accuracy=0.7232, gradient_norm=3.1094, 
[2025-09-16 19:14:57,461][__main__][INFO] - Train, Round 069: loss=0.7180, accuracy=0.7234, gradient_norm=2.9298, 
[2025-09-16 19:14:57,461][__main__][INFO] - Train, Round 070: loss=0.7058, accuracy=0.7263, gradient_norm=3.2172, 
[2025-09-16 19:14:57,461][__main__][INFO] - Train, Round 071: loss=0.7207, accuracy=0.7228, gradient_norm=3.2931, 
[2025-09-16 19:14:57,461][__main__][INFO] - Train, Round 072: loss=0.7254, accuracy=0.7255, gradient_norm=3.0495, 
[2025-09-16 19:14:57,461][__main__][INFO] - Train, Round 073: loss=0.7031, accuracy=0.7276, gradient_norm=3.0591, 
[2025-09-16 19:14:57,461][__main__][INFO] - Train, Round 074: loss=0.7049, accuracy=0.7288, gradient_norm=3.0654, 
[2025-09-16 19:14:57,461][__main__][INFO] - Train, Round 075: loss=0.7003, accuracy=0.7323, gradient_norm=2.9836, 
[2025-09-16 19:14:57,461][__main__][INFO] - Train, Round 076: loss=0.6885, accuracy=0.7360, gradient_norm=3.1585, 
[2025-09-16 19:14:57,461][__main__][INFO] - Train, Round 077: loss=0.6893, accuracy=0.7340, gradient_norm=3.0046, 
[2025-09-16 19:14:57,461][__main__][INFO] - Train, Round 078: loss=0.6870, accuracy=0.7365, gradient_norm=2.7814, 
[2025-09-16 19:14:57,461][__main__][INFO] - Train, Round 079: loss=0.6662, accuracy=0.7439, gradient_norm=2.9033, 
[2025-09-16 19:14:57,461][__main__][INFO] - Train, Round 080: loss=0.6684, accuracy=0.7399, gradient_norm=3.0613, 
[2025-09-16 19:14:57,461][__main__][INFO] - Train, Round 081: loss=0.6813, accuracy=0.7375, gradient_norm=3.0757, 
[2025-09-16 19:14:57,461][__main__][INFO] - Train, Round 082: loss=0.6768, accuracy=0.7402, gradient_norm=2.8612, 
[2025-09-16 19:14:57,461][__main__][INFO] - Train, Round 083: loss=0.8614, accuracy=0.7452, gradient_norm=2.9872, 
[2025-09-16 19:14:57,461][__main__][INFO] - Train, Round 084: loss=0.6594, accuracy=0.7467, gradient_norm=2.9393, 
[2025-09-16 19:14:57,461][__main__][INFO] - Train, Round 085: loss=0.6543, accuracy=0.7498, gradient_norm=3.1376, 
[2025-09-16 19:14:57,461][__main__][INFO] - Train, Round 086: loss=0.6657, accuracy=0.7437, gradient_norm=2.9529, 
[2025-09-16 19:14:57,461][__main__][INFO] - Train, Round 087: loss=0.6488, accuracy=0.7513, gradient_norm=2.9707, 
[2025-09-16 19:14:57,461][__main__][INFO] - Train, Round 088: loss=0.6484, accuracy=0.7503, gradient_norm=2.9922, 
[2025-09-16 19:14:57,461][__main__][INFO] - Train, Round 089: loss=0.6457, accuracy=0.7515, gradient_norm=2.8478, 
[2025-09-16 19:14:57,461][__main__][INFO] - Train, Round 090: loss=0.6326, accuracy=0.7567, gradient_norm=3.1251, 
[2025-09-16 19:14:57,461][__main__][INFO] - Train, Round 091: loss=0.6375, accuracy=0.7517, gradient_norm=3.0216, 
[2025-09-16 19:14:57,461][__main__][INFO] - Train, Round 092: loss=0.6455, accuracy=0.7506, gradient_norm=2.7795, 
[2025-09-16 19:14:57,461][__main__][INFO] - Train, Round 093: loss=0.6356, accuracy=0.7590, gradient_norm=2.8663, 
[2025-09-16 19:14:57,461][__main__][INFO] - Train, Round 094: loss=0.6347, accuracy=0.7563, gradient_norm=3.0260, 
[2025-09-16 19:14:57,461][__main__][INFO] - Train, Round 095: loss=0.6345, accuracy=0.7577, gradient_norm=2.8388, 
[2025-09-16 19:14:57,461][__main__][INFO] - Train, Round 096: loss=0.6171, accuracy=0.7626, gradient_norm=2.8575, 
[2025-09-16 19:14:57,461][__main__][INFO] - Train, Round 097: loss=0.6454, accuracy=0.7623, gradient_norm=2.8461, 
[2025-09-16 19:14:57,461][__main__][INFO] - Train, Round 098: loss=0.6113, accuracy=0.7641, gradient_norm=2.9066, 
[2025-09-16 19:14:57,461][__main__][INFO] - Train, Round 099: loss=0.6146, accuracy=0.7639, gradient_norm=4.5199, 
[2025-09-16 19:14:57,461][__main__][INFO] - Train, Round 100: loss=0.6346, accuracy=0.7577, gradient_norm=3.0361, 
[2025-09-16 19:14:57,461][__main__][INFO] - Train, Round 101: loss=0.6284, accuracy=0.7597, gradient_norm=2.9530, 
[2025-09-16 19:14:57,461][__main__][INFO] - Train, Round 102: loss=0.6159, accuracy=0.7651, gradient_norm=2.9326, 
[2025-09-16 19:14:57,461][__main__][INFO] - Train, Round 103: loss=0.6265, accuracy=0.7650, gradient_norm=2.9755, 
[2025-09-16 19:14:57,462][__main__][INFO] - Train, Round 104: loss=0.6111, accuracy=0.7667, gradient_norm=2.8499, 
[2025-09-16 19:14:57,462][__main__][INFO] - Train, Round 105: loss=0.5973, accuracy=0.7691, gradient_norm=2.9602, 
[2025-09-16 19:14:57,462][__main__][INFO] - Train, Round 106: loss=0.6039, accuracy=0.7686, gradient_norm=2.8178, 
[2025-09-16 19:14:57,462][__main__][INFO] - Train, Round 107: loss=0.5881, accuracy=0.7736, gradient_norm=2.8014, 
[2025-09-16 19:14:57,462][__main__][INFO] - Train, Round 108: loss=0.6018, accuracy=0.7745, gradient_norm=2.9932, 
[2025-09-16 19:14:57,462][__main__][INFO] - Train, Round 109: loss=0.5969, accuracy=0.7712, gradient_norm=2.7936, 
[2025-09-16 19:14:57,462][__main__][INFO] - Train, Round 110: loss=0.5744, accuracy=0.7784, gradient_norm=2.9623, 
[2025-09-16 19:14:57,462][__main__][INFO] - Train, Round 111: loss=0.5843, accuracy=0.7764, gradient_norm=3.0653, 
[2025-09-16 19:14:57,462][__main__][INFO] - Train, Round 112: loss=0.5897, accuracy=0.7754, gradient_norm=2.8702, 
[2025-09-16 19:14:57,462][__main__][INFO] - Train, Round 113: loss=0.5706, accuracy=0.7790, gradient_norm=2.8828, 
[2025-09-16 19:14:57,462][__main__][INFO] - Train, Round 114: loss=0.5693, accuracy=0.7820, gradient_norm=3.0229, 
[2025-09-16 19:14:57,462][__main__][INFO] - Train, Round 115: loss=0.5828, accuracy=0.7779, gradient_norm=2.9038, 
[2025-09-16 19:14:57,462][__main__][INFO] - Train, Round 116: loss=0.5680, accuracy=0.7827, gradient_norm=2.7675, 
[2025-09-16 19:14:57,462][__main__][INFO] - Train, Round 117: loss=0.5528, accuracy=0.7852, gradient_norm=2.9489, 
[2025-09-16 19:14:57,462][__main__][INFO] - Train, Round 118: loss=0.5617, accuracy=0.7832, gradient_norm=2.7008, 
[2025-09-16 19:14:57,462][__main__][INFO] - Train, Round 119: loss=0.5445, accuracy=0.7888, gradient_norm=2.8380, 
[2025-09-16 19:14:57,462][__main__][INFO] - Train, Round 120: loss=0.5545, accuracy=0.7882, gradient_norm=2.8879, 
[2025-09-16 19:14:57,462][__main__][INFO] - Train, Round 121: loss=0.5563, accuracy=0.7894, gradient_norm=2.9312, 
[2025-09-16 19:14:57,462][__main__][INFO] - Train, Round 122: loss=0.5600, accuracy=0.7884, gradient_norm=2.8095, 
[2025-09-16 19:14:57,462][__main__][INFO] - Train, Round 123: loss=0.5408, accuracy=0.7926, gradient_norm=2.9263, 
[2025-09-16 19:14:57,462][__main__][INFO] - Train, Round 124: loss=0.5478, accuracy=0.7914, gradient_norm=2.9397, 
[2025-09-16 19:14:57,462][__main__][INFO] - Train, Round 125: loss=0.5449, accuracy=0.7915, gradient_norm=2.7766, 
[2025-09-16 19:14:57,462][__main__][INFO] - Train, Round 126: loss=0.5367, accuracy=0.7962, gradient_norm=2.8169, 
[2025-09-16 19:14:57,462][__main__][INFO] - Train, Round 127: loss=0.5349, accuracy=0.7952, gradient_norm=2.7774, 
[2025-09-16 19:14:57,462][__main__][INFO] - Train, Round 128: loss=0.5243, accuracy=0.7997, gradient_norm=2.9187, 
[2025-09-16 19:14:57,462][__main__][INFO] - Train, Round 129: loss=0.5331, accuracy=0.7965, gradient_norm=2.7545, 
[2025-09-16 19:14:57,462][__main__][INFO] - Train, Round 130: loss=0.5245, accuracy=0.8004, gradient_norm=2.8301, 
[2025-09-16 19:14:57,462][__main__][INFO] - Train, Round 131: loss=0.5723, accuracy=0.7991, gradient_norm=2.9326, 
[2025-09-16 19:14:57,462][__main__][INFO] - Train, Round 132: loss=0.5300, accuracy=0.7980, gradient_norm=2.7129, 
[2025-09-16 19:14:57,462][__main__][INFO] - Train, Round 133: loss=0.5106, accuracy=0.8041, gradient_norm=2.6804, 
[2025-09-16 19:14:57,462][__main__][INFO] - Train, Round 134: loss=0.5073, accuracy=0.8066, gradient_norm=2.8031, 
[2025-09-16 19:14:57,462][__main__][INFO] - Train, Round 135: loss=0.5136, accuracy=0.8046, gradient_norm=2.8528, 
[2025-09-16 19:14:57,462][__main__][INFO] - Train, Round 136: loss=0.5152, accuracy=0.8048, gradient_norm=2.7663, 
[2025-09-16 19:14:57,462][__main__][INFO] - Train, Round 137: loss=0.5037, accuracy=0.8082, gradient_norm=2.9298, 
[2025-09-16 19:14:57,462][__main__][INFO] - Train, Round 138: loss=0.5105, accuracy=0.8058, gradient_norm=2.7610, 
[2025-09-16 19:14:57,462][__main__][INFO] - Train, Round 139: loss=0.4965, accuracy=0.8098, gradient_norm=2.7799, 
[2025-09-16 19:14:57,462][__main__][INFO] - Train, Round 140: loss=0.4987, accuracy=0.8098, gradient_norm=2.7502, 
[2025-09-16 19:14:57,462][__main__][INFO] - Train, Round 141: loss=0.4957, accuracy=0.8133, gradient_norm=2.7711, 
[2025-09-16 19:14:57,462][__main__][INFO] - Train, Round 142: loss=0.4903, accuracy=0.8120, gradient_norm=2.9065, 
[2025-09-16 19:14:57,462][__main__][INFO] - Train, Round 143: loss=0.4994, accuracy=0.8106, gradient_norm=2.8401, 
[2025-09-16 19:14:57,462][__main__][INFO] - Train, Round 144: loss=0.4935, accuracy=0.8133, gradient_norm=2.8980, 
[2025-09-16 19:14:57,462][__main__][INFO] - Train, Round 145: loss=0.4953, accuracy=0.8133, gradient_norm=2.7661, 
[2025-09-16 19:14:57,462][__main__][INFO] - Train, Round 146: loss=0.4864, accuracy=0.8162, gradient_norm=2.6553, 
[2025-09-16 19:14:57,462][__main__][INFO] - Train, Round 147: loss=0.4741, accuracy=0.8188, gradient_norm=2.8355, 
[2025-09-16 19:14:57,462][__main__][INFO] - Train, Round 148: loss=0.4817, accuracy=0.8155, gradient_norm=2.8597, 
[2025-09-16 19:14:57,462][__main__][INFO] - Train, Round 149: loss=0.4880, accuracy=0.8181, gradient_norm=2.8568, 
[2025-09-16 19:14:57,462][__main__][INFO] - Train, Round 150: loss=0.4819, accuracy=0.8197, gradient_norm=2.8812, 
[2025-09-16 19:14:57,462][__main__][INFO] - Train, Round 151: loss=2.4198, accuracy=0.8182, gradient_norm=2.7201, 
[2025-09-16 19:14:57,462][__main__][INFO] - Train, Round 152: loss=0.4656, accuracy=0.8227, gradient_norm=2.7101, 
[2025-09-16 19:14:57,462][__main__][INFO] - Train, Round 153: loss=0.4691, accuracy=0.8245, gradient_norm=2.8581, 
[2025-09-16 19:14:57,462][__main__][INFO] - Train, Round 154: loss=0.4692, accuracy=0.8224, gradient_norm=2.9392, 
[2025-09-16 19:14:57,462][__main__][INFO] - Train, Round 155: loss=0.4726, accuracy=0.8219, gradient_norm=2.9492, 
[2025-09-16 19:14:57,462][__main__][INFO] - Train, Round 156: loss=0.4711, accuracy=0.8230, gradient_norm=2.6924, 
[2025-09-16 19:14:57,462][__main__][INFO] - Train, Round 157: loss=0.4499, accuracy=0.8284, gradient_norm=2.6563, 
[2025-09-16 19:14:57,462][__main__][INFO] - Train, Round 158: loss=0.4471, accuracy=0.8307, gradient_norm=2.5063, 
[2025-09-16 19:14:57,462][__main__][INFO] - Train, Round 159: loss=0.4361, accuracy=0.8333, gradient_norm=2.6889, 
[2025-09-16 19:14:57,462][__main__][INFO] - Train, Round 160: loss=0.4469, accuracy=0.8306, gradient_norm=2.7656, 
[2025-09-16 19:14:57,462][__main__][INFO] - Train, Round 161: loss=0.4487, accuracy=0.8310, gradient_norm=2.7327, 
[2025-09-16 19:14:57,462][__main__][INFO] - Train, Round 162: loss=0.4435, accuracy=0.8329, gradient_norm=2.7074, 
[2025-09-16 19:14:57,462][__main__][INFO] - Train, Round 163: loss=0.4910, accuracy=0.8351, gradient_norm=2.6139, 
[2025-09-16 19:14:57,462][__main__][INFO] - Train, Round 164: loss=0.4324, accuracy=0.8379, gradient_norm=2.9712, 
[2025-09-16 19:14:57,462][__main__][INFO] - Train, Round 165: loss=0.4505, accuracy=0.8306, gradient_norm=2.7100, 
[2025-09-16 19:14:57,462][__main__][INFO] - Train, Round 166: loss=0.4326, accuracy=0.8372, gradient_norm=2.6137, 
[2025-09-16 19:14:57,462][__main__][INFO] - Train, Round 167: loss=0.4255, accuracy=0.8394, gradient_norm=2.5965, 
[2025-09-16 19:14:57,462][__main__][INFO] - Train, Round 168: loss=0.4237, accuracy=0.8410, gradient_norm=2.7470, 
[2025-09-16 19:14:57,462][__main__][INFO] - Train, Round 169: loss=0.4316, accuracy=0.8383, gradient_norm=2.7164, 
[2025-09-16 19:14:57,462][__main__][INFO] - Train, Round 170: loss=0.4235, accuracy=0.8407, gradient_norm=2.8143, 
[2025-09-16 19:14:57,462][__main__][INFO] - Train, Round 171: loss=0.4291, accuracy=0.8398, gradient_norm=2.7465, 
[2025-09-16 19:14:57,462][__main__][INFO] - Train, Round 172: loss=0.4191, accuracy=0.8435, gradient_norm=2.7739, 
[2025-09-16 19:14:57,462][__main__][INFO] - Train, Round 173: loss=0.4265, accuracy=0.8415, gradient_norm=2.7109, 
[2025-09-16 19:14:57,462][__main__][INFO] - Train, Round 174: loss=0.4165, accuracy=0.8437, gradient_norm=2.7045, 
[2025-09-16 19:14:57,462][__main__][INFO] - Train, Round 175: loss=33.0113, accuracy=0.8466, gradient_norm=2.7490, 
[2025-09-16 19:14:57,462][__main__][INFO] - Train, Round 176: loss=0.4161, accuracy=0.8454, gradient_norm=2.8382, 
[2025-09-16 19:14:57,462][__main__][INFO] - Train, Round 177: loss=0.4181, accuracy=0.8439, gradient_norm=2.7390, 
[2025-09-16 19:14:57,462][__main__][INFO] - Train, Round 178: loss=0.4096, accuracy=0.8472, gradient_norm=2.6134, 
[2025-09-16 19:14:57,462][__main__][INFO] - Train, Round 179: loss=0.4009, accuracy=0.8509, gradient_norm=2.6605, 
[2025-09-16 19:14:57,462][__main__][INFO] - Train, Round 180: loss=0.4019, accuracy=0.8503, gradient_norm=2.8227, 
[2025-09-16 19:14:57,462][__main__][INFO] - Train, Round 181: loss=0.4096, accuracy=0.8467, gradient_norm=2.5800, 
[2025-09-16 19:14:57,462][__main__][INFO] - Train, Round 182: loss=0.3990, accuracy=0.8529, gradient_norm=2.6699, 
[2025-09-16 19:14:57,463][__main__][INFO] - Train, Round 183: loss=0.3951, accuracy=0.8529, gradient_norm=2.5912, 
[2025-09-16 19:14:57,463][__main__][INFO] - Train, Round 184: loss=0.3874, accuracy=0.8557, gradient_norm=2.4857, 
[2025-09-16 19:14:57,463][__main__][INFO] - Train, Round 185: loss=0.3821, accuracy=0.8581, gradient_norm=2.5952, 
[2025-09-16 19:14:57,463][__main__][INFO] - Train, Round 186: loss=0.3886, accuracy=0.8566, gradient_norm=2.6369, 
[2025-09-16 19:14:57,463][__main__][INFO] - Train, Round 187: loss=0.3923, accuracy=0.8557, gradient_norm=2.6857, 
[2025-09-16 19:14:57,463][__main__][INFO] - Train, Round 188: loss=0.3880, accuracy=0.8580, gradient_norm=2.6072, 
[2025-09-16 19:14:57,463][__main__][INFO] - Train, Round 189: loss=0.3829, accuracy=0.8583, gradient_norm=2.7234, 
[2025-09-16 19:14:57,463][__main__][INFO] - Train, Round 190: loss=0.3841, accuracy=0.8583, gradient_norm=2.6891, 
[2025-09-16 19:14:57,463][__main__][INFO] - Train, Round 191: loss=0.3775, accuracy=0.8599, gradient_norm=2.7300, 
[2025-09-16 19:14:57,463][__main__][INFO] - Train, Round 192: loss=0.3782, accuracy=0.8597, gradient_norm=2.5684, 
[2025-09-16 19:14:57,463][__main__][INFO] - Train, Round 193: loss=0.3690, accuracy=0.8627, gradient_norm=2.7157, 
[2025-09-16 19:14:57,463][__main__][INFO] - Train, Round 194: loss=0.3774, accuracy=0.8626, gradient_norm=2.7743, 
[2025-09-16 19:14:57,463][__main__][INFO] - Train, Round 195: loss=0.3766, accuracy=0.8620, gradient_norm=2.5436, 
[2025-09-16 19:14:57,463][__main__][INFO] - Train, Round 196: loss=0.3656, accuracy=0.8657, gradient_norm=2.6571, 
[2025-09-16 19:14:57,463][__main__][INFO] - Train, Round 197: loss=0.3674, accuracy=0.8650, gradient_norm=2.7096, 
[2025-09-16 19:14:57,463][__main__][INFO] - Train, Round 198: loss=0.3699, accuracy=0.8656, gradient_norm=2.7954, 
[2025-09-16 19:14:57,463][__main__][INFO] - Train, Round 199: loss=0.3767, accuracy=0.8633, gradient_norm=2.5293, 
[2025-09-16 19:14:57,463][__main__][INFO] - Train, Round 200: loss=0.3562, accuracy=0.8698, gradient_norm=2.6780, 
[2025-09-16 19:14:57,463][__main__][INFO] - Test, Round 001: loss=2.2945, accuracy=0.1170, 
[2025-09-16 19:14:57,463][__main__][INFO] - Test, Round 002: loss=2.2835, accuracy=0.1735, 
[2025-09-16 19:14:57,463][__main__][INFO] - Test, Round 003: loss=2.2685, accuracy=0.1918, 
[2025-09-16 19:14:57,463][__main__][INFO] - Test, Round 004: loss=2.2470, accuracy=0.2239, 
[2025-09-16 19:14:57,463][__main__][INFO] - Test, Round 005: loss=2.2383, accuracy=0.2778, 
[2025-09-16 19:14:57,463][__main__][INFO] - Test, Round 006: loss=2.2709, accuracy=0.2054, 
[2025-09-16 19:14:57,463][__main__][INFO] - Test, Round 007: loss=2.3246, accuracy=0.2213, 
[2025-09-16 19:14:57,463][__main__][INFO] - Test, Round 008: loss=2.2910, accuracy=0.2428, 
[2025-09-16 19:14:57,463][__main__][INFO] - Test, Round 009: loss=2.3349, accuracy=0.1922, 
[2025-09-16 19:14:57,463][__main__][INFO] - Test, Round 010: loss=2.2100, accuracy=0.2840, 
[2025-09-16 19:14:57,463][__main__][INFO] - Test, Round 011: loss=2.2444, accuracy=0.2843, 
[2025-09-16 19:14:57,463][__main__][INFO] - Test, Round 012: loss=2.2343, accuracy=0.2831, 
[2025-09-16 19:14:57,463][__main__][INFO] - Test, Round 013: loss=2.0396, accuracy=0.2917, 
[2025-09-16 19:14:57,463][__main__][INFO] - Test, Round 014: loss=2.2236, accuracy=0.2970, 
[2025-09-16 19:14:57,463][__main__][INFO] - Test, Round 015: loss=1.9662, accuracy=0.3169, 
[2025-09-16 19:14:57,463][__main__][INFO] - Test, Round 016: loss=4.7615, accuracy=0.1941, 
[2025-09-16 19:14:57,463][__main__][INFO] - Test, Round 017: loss=1.9952, accuracy=0.2909, 
[2025-09-16 19:14:57,463][__main__][INFO] - Test, Round 018: loss=1.9528, accuracy=0.3154, 
[2025-09-16 19:14:57,463][__main__][INFO] - Test, Round 019: loss=1.9696, accuracy=0.3333, 
[2025-09-16 19:14:57,463][__main__][INFO] - Test, Round 020: loss=2.2606, accuracy=0.2998, 
[2025-09-16 19:14:57,463][__main__][INFO] - Test, Round 021: loss=2.1092, accuracy=0.3384, 
[2025-09-16 19:14:57,463][__main__][INFO] - Test, Round 022: loss=2.1888, accuracy=0.3616, 
[2025-09-16 19:14:57,463][__main__][INFO] - Test, Round 023: loss=1.8317, accuracy=0.3900, 
[2025-09-16 19:14:57,463][__main__][INFO] - Test, Round 024: loss=1.9447, accuracy=0.3652, 
[2025-09-16 19:14:57,463][__main__][INFO] - Test, Round 025: loss=2.4956, accuracy=0.3751, 
[2025-09-16 19:14:57,463][__main__][INFO] - Test, Round 026: loss=1.8622, accuracy=0.3890, 
[2025-09-16 19:14:57,463][__main__][INFO] - Test, Round 027: loss=1.8599, accuracy=0.3934, 
[2025-09-16 19:14:57,463][__main__][INFO] - Test, Round 028: loss=1.9311, accuracy=0.4027, 
[2025-09-16 19:14:57,463][__main__][INFO] - Test, Round 029: loss=1.8185, accuracy=0.4122, 
[2025-09-16 19:14:57,463][__main__][INFO] - Test, Round 030: loss=1.8152, accuracy=0.4158, 
[2025-09-16 19:14:57,463][__main__][INFO] - Test, Round 031: loss=1.7580, accuracy=0.4220, 
[2025-09-16 19:14:57,463][__main__][INFO] - Test, Round 032: loss=1.6931, accuracy=0.4547, 
[2025-09-16 19:14:57,463][__main__][INFO] - Test, Round 033: loss=1.8045, accuracy=0.4378, 
[2025-09-16 19:14:57,463][__main__][INFO] - Test, Round 034: loss=1.7252, accuracy=0.4669, 
[2025-09-16 19:14:57,463][__main__][INFO] - Test, Round 035: loss=1.6276, accuracy=0.4622, 
[2025-09-16 19:14:57,463][__main__][INFO] - Test, Round 036: loss=1.6096, accuracy=0.4708, 
[2025-09-16 19:14:57,463][__main__][INFO] - Test, Round 037: loss=1.6956, accuracy=0.4577, 
[2025-09-16 19:14:57,463][__main__][INFO] - Test, Round 038: loss=1.6257, accuracy=0.4761, 
[2025-09-16 19:14:57,463][__main__][INFO] - Test, Round 039: loss=1.5874, accuracy=0.4770, 
[2025-09-16 19:14:57,463][__main__][INFO] - Test, Round 040: loss=1.5879, accuracy=0.4822, 
[2025-09-16 19:14:57,463][__main__][INFO] - Test, Round 041: loss=1.4487, accuracy=0.5058, 
[2025-09-16 19:14:57,463][__main__][INFO] - Test, Round 042: loss=1.4940, accuracy=0.4920, 
[2025-09-16 19:14:57,463][__main__][INFO] - Test, Round 043: loss=1.6258, accuracy=0.5000, 
[2025-09-16 19:14:57,463][__main__][INFO] - Test, Round 044: loss=1.5118, accuracy=0.5059, 
[2025-09-16 19:14:57,463][__main__][INFO] - Test, Round 045: loss=1.4073, accuracy=0.5244, 
[2025-09-16 19:14:57,463][__main__][INFO] - Test, Round 046: loss=1.4267, accuracy=0.5210, 
[2025-09-16 19:14:57,463][__main__][INFO] - Test, Round 047: loss=1.5209, accuracy=0.4984, 
[2025-09-16 19:14:57,463][__main__][INFO] - Test, Round 048: loss=1.7551, accuracy=0.5185, 
[2025-09-16 19:14:57,463][__main__][INFO] - Test, Round 049: loss=1.4168, accuracy=0.5302, 
[2025-09-16 19:14:57,463][__main__][INFO] - Test, Round 050: loss=1.3443, accuracy=0.5380, 
[2025-09-16 19:14:57,463][__main__][INFO] - Test, Round 051: loss=1.3991, accuracy=0.5326, 
[2025-09-16 19:14:57,463][__main__][INFO] - Test, Round 052: loss=1.3099, accuracy=0.5547, 
[2025-09-16 19:14:57,463][__main__][INFO] - Test, Round 053: loss=1.3528, accuracy=0.5393, 
[2025-09-16 19:14:57,463][__main__][INFO] - Test, Round 054: loss=1.3082, accuracy=0.5528, 
[2025-09-16 19:14:57,463][__main__][INFO] - Test, Round 055: loss=1.3537, accuracy=0.5418, 
[2025-09-16 19:14:57,463][__main__][INFO] - Test, Round 056: loss=1.3993, accuracy=0.5323, 
[2025-09-16 19:14:57,463][__main__][INFO] - Test, Round 057: loss=1.3259, accuracy=0.5544, 
[2025-09-16 19:14:57,463][__main__][INFO] - Test, Round 058: loss=1.3264, accuracy=0.5530, 
[2025-09-16 19:14:57,464][__main__][INFO] - Test, Round 059: loss=1.3238, accuracy=0.5580, 
[2025-09-16 19:14:57,464][__main__][INFO] - Test, Round 060: loss=1.3341, accuracy=0.5544, 
[2025-09-16 19:14:57,464][__main__][INFO] - Test, Round 061: loss=1.3166, accuracy=0.5639, 
[2025-09-16 19:14:57,464][__main__][INFO] - Test, Round 062: loss=1.5310, accuracy=0.5493, 
[2025-09-16 19:14:57,464][__main__][INFO] - Test, Round 063: loss=1.2395, accuracy=0.5703, 
[2025-09-16 19:14:57,464][__main__][INFO] - Test, Round 064: loss=1.2255, accuracy=0.5786, 
[2025-09-16 19:14:57,464][__main__][INFO] - Test, Round 065: loss=1.2290, accuracy=0.5773, 
[2025-09-16 19:14:57,464][__main__][INFO] - Test, Round 066: loss=1.2894, accuracy=0.5640, 
[2025-09-16 19:14:57,464][__main__][INFO] - Test, Round 067: loss=1.2367, accuracy=0.5802, 
[2025-09-16 19:14:57,464][__main__][INFO] - Test, Round 068: loss=1.2081, accuracy=0.5841, 
[2025-09-16 19:14:57,464][__main__][INFO] - Test, Round 069: loss=1.1837, accuracy=0.5895, 
[2025-09-16 19:14:57,464][__main__][INFO] - Test, Round 070: loss=1.2856, accuracy=0.5674, 
[2025-09-16 19:14:57,464][__main__][INFO] - Test, Round 071: loss=1.3114, accuracy=0.5692, 
[2025-09-16 19:14:57,464][__main__][INFO] - Test, Round 072: loss=1.2031, accuracy=0.5958, 
[2025-09-16 19:14:57,464][__main__][INFO] - Test, Round 073: loss=1.2330, accuracy=0.5783, 
[2025-09-16 19:14:57,464][__main__][INFO] - Test, Round 074: loss=1.2381, accuracy=0.5954, 
[2025-09-16 19:14:57,464][__main__][INFO] - Test, Round 075: loss=1.1781, accuracy=0.5911, 
[2025-09-16 19:14:57,464][__main__][INFO] - Test, Round 076: loss=1.1642, accuracy=0.6001, 
[2025-09-16 19:14:57,464][__main__][INFO] - Test, Round 077: loss=1.2008, accuracy=0.5893, 
[2025-09-16 19:14:57,464][__main__][INFO] - Test, Round 078: loss=1.0907, accuracy=0.6197, 
[2025-09-16 19:14:57,464][__main__][INFO] - Test, Round 079: loss=1.1424, accuracy=0.5981, 
[2025-09-16 19:14:57,464][__main__][INFO] - Test, Round 080: loss=1.1969, accuracy=0.5962, 
[2025-09-16 19:14:57,464][__main__][INFO] - Test, Round 081: loss=1.2219, accuracy=0.5953, 
[2025-09-16 19:14:57,464][__main__][INFO] - Test, Round 082: loss=2.9537, accuracy=0.6078, 
[2025-09-16 19:14:57,464][__main__][INFO] - Test, Round 083: loss=1.1362, accuracy=0.6144, 
[2025-09-16 19:14:57,464][__main__][INFO] - Test, Round 084: loss=1.1447, accuracy=0.6092, 
[2025-09-16 19:14:57,464][__main__][INFO] - Test, Round 085: loss=1.2242, accuracy=0.5947, 
[2025-09-16 19:14:57,464][__main__][INFO] - Test, Round 086: loss=1.1506, accuracy=0.6116, 
[2025-09-16 19:14:57,464][__main__][INFO] - Test, Round 087: loss=1.1368, accuracy=0.6110, 
[2025-09-16 19:14:57,464][__main__][INFO] - Test, Round 088: loss=1.1584, accuracy=0.6080, 
[2025-09-16 19:14:57,464][__main__][INFO] - Test, Round 089: loss=1.1274, accuracy=0.6191, 
[2025-09-16 19:14:57,464][__main__][INFO] - Test, Round 090: loss=1.1543, accuracy=0.6074, 
[2025-09-16 19:14:57,464][__main__][INFO] - Test, Round 091: loss=1.1504, accuracy=0.5996, 
[2025-09-16 19:14:57,464][__main__][INFO] - Test, Round 092: loss=1.0723, accuracy=0.6278, 
[2025-09-16 19:14:57,464][__main__][INFO] - Test, Round 093: loss=1.1400, accuracy=0.6194, 
[2025-09-16 19:14:57,464][__main__][INFO] - Test, Round 094: loss=1.1858, accuracy=0.6121, 
[2025-09-16 19:14:57,464][__main__][INFO] - Test, Round 095: loss=1.1106, accuracy=0.6287, 
[2025-09-16 19:14:57,464][__main__][INFO] - Test, Round 096: loss=1.1096, accuracy=0.6199, 
[2025-09-16 19:14:57,464][__main__][INFO] - Test, Round 097: loss=1.1026, accuracy=0.6274, 
[2025-09-16 19:14:57,464][__main__][INFO] - Test, Round 098: loss=1.1270, accuracy=0.6265, 
[2025-09-16 19:14:57,464][__main__][INFO] - Test, Round 099: loss=1.2257, accuracy=0.6082, 
[2025-09-16 19:14:57,464][__main__][INFO] - Test, Round 100: loss=1.2076, accuracy=0.6087, 
[2025-09-16 19:14:57,464][__main__][INFO] - Test, Round 101: loss=1.1295, accuracy=0.6224, 
[2025-09-16 19:14:57,464][__main__][INFO] - Test, Round 102: loss=1.1609, accuracy=0.6185, 
[2025-09-16 19:14:57,464][__main__][INFO] - Test, Round 103: loss=1.1567, accuracy=0.6268, 
[2025-09-16 19:14:57,464][__main__][INFO] - Test, Round 104: loss=1.1180, accuracy=0.6253, 
[2025-09-16 19:14:57,464][__main__][INFO] - Test, Round 105: loss=1.1778, accuracy=0.6183, 
[2025-09-16 19:14:57,464][__main__][INFO] - Test, Round 106: loss=1.1633, accuracy=0.6287, 
[2025-09-16 19:14:57,464][__main__][INFO] - Test, Round 107: loss=1.0802, accuracy=0.6326, 
[2025-09-16 19:14:57,464][__main__][INFO] - Test, Round 108: loss=1.1763, accuracy=0.6196, 
[2025-09-16 19:14:57,464][__main__][INFO] - Test, Round 109: loss=1.0624, accuracy=0.6432, 
[2025-09-16 19:14:57,464][__main__][INFO] - Test, Round 110: loss=1.1198, accuracy=0.6324, 
[2025-09-16 19:14:57,464][__main__][INFO] - Test, Round 111: loss=1.2271, accuracy=0.6219, 
[2025-09-16 19:14:57,464][__main__][INFO] - Test, Round 112: loss=1.0890, accuracy=0.6313, 
[2025-09-16 19:14:57,464][__main__][INFO] - Test, Round 113: loss=1.0748, accuracy=0.6477, 
[2025-09-16 19:14:57,464][__main__][INFO] - Test, Round 114: loss=1.1824, accuracy=0.6299, 
[2025-09-16 19:14:57,464][__main__][INFO] - Test, Round 115: loss=1.1190, accuracy=0.6336, 
[2025-09-16 19:14:57,464][__main__][INFO] - Test, Round 116: loss=1.0641, accuracy=0.6440, 
[2025-09-16 19:14:57,464][__main__][INFO] - Test, Round 117: loss=1.1217, accuracy=0.6373, 
[2025-09-16 19:14:57,464][__main__][INFO] - Test, Round 118: loss=1.0697, accuracy=0.6406, 
[2025-09-16 19:14:57,464][__main__][INFO] - Test, Round 119: loss=1.0952, accuracy=0.6347, 
[2025-09-16 19:14:57,464][__main__][INFO] - Test, Round 120: loss=1.1943, accuracy=0.6293, 
[2025-09-16 19:14:57,464][__main__][INFO] - Test, Round 121: loss=1.1129, accuracy=0.6395, 
[2025-09-16 19:14:57,464][__main__][INFO] - Test, Round 122: loss=1.0718, accuracy=0.6462, 
[2025-09-16 19:14:57,464][__main__][INFO] - Test, Round 123: loss=1.1078, accuracy=0.6402, 
[2025-09-16 19:14:57,464][__main__][INFO] - Test, Round 124: loss=1.1272, accuracy=0.6377, 
[2025-09-16 19:14:57,464][__main__][INFO] - Test, Round 125: loss=1.0973, accuracy=0.6457, 
[2025-09-16 19:14:57,464][__main__][INFO] - Test, Round 126: loss=1.0938, accuracy=0.6441, 
[2025-09-16 19:14:57,464][__main__][INFO] - Test, Round 127: loss=1.0604, accuracy=0.6489, 
[2025-09-16 19:14:57,464][__main__][INFO] - Test, Round 128: loss=1.1124, accuracy=0.6442, 
[2025-09-16 19:14:57,464][__main__][INFO] - Test, Round 129: loss=1.0919, accuracy=0.6520, 
[2025-09-16 19:14:57,464][__main__][INFO] - Test, Round 130: loss=1.0922, accuracy=0.6455, 
[2025-09-16 19:14:57,464][__main__][INFO] - Test, Round 131: loss=1.1219, accuracy=0.6466, 
[2025-09-16 19:14:57,464][__main__][INFO] - Test, Round 132: loss=1.1703, accuracy=0.6500, 
[2025-09-16 19:14:57,464][__main__][INFO] - Test, Round 133: loss=1.0471, accuracy=0.6526, 
[2025-09-16 19:14:57,464][__main__][INFO] - Test, Round 134: loss=1.0964, accuracy=0.6495, 
[2025-09-16 19:14:57,464][__main__][INFO] - Test, Round 135: loss=1.1817, accuracy=0.6417, 
[2025-09-16 19:14:57,464][__main__][INFO] - Test, Round 136: loss=1.1412, accuracy=0.6555, 
[2025-09-16 19:14:57,465][__main__][INFO] - Test, Round 137: loss=1.1597, accuracy=0.6466, 
[2025-09-16 19:14:57,465][__main__][INFO] - Test, Round 138: loss=1.1237, accuracy=0.6610, 
[2025-09-16 19:14:57,465][__main__][INFO] - Test, Round 139: loss=1.0980, accuracy=0.6531, 
[2025-09-16 19:14:57,465][__main__][INFO] - Test, Round 140: loss=1.0823, accuracy=0.6558, 
[2025-09-16 19:14:57,465][__main__][INFO] - Test, Round 141: loss=1.1425, accuracy=0.6581, 
[2025-09-16 19:14:57,465][__main__][INFO] - Test, Round 142: loss=1.3056, accuracy=0.6465, 
[2025-09-16 19:14:57,465][__main__][INFO] - Test, Round 143: loss=1.2935, accuracy=0.6537, 
[2025-09-16 19:14:57,465][__main__][INFO] - Test, Round 144: loss=1.1462, accuracy=0.6464, 
[2025-09-16 19:14:57,465][__main__][INFO] - Test, Round 145: loss=1.0874, accuracy=0.6595, 
[2025-09-16 19:14:57,465][__main__][INFO] - Test, Round 146: loss=1.1178, accuracy=0.6716, 
[2025-09-16 19:14:57,465][__main__][INFO] - Test, Round 147: loss=1.3494, accuracy=0.6526, 
[2025-09-16 19:14:57,465][__main__][INFO] - Test, Round 148: loss=1.1437, accuracy=0.6491, 
[2025-09-16 19:14:57,465][__main__][INFO] - Test, Round 149: loss=1.1098, accuracy=0.6533, 
[2025-09-16 19:14:57,465][__main__][INFO] - Test, Round 150: loss=1.1930, accuracy=0.6530, 
[2025-09-16 19:14:57,465][__main__][INFO] - Test, Round 151: loss=1.0400, accuracy=0.6649, 
[2025-09-16 19:14:57,465][__main__][INFO] - Test, Round 152: loss=1.0358, accuracy=0.6647, 
[2025-09-16 19:14:57,465][__main__][INFO] - Test, Round 153: loss=1.0756, accuracy=0.6616, 
[2025-09-16 19:14:57,465][__main__][INFO] - Test, Round 154: loss=1.1119, accuracy=0.6599, 
[2025-09-16 19:14:57,465][__main__][INFO] - Test, Round 155: loss=1.1357, accuracy=0.6489, 
[2025-09-16 19:14:57,465][__main__][INFO] - Test, Round 156: loss=1.0271, accuracy=0.6648, 
[2025-09-16 19:14:57,465][__main__][INFO] - Test, Round 157: loss=1.0301, accuracy=0.6714, 
[2025-09-16 19:14:57,465][__main__][INFO] - Test, Round 158: loss=0.9873, accuracy=0.6737, 
[2025-09-16 19:14:57,465][__main__][INFO] - Test, Round 159: loss=1.0628, accuracy=0.6659, 
[2025-09-16 19:14:57,465][__main__][INFO] - Test, Round 160: loss=1.0642, accuracy=0.6705, 
[2025-09-16 19:14:57,465][__main__][INFO] - Test, Round 161: loss=1.0272, accuracy=0.6790, 
[2025-09-16 19:14:57,465][__main__][INFO] - Test, Round 162: loss=1.3606, accuracy=0.6769, 
[2025-09-16 19:14:57,465][__main__][INFO] - Test, Round 163: loss=1.0148, accuracy=0.6748, 
[2025-09-16 19:14:57,465][__main__][INFO] - Test, Round 164: loss=1.1517, accuracy=0.6470, 
[2025-09-16 19:14:57,465][__main__][INFO] - Test, Round 165: loss=1.0591, accuracy=0.6741, 
[2025-09-16 19:14:57,465][__main__][INFO] - Test, Round 166: loss=1.0380, accuracy=0.6720, 
[2025-09-16 19:14:57,465][__main__][INFO] - Test, Round 167: loss=1.0290, accuracy=0.6794, 
[2025-09-16 19:14:57,465][__main__][INFO] - Test, Round 168: loss=1.0708, accuracy=0.6704, 
[2025-09-16 19:14:57,465][__main__][INFO] - Test, Round 169: loss=1.0593, accuracy=0.6676, 
[2025-09-16 19:14:57,465][__main__][INFO] - Test, Round 170: loss=1.0953, accuracy=0.6660, 
[2025-09-16 19:14:57,465][__main__][INFO] - Test, Round 171: loss=1.0584, accuracy=0.6710, 
[2025-09-16 19:14:57,465][__main__][INFO] - Test, Round 172: loss=1.1138, accuracy=0.6635, 
[2025-09-16 19:14:57,465][__main__][INFO] - Test, Round 173: loss=1.0664, accuracy=0.6697, 
[2025-09-16 19:14:57,465][__main__][INFO] - Test, Round 174: loss=5.5164, accuracy=0.6692, 
[2025-09-16 19:14:57,465][__main__][INFO] - Test, Round 175: loss=1.0976, accuracy=0.6747, 
[2025-09-16 19:14:57,465][__main__][INFO] - Test, Round 176: loss=1.1265, accuracy=0.6727, 
[2025-09-16 19:14:57,465][__main__][INFO] - Test, Round 177: loss=1.0964, accuracy=0.6741, 
[2025-09-16 19:14:57,465][__main__][INFO] - Test, Round 178: loss=1.0402, accuracy=0.6833, 
[2025-09-16 19:14:57,465][__main__][INFO] - Test, Round 179: loss=1.0560, accuracy=0.6732, 
[2025-09-16 19:14:57,465][__main__][INFO] - Test, Round 180: loss=1.1202, accuracy=0.6668, 
[2025-09-16 19:14:57,465][__main__][INFO] - Test, Round 181: loss=1.0746, accuracy=0.6714, 
[2025-09-16 19:14:57,465][__main__][INFO] - Test, Round 182: loss=1.0459, accuracy=0.6862, 
[2025-09-16 19:14:57,465][__main__][INFO] - Test, Round 183: loss=1.0525, accuracy=0.6774, 
[2025-09-16 19:14:57,465][__main__][INFO] - Test, Round 184: loss=1.0152, accuracy=0.6817, 
[2025-09-16 19:14:57,465][__main__][INFO] - Test, Round 185: loss=1.0527, accuracy=0.6841, 
[2025-09-16 19:14:57,465][__main__][INFO] - Test, Round 186: loss=1.0803, accuracy=0.6812, 
[2025-09-16 19:14:57,465][__main__][INFO] - Test, Round 187: loss=1.0656, accuracy=0.6806, 
[2025-09-16 19:14:57,465][__main__][INFO] - Test, Round 188: loss=1.0375, accuracy=0.6841, 
[2025-09-16 19:14:57,465][__main__][INFO] - Test, Round 189: loss=1.0802, accuracy=0.6775, 
[2025-09-16 19:14:57,465][__main__][INFO] - Test, Round 190: loss=1.0565, accuracy=0.6787, 
[2025-09-16 19:14:57,465][__main__][INFO] - Test, Round 191: loss=1.0902, accuracy=0.6777, 
[2025-09-16 19:14:57,465][__main__][INFO] - Test, Round 192: loss=1.0411, accuracy=0.6906, 
[2025-09-16 19:14:57,465][__main__][INFO] - Test, Round 193: loss=1.0912, accuracy=0.6753, 
[2025-09-16 19:14:57,465][__main__][INFO] - Test, Round 194: loss=1.1039, accuracy=0.6797, 
[2025-09-16 19:14:57,465][__main__][INFO] - Test, Round 195: loss=1.0789, accuracy=0.6775, 
[2025-09-16 19:14:57,465][__main__][INFO] - Test, Round 196: loss=1.0720, accuracy=0.6843, 
[2025-09-16 19:14:57,465][__main__][INFO] - Test, Round 197: loss=1.1054, accuracy=0.6798, 
[2025-09-16 19:14:57,465][__main__][INFO] - Test, Round 198: loss=1.1548, accuracy=0.6730, 
[2025-09-16 19:14:57,465][__main__][INFO] - Test, Round 199: loss=1.0382, accuracy=0.6927, 
[2025-09-16 19:14:57,465][__main__][INFO] - Test, Round 200: loss=1.1159, accuracy=0.6787, 
