[2025-09-19 11:03:41,709][flp2p.graph_runner][INFO] - Train, Round 0 : loss => 1.804711580742287,  accuracy: 0.40836670003336667, gradient_norm : 0.32683791871340667
[2025-09-19 11:03:45,600][flp2p.graph_runner][INFO] - Test, Round 0 : loss => 2.0509699398741836,  accuracy: 0.2288
[2025-09-19 11:03:51,711][flp2p.graph_runner][INFO] - Train, Round 1 : loss => 1.8805913501756464,  accuracy: 0.3308304757400182, gradient_norm : 0.22093195281504951
[2025-09-19 11:03:55,495][flp2p.graph_runner][INFO] - Test, Round 1 : loss => 1.9318750936223776,  accuracy: 0.2848
[2025-09-19 11:04:01,917][flp2p.graph_runner][INFO] - Train, Round 2 : loss => 1.6814591013069269,  accuracy: 0.4098616456881306, gradient_norm : 0.2062281402175749
[2025-09-19 11:04:05,790][flp2p.graph_runner][INFO] - Test, Round 2 : loss => 1.8426956424084684,  accuracy: 0.3191
[2025-09-19 11:04:12,829][flp2p.graph_runner][INFO] - Train, Round 3 : loss => 1.5961491936515317,  accuracy: 0.4338207539428552, gradient_norm : 0.21768041921682219
[2025-09-19 11:04:16,698][flp2p.graph_runner][INFO] - Test, Round 3 : loss => 1.7386244154036088,  accuracy: 0.3682
[2025-09-19 11:04:21,887][flp2p.graph_runner][INFO] - Train, Round 4 : loss => 1.5805458657528586,  accuracy: 0.4304437190182566, gradient_norm : 0.21834658118349426
[2025-09-19 11:04:25,760][flp2p.graph_runner][INFO] - Test, Round 4 : loss => 1.7027018068130855,  accuracy: 0.3773
[2025-09-19 11:04:32,800][flp2p.graph_runner][INFO] - Train, Round 5 : loss => 1.4176039042998472,  accuracy: 0.4973626905395597, gradient_norm : 0.18250372903936551
[2025-09-19 11:04:36,651][flp2p.graph_runner][INFO] - Test, Round 5 : loss => 1.626273560559059,  accuracy: 0.3997
[2025-09-19 11:04:43,703][flp2p.graph_runner][INFO] - Train, Round 6 : loss => 1.653010022165108,  accuracy: 0.4145135649773771, gradient_norm : 0.21823936645666037
[2025-09-19 11:04:47,560][flp2p.graph_runner][INFO] - Test, Round 6 : loss => 1.551108004501293,  accuracy: 0.4282
[2025-09-19 11:04:54,701][flp2p.graph_runner][INFO] - Train, Round 7 : loss => 1.3834201925611764,  accuracy: 0.5089796124854556, gradient_norm : 0.19378261611208158
[2025-09-19 11:04:58,564][flp2p.graph_runner][INFO] - Test, Round 7 : loss => 1.533272769291782,  accuracy: 0.4355
[2025-09-19 11:05:05,610][flp2p.graph_runner][INFO] - Train, Round 8 : loss => 1.4569087908441747,  accuracy: 0.46976843208542135, gradient_norm : 0.19836040669535754
[2025-09-19 11:05:09,454][flp2p.graph_runner][INFO] - Test, Round 8 : loss => 1.4814833122452788,  accuracy: 0.4632
[2025-09-19 11:05:16,341][flp2p.graph_runner][INFO] - Train, Round 9 : loss => 1.4168800724179786,  accuracy: 0.49096665029201464, gradient_norm : 0.21667879771036774
[2025-09-19 11:05:20,206][flp2p.graph_runner][INFO] - Test, Round 9 : loss => 1.439602535454302,  accuracy: 0.4791
[2025-09-19 11:05:27,364][flp2p.graph_runner][INFO] - Train, Round 10 : loss => 1.2226476812703324,  accuracy: 0.568120385021097, gradient_norm : 0.20041700861577555
[2025-09-19 11:05:31,180][flp2p.graph_runner][INFO] - Test, Round 10 : loss => 1.401609664840386,  accuracy: 0.4926
[2025-09-19 11:05:38,283][flp2p.graph_runner][INFO] - Train, Round 11 : loss => 1.2082222775734277,  accuracy: 0.5696086778292814, gradient_norm : 0.20153031768390983
[2025-09-19 11:05:42,132][flp2p.graph_runner][INFO] - Test, Round 11 : loss => 1.3852829915049611,  accuracy: 0.501
[2025-09-19 11:05:48,907][flp2p.graph_runner][INFO] - Train, Round 12 : loss => 1.3254789730126904,  accuracy: 0.5239612013892638, gradient_norm : 0.2049722324892052
[2025-09-19 11:05:52,750][flp2p.graph_runner][INFO] - Test, Round 12 : loss => 1.3433168908140003,  accuracy: 0.5216
[2025-09-19 11:05:59,506][flp2p.graph_runner][INFO] - Train, Round 13 : loss => 1.2388452497655944,  accuracy: 0.5588910408148626, gradient_norm : 0.21457732347981393
[2025-09-19 11:06:03,334][flp2p.graph_runner][INFO] - Test, Round 13 : loss => 1.330165811383594,  accuracy: 0.5284
[2025-09-19 11:06:09,297][flp2p.graph_runner][INFO] - Train, Round 14 : loss => 1.2125417502552764,  accuracy: 0.5591657147692178, gradient_norm : 0.2192875513935498
[2025-09-19 11:06:13,136][flp2p.graph_runner][INFO] - Test, Round 14 : loss => 1.310703491413009,  accuracy: 0.5339
[2025-09-19 11:06:20,182][flp2p.graph_runner][INFO] - Train, Round 15 : loss => 1.1556719082191274,  accuracy: 0.5904014055468902, gradient_norm : 0.24000081728208536
[2025-09-19 11:06:24,024][flp2p.graph_runner][INFO] - Test, Round 15 : loss => 1.3072813324695074,  accuracy: 0.5391
[2025-09-19 11:06:30,613][flp2p.graph_runner][INFO] - Train, Round 16 : loss => 1.1406597228795847,  accuracy: 0.592542270531401, gradient_norm : 0.22814088504396235
[2025-09-19 11:06:34,423][flp2p.graph_runner][INFO] - Test, Round 16 : loss => 1.2959578141198627,  accuracy: 0.5458
[2025-09-19 11:06:40,931][flp2p.graph_runner][INFO] - Train, Round 17 : loss => 1.1260031416660705,  accuracy: 0.5928453387770972, gradient_norm : 0.24825311970746133
[2025-09-19 11:06:44,739][flp2p.graph_runner][INFO] - Test, Round 17 : loss => 1.27954952911927,  accuracy: 0.5533
[2025-09-19 11:06:51,328][flp2p.graph_runner][INFO] - Train, Round 18 : loss => 0.9971644781088977,  accuracy: 0.6482821491652897, gradient_norm : 0.24592934762899135
[2025-09-19 11:06:55,173][flp2p.graph_runner][INFO] - Test, Round 18 : loss => 1.2876033814189451,  accuracy: 0.5554
[2025-09-19 11:07:00,328][flp2p.graph_runner][INFO] - Train, Round 19 : loss => 0.8502861701332229,  accuracy: 0.699336009492792, gradient_norm : 0.25740088301674263
[2025-09-19 11:07:04,101][flp2p.graph_runner][INFO] - Test, Round 19 : loss => 1.296631668105397,  accuracy: 0.5591
[2025-09-19 11:07:10,907][flp2p.graph_runner][INFO] - Train, Round 20 : loss => 0.8401453279864519,  accuracy: 0.7098195329087049, gradient_norm : 0.25764229416369233
[2025-09-19 11:07:14,739][flp2p.graph_runner][INFO] - Test, Round 20 : loss => 1.3055608788931439,  accuracy: 0.5622
[2025-09-19 11:07:21,226][flp2p.graph_runner][INFO] - Train, Round 21 : loss => 0.8615744454345269,  accuracy: 0.7003105282936766, gradient_norm : 0.24532376673391668
[2025-09-19 11:07:25,069][flp2p.graph_runner][INFO] - Test, Round 21 : loss => 1.3034594114738656,  accuracy: 0.5668
[2025-09-19 11:07:32,171][flp2p.graph_runner][INFO] - Train, Round 22 : loss => 0.9323770253589422,  accuracy: 0.6689019279128248, gradient_norm : 0.24086275235678525
[2025-09-19 11:07:35,994][flp2p.graph_runner][INFO] - Test, Round 22 : loss => 1.3238516609768027,  accuracy: 0.567
[2025-09-19 11:07:42,218][flp2p.graph_runner][INFO] - Train, Round 23 : loss => 0.5849499847322827,  accuracy: 0.8014866718570421, gradient_norm : 0.2813012942180918
[2025-09-19 11:07:46,071][flp2p.graph_runner][INFO] - Test, Round 23 : loss => 1.3556788019967363,  accuracy: 0.5725
[2025-09-19 11:07:52,653][flp2p.graph_runner][INFO] - Train, Round 24 : loss => 0.6480684895853707,  accuracy: 0.7741286746060891, gradient_norm : 0.2418482799838657
[2025-09-19 11:07:56,470][flp2p.graph_runner][INFO] - Test, Round 24 : loss => 1.395997181668608,  accuracy: 0.5709
[2025-09-19 11:08:04,015][flp2p.graph_runner][INFO] - Train, Round 25 : loss => 0.6871348139436279,  accuracy: 0.7600562718362552, gradient_norm : 0.23766511653996816
[2025-09-19 11:08:07,901][flp2p.graph_runner][INFO] - Test, Round 25 : loss => 1.4157637179963076,  accuracy: 0.5732
[2025-09-19 11:08:15,165][flp2p.graph_runner][INFO] - Train, Round 26 : loss => 0.5190586276991166,  accuracy: 0.8212908785775841, gradient_norm : 0.23567286143451077
[2025-09-19 11:08:19,034][flp2p.graph_runner][INFO] - Test, Round 26 : loss => 1.4670603447665727,  accuracy: 0.5767
[2025-09-19 11:08:26,092][flp2p.graph_runner][INFO] - Train, Round 27 : loss => 0.6240512403764457,  accuracy: 0.7766360018268416, gradient_norm : 0.21343111300456785
[2025-09-19 11:08:29,964][flp2p.graph_runner][INFO] - Test, Round 27 : loss => 1.514057521150182,  accuracy: 0.5762
[2025-09-19 11:08:37,435][flp2p.graph_runner][INFO] - Train, Round 28 : loss => 0.5536329715680632,  accuracy: 0.810824238344792, gradient_norm : 0.2562786779120886
[2025-09-19 11:08:41,304][flp2p.graph_runner][INFO] - Test, Round 28 : loss => 1.5483774384532736,  accuracy: 0.5791
[2025-09-19 11:08:48,641][flp2p.graph_runner][INFO] - Train, Round 29 : loss => 0.45329052798101643,  accuracy: 0.8454982887736777, gradient_norm : 0.2422586688909816
[2025-09-19 11:08:52,523][flp2p.graph_runner][INFO] - Test, Round 29 : loss => 1.6260372044006357,  accuracy: 0.5794
[2025-09-19 11:08:59,081][flp2p.graph_runner][INFO] - Train, Round 30 : loss => 0.3703341606822129,  accuracy: 0.874861868605853, gradient_norm : 0.21756974251337016
[2025-09-19 11:09:02,930][flp2p.graph_runner][INFO] - Test, Round 30 : loss => 1.681792795704971,  accuracy: 0.5811
[2025-09-19 11:09:10,797][flp2p.graph_runner][INFO] - Train, Round 31 : loss => 0.48447942157072355,  accuracy: 0.830050437222073, gradient_norm : 0.22296569021266147
[2025-09-19 11:09:14,667][flp2p.graph_runner][INFO] - Test, Round 31 : loss => 1.7446892558649316,  accuracy: 0.5811
[2025-09-19 11:09:20,135][flp2p.graph_runner][INFO] - Train, Round 32 : loss => 0.4999524163235638,  accuracy: 0.821301247771836, gradient_norm : 0.21472036676615655
[2025-09-19 11:09:24,031][flp2p.graph_runner][INFO] - Test, Round 32 : loss => 1.757363240781752,  accuracy: 0.5821
[2025-09-19 11:09:28,974][flp2p.graph_runner][INFO] - Train, Round 33 : loss => 0.41568595135875314,  accuracy: 0.8568031906734161, gradient_norm : 0.21936637700412998
[2025-09-19 11:09:32,845][flp2p.graph_runner][INFO] - Test, Round 33 : loss => 1.7986206756368552,  accuracy: 0.5844
[2025-09-19 11:09:38,757][flp2p.graph_runner][INFO] - Train, Round 34 : loss => 0.22480536421093958,  accuracy: 0.926521082268919, gradient_norm : 0.2007350935678283
[2025-09-19 11:09:42,643][flp2p.graph_runner][INFO] - Test, Round 34 : loss => 1.9121079505688228,  accuracy: 0.5852
[2025-09-19 11:09:48,580][flp2p.graph_runner][INFO] - Train, Round 35 : loss => 0.3920933641142556,  accuracy: 0.8648229460209276, gradient_norm : 0.22841712727879301
[2025-09-19 11:09:52,475][flp2p.graph_runner][INFO] - Test, Round 35 : loss => 1.9252697587649208,  accuracy: 0.5857
[2025-09-19 11:09:59,001][flp2p.graph_runner][INFO] - Train, Round 36 : loss => 0.3222393409832107,  accuracy: 0.8916812231175222, gradient_norm : 0.20380484847213673
[2025-09-19 11:10:02,878][flp2p.graph_runner][INFO] - Test, Round 36 : loss => 1.9702073220591922,  accuracy: 0.5875
[2025-09-19 11:10:10,069][flp2p.graph_runner][INFO] - Train, Round 37 : loss => 0.33886673476010704,  accuracy: 0.8854589228587448, gradient_norm : 0.1837432230690787
[2025-09-19 11:10:13,891][flp2p.graph_runner][INFO] - Test, Round 37 : loss => 1.956100735012711,  accuracy: 0.5889
[2025-09-19 11:10:20,606][flp2p.graph_runner][INFO] - Train, Round 38 : loss => 0.3034663717274662,  accuracy: 0.8987280631138304, gradient_norm : 0.17966558226804305
[2025-09-19 11:10:24,492][flp2p.graph_runner][INFO] - Test, Round 38 : loss => 2.024714095655651,  accuracy: 0.591
[2025-09-19 11:10:30,826][flp2p.graph_runner][INFO] - Train, Round 39 : loss => 0.2832178799896405,  accuracy: 0.9065794046102166, gradient_norm : 0.20841876106454693
[2025-09-19 11:10:34,747][flp2p.graph_runner][INFO] - Test, Round 39 : loss => 2.0665989827088227,  accuracy: 0.5933
[2025-09-19 11:10:41,523][flp2p.graph_runner][INFO] - Train, Round 40 : loss => 0.20840336721218322,  accuracy: 0.9282879669935841, gradient_norm : 0.1505814313114768
[2025-09-19 11:10:45,442][flp2p.graph_runner][INFO] - Test, Round 40 : loss => 2.116488726131523,  accuracy: 0.5918
[2025-09-19 11:10:53,264][flp2p.graph_runner][INFO] - Train, Round 41 : loss => 0.11298815748468262,  accuracy: 0.9635135135135133, gradient_norm : 0.14582006976071624
[2025-09-19 11:10:57,172][flp2p.graph_runner][INFO] - Test, Round 41 : loss => 2.182636532074532,  accuracy: 0.5927
[2025-09-19 11:11:03,894][flp2p.graph_runner][INFO] - Train, Round 42 : loss => 0.2805591728580341,  accuracy: 0.901443716370998, gradient_norm : 0.16522264121074737
[2025-09-19 11:11:07,776][flp2p.graph_runner][INFO] - Test, Round 42 : loss => 2.237070689126294,  accuracy: 0.5929
[2025-09-19 11:11:14,431][flp2p.graph_runner][INFO] - Train, Round 43 : loss => 0.2385176864808399,  accuracy: 0.918938962763664, gradient_norm : 0.13524174416493692
[2025-09-19 11:11:18,346][flp2p.graph_runner][INFO] - Test, Round 43 : loss => 2.28929785313098,  accuracy: 0.5916
[2025-09-19 11:11:25,348][flp2p.graph_runner][INFO] - Train, Round 44 : loss => 0.20187444006470634,  accuracy: 0.9309228760974911, gradient_norm : 0.200918573174592
[2025-09-19 11:11:29,246][flp2p.graph_runner][INFO] - Test, Round 44 : loss => 2.3309199151983035,  accuracy: 0.5923
[2025-09-19 11:11:36,496][flp2p.graph_runner][INFO] - Train, Round 45 : loss => 0.17170360880245283,  accuracy: 0.9428957920672496, gradient_norm : 0.17929098858810097
[2025-09-19 11:11:40,332][flp2p.graph_runner][INFO] - Test, Round 45 : loss => 2.368256242883169,  accuracy: 0.5926
[2025-09-19 11:11:47,901][flp2p.graph_runner][INFO] - Train, Round 46 : loss => 0.15938442011536405,  accuracy: 0.945807372793354, gradient_norm : 0.13766189535094736
[2025-09-19 11:11:51,726][flp2p.graph_runner][INFO] - Test, Round 46 : loss => 2.413286847025119,  accuracy: 0.5947
[2025-09-19 11:11:58,180][flp2p.graph_runner][INFO] - Train, Round 47 : loss => 0.13179386070376015,  accuracy: 0.9572086899275839, gradient_norm : 0.12002758759207899
[2025-09-19 11:12:02,033][flp2p.graph_runner][INFO] - Test, Round 47 : loss => 2.4568623282815647,  accuracy: 0.5959
[2025-09-19 11:12:08,291][flp2p.graph_runner][INFO] - Train, Round 48 : loss => 0.2514093450303459,  accuracy: 0.9128478164416556, gradient_norm : 0.21431672718469869
[2025-09-19 11:12:12,194][flp2p.graph_runner][INFO] - Test, Round 48 : loss => 2.4834934805302282,  accuracy: 0.5934
[2025-09-19 11:12:18,301][flp2p.graph_runner][INFO] - Train, Round 49 : loss => 0.11457912606018185,  accuracy: 0.9632069313393863, gradient_norm : 0.14475511117311177
[2025-09-19 11:12:22,170][flp2p.graph_runner][INFO] - Test, Round 49 : loss => 2.5198871854473612,  accuracy: 0.5927
[2025-09-19 11:12:28,372][flp2p.graph_runner][INFO] - Train, Round 50 : loss => 0.126250981881734,  accuracy: 0.9595017326964504, gradient_norm : 0.09376980109652855
[2025-09-19 11:12:32,319][flp2p.graph_runner][INFO] - Test, Round 50 : loss => 2.5519350622548846,  accuracy: 0.5945
[2025-09-19 11:12:39,045][flp2p.graph_runner][INFO] - Train, Round 51 : loss => 0.039245848979927216,  accuracy: 0.9883142842499103, gradient_norm : 0.1188630740345457
[2025-09-19 11:12:42,987][flp2p.graph_runner][INFO] - Test, Round 51 : loss => 2.5758685259528478,  accuracy: 0.5934
[2025-09-19 11:12:50,304][flp2p.graph_runner][INFO] - Train, Round 52 : loss => 0.06574201660261927,  accuracy: 0.9795808966861599, gradient_norm : 0.12017188680589024
[2025-09-19 11:12:54,194][flp2p.graph_runner][INFO] - Test, Round 52 : loss => 2.626644768728171,  accuracy: 0.5913
[2025-09-19 11:13:01,348][flp2p.graph_runner][INFO] - Train, Round 53 : loss => 0.10323548985632865,  accuracy: 0.9629744490963286, gradient_norm : 0.16203784621450718
[2025-09-19 11:13:05,201][flp2p.graph_runner][INFO] - Test, Round 53 : loss => 2.6654776121026296,  accuracy: 0.5918
[2025-09-19 11:13:10,338][flp2p.graph_runner][INFO] - Train, Round 54 : loss => 0.036132997520441926,  accuracy: 0.9901953835613202, gradient_norm : 0.1196897684165707
[2025-09-19 11:13:14,228][flp2p.graph_runner][INFO] - Test, Round 54 : loss => 2.6886703876893057,  accuracy: 0.592
[2025-09-19 11:13:21,943][flp2p.graph_runner][INFO] - Train, Round 55 : loss => 0.07738511173118323,  accuracy: 0.9743366614759854, gradient_norm : 0.09500894540272077
[2025-09-19 11:13:25,835][flp2p.graph_runner][INFO] - Test, Round 55 : loss => 2.733784725472033,  accuracy: 0.5926
[2025-09-19 11:13:32,385][flp2p.graph_runner][INFO] - Train, Round 56 : loss => 0.028110240513174397,  accuracy: 0.9923722778215077, gradient_norm : 0.09889474519127915
[2025-09-19 11:13:36,280][flp2p.graph_runner][INFO] - Test, Round 56 : loss => 2.763615276578168,  accuracy: 0.5937
[2025-09-19 11:13:42,718][flp2p.graph_runner][INFO] - Train, Round 57 : loss => 0.03695201491319539,  accuracy: 0.9886715887955814, gradient_norm : 0.13227367649727054
[2025-09-19 11:13:46,605][flp2p.graph_runner][INFO] - Test, Round 57 : loss => 2.788467647045818,  accuracy: 0.5934
[2025-09-19 11:13:52,347][flp2p.graph_runner][INFO] - Train, Round 58 : loss => 0.08364573232669685,  accuracy: 0.9710227392468019, gradient_norm : 0.17588466423332338
[2025-09-19 11:13:56,220][flp2p.graph_runner][INFO] - Test, Round 58 : loss => 2.8064652175293365,  accuracy: 0.5926
[2025-09-19 11:14:03,581][flp2p.graph_runner][INFO] - Train, Round 59 : loss => 0.09510158856374212,  accuracy: 0.9690652109118859, gradient_norm : 0.0769709200720812
[2025-09-19 11:14:07,464][flp2p.graph_runner][INFO] - Test, Round 59 : loss => 2.841731600238322,  accuracy: 0.5941
[2025-09-19 11:14:14,483][flp2p.graph_runner][INFO] - Train, Round 60 : loss => 0.05451190393580037,  accuracy: 0.9819353578752077, gradient_norm : 0.11419445201619435
[2025-09-19 11:14:18,403][flp2p.graph_runner][INFO] - Test, Round 60 : loss => 2.8772824642569876,  accuracy: 0.5924
[2025-09-19 11:14:25,502][flp2p.graph_runner][INFO] - Train, Round 61 : loss => 0.06858947513939552,  accuracy: 0.977608168216741, gradient_norm : 0.11208544326457599
[2025-09-19 11:14:29,396][flp2p.graph_runner][INFO] - Test, Round 61 : loss => 2.908610630563489,  accuracy: 0.5925
[2025-09-19 11:14:35,764][flp2p.graph_runner][INFO] - Train, Round 62 : loss => 0.051601444278016106,  accuracy: 0.9847265464371733, gradient_norm : 0.06538555079135898
[2025-09-19 11:14:39,642][flp2p.graph_runner][INFO] - Test, Round 62 : loss => 2.9252677062608714,  accuracy: 0.5923
[2025-09-19 11:14:45,708][flp2p.graph_runner][INFO] - Train, Round 63 : loss => 0.050211024916783586,  accuracy: 0.9845757319479682, gradient_norm : 0.05718403923408625
[2025-09-19 11:14:49,631][flp2p.graph_runner][INFO] - Test, Round 63 : loss => 2.945209884114256,  accuracy: 0.592
[2025-09-19 11:14:55,478][flp2p.graph_runner][INFO] - Train, Round 64 : loss => 0.08727912367296058,  accuracy: 0.9703988849804207, gradient_norm : 0.12393971987988937
[2025-09-19 11:14:59,361][flp2p.graph_runner][INFO] - Test, Round 64 : loss => 2.9653368774407443,  accuracy: 0.5918
[2025-09-19 11:15:05,249][flp2p.graph_runner][INFO] - Train, Round 65 : loss => 0.058499306390686755,  accuracy: 0.9814725601188325, gradient_norm : 0.08128210669799614
[2025-09-19 11:15:09,109][flp2p.graph_runner][INFO] - Test, Round 65 : loss => 2.9865573443675375,  accuracy: 0.5935
[2025-09-19 11:15:15,784][flp2p.graph_runner][INFO] - Train, Round 66 : loss => 0.019709484593897826,  accuracy: 0.9953828323612981, gradient_norm : 0.12753386395764382
[2025-09-19 11:15:19,683][flp2p.graph_runner][INFO] - Test, Round 66 : loss => 3.0085986123470922,  accuracy: 0.5939
[2025-09-19 11:15:27,266][flp2p.graph_runner][INFO] - Train, Round 67 : loss => 0.08296857035440572,  accuracy: 0.9755137315152681, gradient_norm : 0.0919223830197875
[2025-09-19 11:15:31,101][flp2p.graph_runner][INFO] - Test, Round 67 : loss => 3.0552387807217993,  accuracy: 0.5929
[2025-09-19 11:15:37,888][flp2p.graph_runner][INFO] - Train, Round 68 : loss => 0.015534161990192672,  accuracy: 0.9962396059530745, gradient_norm : 0.09131852702258139
[2025-09-19 11:15:41,735][flp2p.graph_runner][INFO] - Test, Round 68 : loss => 3.0703041599575767,  accuracy: 0.5926
[2025-09-19 11:15:48,882][flp2p.graph_runner][INFO] - Train, Round 69 : loss => 0.08246513083362694,  accuracy: 0.972437729685586, gradient_norm : 0.14054450928523066
[2025-09-19 11:15:52,744][flp2p.graph_runner][INFO] - Test, Round 69 : loss => 3.0881343543332407,  accuracy: 0.5916
[2025-09-19 11:15:59,052][flp2p.graph_runner][INFO] - Train, Round 70 : loss => 0.0326855308033297,  accuracy: 0.9904095542553834, gradient_norm : 0.08321069888954934
[2025-09-19 11:16:02,909][flp2p.graph_runner][INFO] - Test, Round 70 : loss => 3.105713945804446,  accuracy: 0.5925
[2025-09-19 11:16:10,063][flp2p.graph_runner][INFO] - Train, Round 71 : loss => 0.009514903906851861,  accuracy: 0.9980338703015285, gradient_norm : 0.03619444856386741
[2025-09-19 11:16:13,951][flp2p.graph_runner][INFO] - Test, Round 71 : loss => 3.1307489694194093,  accuracy: 0.5908
[2025-09-19 11:16:21,332][flp2p.graph_runner][INFO] - Train, Round 72 : loss => 0.035115803323508644,  accuracy: 0.9887858398496696, gradient_norm : 0.1110390476728124
[2025-09-19 11:16:25,174][flp2p.graph_runner][INFO] - Test, Round 72 : loss => 3.1526104495646843,  accuracy: 0.5896
[2025-09-19 11:16:31,349][flp2p.graph_runner][INFO] - Train, Round 73 : loss => 0.01139259974564483,  accuracy: 0.9967008161139087, gradient_norm : 0.08169573175022853
[2025-09-19 11:16:35,228][flp2p.graph_runner][INFO] - Test, Round 73 : loss => 3.164850466467593,  accuracy: 0.5906
[2025-09-19 11:16:42,494][flp2p.graph_runner][INFO] - Train, Round 74 : loss => 0.012069850525438656,  accuracy: 0.9973265281322155, gradient_norm : 0.022409692651224992
[2025-09-19 11:16:46,393][flp2p.graph_runner][INFO] - Test, Round 74 : loss => 3.1956910382054975,  accuracy: 0.5925
[2025-09-19 11:16:52,749][flp2p.graph_runner][INFO] - Train, Round 75 : loss => 0.01315445268112904,  accuracy: 0.9965481741658088, gradient_norm : 0.06365287306891916
[2025-09-19 11:16:56,664][flp2p.graph_runner][INFO] - Test, Round 75 : loss => 3.203106468022174,  accuracy: 0.5924
[2025-09-19 11:17:03,570][flp2p.graph_runner][INFO] - Train, Round 76 : loss => 0.031049184687441223,  accuracy: 0.990909090909091, gradient_norm : 0.11114873023230346
[2025-09-19 11:17:07,451][flp2p.graph_runner][INFO] - Test, Round 76 : loss => 3.2252371384138274,  accuracy: 0.5932
[2025-09-19 11:17:13,604][flp2p.graph_runner][INFO] - Train, Round 77 : loss => 0.06764404204924415,  accuracy: 0.9781408191440407, gradient_norm : 0.12829056754284734
[2025-09-19 11:17:17,451][flp2p.graph_runner][INFO] - Test, Round 77 : loss => 3.2631695850481472,  accuracy: 0.5935
[2025-09-19 11:17:23,343][flp2p.graph_runner][INFO] - Train, Round 78 : loss => 0.007307405956985635,  accuracy: 0.9987291330102316, gradient_norm : 0.06531268704375916
[2025-09-19 11:17:27,191][flp2p.graph_runner][INFO] - Test, Round 78 : loss => 3.2812859583093363,  accuracy: 0.5938
[2025-09-19 11:17:33,431][flp2p.graph_runner][INFO] - Train, Round 79 : loss => 0.008424508357239622,  accuracy: 0.9978823050295255, gradient_norm : 0.08696526596667169
[2025-09-19 11:17:37,285][flp2p.graph_runner][INFO] - Test, Round 79 : loss => 3.2920749826620894,  accuracy: 0.593
[2025-09-19 11:17:44,545][flp2p.graph_runner][INFO] - Train, Round 80 : loss => 0.013237991144542206,  accuracy: 0.9965479416698929, gradient_norm : 0.11097219582064105
[2025-09-19 11:17:48,429][flp2p.graph_runner][INFO] - Test, Round 80 : loss => 3.314225115335045,  accuracy: 0.5934
[2025-09-19 11:17:55,203][flp2p.graph_runner][INFO] - Train, Round 81 : loss => 0.00441521185627419,  accuracy: 0.9992098609355247, gradient_norm : 0.020930091953905543
[2025-09-19 11:17:59,131][flp2p.graph_runner][INFO] - Test, Round 81 : loss => 3.3262866592499196,  accuracy: 0.5921
[2025-09-19 11:18:05,739][flp2p.graph_runner][INFO] - Train, Round 82 : loss => 0.011888795776508566,  accuracy: 0.9973070017953322, gradient_norm : 0.13950435003174677
[2025-09-19 11:18:09,629][flp2p.graph_runner][INFO] - Test, Round 82 : loss => 3.3349539682770106,  accuracy: 0.5917
[2025-09-19 11:18:16,035][flp2p.graph_runner][INFO] - Train, Round 83 : loss => 0.004273916413035776,  accuracy: 0.9997127427323911, gradient_norm : 0.02302036189261495
[2025-09-19 11:18:19,887][flp2p.graph_runner][INFO] - Test, Round 83 : loss => 3.3559727400456896,  accuracy: 0.5911
[2025-09-19 11:18:25,841][flp2p.graph_runner][INFO] - Train, Round 84 : loss => 0.0021231888957557233,  accuracy: 0.9999041043344842, gradient_norm : 0.010886838031889832
[2025-09-19 11:18:29,746][flp2p.graph_runner][INFO] - Test, Round 84 : loss => 3.367073888231221,  accuracy: 0.591
[2025-09-19 11:18:36,816][flp2p.graph_runner][INFO] - Train, Round 85 : loss => 0.016090040892941842,  accuracy: 0.9955553408377217, gradient_norm : 0.08775187826906548
[2025-09-19 11:18:40,755][flp2p.graph_runner][INFO] - Test, Round 85 : loss => 3.378458621014213,  accuracy: 0.5919
[2025-09-19 11:18:47,857][flp2p.graph_runner][INFO] - Train, Round 86 : loss => 0.0068365336951869,  accuracy: 0.9986055952696142, gradient_norm : 0.0843950749511208
[2025-09-19 11:18:51,771][flp2p.graph_runner][INFO] - Test, Round 86 : loss => 3.392801330385771,  accuracy: 0.5924
[2025-09-19 11:18:57,716][flp2p.graph_runner][INFO] - Train, Round 87 : loss => 0.007559128756236831,  accuracy: 0.9981263383297645, gradient_norm : 0.08178475536501886
[2025-09-19 11:19:01,586][flp2p.graph_runner][INFO] - Test, Round 87 : loss => 3.3963149677498805,  accuracy: 0.5919
[2025-09-19 11:19:08,279][flp2p.graph_runner][INFO] - Train, Round 88 : loss => 0.004246429156114073,  accuracy: 0.99940034888792, gradient_norm : 0.057331549944057404
[2025-09-19 11:19:12,128][flp2p.graph_runner][INFO] - Test, Round 88 : loss => 3.4072458338286276,  accuracy: 0.5926
[2025-09-19 11:19:18,466][flp2p.graph_runner][INFO] - Train, Round 89 : loss => 0.008081306932841443,  accuracy: 0.9989189189189189, gradient_norm : 0.09783327858845244
[2025-09-19 11:19:22,390][flp2p.graph_runner][INFO] - Test, Round 89 : loss => 3.4327248980166596,  accuracy: 0.5946
[2025-09-19 11:19:29,118][flp2p.graph_runner][INFO] - Train, Round 90 : loss => 0.007038731168354291,  accuracy: 0.9985882698054747, gradient_norm : 0.10112290470580196
[2025-09-19 11:19:32,964][flp2p.graph_runner][INFO] - Test, Round 90 : loss => 3.4491153474287266,  accuracy: 0.5956
[2025-09-19 11:19:40,187][flp2p.graph_runner][INFO] - Train, Round 91 : loss => 0.0019628605200064267,  accuracy: 1.0, gradient_norm : 0.007986330108431697
[2025-09-19 11:19:44,132][flp2p.graph_runner][INFO] - Test, Round 91 : loss => 3.4657289169588994,  accuracy: 0.595
[2025-09-19 11:19:51,387][flp2p.graph_runner][INFO] - Train, Round 92 : loss => 0.005415296949304859,  accuracy: 0.9990235566947395, gradient_norm : 0.08901487824024538
[2025-09-19 11:19:55,312][flp2p.graph_runner][INFO] - Test, Round 92 : loss => 3.481088923392143,  accuracy: 0.5945
[2025-09-19 11:20:00,954][flp2p.graph_runner][INFO] - Train, Round 93 : loss => 0.002591982047702408,  accuracy: 0.9993766557581425, gradient_norm : 0.0619471685645301
[2025-09-19 11:20:04,858][flp2p.graph_runner][INFO] - Test, Round 93 : loss => 3.4868458491001975,  accuracy: 0.5946
[2025-09-19 11:20:11,885][flp2p.graph_runner][INFO] - Train, Round 94 : loss => 0.002376469534027759,  accuracy: 0.9997732646649176, gradient_norm : 0.029569259221700107
[2025-09-19 11:20:15,792][flp2p.graph_runner][INFO] - Test, Round 94 : loss => 3.4990166034756593,  accuracy: 0.5947
[2025-09-19 11:20:22,588][flp2p.graph_runner][INFO] - Train, Round 95 : loss => 0.0014827042328320107,  accuracy: 0.9999461584019814, gradient_norm : 0.006948662997876306
[2025-09-19 11:20:26,459][flp2p.graph_runner][INFO] - Test, Round 95 : loss => 3.506961208958451,  accuracy: 0.5947
[2025-09-19 11:20:32,828][flp2p.graph_runner][INFO] - Train, Round 96 : loss => 0.027641356764655803,  accuracy: 0.9910362648133297, gradient_norm : 0.04771076130405741
[2025-09-19 11:20:36,677][flp2p.graph_runner][INFO] - Test, Round 96 : loss => 3.5181120331696243,  accuracy: 0.5943
[2025-09-19 11:20:42,862][flp2p.graph_runner][INFO] - Train, Round 97 : loss => 0.0014211564793578807,  accuracy: 1.0, gradient_norm : 0.006978065255768981
[2025-09-19 11:20:46,765][flp2p.graph_runner][INFO] - Test, Round 97 : loss => 3.527010695139408,  accuracy: 0.594
[2025-09-19 11:20:53,793][flp2p.graph_runner][INFO] - Train, Round 98 : loss => 0.002455338389970502,  accuracy: 0.9996141772329493, gradient_norm : 0.02167792971430027
[2025-09-19 11:20:57,714][flp2p.graph_runner][INFO] - Test, Round 98 : loss => 3.5343965904502554,  accuracy: 0.5935
[2025-09-19 11:21:04,082][flp2p.graph_runner][INFO] - Train, Round 99 : loss => 0.0202680294871258,  accuracy: 0.9929914623268343, gradient_norm : 0.026739630626603957
[2025-09-19 11:21:07,990][flp2p.graph_runner][INFO] - Test, Round 99 : loss => 3.53649104189402,  accuracy: 0.5937
[2025-09-19 11:21:13,925][flp2p.graph_runner][INFO] - Train, Round 100 : loss => 0.001112758081504782,  accuracy: 1.0, gradient_norm : 0.007843880039249837
[2025-09-19 11:21:17,829][flp2p.graph_runner][INFO] - Test, Round 100 : loss => 3.544791659638196,  accuracy: 0.5937
[2025-09-19 11:21:24,528][flp2p.graph_runner][INFO] - Train, Round 101 : loss => 0.018713632636342833,  accuracy: 0.9949647532729103, gradient_norm : 0.051182759768646
[2025-09-19 11:21:28,420][flp2p.graph_runner][INFO] - Test, Round 101 : loss => 3.554770329156158,  accuracy: 0.5931
[2025-09-19 11:21:36,147][flp2p.graph_runner][INFO] - Train, Round 102 : loss => 0.0023572209533253563,  accuracy: 0.999687242798354, gradient_norm : 0.08392297908547455
[2025-09-19 11:21:40,035][flp2p.graph_runner][INFO] - Test, Round 102 : loss => 3.562133707702425,  accuracy: 0.5927
[2025-09-19 11:21:47,664][flp2p.graph_runner][INFO] - Train, Round 103 : loss => 0.010263440525130445,  accuracy: 0.9971494657284338, gradient_norm : 0.039756001714613604
[2025-09-19 11:21:51,571][flp2p.graph_runner][INFO] - Test, Round 103 : loss => 3.573670799552621,  accuracy: 0.5944
[2025-09-19 11:21:58,459][flp2p.graph_runner][INFO] - Train, Round 104 : loss => 0.0019950759061739816,  accuracy: 0.9997586539787615, gradient_norm : 0.03619537171572938
[2025-09-19 11:22:02,360][flp2p.graph_runner][INFO] - Test, Round 104 : loss => 3.579975331679239,  accuracy: 0.594
[2025-09-19 11:22:09,161][flp2p.graph_runner][INFO] - Train, Round 105 : loss => 0.0017511622366748409,  accuracy: 0.9998599366213212, gradient_norm : 0.021543910231830263
[2025-09-19 11:22:13,005][flp2p.graph_runner][INFO] - Test, Round 105 : loss => 3.591893184432621,  accuracy: 0.5946
[2025-09-19 11:22:19,944][flp2p.graph_runner][INFO] - Train, Round 106 : loss => 0.0011910008707957226,  accuracy: 0.9999491301251399, gradient_norm : 0.03482755284675515
[2025-09-19 11:22:23,819][flp2p.graph_runner][INFO] - Test, Round 106 : loss => 3.597521142870107,  accuracy: 0.5943
[2025-09-19 11:22:30,381][flp2p.graph_runner][INFO] - Train, Round 107 : loss => 0.001585452080258039,  accuracy: 0.9999417419166909, gradient_norm : 0.011050174385405238
[2025-09-19 11:22:34,304][flp2p.graph_runner][INFO] - Test, Round 107 : loss => 3.603732276029645,  accuracy: 0.5942
[2025-09-19 11:22:40,919][flp2p.graph_runner][INFO] - Train, Round 108 : loss => 0.0014865194016776793,  accuracy: 0.9999121188153618, gradient_norm : 0.03491567464140933
[2025-09-19 11:22:44,823][flp2p.graph_runner][INFO] - Test, Round 108 : loss => 3.61090578821784,  accuracy: 0.5944
[2025-09-19 11:22:51,811][flp2p.graph_runner][INFO] - Train, Round 109 : loss => 0.0009589817005512852,  accuracy: 1.0, gradient_norm : 0.004222527033976998
[2025-09-19 11:22:55,647][flp2p.graph_runner][INFO] - Test, Round 109 : loss => 3.617239157105691,  accuracy: 0.5945
[2025-09-19 11:23:02,989][flp2p.graph_runner][INFO] - Train, Round 110 : loss => 0.009096007372350917,  accuracy: 0.9974407582938388, gradient_norm : 0.03836569548005605
[2025-09-19 11:23:06,857][flp2p.graph_runner][INFO] - Test, Round 110 : loss => 3.6267745315953492,  accuracy: 0.5934
[2025-09-19 11:23:12,070][flp2p.graph_runner][INFO] - Train, Round 111 : loss => 0.00880163405514068,  accuracy: 0.9977154867155553, gradient_norm : 0.044385689142369976
[2025-09-19 11:23:15,927][flp2p.graph_runner][INFO] - Test, Round 111 : loss => 3.634709148024946,  accuracy: 0.594
[2025-09-19 11:23:22,342][flp2p.graph_runner][INFO] - Train, Round 112 : loss => 0.0014620455234843036,  accuracy: 0.999922772468385, gradient_norm : 0.019057426296842383
[2025-09-19 11:23:26,229][flp2p.graph_runner][INFO] - Test, Round 112 : loss => 3.6393256159377576,  accuracy: 0.5942
[2025-09-19 11:23:32,841][flp2p.graph_runner][INFO] - Train, Round 113 : loss => 0.0011373787293605423,  accuracy: 1.0, gradient_norm : 0.02402342517216546
[2025-09-19 11:23:36,765][flp2p.graph_runner][INFO] - Test, Round 113 : loss => 3.644481476211794,  accuracy: 0.5941
[2025-09-19 11:23:43,513][flp2p.graph_runner][INFO] - Train, Round 114 : loss => 0.0007097211498793147,  accuracy: 1.0, gradient_norm : 0.0034902394539451428
[2025-09-19 11:23:47,375][flp2p.graph_runner][INFO] - Test, Round 114 : loss => 3.6501561710925854,  accuracy: 0.5942
[2025-09-19 11:23:55,388][flp2p.graph_runner][INFO] - Train, Round 115 : loss => 0.00242070964056981,  accuracy: 0.99983922829582, gradient_norm : 0.009480960526968639
[2025-09-19 11:23:59,261][flp2p.graph_runner][INFO] - Test, Round 115 : loss => 3.6660326848617864,  accuracy: 0.5955
[2025-09-19 11:24:06,077][flp2p.graph_runner][INFO] - Train, Round 116 : loss => 0.0009335896115524305,  accuracy: 0.9999829619028148, gradient_norm : 0.007200979873523726
[2025-09-19 11:24:09,953][flp2p.graph_runner][INFO] - Test, Round 116 : loss => 3.673198440324731,  accuracy: 0.5956
[2025-09-19 11:24:17,415][flp2p.graph_runner][INFO] - Train, Round 117 : loss => 0.0019350301820321308,  accuracy: 0.9999516136836503, gradient_norm : 0.017413105066454233
[2025-09-19 11:24:21,288][flp2p.graph_runner][INFO] - Test, Round 117 : loss => 3.6808765557500718,  accuracy: 0.5952
[2025-09-19 11:24:27,872][flp2p.graph_runner][INFO] - Train, Round 118 : loss => 0.0012493845745856368,  accuracy: 1.0, gradient_norm : 0.0159188244559375
[2025-09-19 11:24:31,721][flp2p.graph_runner][INFO] - Test, Round 118 : loss => 3.68576780293415,  accuracy: 0.595
[2025-09-19 11:24:39,199][flp2p.graph_runner][INFO] - Train, Round 119 : loss => 0.0012037796653363948,  accuracy: 1.0, gradient_norm : 0.014361215929327964
[2025-09-19 11:24:43,089][flp2p.graph_runner][INFO] - Test, Round 119 : loss => 3.69245386990973,  accuracy: 0.5944
[2025-09-19 11:24:49,992][flp2p.graph_runner][INFO] - Train, Round 120 : loss => 0.0013335637010806428,  accuracy: 0.9999825397656835, gradient_norm : 0.007684178909230975
[2025-09-19 11:24:53,854][flp2p.graph_runner][INFO] - Test, Round 120 : loss => 3.6998958121279246,  accuracy: 0.594
[2025-09-19 11:25:00,277][flp2p.graph_runner][INFO] - Train, Round 121 : loss => 0.0009359104608286957,  accuracy: 1.0, gradient_norm : 0.0070977055706875
[2025-09-19 11:25:04,155][flp2p.graph_runner][INFO] - Test, Round 121 : loss => 3.7040250307993463,  accuracy: 0.5944
[2025-09-19 11:25:10,891][flp2p.graph_runner][INFO] - Train, Round 122 : loss => 0.0014899800573540335,  accuracy: 1.0, gradient_norm : 0.02308179163192082
[2025-09-19 11:25:14,768][flp2p.graph_runner][INFO] - Test, Round 122 : loss => 3.709655270387431,  accuracy: 0.5947
[2025-09-19 11:25:21,202][flp2p.graph_runner][INFO] - Train, Round 123 : loss => 0.0014486572085036186,  accuracy: 1.0, gradient_norm : 0.016120191616213825
[2025-09-19 11:25:25,137][flp2p.graph_runner][INFO] - Test, Round 123 : loss => 3.715953310296635,  accuracy: 0.5945
[2025-09-19 11:25:32,253][flp2p.graph_runner][INFO] - Train, Round 124 : loss => 0.0006812777492957668,  accuracy: 1.0, gradient_norm : 0.01288906772585668
[2025-09-19 11:25:36,149][flp2p.graph_runner][INFO] - Test, Round 124 : loss => 3.7202399628775225,  accuracy: 0.5944
[2025-09-19 11:25:42,745][flp2p.graph_runner][INFO] - Train, Round 125 : loss => 0.0009414850624708109,  accuracy: 1.0, gradient_norm : 0.012893645257418526
[2025-09-19 11:25:46,643][flp2p.graph_runner][INFO] - Test, Round 125 : loss => 3.723608488874659,  accuracy: 0.5941
[2025-09-19 11:25:53,537][flp2p.graph_runner][INFO] - Train, Round 126 : loss => 0.00111062986815728,  accuracy: 1.0, gradient_norm : 0.013971530418297745
[2025-09-19 11:25:57,429][flp2p.graph_runner][INFO] - Test, Round 126 : loss => 3.7331055994936206,  accuracy: 0.5937
[2025-09-19 11:26:04,032][flp2p.graph_runner][INFO] - Train, Round 127 : loss => 0.0009181432918170505,  accuracy: 0.9999603056465219, gradient_norm : 0.0218819605897307
[2025-09-19 11:26:07,931][flp2p.graph_runner][INFO] - Test, Round 127 : loss => 3.737168760783015,  accuracy: 0.5938
[2025-09-19 11:26:14,908][flp2p.graph_runner][INFO] - Train, Round 128 : loss => 0.0009630123073247547,  accuracy: 0.9998870330025013, gradient_norm : 0.03273360486651174
[2025-09-19 11:26:18,317][flp2p.graph_runner][INFO] - Test, Round 128 : loss => 3.73983677549962,  accuracy: 0.5941
[2025-09-19 11:26:25,445][flp2p.graph_runner][INFO] - Train, Round 129 : loss => 0.0005577689805297731,  accuracy: 1.0, gradient_norm : 0.002897354498240707
[2025-09-19 11:26:28,943][flp2p.graph_runner][INFO] - Test, Round 129 : loss => 3.7435715096806876,  accuracy: 0.5938
[2025-09-19 11:26:35,798][flp2p.graph_runner][INFO] - Train, Round 130 : loss => 0.0006629033767864564,  accuracy: 0.9999830364715859, gradient_norm : 0.017005281279344517
[2025-09-19 11:26:39,348][flp2p.graph_runner][INFO] - Test, Round 130 : loss => 3.747293362186412,  accuracy: 0.594
[2025-09-19 11:26:45,242][flp2p.graph_runner][INFO] - Train, Round 131 : loss => 0.000460304111890769,  accuracy: 1.0, gradient_norm : 0.002601054189769966
[2025-09-19 11:26:48,650][flp2p.graph_runner][INFO] - Test, Round 131 : loss => 3.750412222185194,  accuracy: 0.594
[2025-09-19 11:26:55,683][flp2p.graph_runner][INFO] - Train, Round 132 : loss => 0.0008518129287392211,  accuracy: 1.0, gradient_norm : 0.010187034174536449
[2025-09-19 11:26:59,028][flp2p.graph_runner][INFO] - Test, Round 132 : loss => 3.7568695621975587,  accuracy: 0.5939
[2025-09-19 11:27:05,518][flp2p.graph_runner][INFO] - Train, Round 133 : loss => 0.000928549413565988,  accuracy: 1.0, gradient_norm : 0.007041660496511347
[2025-09-19 11:27:08,855][flp2p.graph_runner][INFO] - Test, Round 133 : loss => 3.761148066992792,  accuracy: 0.5939
[2025-09-19 11:27:15,505][flp2p.graph_runner][INFO] - Train, Round 134 : loss => 0.0008368976391576515,  accuracy: 1.0, gradient_norm : 0.003881297744917565
[2025-09-19 11:27:18,835][flp2p.graph_runner][INFO] - Test, Round 134 : loss => 3.7686811138633094,  accuracy: 0.5939
[2025-09-19 11:27:25,429][flp2p.graph_runner][INFO] - Train, Round 135 : loss => 0.0006387009993786001,  accuracy: 1.0, gradient_norm : 0.00961889324985972
[2025-09-19 11:27:28,817][flp2p.graph_runner][INFO] - Test, Round 135 : loss => 3.771404725195763,  accuracy: 0.5942
[2025-09-19 11:27:36,695][flp2p.graph_runner][INFO] - Train, Round 136 : loss => 0.0006748485125041384,  accuracy: 1.0, gradient_norm : 0.008475029711058879
[2025-09-19 11:27:40,038][flp2p.graph_runner][INFO] - Test, Round 136 : loss => 3.775394650905113,  accuracy: 0.5942
[2025-09-19 11:27:45,713][flp2p.graph_runner][INFO] - Train, Round 137 : loss => 0.0008633759414624795,  accuracy: 1.0, gradient_norm : 0.009048218462159884
[2025-09-19 11:27:49,020][flp2p.graph_runner][INFO] - Test, Round 137 : loss => 3.7808702048847858,  accuracy: 0.5944
[2025-09-19 11:27:55,480][flp2p.graph_runner][INFO] - Train, Round 138 : loss => 0.000650420296275484,  accuracy: 1.0, gradient_norm : 0.003962983467580958
[2025-09-19 11:27:58,757][flp2p.graph_runner][INFO] - Test, Round 138 : loss => 3.7853474457991987,  accuracy: 0.5941
[2025-09-19 11:28:05,631][flp2p.graph_runner][INFO] - Train, Round 139 : loss => 0.0005999645833737107,  accuracy: 1.0, gradient_norm : 0.00792788915080397
[2025-09-19 11:28:08,896][flp2p.graph_runner][INFO] - Test, Round 139 : loss => 3.7896914124212278,  accuracy: 0.5941
[2025-09-19 11:28:14,708][flp2p.graph_runner][INFO] - Train, Round 140 : loss => 0.0006798050327028621,  accuracy: 1.0, gradient_norm : 0.0048291629760746375
[2025-09-19 11:28:17,960][flp2p.graph_runner][INFO] - Test, Round 140 : loss => 3.7942074536374815,  accuracy: 0.5943
[2025-09-19 11:28:24,228][flp2p.graph_runner][INFO] - Train, Round 141 : loss => 0.0007157666529701102,  accuracy: 1.0, gradient_norm : 0.00804373121163905
[2025-09-19 11:28:27,550][flp2p.graph_runner][INFO] - Test, Round 141 : loss => 3.7973328458181625,  accuracy: 0.5943
[2025-09-19 11:28:34,563][flp2p.graph_runner][INFO] - Train, Round 142 : loss => 0.0008245773401158247,  accuracy: 1.0, gradient_norm : 0.00840039919825603
[2025-09-19 11:28:37,927][flp2p.graph_runner][INFO] - Test, Round 142 : loss => 3.805520188780784,  accuracy: 0.594
[2025-09-19 11:28:44,210][flp2p.graph_runner][INFO] - Train, Round 143 : loss => 0.0005528937213006858,  accuracy: 1.0, gradient_norm : 0.007415229907923354
[2025-09-19 11:28:47,497][flp2p.graph_runner][INFO] - Test, Round 143 : loss => 3.8066989087365894,  accuracy: 0.594
[2025-09-19 11:28:53,571][flp2p.graph_runner][INFO] - Train, Round 144 : loss => 0.0008040141236769922,  accuracy: 1.0, gradient_norm : 0.008535601191172339
[2025-09-19 11:28:56,926][flp2p.graph_runner][INFO] - Test, Round 144 : loss => 3.8097444029529384,  accuracy: 0.5939
[2025-09-19 11:29:04,342][flp2p.graph_runner][INFO] - Train, Round 145 : loss => 0.0004869035332428112,  accuracy: 1.0, gradient_norm : 0.0029788766513333437
[2025-09-19 11:29:07,703][flp2p.graph_runner][INFO] - Test, Round 145 : loss => 3.8144564825248715,  accuracy: 0.5942
[2025-09-19 11:29:15,038][flp2p.graph_runner][INFO] - Train, Round 146 : loss => 0.0006180995488677895,  accuracy: 1.0, gradient_norm : 0.0030244756014574273
[2025-09-19 11:29:18,332][flp2p.graph_runner][INFO] - Test, Round 146 : loss => 3.8198073664102687,  accuracy: 0.5942
[2025-09-19 11:29:23,816][flp2p.graph_runner][INFO] - Train, Round 147 : loss => 0.0006369461144678223,  accuracy: 1.0, gradient_norm : 0.007935997294827017
[2025-09-19 11:29:27,111][flp2p.graph_runner][INFO] - Test, Round 147 : loss => 3.821847854792924,  accuracy: 0.5942
[2025-09-19 11:29:33,560][flp2p.graph_runner][INFO] - Train, Round 148 : loss => 0.0006107716255876284,  accuracy: 1.0, gradient_norm : 0.003864507517776012
[2025-09-19 11:29:36,912][flp2p.graph_runner][INFO] - Test, Round 148 : loss => 3.824904306198642,  accuracy: 0.5939
[2025-09-19 11:29:43,219][flp2p.graph_runner][INFO] - Train, Round 149 : loss => 0.0006035614459797356,  accuracy: 1.0, gradient_norm : 0.007498888349348437
[2025-09-19 11:29:46,526][flp2p.graph_runner][INFO] - Test, Round 149 : loss => 3.827671979642428,  accuracy: 0.5937
[2025-09-19 11:29:51,885][flp2p.graph_runner][INFO] - Train, Round 150 : loss => 0.0004528539838902403,  accuracy: 1.0, gradient_norm : 0.0025447395072593177
[2025-09-19 11:29:55,421][flp2p.graph_runner][INFO] - Test, Round 150 : loss => 3.8293679018822475,  accuracy: 0.5942
[2025-09-19 11:30:06,872][flp2p.graph_runner][INFO] - Train, Round 151 : loss => 0.0006514892652211574,  accuracy: 1.0, gradient_norm : 0.007710573731863877
[2025-09-19 11:30:10,289][flp2p.graph_runner][INFO] - Test, Round 151 : loss => 3.833813104252797,  accuracy: 0.5943
[2025-09-19 11:30:16,796][flp2p.graph_runner][INFO] - Train, Round 152 : loss => 0.0005020669981457023,  accuracy: 1.0, gradient_norm : 0.003469904535138997
[2025-09-19 11:30:20,277][flp2p.graph_runner][INFO] - Test, Round 152 : loss => 3.837018433458588,  accuracy: 0.5941
[2025-09-19 11:30:26,845][flp2p.graph_runner][INFO] - Train, Round 153 : loss => 0.0005585296556781368,  accuracy: 1.0, gradient_norm : 0.0033087633336843516
[2025-09-19 11:30:30,274][flp2p.graph_runner][INFO] - Test, Round 153 : loss => 3.840791799675407,  accuracy: 0.594
[2025-09-19 11:30:36,452][flp2p.graph_runner][INFO] - Train, Round 154 : loss => 0.0006447634237500618,  accuracy: 1.0, gradient_norm : 0.004851785181031567
[2025-09-19 11:30:39,870][flp2p.graph_runner][INFO] - Test, Round 154 : loss => 3.8443011178497035,  accuracy: 0.5937
[2025-09-19 11:30:45,784][flp2p.graph_runner][INFO] - Train, Round 155 : loss => 0.000488311587151236,  accuracy: 1.0, gradient_norm : 0.002564842465451995
[2025-09-19 11:30:49,216][flp2p.graph_runner][INFO] - Test, Round 155 : loss => 3.8465210475583436,  accuracy: 0.5936
[2025-09-19 11:30:56,293][flp2p.graph_runner][INFO] - Train, Round 156 : loss => 0.0005013172090194691,  accuracy: 1.0, gradient_norm : 0.005810802312459999
[2025-09-19 11:30:59,730][flp2p.graph_runner][INFO] - Test, Round 156 : loss => 3.850601199225797,  accuracy: 0.5939
[2025-09-19 11:31:05,344][flp2p.graph_runner][INFO] - Train, Round 157 : loss => 0.0004895541578576615,  accuracy: 1.0, gradient_norm : 0.002649087412958454
[2025-09-19 11:31:08,794][flp2p.graph_runner][INFO] - Test, Round 157 : loss => 3.853661623244094,  accuracy: 0.5939
[2025-09-19 11:31:15,790][flp2p.graph_runner][INFO] - Train, Round 158 : loss => 0.0005108176743013655,  accuracy: 1.0, gradient_norm : 0.0031883319350924006
[2025-09-19 11:31:19,215][flp2p.graph_runner][INFO] - Test, Round 158 : loss => 3.8577886047223044,  accuracy: 0.5945
[2025-09-19 11:31:25,080][flp2p.graph_runner][INFO] - Train, Round 159 : loss => 0.0005308450939421557,  accuracy: 1.0, gradient_norm : 0.005944395478552405
[2025-09-19 11:31:28,589][flp2p.graph_runner][INFO] - Test, Round 159 : loss => 3.860142464643721,  accuracy: 0.5943
[2025-09-19 11:31:35,135][flp2p.graph_runner][INFO] - Train, Round 160 : loss => 0.0005526594253200647,  accuracy: 1.0, gradient_norm : 0.004091714633294217
[2025-09-19 11:31:38,559][flp2p.graph_runner][INFO] - Test, Round 160 : loss => 3.8619037670886573,  accuracy: 0.5939
[2025-09-19 11:31:44,599][flp2p.graph_runner][INFO] - Train, Round 161 : loss => 0.0005483555448210864,  accuracy: 1.0, gradient_norm : 0.006416271335641444
[2025-09-19 11:31:48,818][flp2p.graph_runner][INFO] - Test, Round 161 : loss => 3.864116796864283,  accuracy: 0.5942
[2025-09-19 11:31:57,342][flp2p.graph_runner][INFO] - Train, Round 162 : loss => 0.00046812944905064147,  accuracy: 1.0, gradient_norm : 0.0030419697116971724
[2025-09-19 11:32:02,346][flp2p.graph_runner][INFO] - Test, Round 162 : loss => 3.865962035244318,  accuracy: 0.5944
[2025-09-19 11:32:10,601][flp2p.graph_runner][INFO] - Train, Round 163 : loss => 0.0005090490755669535,  accuracy: 1.0, gradient_norm : 0.0034197581578809294
[2025-09-19 11:32:14,183][flp2p.graph_runner][INFO] - Test, Round 163 : loss => 3.869573525816935,  accuracy: 0.594
[2025-09-19 11:32:21,564][flp2p.graph_runner][INFO] - Train, Round 164 : loss => 0.00035902189353049515,  accuracy: 1.0, gradient_norm : 0.0025624360713687286
[2025-09-19 11:32:26,080][flp2p.graph_runner][INFO] - Test, Round 164 : loss => 3.871356415078578,  accuracy: 0.5936
[2025-09-19 11:32:35,291][flp2p.graph_runner][INFO] - Train, Round 165 : loss => 0.00044524438147582887,  accuracy: 1.0, gradient_norm : 0.0026532930870397436
[2025-09-19 11:32:38,817][flp2p.graph_runner][INFO] - Test, Round 165 : loss => 3.8730482807714632,  accuracy: 0.5939
[2025-09-19 11:32:45,204][flp2p.graph_runner][INFO] - Train, Round 166 : loss => 0.0004255242482368897,  accuracy: 1.0, gradient_norm : 0.0027125605353299958
[2025-09-19 11:32:48,772][flp2p.graph_runner][INFO] - Test, Round 166 : loss => 3.875298230958572,  accuracy: 0.5942
[2025-09-19 11:32:55,789][flp2p.graph_runner][INFO] - Train, Round 167 : loss => 0.0005016903883859107,  accuracy: 1.0, gradient_norm : 0.005540396984604354
[2025-09-19 11:32:59,285][flp2p.graph_runner][INFO] - Test, Round 167 : loss => 3.878863403954929,  accuracy: 0.5943
[2025-09-19 11:33:05,784][flp2p.graph_runner][INFO] - Train, Round 168 : loss => 0.00044110713866887466,  accuracy: 1.0, gradient_norm : 0.0050524025936471316
[2025-09-19 11:33:09,247][flp2p.graph_runner][INFO] - Test, Round 168 : loss => 3.8809171816206733,  accuracy: 0.5942
[2025-09-19 11:33:15,420][flp2p.graph_runner][INFO] - Train, Round 169 : loss => 0.0005268380165985538,  accuracy: 1.0, gradient_norm : 0.005876310896485573
[2025-09-19 11:33:18,925][flp2p.graph_runner][INFO] - Test, Round 169 : loss => 3.8837512993995857,  accuracy: 0.5941
[2025-09-19 11:33:26,499][flp2p.graph_runner][INFO] - Train, Round 170 : loss => 0.0004793901936754363,  accuracy: 1.0, gradient_norm : 0.004793585765942005
[2025-09-19 11:33:29,962][flp2p.graph_runner][INFO] - Test, Round 170 : loss => 3.886336794951324,  accuracy: 0.5942
[2025-09-19 11:33:36,563][flp2p.graph_runner][INFO] - Train, Round 171 : loss => 0.0004185388887937587,  accuracy: 1.0, gradient_norm : 0.0026374139281993545
[2025-09-19 11:33:39,998][flp2p.graph_runner][INFO] - Test, Round 171 : loss => 3.8883381960385472,  accuracy: 0.5942
[2025-09-19 11:33:46,665][flp2p.graph_runner][INFO] - Train, Round 172 : loss => 0.00037817531986769973,  accuracy: 1.0, gradient_norm : 0.0018490241420198819
[2025-09-19 11:33:50,348][flp2p.graph_runner][INFO] - Test, Round 172 : loss => 3.890191900848816,  accuracy: 0.5944
[2025-09-19 11:33:56,801][flp2p.graph_runner][INFO] - Train, Round 173 : loss => 0.0003919892547104126,  accuracy: 1.0, gradient_norm : 0.002578204437162208
[2025-09-19 11:34:00,295][flp2p.graph_runner][INFO] - Test, Round 173 : loss => 3.8917963069493524,  accuracy: 0.5943
[2025-09-19 11:34:07,401][flp2p.graph_runner][INFO] - Train, Round 174 : loss => 0.0004525857931419974,  accuracy: 1.0, gradient_norm : 0.0053690741832562415
[2025-09-19 11:34:10,790][flp2p.graph_runner][INFO] - Test, Round 174 : loss => 3.893991508815914,  accuracy: 0.5945
[2025-09-19 11:34:17,141][flp2p.graph_runner][INFO] - Train, Round 175 : loss => 0.0004820042402195943,  accuracy: 1.0, gradient_norm : 0.0031224423911823543
[2025-09-19 11:34:20,534][flp2p.graph_runner][INFO] - Test, Round 175 : loss => 3.897106919296016,  accuracy: 0.5946
[2025-09-19 11:34:27,275][flp2p.graph_runner][INFO] - Train, Round 176 : loss => 0.00037370848912670495,  accuracy: 1.0, gradient_norm : 0.0023133051036575087
[2025-09-19 11:34:30,667][flp2p.graph_runner][INFO] - Test, Round 176 : loss => 3.898517544897868,  accuracy: 0.5944
[2025-09-19 11:34:37,311][flp2p.graph_runner][INFO] - Train, Round 177 : loss => 0.00040251897179640646,  accuracy: 1.0, gradient_norm : 0.0027024164281007415
[2025-09-19 11:34:40,689][flp2p.graph_runner][INFO] - Test, Round 177 : loss => 3.9001447931116564,  accuracy: 0.5942
[2025-09-19 11:34:47,486][flp2p.graph_runner][INFO] - Train, Round 178 : loss => 0.00033841211075198615,  accuracy: 1.0, gradient_norm : 0.0019102394837787042
[2025-09-19 11:34:52,099][flp2p.graph_runner][INFO] - Test, Round 178 : loss => 3.9016168728465304,  accuracy: 0.5939
[2025-09-19 11:35:00,459][flp2p.graph_runner][INFO] - Train, Round 179 : loss => 0.00035152682105233306,  accuracy: 1.0, gradient_norm : 0.0043121909147222
[2025-09-19 11:35:05,233][flp2p.graph_runner][INFO] - Test, Round 179 : loss => 3.903042890347916,  accuracy: 0.5941
[2025-09-19 11:35:14,559][flp2p.graph_runner][INFO] - Train, Round 180 : loss => 0.0004472349902554232,  accuracy: 1.0, gradient_norm : 0.004688055894111732
[2025-09-19 11:35:19,274][flp2p.graph_runner][INFO] - Test, Round 180 : loss => 3.905686472215817,  accuracy: 0.594
[2025-09-19 11:35:27,925][flp2p.graph_runner][INFO] - Train, Round 181 : loss => 0.00035081498824650727,  accuracy: 1.0, gradient_norm : 0.0018252517450854132
[2025-09-19 11:35:32,727][flp2p.graph_runner][INFO] - Test, Round 181 : loss => 3.9066904542507377,  accuracy: 0.594
[2025-09-19 11:35:39,853][flp2p.graph_runner][INFO] - Train, Round 182 : loss => 0.0003785776519841707,  accuracy: 1.0, gradient_norm : 0.0019999018912072626
[2025-09-19 11:35:43,363][flp2p.graph_runner][INFO] - Test, Round 182 : loss => 3.9091637790593863,  accuracy: 0.5938
[2025-09-19 11:35:49,460][flp2p.graph_runner][INFO] - Train, Round 183 : loss => 0.0003982623927039137,  accuracy: 1.0, gradient_norm : 0.0028816543946336747
[2025-09-19 11:35:53,105][flp2p.graph_runner][INFO] - Test, Round 183 : loss => 3.911084533970044,  accuracy: 0.5934
[2025-09-19 11:35:59,515][flp2p.graph_runner][INFO] - Train, Round 184 : loss => 0.000386115932704541,  accuracy: 1.0, gradient_norm : 0.0028484504984776448
[2025-09-19 11:36:03,142][flp2p.graph_runner][INFO] - Test, Round 184 : loss => 3.9128749942065806,  accuracy: 0.5936
[2025-09-19 11:36:09,292][flp2p.graph_runner][INFO] - Train, Round 185 : loss => 0.0004413115481907336,  accuracy: 1.0, gradient_norm : 0.0046781471664644435
[2025-09-19 11:36:12,817][flp2p.graph_runner][INFO] - Test, Round 185 : loss => 3.914853957036351,  accuracy: 0.5939
[2025-09-19 11:36:19,494][flp2p.graph_runner][INFO] - Train, Round 186 : loss => 0.0004364325830554807,  accuracy: 1.0, gradient_norm : 0.004676432243157729
[2025-09-19 11:36:23,034][flp2p.graph_runner][INFO] - Test, Round 186 : loss => 3.9165752192578505,  accuracy: 0.5938
[2025-09-19 11:36:29,914][flp2p.graph_runner][INFO] - Train, Round 187 : loss => 0.0003834007930494968,  accuracy: 1.0, gradient_norm : 0.002354350346732432
[2025-09-19 11:36:33,373][flp2p.graph_runner][INFO] - Test, Round 187 : loss => 3.918436593387945,  accuracy: 0.5942
[2025-09-19 11:36:39,451][flp2p.graph_runner][INFO] - Train, Round 188 : loss => 0.00044791160584558735,  accuracy: 1.0, gradient_norm : 0.004781943930020489
[2025-09-19 11:36:42,865][flp2p.graph_runner][INFO] - Test, Round 188 : loss => 3.9213489836625524,  accuracy: 0.5942
[2025-09-19 11:36:49,075][flp2p.graph_runner][INFO] - Train, Round 189 : loss => 0.00046592669646675755,  accuracy: 1.0, gradient_norm : 0.005012752347726591
[2025-09-19 11:36:53,254][flp2p.graph_runner][INFO] - Test, Round 189 : loss => 3.9223558408462007,  accuracy: 0.5943
[2025-09-19 11:37:03,058][flp2p.graph_runner][INFO] - Train, Round 190 : loss => 0.00045994799026745803,  accuracy: 1.0, gradient_norm : 0.004194836224227058
[2025-09-19 11:37:07,810][flp2p.graph_runner][INFO] - Test, Round 190 : loss => 3.924059563381468,  accuracy: 0.5947
[2025-09-19 11:37:16,272][flp2p.graph_runner][INFO] - Train, Round 191 : loss => 0.00038097417012056256,  accuracy: 1.0, gradient_norm : 0.0021173655472962196
[2025-09-19 11:37:21,600][flp2p.graph_runner][INFO] - Test, Round 191 : loss => 3.9258444717981367,  accuracy: 0.5946
[2025-09-19 11:37:28,917][flp2p.graph_runner][INFO] - Train, Round 192 : loss => 0.0003570814507436934,  accuracy: 1.0, gradient_norm : 0.004445089130316583
[2025-09-19 11:37:33,534][flp2p.graph_runner][INFO] - Test, Round 192 : loss => 3.9268281139643193,  accuracy: 0.5944
[2025-09-19 11:37:41,298][flp2p.graph_runner][INFO] - Train, Round 193 : loss => 0.00038888788854437917,  accuracy: 1.0, gradient_norm : 0.0039913603813524455
[2025-09-19 11:37:44,885][flp2p.graph_runner][INFO] - Test, Round 193 : loss => 3.9287969796254445,  accuracy: 0.5942
[2025-09-19 11:37:50,800][flp2p.graph_runner][INFO] - Train, Round 194 : loss => 0.0003727240880022533,  accuracy: 1.0, gradient_norm : 0.0021621130550279026
[2025-09-19 11:37:54,413][flp2p.graph_runner][INFO] - Test, Round 194 : loss => 3.929568908650772,  accuracy: 0.5945
[2025-09-19 11:38:02,186][flp2p.graph_runner][INFO] - Train, Round 195 : loss => 0.0003998122236854123,  accuracy: 1.0, gradient_norm : 0.003978438561000458
[2025-09-19 11:38:05,643][flp2p.graph_runner][INFO] - Test, Round 195 : loss => 3.9318757006249214,  accuracy: 0.5943
[2025-09-19 11:38:11,408][flp2p.graph_runner][INFO] - Train, Round 196 : loss => 0.0004406569680610305,  accuracy: 1.0, gradient_norm : 0.0028354725950572057
[2025-09-19 11:38:15,675][flp2p.graph_runner][INFO] - Test, Round 196 : loss => 3.93318951564289,  accuracy: 0.5944
[2025-09-19 11:38:23,450][flp2p.graph_runner][INFO] - Train, Round 197 : loss => 0.0003523728589817835,  accuracy: 1.0, gradient_norm : 0.0036750832225881758
[2025-09-19 11:38:28,126][flp2p.graph_runner][INFO] - Test, Round 197 : loss => 3.934538500043962,  accuracy: 0.5942
[2025-09-19 11:38:36,198][flp2p.graph_runner][INFO] - Train, Round 198 : loss => 0.000430254212686915,  accuracy: 1.0, gradient_norm : 0.004200012614823397
[2025-09-19 11:38:40,941][flp2p.graph_runner][INFO] - Test, Round 198 : loss => 3.9359174521133005,  accuracy: 0.5943
[2025-09-19 11:38:49,561][flp2p.graph_runner][INFO] - Train, Round 199 : loss => 0.00033106902041161264,  accuracy: 1.0, gradient_norm : 0.0036700214630968394
[2025-09-19 11:38:54,207][flp2p.graph_runner][INFO] - Test, Round 199 : loss => 3.937618309353262,  accuracy: 0.5943
[2025-09-19 11:38:54,208][__main__][INFO] - Train, Round 001: loss=1.8047, accuracy=0.4084, gradient_norm=0.3268, 
[2025-09-19 11:38:54,208][__main__][INFO] - Train, Round 002: loss=1.8806, accuracy=0.3308, gradient_norm=0.2209, 
[2025-09-19 11:38:54,208][__main__][INFO] - Train, Round 003: loss=1.6815, accuracy=0.4099, gradient_norm=0.2062, 
[2025-09-19 11:38:54,208][__main__][INFO] - Train, Round 004: loss=1.5961, accuracy=0.4338, gradient_norm=0.2177, 
[2025-09-19 11:38:54,209][__main__][INFO] - Train, Round 005: loss=1.5805, accuracy=0.4304, gradient_norm=0.2183, 
[2025-09-19 11:38:54,209][__main__][INFO] - Train, Round 006: loss=1.4176, accuracy=0.4974, gradient_norm=0.1825, 
[2025-09-19 11:38:54,209][__main__][INFO] - Train, Round 007: loss=1.6530, accuracy=0.4145, gradient_norm=0.2182, 
[2025-09-19 11:38:54,209][__main__][INFO] - Train, Round 008: loss=1.3834, accuracy=0.5090, gradient_norm=0.1938, 
[2025-09-19 11:38:54,209][__main__][INFO] - Train, Round 009: loss=1.4569, accuracy=0.4698, gradient_norm=0.1984, 
[2025-09-19 11:38:54,209][__main__][INFO] - Train, Round 010: loss=1.4169, accuracy=0.4910, gradient_norm=0.2167, 
[2025-09-19 11:38:54,209][__main__][INFO] - Train, Round 011: loss=1.2226, accuracy=0.5681, gradient_norm=0.2004, 
[2025-09-19 11:38:54,209][__main__][INFO] - Train, Round 012: loss=1.2082, accuracy=0.5696, gradient_norm=0.2015, 
[2025-09-19 11:38:54,209][__main__][INFO] - Train, Round 013: loss=1.3255, accuracy=0.5240, gradient_norm=0.2050, 
[2025-09-19 11:38:54,209][__main__][INFO] - Train, Round 014: loss=1.2388, accuracy=0.5589, gradient_norm=0.2146, 
[2025-09-19 11:38:54,209][__main__][INFO] - Train, Round 015: loss=1.2125, accuracy=0.5592, gradient_norm=0.2193, 
[2025-09-19 11:38:54,209][__main__][INFO] - Train, Round 016: loss=1.1557, accuracy=0.5904, gradient_norm=0.2400, 
[2025-09-19 11:38:54,209][__main__][INFO] - Train, Round 017: loss=1.1407, accuracy=0.5925, gradient_norm=0.2281, 
[2025-09-19 11:38:54,209][__main__][INFO] - Train, Round 018: loss=1.1260, accuracy=0.5928, gradient_norm=0.2483, 
[2025-09-19 11:38:54,209][__main__][INFO] - Train, Round 019: loss=0.9972, accuracy=0.6483, gradient_norm=0.2459, 
[2025-09-19 11:38:54,209][__main__][INFO] - Train, Round 020: loss=0.8503, accuracy=0.6993, gradient_norm=0.2574, 
[2025-09-19 11:38:54,209][__main__][INFO] - Train, Round 021: loss=0.8401, accuracy=0.7098, gradient_norm=0.2576, 
[2025-09-19 11:38:54,209][__main__][INFO] - Train, Round 022: loss=0.8616, accuracy=0.7003, gradient_norm=0.2453, 
[2025-09-19 11:38:54,209][__main__][INFO] - Train, Round 023: loss=0.9324, accuracy=0.6689, gradient_norm=0.2409, 
[2025-09-19 11:38:54,209][__main__][INFO] - Train, Round 024: loss=0.5849, accuracy=0.8015, gradient_norm=0.2813, 
[2025-09-19 11:38:54,209][__main__][INFO] - Train, Round 025: loss=0.6481, accuracy=0.7741, gradient_norm=0.2418, 
[2025-09-19 11:38:54,209][__main__][INFO] - Train, Round 026: loss=0.6871, accuracy=0.7601, gradient_norm=0.2377, 
[2025-09-19 11:38:54,210][__main__][INFO] - Train, Round 027: loss=0.5191, accuracy=0.8213, gradient_norm=0.2357, 
[2025-09-19 11:38:54,210][__main__][INFO] - Train, Round 028: loss=0.6241, accuracy=0.7766, gradient_norm=0.2134, 
[2025-09-19 11:38:54,210][__main__][INFO] - Train, Round 029: loss=0.5536, accuracy=0.8108, gradient_norm=0.2563, 
[2025-09-19 11:38:54,210][__main__][INFO] - Train, Round 030: loss=0.4533, accuracy=0.8455, gradient_norm=0.2423, 
[2025-09-19 11:38:54,210][__main__][INFO] - Train, Round 031: loss=0.3703, accuracy=0.8749, gradient_norm=0.2176, 
[2025-09-19 11:38:54,210][__main__][INFO] - Train, Round 032: loss=0.4845, accuracy=0.8301, gradient_norm=0.2230, 
[2025-09-19 11:38:54,210][__main__][INFO] - Train, Round 033: loss=0.5000, accuracy=0.8213, gradient_norm=0.2147, 
[2025-09-19 11:38:54,210][__main__][INFO] - Train, Round 034: loss=0.4157, accuracy=0.8568, gradient_norm=0.2194, 
[2025-09-19 11:38:54,210][__main__][INFO] - Train, Round 035: loss=0.2248, accuracy=0.9265, gradient_norm=0.2007, 
[2025-09-19 11:38:54,210][__main__][INFO] - Train, Round 036: loss=0.3921, accuracy=0.8648, gradient_norm=0.2284, 
[2025-09-19 11:38:54,210][__main__][INFO] - Train, Round 037: loss=0.3222, accuracy=0.8917, gradient_norm=0.2038, 
[2025-09-19 11:38:54,210][__main__][INFO] - Train, Round 038: loss=0.3389, accuracy=0.8855, gradient_norm=0.1837, 
[2025-09-19 11:38:54,210][__main__][INFO] - Train, Round 039: loss=0.3035, accuracy=0.8987, gradient_norm=0.1797, 
[2025-09-19 11:38:54,210][__main__][INFO] - Train, Round 040: loss=0.2832, accuracy=0.9066, gradient_norm=0.2084, 
[2025-09-19 11:38:54,210][__main__][INFO] - Train, Round 041: loss=0.2084, accuracy=0.9283, gradient_norm=0.1506, 
[2025-09-19 11:38:54,210][__main__][INFO] - Train, Round 042: loss=0.1130, accuracy=0.9635, gradient_norm=0.1458, 
[2025-09-19 11:38:54,210][__main__][INFO] - Train, Round 043: loss=0.2806, accuracy=0.9014, gradient_norm=0.1652, 
[2025-09-19 11:38:54,210][__main__][INFO] - Train, Round 044: loss=0.2385, accuracy=0.9189, gradient_norm=0.1352, 
[2025-09-19 11:38:54,210][__main__][INFO] - Train, Round 045: loss=0.2019, accuracy=0.9309, gradient_norm=0.2009, 
[2025-09-19 11:38:54,210][__main__][INFO] - Train, Round 046: loss=0.1717, accuracy=0.9429, gradient_norm=0.1793, 
[2025-09-19 11:38:54,210][__main__][INFO] - Train, Round 047: loss=0.1594, accuracy=0.9458, gradient_norm=0.1377, 
[2025-09-19 11:38:54,210][__main__][INFO] - Train, Round 048: loss=0.1318, accuracy=0.9572, gradient_norm=0.1200, 
[2025-09-19 11:38:54,210][__main__][INFO] - Train, Round 049: loss=0.2514, accuracy=0.9128, gradient_norm=0.2143, 
[2025-09-19 11:38:54,210][__main__][INFO] - Train, Round 050: loss=0.1146, accuracy=0.9632, gradient_norm=0.1448, 
[2025-09-19 11:38:54,211][__main__][INFO] - Train, Round 051: loss=0.1263, accuracy=0.9595, gradient_norm=0.0938, 
[2025-09-19 11:38:54,211][__main__][INFO] - Train, Round 052: loss=0.0392, accuracy=0.9883, gradient_norm=0.1189, 
[2025-09-19 11:38:54,211][__main__][INFO] - Train, Round 053: loss=0.0657, accuracy=0.9796, gradient_norm=0.1202, 
[2025-09-19 11:38:54,211][__main__][INFO] - Train, Round 054: loss=0.1032, accuracy=0.9630, gradient_norm=0.1620, 
[2025-09-19 11:38:54,211][__main__][INFO] - Train, Round 055: loss=0.0361, accuracy=0.9902, gradient_norm=0.1197, 
[2025-09-19 11:38:54,211][__main__][INFO] - Train, Round 056: loss=0.0774, accuracy=0.9743, gradient_norm=0.0950, 
[2025-09-19 11:38:54,211][__main__][INFO] - Train, Round 057: loss=0.0281, accuracy=0.9924, gradient_norm=0.0989, 
[2025-09-19 11:38:54,211][__main__][INFO] - Train, Round 058: loss=0.0370, accuracy=0.9887, gradient_norm=0.1323, 
[2025-09-19 11:38:54,211][__main__][INFO] - Train, Round 059: loss=0.0836, accuracy=0.9710, gradient_norm=0.1759, 
[2025-09-19 11:38:54,211][__main__][INFO] - Train, Round 060: loss=0.0951, accuracy=0.9691, gradient_norm=0.0770, 
[2025-09-19 11:38:54,211][__main__][INFO] - Train, Round 061: loss=0.0545, accuracy=0.9819, gradient_norm=0.1142, 
[2025-09-19 11:38:54,211][__main__][INFO] - Train, Round 062: loss=0.0686, accuracy=0.9776, gradient_norm=0.1121, 
[2025-09-19 11:38:54,211][__main__][INFO] - Train, Round 063: loss=0.0516, accuracy=0.9847, gradient_norm=0.0654, 
[2025-09-19 11:38:54,211][__main__][INFO] - Train, Round 064: loss=0.0502, accuracy=0.9846, gradient_norm=0.0572, 
[2025-09-19 11:38:54,211][__main__][INFO] - Train, Round 065: loss=0.0873, accuracy=0.9704, gradient_norm=0.1239, 
[2025-09-19 11:38:54,211][__main__][INFO] - Train, Round 066: loss=0.0585, accuracy=0.9815, gradient_norm=0.0813, 
[2025-09-19 11:38:54,211][__main__][INFO] - Train, Round 067: loss=0.0197, accuracy=0.9954, gradient_norm=0.1275, 
[2025-09-19 11:38:54,211][__main__][INFO] - Train, Round 068: loss=0.0830, accuracy=0.9755, gradient_norm=0.0919, 
[2025-09-19 11:38:54,211][__main__][INFO] - Train, Round 069: loss=0.0155, accuracy=0.9962, gradient_norm=0.0913, 
[2025-09-19 11:38:54,211][__main__][INFO] - Train, Round 070: loss=0.0825, accuracy=0.9724, gradient_norm=0.1405, 
[2025-09-19 11:38:54,211][__main__][INFO] - Train, Round 071: loss=0.0327, accuracy=0.9904, gradient_norm=0.0832, 
[2025-09-19 11:38:54,211][__main__][INFO] - Train, Round 072: loss=0.0095, accuracy=0.9980, gradient_norm=0.0362, 
[2025-09-19 11:38:54,212][__main__][INFO] - Train, Round 073: loss=0.0351, accuracy=0.9888, gradient_norm=0.1110, 
[2025-09-19 11:38:54,212][__main__][INFO] - Train, Round 074: loss=0.0114, accuracy=0.9967, gradient_norm=0.0817, 
[2025-09-19 11:38:54,212][__main__][INFO] - Train, Round 075: loss=0.0121, accuracy=0.9973, gradient_norm=0.0224, 
[2025-09-19 11:38:54,212][__main__][INFO] - Train, Round 076: loss=0.0132, accuracy=0.9965, gradient_norm=0.0637, 
[2025-09-19 11:38:54,212][__main__][INFO] - Train, Round 077: loss=0.0310, accuracy=0.9909, gradient_norm=0.1111, 
[2025-09-19 11:38:54,212][__main__][INFO] - Train, Round 078: loss=0.0676, accuracy=0.9781, gradient_norm=0.1283, 
[2025-09-19 11:38:54,212][__main__][INFO] - Train, Round 079: loss=0.0073, accuracy=0.9987, gradient_norm=0.0653, 
[2025-09-19 11:38:54,212][__main__][INFO] - Train, Round 080: loss=0.0084, accuracy=0.9979, gradient_norm=0.0870, 
[2025-09-19 11:38:54,212][__main__][INFO] - Train, Round 081: loss=0.0132, accuracy=0.9965, gradient_norm=0.1110, 
[2025-09-19 11:38:54,212][__main__][INFO] - Train, Round 082: loss=0.0044, accuracy=0.9992, gradient_norm=0.0209, 
[2025-09-19 11:38:54,212][__main__][INFO] - Train, Round 083: loss=0.0119, accuracy=0.9973, gradient_norm=0.1395, 
[2025-09-19 11:38:54,212][__main__][INFO] - Train, Round 084: loss=0.0043, accuracy=0.9997, gradient_norm=0.0230, 
[2025-09-19 11:38:54,212][__main__][INFO] - Train, Round 085: loss=0.0021, accuracy=0.9999, gradient_norm=0.0109, 
[2025-09-19 11:38:54,212][__main__][INFO] - Train, Round 086: loss=0.0161, accuracy=0.9956, gradient_norm=0.0878, 
[2025-09-19 11:38:54,212][__main__][INFO] - Train, Round 087: loss=0.0068, accuracy=0.9986, gradient_norm=0.0844, 
[2025-09-19 11:38:54,212][__main__][INFO] - Train, Round 088: loss=0.0076, accuracy=0.9981, gradient_norm=0.0818, 
[2025-09-19 11:38:54,212][__main__][INFO] - Train, Round 089: loss=0.0042, accuracy=0.9994, gradient_norm=0.0573, 
[2025-09-19 11:38:54,212][__main__][INFO] - Train, Round 090: loss=0.0081, accuracy=0.9989, gradient_norm=0.0978, 
[2025-09-19 11:38:54,212][__main__][INFO] - Train, Round 091: loss=0.0070, accuracy=0.9986, gradient_norm=0.1011, 
[2025-09-19 11:38:54,212][__main__][INFO] - Train, Round 092: loss=0.0020, accuracy=1.0000, gradient_norm=0.0080, 
[2025-09-19 11:38:54,212][__main__][INFO] - Train, Round 093: loss=0.0054, accuracy=0.9990, gradient_norm=0.0890, 
[2025-09-19 11:38:54,212][__main__][INFO] - Train, Round 094: loss=0.0026, accuracy=0.9994, gradient_norm=0.0619, 
[2025-09-19 11:38:54,212][__main__][INFO] - Train, Round 095: loss=0.0024, accuracy=0.9998, gradient_norm=0.0296, 
[2025-09-19 11:38:54,212][__main__][INFO] - Train, Round 096: loss=0.0015, accuracy=0.9999, gradient_norm=0.0069, 
[2025-09-19 11:38:54,212][__main__][INFO] - Train, Round 097: loss=0.0276, accuracy=0.9910, gradient_norm=0.0477, 
[2025-09-19 11:38:54,212][__main__][INFO] - Train, Round 098: loss=0.0014, accuracy=1.0000, gradient_norm=0.0070, 
[2025-09-19 11:38:54,212][__main__][INFO] - Train, Round 099: loss=0.0025, accuracy=0.9996, gradient_norm=0.0217, 
[2025-09-19 11:38:54,212][__main__][INFO] - Train, Round 100: loss=0.0203, accuracy=0.9930, gradient_norm=0.0267, 
[2025-09-19 11:38:54,212][__main__][INFO] - Train, Round 101: loss=0.0011, accuracy=1.0000, gradient_norm=0.0078, 
[2025-09-19 11:38:54,212][__main__][INFO] - Train, Round 102: loss=0.0187, accuracy=0.9950, gradient_norm=0.0512, 
[2025-09-19 11:38:54,212][__main__][INFO] - Train, Round 103: loss=0.0024, accuracy=0.9997, gradient_norm=0.0839, 
[2025-09-19 11:38:54,212][__main__][INFO] - Train, Round 104: loss=0.0103, accuracy=0.9971, gradient_norm=0.0398, 
[2025-09-19 11:38:54,212][__main__][INFO] - Train, Round 105: loss=0.0020, accuracy=0.9998, gradient_norm=0.0362, 
[2025-09-19 11:38:54,212][__main__][INFO] - Train, Round 106: loss=0.0018, accuracy=0.9999, gradient_norm=0.0215, 
[2025-09-19 11:38:54,212][__main__][INFO] - Train, Round 107: loss=0.0012, accuracy=0.9999, gradient_norm=0.0348, 
[2025-09-19 11:38:54,212][__main__][INFO] - Train, Round 108: loss=0.0016, accuracy=0.9999, gradient_norm=0.0111, 
[2025-09-19 11:38:54,213][__main__][INFO] - Train, Round 109: loss=0.0015, accuracy=0.9999, gradient_norm=0.0349, 
[2025-09-19 11:38:54,213][__main__][INFO] - Train, Round 110: loss=0.0010, accuracy=1.0000, gradient_norm=0.0042, 
[2025-09-19 11:38:54,213][__main__][INFO] - Train, Round 111: loss=0.0091, accuracy=0.9974, gradient_norm=0.0384, 
[2025-09-19 11:38:54,213][__main__][INFO] - Train, Round 112: loss=0.0088, accuracy=0.9977, gradient_norm=0.0444, 
[2025-09-19 11:38:54,213][__main__][INFO] - Train, Round 113: loss=0.0015, accuracy=0.9999, gradient_norm=0.0191, 
[2025-09-19 11:38:54,213][__main__][INFO] - Train, Round 114: loss=0.0011, accuracy=1.0000, gradient_norm=0.0240, 
[2025-09-19 11:38:54,213][__main__][INFO] - Train, Round 115: loss=0.0007, accuracy=1.0000, gradient_norm=0.0035, 
[2025-09-19 11:38:54,213][__main__][INFO] - Train, Round 116: loss=0.0024, accuracy=0.9998, gradient_norm=0.0095, 
[2025-09-19 11:38:54,213][__main__][INFO] - Train, Round 117: loss=0.0009, accuracy=1.0000, gradient_norm=0.0072, 
[2025-09-19 11:38:54,213][__main__][INFO] - Train, Round 118: loss=0.0019, accuracy=1.0000, gradient_norm=0.0174, 
[2025-09-19 11:38:54,213][__main__][INFO] - Train, Round 119: loss=0.0012, accuracy=1.0000, gradient_norm=0.0159, 
[2025-09-19 11:38:54,213][__main__][INFO] - Train, Round 120: loss=0.0012, accuracy=1.0000, gradient_norm=0.0144, 
[2025-09-19 11:38:54,213][__main__][INFO] - Train, Round 121: loss=0.0013, accuracy=1.0000, gradient_norm=0.0077, 
[2025-09-19 11:38:54,213][__main__][INFO] - Train, Round 122: loss=0.0009, accuracy=1.0000, gradient_norm=0.0071, 
[2025-09-19 11:38:54,213][__main__][INFO] - Train, Round 123: loss=0.0015, accuracy=1.0000, gradient_norm=0.0231, 
[2025-09-19 11:38:54,213][__main__][INFO] - Train, Round 124: loss=0.0014, accuracy=1.0000, gradient_norm=0.0161, 
[2025-09-19 11:38:54,213][__main__][INFO] - Train, Round 125: loss=0.0007, accuracy=1.0000, gradient_norm=0.0129, 
[2025-09-19 11:38:54,213][__main__][INFO] - Train, Round 126: loss=0.0009, accuracy=1.0000, gradient_norm=0.0129, 
[2025-09-19 11:38:54,213][__main__][INFO] - Train, Round 127: loss=0.0011, accuracy=1.0000, gradient_norm=0.0140, 
[2025-09-19 11:38:54,213][__main__][INFO] - Train, Round 128: loss=0.0009, accuracy=1.0000, gradient_norm=0.0219, 
[2025-09-19 11:38:54,213][__main__][INFO] - Train, Round 129: loss=0.0010, accuracy=0.9999, gradient_norm=0.0327, 
[2025-09-19 11:38:54,213][__main__][INFO] - Train, Round 130: loss=0.0006, accuracy=1.0000, gradient_norm=0.0029, 
[2025-09-19 11:38:54,213][__main__][INFO] - Train, Round 131: loss=0.0007, accuracy=1.0000, gradient_norm=0.0170, 
[2025-09-19 11:38:54,213][__main__][INFO] - Train, Round 132: loss=0.0005, accuracy=1.0000, gradient_norm=0.0026, 
[2025-09-19 11:38:54,213][__main__][INFO] - Train, Round 133: loss=0.0009, accuracy=1.0000, gradient_norm=0.0102, 
[2025-09-19 11:38:54,213][__main__][INFO] - Train, Round 134: loss=0.0009, accuracy=1.0000, gradient_norm=0.0070, 
[2025-09-19 11:38:54,213][__main__][INFO] - Train, Round 135: loss=0.0008, accuracy=1.0000, gradient_norm=0.0039, 
[2025-09-19 11:38:54,213][__main__][INFO] - Train, Round 136: loss=0.0006, accuracy=1.0000, gradient_norm=0.0096, 
[2025-09-19 11:38:54,213][__main__][INFO] - Train, Round 137: loss=0.0007, accuracy=1.0000, gradient_norm=0.0085, 
[2025-09-19 11:38:54,213][__main__][INFO] - Train, Round 138: loss=0.0009, accuracy=1.0000, gradient_norm=0.0090, 
[2025-09-19 11:38:54,213][__main__][INFO] - Train, Round 139: loss=0.0007, accuracy=1.0000, gradient_norm=0.0040, 
[2025-09-19 11:38:54,213][__main__][INFO] - Train, Round 140: loss=0.0006, accuracy=1.0000, gradient_norm=0.0079, 
[2025-09-19 11:38:54,213][__main__][INFO] - Train, Round 141: loss=0.0007, accuracy=1.0000, gradient_norm=0.0048, 
[2025-09-19 11:38:54,213][__main__][INFO] - Train, Round 142: loss=0.0007, accuracy=1.0000, gradient_norm=0.0080, 
[2025-09-19 11:38:54,213][__main__][INFO] - Train, Round 143: loss=0.0008, accuracy=1.0000, gradient_norm=0.0084, 
[2025-09-19 11:38:54,213][__main__][INFO] - Train, Round 144: loss=0.0006, accuracy=1.0000, gradient_norm=0.0074, 
[2025-09-19 11:38:54,213][__main__][INFO] - Train, Round 145: loss=0.0008, accuracy=1.0000, gradient_norm=0.0085, 
[2025-09-19 11:38:54,213][__main__][INFO] - Train, Round 146: loss=0.0005, accuracy=1.0000, gradient_norm=0.0030, 
[2025-09-19 11:38:54,213][__main__][INFO] - Train, Round 147: loss=0.0006, accuracy=1.0000, gradient_norm=0.0030, 
[2025-09-19 11:38:54,213][__main__][INFO] - Train, Round 148: loss=0.0006, accuracy=1.0000, gradient_norm=0.0079, 
[2025-09-19 11:38:54,213][__main__][INFO] - Train, Round 149: loss=0.0006, accuracy=1.0000, gradient_norm=0.0039, 
[2025-09-19 11:38:54,213][__main__][INFO] - Train, Round 150: loss=0.0006, accuracy=1.0000, gradient_norm=0.0075, 
[2025-09-19 11:38:54,213][__main__][INFO] - Train, Round 151: loss=0.0005, accuracy=1.0000, gradient_norm=0.0025, 
[2025-09-19 11:38:54,213][__main__][INFO] - Train, Round 152: loss=0.0007, accuracy=1.0000, gradient_norm=0.0077, 
[2025-09-19 11:38:54,213][__main__][INFO] - Train, Round 153: loss=0.0005, accuracy=1.0000, gradient_norm=0.0035, 
[2025-09-19 11:38:54,213][__main__][INFO] - Train, Round 154: loss=0.0006, accuracy=1.0000, gradient_norm=0.0033, 
[2025-09-19 11:38:54,213][__main__][INFO] - Train, Round 155: loss=0.0006, accuracy=1.0000, gradient_norm=0.0049, 
[2025-09-19 11:38:54,214][__main__][INFO] - Train, Round 156: loss=0.0005, accuracy=1.0000, gradient_norm=0.0026, 
[2025-09-19 11:38:54,214][__main__][INFO] - Train, Round 157: loss=0.0005, accuracy=1.0000, gradient_norm=0.0058, 
[2025-09-19 11:38:54,214][__main__][INFO] - Train, Round 158: loss=0.0005, accuracy=1.0000, gradient_norm=0.0026, 
[2025-09-19 11:38:54,214][__main__][INFO] - Train, Round 159: loss=0.0005, accuracy=1.0000, gradient_norm=0.0032, 
[2025-09-19 11:38:54,214][__main__][INFO] - Train, Round 160: loss=0.0005, accuracy=1.0000, gradient_norm=0.0059, 
[2025-09-19 11:38:54,214][__main__][INFO] - Train, Round 161: loss=0.0006, accuracy=1.0000, gradient_norm=0.0041, 
[2025-09-19 11:38:54,214][__main__][INFO] - Train, Round 162: loss=0.0005, accuracy=1.0000, gradient_norm=0.0064, 
[2025-09-19 11:38:54,214][__main__][INFO] - Train, Round 163: loss=0.0005, accuracy=1.0000, gradient_norm=0.0030, 
[2025-09-19 11:38:54,214][__main__][INFO] - Train, Round 164: loss=0.0005, accuracy=1.0000, gradient_norm=0.0034, 
[2025-09-19 11:38:54,214][__main__][INFO] - Train, Round 165: loss=0.0004, accuracy=1.0000, gradient_norm=0.0026, 
[2025-09-19 11:38:54,214][__main__][INFO] - Train, Round 166: loss=0.0004, accuracy=1.0000, gradient_norm=0.0027, 
[2025-09-19 11:38:54,214][__main__][INFO] - Train, Round 167: loss=0.0004, accuracy=1.0000, gradient_norm=0.0027, 
[2025-09-19 11:38:54,214][__main__][INFO] - Train, Round 168: loss=0.0005, accuracy=1.0000, gradient_norm=0.0055, 
[2025-09-19 11:38:54,214][__main__][INFO] - Train, Round 169: loss=0.0004, accuracy=1.0000, gradient_norm=0.0051, 
[2025-09-19 11:38:54,214][__main__][INFO] - Train, Round 170: loss=0.0005, accuracy=1.0000, gradient_norm=0.0059, 
[2025-09-19 11:38:54,214][__main__][INFO] - Train, Round 171: loss=0.0005, accuracy=1.0000, gradient_norm=0.0048, 
[2025-09-19 11:38:54,214][__main__][INFO] - Train, Round 172: loss=0.0004, accuracy=1.0000, gradient_norm=0.0026, 
[2025-09-19 11:38:54,214][__main__][INFO] - Train, Round 173: loss=0.0004, accuracy=1.0000, gradient_norm=0.0018, 
[2025-09-19 11:38:54,214][__main__][INFO] - Train, Round 174: loss=0.0004, accuracy=1.0000, gradient_norm=0.0026, 
[2025-09-19 11:38:54,214][__main__][INFO] - Train, Round 175: loss=0.0005, accuracy=1.0000, gradient_norm=0.0054, 
[2025-09-19 11:38:54,214][__main__][INFO] - Train, Round 176: loss=0.0005, accuracy=1.0000, gradient_norm=0.0031, 
[2025-09-19 11:38:54,214][__main__][INFO] - Train, Round 177: loss=0.0004, accuracy=1.0000, gradient_norm=0.0023, 
[2025-09-19 11:38:54,214][__main__][INFO] - Train, Round 178: loss=0.0004, accuracy=1.0000, gradient_norm=0.0027, 
[2025-09-19 11:38:54,214][__main__][INFO] - Train, Round 179: loss=0.0003, accuracy=1.0000, gradient_norm=0.0019, 
[2025-09-19 11:38:54,214][__main__][INFO] - Train, Round 180: loss=0.0004, accuracy=1.0000, gradient_norm=0.0043, 
[2025-09-19 11:38:54,214][__main__][INFO] - Train, Round 181: loss=0.0004, accuracy=1.0000, gradient_norm=0.0047, 
[2025-09-19 11:38:54,214][__main__][INFO] - Train, Round 182: loss=0.0004, accuracy=1.0000, gradient_norm=0.0018, 
[2025-09-19 11:38:54,214][__main__][INFO] - Train, Round 183: loss=0.0004, accuracy=1.0000, gradient_norm=0.0020, 
[2025-09-19 11:38:54,214][__main__][INFO] - Train, Round 184: loss=0.0004, accuracy=1.0000, gradient_norm=0.0029, 
[2025-09-19 11:38:54,214][__main__][INFO] - Train, Round 185: loss=0.0004, accuracy=1.0000, gradient_norm=0.0028, 
[2025-09-19 11:38:54,214][__main__][INFO] - Train, Round 186: loss=0.0004, accuracy=1.0000, gradient_norm=0.0047, 
[2025-09-19 11:38:54,214][__main__][INFO] - Train, Round 187: loss=0.0004, accuracy=1.0000, gradient_norm=0.0047, 
[2025-09-19 11:38:54,214][__main__][INFO] - Train, Round 188: loss=0.0004, accuracy=1.0000, gradient_norm=0.0024, 
[2025-09-19 11:38:54,214][__main__][INFO] - Train, Round 189: loss=0.0004, accuracy=1.0000, gradient_norm=0.0048, 
[2025-09-19 11:38:54,214][__main__][INFO] - Train, Round 190: loss=0.0005, accuracy=1.0000, gradient_norm=0.0050, 
[2025-09-19 11:38:54,214][__main__][INFO] - Train, Round 191: loss=0.0005, accuracy=1.0000, gradient_norm=0.0042, 
[2025-09-19 11:38:54,214][__main__][INFO] - Train, Round 192: loss=0.0004, accuracy=1.0000, gradient_norm=0.0021, 
[2025-09-19 11:38:54,214][__main__][INFO] - Train, Round 193: loss=0.0004, accuracy=1.0000, gradient_norm=0.0044, 
[2025-09-19 11:38:54,214][__main__][INFO] - Train, Round 194: loss=0.0004, accuracy=1.0000, gradient_norm=0.0040, 
[2025-09-19 11:38:54,214][__main__][INFO] - Train, Round 195: loss=0.0004, accuracy=1.0000, gradient_norm=0.0022, 
[2025-09-19 11:38:54,214][__main__][INFO] - Train, Round 196: loss=0.0004, accuracy=1.0000, gradient_norm=0.0040, 
[2025-09-19 11:38:54,214][__main__][INFO] - Train, Round 197: loss=0.0004, accuracy=1.0000, gradient_norm=0.0028, 
[2025-09-19 11:38:54,214][__main__][INFO] - Train, Round 198: loss=0.0004, accuracy=1.0000, gradient_norm=0.0037, 
[2025-09-19 11:38:54,214][__main__][INFO] - Train, Round 199: loss=0.0004, accuracy=1.0000, gradient_norm=0.0042, 
[2025-09-19 11:38:54,214][__main__][INFO] - Train, Round 200: loss=0.0003, accuracy=1.0000, gradient_norm=0.0037, 
[2025-09-19 11:38:54,215][__main__][INFO] - Test, Round 001: loss=2.0510, accuracy=0.2288, 
[2025-09-19 11:38:54,215][__main__][INFO] - Test, Round 002: loss=1.9319, accuracy=0.2848, 
[2025-09-19 11:38:54,215][__main__][INFO] - Test, Round 003: loss=1.8427, accuracy=0.3191, 
[2025-09-19 11:38:54,215][__main__][INFO] - Test, Round 004: loss=1.7386, accuracy=0.3682, 
[2025-09-19 11:38:54,215][__main__][INFO] - Test, Round 005: loss=1.7027, accuracy=0.3773, 
[2025-09-19 11:38:54,215][__main__][INFO] - Test, Round 006: loss=1.6263, accuracy=0.3997, 
[2025-09-19 11:38:54,215][__main__][INFO] - Test, Round 007: loss=1.5511, accuracy=0.4282, 
[2025-09-19 11:38:54,215][__main__][INFO] - Test, Round 008: loss=1.5333, accuracy=0.4355, 
[2025-09-19 11:38:54,215][__main__][INFO] - Test, Round 009: loss=1.4815, accuracy=0.4632, 
[2025-09-19 11:38:54,215][__main__][INFO] - Test, Round 010: loss=1.4396, accuracy=0.4791, 
[2025-09-19 11:38:54,215][__main__][INFO] - Test, Round 011: loss=1.4016, accuracy=0.4926, 
[2025-09-19 11:38:54,215][__main__][INFO] - Test, Round 012: loss=1.3853, accuracy=0.5010, 
[2025-09-19 11:38:54,215][__main__][INFO] - Test, Round 013: loss=1.3433, accuracy=0.5216, 
[2025-09-19 11:38:54,215][__main__][INFO] - Test, Round 014: loss=1.3302, accuracy=0.5284, 
[2025-09-19 11:38:54,215][__main__][INFO] - Test, Round 015: loss=1.3107, accuracy=0.5339, 
[2025-09-19 11:38:54,215][__main__][INFO] - Test, Round 016: loss=1.3073, accuracy=0.5391, 
[2025-09-19 11:38:54,215][__main__][INFO] - Test, Round 017: loss=1.2960, accuracy=0.5458, 
[2025-09-19 11:38:54,215][__main__][INFO] - Test, Round 018: loss=1.2795, accuracy=0.5533, 
[2025-09-19 11:38:54,215][__main__][INFO] - Test, Round 019: loss=1.2876, accuracy=0.5554, 
[2025-09-19 11:38:54,215][__main__][INFO] - Test, Round 020: loss=1.2966, accuracy=0.5591, 
[2025-09-19 11:38:54,215][__main__][INFO] - Test, Round 021: loss=1.3056, accuracy=0.5622, 
[2025-09-19 11:38:54,215][__main__][INFO] - Test, Round 022: loss=1.3035, accuracy=0.5668, 
[2025-09-19 11:38:54,215][__main__][INFO] - Test, Round 023: loss=1.3239, accuracy=0.5670, 
[2025-09-19 11:38:54,215][__main__][INFO] - Test, Round 024: loss=1.3557, accuracy=0.5725, 
[2025-09-19 11:38:54,215][__main__][INFO] - Test, Round 025: loss=1.3960, accuracy=0.5709, 
[2025-09-19 11:38:54,215][__main__][INFO] - Test, Round 026: loss=1.4158, accuracy=0.5732, 
[2025-09-19 11:38:54,215][__main__][INFO] - Test, Round 027: loss=1.4671, accuracy=0.5767, 
[2025-09-19 11:38:54,215][__main__][INFO] - Test, Round 028: loss=1.5141, accuracy=0.5762, 
[2025-09-19 11:38:54,215][__main__][INFO] - Test, Round 029: loss=1.5484, accuracy=0.5791, 
[2025-09-19 11:38:54,215][__main__][INFO] - Test, Round 030: loss=1.6260, accuracy=0.5794, 
[2025-09-19 11:38:54,215][__main__][INFO] - Test, Round 031: loss=1.6818, accuracy=0.5811, 
[2025-09-19 11:38:54,215][__main__][INFO] - Test, Round 032: loss=1.7447, accuracy=0.5811, 
[2025-09-19 11:38:54,215][__main__][INFO] - Test, Round 033: loss=1.7574, accuracy=0.5821, 
[2025-09-19 11:38:54,215][__main__][INFO] - Test, Round 034: loss=1.7986, accuracy=0.5844, 
[2025-09-19 11:38:54,215][__main__][INFO] - Test, Round 035: loss=1.9121, accuracy=0.5852, 
[2025-09-19 11:38:54,215][__main__][INFO] - Test, Round 036: loss=1.9253, accuracy=0.5857, 
[2025-09-19 11:38:54,215][__main__][INFO] - Test, Round 037: loss=1.9702, accuracy=0.5875, 
[2025-09-19 11:38:54,215][__main__][INFO] - Test, Round 038: loss=1.9561, accuracy=0.5889, 
[2025-09-19 11:38:54,215][__main__][INFO] - Test, Round 039: loss=2.0247, accuracy=0.5910, 
[2025-09-19 11:38:54,215][__main__][INFO] - Test, Round 040: loss=2.0666, accuracy=0.5933, 
[2025-09-19 11:38:54,215][__main__][INFO] - Test, Round 041: loss=2.1165, accuracy=0.5918, 
[2025-09-19 11:38:54,215][__main__][INFO] - Test, Round 042: loss=2.1826, accuracy=0.5927, 
[2025-09-19 11:38:54,215][__main__][INFO] - Test, Round 043: loss=2.2371, accuracy=0.5929, 
[2025-09-19 11:38:54,215][__main__][INFO] - Test, Round 044: loss=2.2893, accuracy=0.5916, 
[2025-09-19 11:38:54,215][__main__][INFO] - Test, Round 045: loss=2.3309, accuracy=0.5923, 
[2025-09-19 11:38:54,215][__main__][INFO] - Test, Round 046: loss=2.3683, accuracy=0.5926, 
[2025-09-19 11:38:54,215][__main__][INFO] - Test, Round 047: loss=2.4133, accuracy=0.5947, 
[2025-09-19 11:38:54,215][__main__][INFO] - Test, Round 048: loss=2.4569, accuracy=0.5959, 
[2025-09-19 11:38:54,215][__main__][INFO] - Test, Round 049: loss=2.4835, accuracy=0.5934, 
[2025-09-19 11:38:54,215][__main__][INFO] - Test, Round 050: loss=2.5199, accuracy=0.5927, 
[2025-09-19 11:38:54,216][__main__][INFO] - Test, Round 051: loss=2.5519, accuracy=0.5945, 
[2025-09-19 11:38:54,216][__main__][INFO] - Test, Round 052: loss=2.5759, accuracy=0.5934, 
[2025-09-19 11:38:54,216][__main__][INFO] - Test, Round 053: loss=2.6266, accuracy=0.5913, 
[2025-09-19 11:38:54,216][__main__][INFO] - Test, Round 054: loss=2.6655, accuracy=0.5918, 
[2025-09-19 11:38:54,216][__main__][INFO] - Test, Round 055: loss=2.6887, accuracy=0.5920, 
[2025-09-19 11:38:54,216][__main__][INFO] - Test, Round 056: loss=2.7338, accuracy=0.5926, 
[2025-09-19 11:38:54,216][__main__][INFO] - Test, Round 057: loss=2.7636, accuracy=0.5937, 
[2025-09-19 11:38:54,216][__main__][INFO] - Test, Round 058: loss=2.7885, accuracy=0.5934, 
[2025-09-19 11:38:54,216][__main__][INFO] - Test, Round 059: loss=2.8065, accuracy=0.5926, 
[2025-09-19 11:38:54,216][__main__][INFO] - Test, Round 060: loss=2.8417, accuracy=0.5941, 
[2025-09-19 11:38:54,216][__main__][INFO] - Test, Round 061: loss=2.8773, accuracy=0.5924, 
[2025-09-19 11:38:54,216][__main__][INFO] - Test, Round 062: loss=2.9086, accuracy=0.5925, 
[2025-09-19 11:38:54,216][__main__][INFO] - Test, Round 063: loss=2.9253, accuracy=0.5923, 
[2025-09-19 11:38:54,216][__main__][INFO] - Test, Round 064: loss=2.9452, accuracy=0.5920, 
[2025-09-19 11:38:54,216][__main__][INFO] - Test, Round 065: loss=2.9653, accuracy=0.5918, 
[2025-09-19 11:38:54,216][__main__][INFO] - Test, Round 066: loss=2.9866, accuracy=0.5935, 
[2025-09-19 11:38:54,216][__main__][INFO] - Test, Round 067: loss=3.0086, accuracy=0.5939, 
[2025-09-19 11:38:54,216][__main__][INFO] - Test, Round 068: loss=3.0552, accuracy=0.5929, 
[2025-09-19 11:38:54,216][__main__][INFO] - Test, Round 069: loss=3.0703, accuracy=0.5926, 
[2025-09-19 11:38:54,216][__main__][INFO] - Test, Round 070: loss=3.0881, accuracy=0.5916, 
[2025-09-19 11:38:54,216][__main__][INFO] - Test, Round 071: loss=3.1057, accuracy=0.5925, 
[2025-09-19 11:38:54,216][__main__][INFO] - Test, Round 072: loss=3.1307, accuracy=0.5908, 
[2025-09-19 11:38:54,216][__main__][INFO] - Test, Round 073: loss=3.1526, accuracy=0.5896, 
[2025-09-19 11:38:54,216][__main__][INFO] - Test, Round 074: loss=3.1649, accuracy=0.5906, 
[2025-09-19 11:38:54,216][__main__][INFO] - Test, Round 075: loss=3.1957, accuracy=0.5925, 
[2025-09-19 11:38:54,216][__main__][INFO] - Test, Round 076: loss=3.2031, accuracy=0.5924, 
[2025-09-19 11:38:54,216][__main__][INFO] - Test, Round 077: loss=3.2252, accuracy=0.5932, 
[2025-09-19 11:38:54,216][__main__][INFO] - Test, Round 078: loss=3.2632, accuracy=0.5935, 
[2025-09-19 11:38:54,216][__main__][INFO] - Test, Round 079: loss=3.2813, accuracy=0.5938, 
[2025-09-19 11:38:54,216][__main__][INFO] - Test, Round 080: loss=3.2921, accuracy=0.5930, 
[2025-09-19 11:38:54,216][__main__][INFO] - Test, Round 081: loss=3.3142, accuracy=0.5934, 
[2025-09-19 11:38:54,216][__main__][INFO] - Test, Round 082: loss=3.3263, accuracy=0.5921, 
[2025-09-19 11:38:54,216][__main__][INFO] - Test, Round 083: loss=3.3350, accuracy=0.5917, 
[2025-09-19 11:38:54,216][__main__][INFO] - Test, Round 084: loss=3.3560, accuracy=0.5911, 
[2025-09-19 11:38:54,216][__main__][INFO] - Test, Round 085: loss=3.3671, accuracy=0.5910, 
[2025-09-19 11:38:54,216][__main__][INFO] - Test, Round 086: loss=3.3785, accuracy=0.5919, 
[2025-09-19 11:38:54,216][__main__][INFO] - Test, Round 087: loss=3.3928, accuracy=0.5924, 
[2025-09-19 11:38:54,216][__main__][INFO] - Test, Round 088: loss=3.3963, accuracy=0.5919, 
[2025-09-19 11:38:54,216][__main__][INFO] - Test, Round 089: loss=3.4072, accuracy=0.5926, 
[2025-09-19 11:38:54,216][__main__][INFO] - Test, Round 090: loss=3.4327, accuracy=0.5946, 
[2025-09-19 11:38:54,216][__main__][INFO] - Test, Round 091: loss=3.4491, accuracy=0.5956, 
[2025-09-19 11:38:54,216][__main__][INFO] - Test, Round 092: loss=3.4657, accuracy=0.5950, 
[2025-09-19 11:38:54,216][__main__][INFO] - Test, Round 093: loss=3.4811, accuracy=0.5945, 
[2025-09-19 11:38:54,216][__main__][INFO] - Test, Round 094: loss=3.4868, accuracy=0.5946, 
[2025-09-19 11:38:54,216][__main__][INFO] - Test, Round 095: loss=3.4990, accuracy=0.5947, 
[2025-09-19 11:38:54,216][__main__][INFO] - Test, Round 096: loss=3.5070, accuracy=0.5947, 
[2025-09-19 11:38:54,216][__main__][INFO] - Test, Round 097: loss=3.5181, accuracy=0.5943, 
[2025-09-19 11:38:54,216][__main__][INFO] - Test, Round 098: loss=3.5270, accuracy=0.5940, 
[2025-09-19 11:38:54,216][__main__][INFO] - Test, Round 099: loss=3.5344, accuracy=0.5935, 
[2025-09-19 11:38:54,216][__main__][INFO] - Test, Round 100: loss=3.5365, accuracy=0.5937, 
[2025-09-19 11:38:54,216][__main__][INFO] - Test, Round 101: loss=3.5448, accuracy=0.5937, 
[2025-09-19 11:38:54,217][__main__][INFO] - Test, Round 102: loss=3.5548, accuracy=0.5931, 
[2025-09-19 11:38:54,217][__main__][INFO] - Test, Round 103: loss=3.5621, accuracy=0.5927, 
[2025-09-19 11:38:54,217][__main__][INFO] - Test, Round 104: loss=3.5737, accuracy=0.5944, 
[2025-09-19 11:38:54,217][__main__][INFO] - Test, Round 105: loss=3.5800, accuracy=0.5940, 
[2025-09-19 11:38:54,217][__main__][INFO] - Test, Round 106: loss=3.5919, accuracy=0.5946, 
[2025-09-19 11:38:54,217][__main__][INFO] - Test, Round 107: loss=3.5975, accuracy=0.5943, 
[2025-09-19 11:38:54,217][__main__][INFO] - Test, Round 108: loss=3.6037, accuracy=0.5942, 
[2025-09-19 11:38:54,217][__main__][INFO] - Test, Round 109: loss=3.6109, accuracy=0.5944, 
[2025-09-19 11:38:54,217][__main__][INFO] - Test, Round 110: loss=3.6172, accuracy=0.5945, 
[2025-09-19 11:38:54,217][__main__][INFO] - Test, Round 111: loss=3.6268, accuracy=0.5934, 
[2025-09-19 11:38:54,217][__main__][INFO] - Test, Round 112: loss=3.6347, accuracy=0.5940, 
[2025-09-19 11:38:54,217][__main__][INFO] - Test, Round 113: loss=3.6393, accuracy=0.5942, 
[2025-09-19 11:38:54,217][__main__][INFO] - Test, Round 114: loss=3.6445, accuracy=0.5941, 
[2025-09-19 11:38:54,217][__main__][INFO] - Test, Round 115: loss=3.6502, accuracy=0.5942, 
[2025-09-19 11:38:54,217][__main__][INFO] - Test, Round 116: loss=3.6660, accuracy=0.5955, 
[2025-09-19 11:38:54,217][__main__][INFO] - Test, Round 117: loss=3.6732, accuracy=0.5956, 
[2025-09-19 11:38:54,217][__main__][INFO] - Test, Round 118: loss=3.6809, accuracy=0.5952, 
[2025-09-19 11:38:54,217][__main__][INFO] - Test, Round 119: loss=3.6858, accuracy=0.5950, 
[2025-09-19 11:38:54,217][__main__][INFO] - Test, Round 120: loss=3.6925, accuracy=0.5944, 
[2025-09-19 11:38:54,217][__main__][INFO] - Test, Round 121: loss=3.6999, accuracy=0.5940, 
[2025-09-19 11:38:54,217][__main__][INFO] - Test, Round 122: loss=3.7040, accuracy=0.5944, 
[2025-09-19 11:38:54,217][__main__][INFO] - Test, Round 123: loss=3.7097, accuracy=0.5947, 
[2025-09-19 11:38:54,217][__main__][INFO] - Test, Round 124: loss=3.7160, accuracy=0.5945, 
[2025-09-19 11:38:54,217][__main__][INFO] - Test, Round 125: loss=3.7202, accuracy=0.5944, 
[2025-09-19 11:38:54,217][__main__][INFO] - Test, Round 126: loss=3.7236, accuracy=0.5941, 
[2025-09-19 11:38:54,217][__main__][INFO] - Test, Round 127: loss=3.7331, accuracy=0.5937, 
[2025-09-19 11:38:54,217][__main__][INFO] - Test, Round 128: loss=3.7372, accuracy=0.5938, 
[2025-09-19 11:38:54,217][__main__][INFO] - Test, Round 129: loss=3.7398, accuracy=0.5941, 
[2025-09-19 11:38:54,217][__main__][INFO] - Test, Round 130: loss=3.7436, accuracy=0.5938, 
[2025-09-19 11:38:54,217][__main__][INFO] - Test, Round 131: loss=3.7473, accuracy=0.5940, 
[2025-09-19 11:38:54,217][__main__][INFO] - Test, Round 132: loss=3.7504, accuracy=0.5940, 
[2025-09-19 11:38:54,217][__main__][INFO] - Test, Round 133: loss=3.7569, accuracy=0.5939, 
[2025-09-19 11:38:54,217][__main__][INFO] - Test, Round 134: loss=3.7611, accuracy=0.5939, 
[2025-09-19 11:38:54,217][__main__][INFO] - Test, Round 135: loss=3.7687, accuracy=0.5939, 
[2025-09-19 11:38:54,217][__main__][INFO] - Test, Round 136: loss=3.7714, accuracy=0.5942, 
[2025-09-19 11:38:54,217][__main__][INFO] - Test, Round 137: loss=3.7754, accuracy=0.5942, 
[2025-09-19 11:38:54,217][__main__][INFO] - Test, Round 138: loss=3.7809, accuracy=0.5944, 
[2025-09-19 11:38:54,217][__main__][INFO] - Test, Round 139: loss=3.7853, accuracy=0.5941, 
[2025-09-19 11:38:54,217][__main__][INFO] - Test, Round 140: loss=3.7897, accuracy=0.5941, 
[2025-09-19 11:38:54,217][__main__][INFO] - Test, Round 141: loss=3.7942, accuracy=0.5943, 
[2025-09-19 11:38:54,217][__main__][INFO] - Test, Round 142: loss=3.7973, accuracy=0.5943, 
[2025-09-19 11:38:54,217][__main__][INFO] - Test, Round 143: loss=3.8055, accuracy=0.5940, 
[2025-09-19 11:38:54,217][__main__][INFO] - Test, Round 144: loss=3.8067, accuracy=0.5940, 
[2025-09-19 11:38:54,217][__main__][INFO] - Test, Round 145: loss=3.8097, accuracy=0.5939, 
[2025-09-19 11:38:54,217][__main__][INFO] - Test, Round 146: loss=3.8145, accuracy=0.5942, 
[2025-09-19 11:38:54,217][__main__][INFO] - Test, Round 147: loss=3.8198, accuracy=0.5942, 
[2025-09-19 11:38:54,217][__main__][INFO] - Test, Round 148: loss=3.8218, accuracy=0.5942, 
[2025-09-19 11:38:54,217][__main__][INFO] - Test, Round 149: loss=3.8249, accuracy=0.5939, 
[2025-09-19 11:38:54,218][__main__][INFO] - Test, Round 150: loss=3.8277, accuracy=0.5937, 
[2025-09-19 11:38:54,218][__main__][INFO] - Test, Round 151: loss=3.8294, accuracy=0.5942, 
[2025-09-19 11:38:54,218][__main__][INFO] - Test, Round 152: loss=3.8338, accuracy=0.5943, 
[2025-09-19 11:38:54,218][__main__][INFO] - Test, Round 153: loss=3.8370, accuracy=0.5941, 
[2025-09-19 11:38:54,218][__main__][INFO] - Test, Round 154: loss=3.8408, accuracy=0.5940, 
[2025-09-19 11:38:54,218][__main__][INFO] - Test, Round 155: loss=3.8443, accuracy=0.5937, 
[2025-09-19 11:38:54,218][__main__][INFO] - Test, Round 156: loss=3.8465, accuracy=0.5936, 
[2025-09-19 11:38:54,218][__main__][INFO] - Test, Round 157: loss=3.8506, accuracy=0.5939, 
[2025-09-19 11:38:54,218][__main__][INFO] - Test, Round 158: loss=3.8537, accuracy=0.5939, 
[2025-09-19 11:38:54,218][__main__][INFO] - Test, Round 159: loss=3.8578, accuracy=0.5945, 
[2025-09-19 11:38:54,218][__main__][INFO] - Test, Round 160: loss=3.8601, accuracy=0.5943, 
[2025-09-19 11:38:54,218][__main__][INFO] - Test, Round 161: loss=3.8619, accuracy=0.5939, 
[2025-09-19 11:38:54,218][__main__][INFO] - Test, Round 162: loss=3.8641, accuracy=0.5942, 
[2025-09-19 11:38:54,218][__main__][INFO] - Test, Round 163: loss=3.8660, accuracy=0.5944, 
[2025-09-19 11:38:54,218][__main__][INFO] - Test, Round 164: loss=3.8696, accuracy=0.5940, 
[2025-09-19 11:38:54,218][__main__][INFO] - Test, Round 165: loss=3.8714, accuracy=0.5936, 
[2025-09-19 11:38:54,218][__main__][INFO] - Test, Round 166: loss=3.8730, accuracy=0.5939, 
[2025-09-19 11:38:54,218][__main__][INFO] - Test, Round 167: loss=3.8753, accuracy=0.5942, 
[2025-09-19 11:38:54,218][__main__][INFO] - Test, Round 168: loss=3.8789, accuracy=0.5943, 
[2025-09-19 11:38:54,218][__main__][INFO] - Test, Round 169: loss=3.8809, accuracy=0.5942, 
[2025-09-19 11:38:54,218][__main__][INFO] - Test, Round 170: loss=3.8838, accuracy=0.5941, 
[2025-09-19 11:38:54,218][__main__][INFO] - Test, Round 171: loss=3.8863, accuracy=0.5942, 
[2025-09-19 11:38:54,218][__main__][INFO] - Test, Round 172: loss=3.8883, accuracy=0.5942, 
[2025-09-19 11:38:54,218][__main__][INFO] - Test, Round 173: loss=3.8902, accuracy=0.5944, 
[2025-09-19 11:38:54,218][__main__][INFO] - Test, Round 174: loss=3.8918, accuracy=0.5943, 
[2025-09-19 11:38:54,218][__main__][INFO] - Test, Round 175: loss=3.8940, accuracy=0.5945, 
[2025-09-19 11:38:54,218][__main__][INFO] - Test, Round 176: loss=3.8971, accuracy=0.5946, 
[2025-09-19 11:38:54,218][__main__][INFO] - Test, Round 177: loss=3.8985, accuracy=0.5944, 
[2025-09-19 11:38:54,218][__main__][INFO] - Test, Round 178: loss=3.9001, accuracy=0.5942, 
[2025-09-19 11:38:54,218][__main__][INFO] - Test, Round 179: loss=3.9016, accuracy=0.5939, 
[2025-09-19 11:38:54,218][__main__][INFO] - Test, Round 180: loss=3.9030, accuracy=0.5941, 
[2025-09-19 11:38:54,218][__main__][INFO] - Test, Round 181: loss=3.9057, accuracy=0.5940, 
[2025-09-19 11:38:54,218][__main__][INFO] - Test, Round 182: loss=3.9067, accuracy=0.5940, 
[2025-09-19 11:38:54,218][__main__][INFO] - Test, Round 183: loss=3.9092, accuracy=0.5938, 
[2025-09-19 11:38:54,218][__main__][INFO] - Test, Round 184: loss=3.9111, accuracy=0.5934, 
[2025-09-19 11:38:54,218][__main__][INFO] - Test, Round 185: loss=3.9129, accuracy=0.5936, 
[2025-09-19 11:38:54,218][__main__][INFO] - Test, Round 186: loss=3.9149, accuracy=0.5939, 
[2025-09-19 11:38:54,218][__main__][INFO] - Test, Round 187: loss=3.9166, accuracy=0.5938, 
[2025-09-19 11:38:54,218][__main__][INFO] - Test, Round 188: loss=3.9184, accuracy=0.5942, 
[2025-09-19 11:38:54,218][__main__][INFO] - Test, Round 189: loss=3.9213, accuracy=0.5942, 
[2025-09-19 11:38:54,218][__main__][INFO] - Test, Round 190: loss=3.9224, accuracy=0.5943, 
[2025-09-19 11:38:54,218][__main__][INFO] - Test, Round 191: loss=3.9241, accuracy=0.5947, 
[2025-09-19 11:38:54,218][__main__][INFO] - Test, Round 192: loss=3.9258, accuracy=0.5946, 
[2025-09-19 11:38:54,218][__main__][INFO] - Test, Round 193: loss=3.9268, accuracy=0.5944, 
[2025-09-19 11:38:54,218][__main__][INFO] - Test, Round 194: loss=3.9288, accuracy=0.5942, 
[2025-09-19 11:38:54,218][__main__][INFO] - Test, Round 195: loss=3.9296, accuracy=0.5945, 
[2025-09-19 11:38:54,218][__main__][INFO] - Test, Round 196: loss=3.9319, accuracy=0.5943, 
[2025-09-19 11:38:54,218][__main__][INFO] - Test, Round 197: loss=3.9332, accuracy=0.5944, 
[2025-09-19 11:38:54,218][__main__][INFO] - Test, Round 198: loss=3.9345, accuracy=0.5942, 
[2025-09-19 11:38:54,218][__main__][INFO] - Test, Round 199: loss=3.9359, accuracy=0.5943, 
[2025-09-19 11:38:54,218][__main__][INFO] - Test, Round 200: loss=3.9376, accuracy=0.5943, 
