[2025-09-17 16:19:49,721][flp2p.graph_runner][INFO] - Train, Round 0 : loss => 2.3004941371354195,  accuracy: 0.11696969696969697, gradient_norm : 0.07976990167751642
[2025-09-17 16:19:50,514][flp2p.graph_runner][INFO] - Test, Round 0 : loss => 2.2937065711703855,  accuracy: 0.14855072463768115
[2025-09-17 16:19:53,573][flp2p.graph_runner][INFO] - Train, Round 1 : loss => 2.297836279327219,  accuracy: 0.12321212121212123, gradient_norm : 0.08399827462661712
[2025-09-17 16:19:54,315][flp2p.graph_runner][INFO] - Test, Round 1 : loss => 2.2922169271098003,  accuracy: 0.12941176470588237
[2025-09-17 16:19:57,377][flp2p.graph_runner][INFO] - Train, Round 2 : loss => 2.2943219307697182,  accuracy: 0.12751515151515153, gradient_norm : 0.08658090582050894
[2025-09-17 16:19:58,150][flp2p.graph_runner][INFO] - Test, Round 2 : loss => 2.288622185899736,  accuracy: 0.1410373066424022
[2025-09-17 16:20:01,242][flp2p.graph_runner][INFO] - Train, Round 3 : loss => 2.2886694489103374,  accuracy: 0.12806060606060607, gradient_norm : 0.09747169191322716
[2025-09-17 16:20:01,992][flp2p.graph_runner][INFO] - Test, Round 3 : loss => 2.282350183940822,  accuracy: 0.15258855585831063
[2025-09-17 16:20:05,057][flp2p.graph_runner][INFO] - Train, Round 4 : loss => 2.2782943736423147,  accuracy: 0.14539393939393938, gradient_norm : 0.10867005709560766
[2025-09-17 16:20:05,850][flp2p.graph_runner][INFO] - Test, Round 4 : loss => 2.2741894533874794,  accuracy: 0.15321100917431194
[2025-09-17 16:20:08,853][flp2p.graph_runner][INFO] - Train, Round 5 : loss => 2.2633618739518253,  accuracy: 0.14684848484848484, gradient_norm : 0.12166494037116321
[2025-09-17 16:20:09,651][flp2p.graph_runner][INFO] - Test, Round 5 : loss => 2.2690396281657317,  accuracy: 0.1453382084095064
[2025-09-17 16:20:12,713][flp2p.graph_runner][INFO] - Train, Round 6 : loss => 2.248613692594297,  accuracy: 0.15351515151515155, gradient_norm : 0.12846385670528415
[2025-09-17 16:20:13,505][flp2p.graph_runner][INFO] - Test, Round 6 : loss => 2.2619704456903116,  accuracy: 0.14767547857793983
[2025-09-17 16:20:16,581][flp2p.graph_runner][INFO] - Train, Round 7 : loss => 2.222563089294867,  accuracy: 0.17254545454545456, gradient_norm : 0.12578241646705093
[2025-09-17 16:20:17,356][flp2p.graph_runner][INFO] - Test, Round 7 : loss => 2.2539559528977358,  accuracy: 0.1583257506824386
[2025-09-17 16:20:20,480][flp2p.graph_runner][INFO] - Train, Round 8 : loss => 2.2094985549197053,  accuracy: 0.1876363636363636, gradient_norm : 0.13005885206596884
[2025-09-17 16:20:21,275][flp2p.graph_runner][INFO] - Test, Round 8 : loss => 2.244025616237963,  accuracy: 0.17652411282984531
[2025-09-17 16:20:24,356][flp2p.graph_runner][INFO] - Train, Round 9 : loss => 2.1910057076902096,  accuracy: 0.1826060606060606, gradient_norm : 0.126766539981294
[2025-09-17 16:20:25,177][flp2p.graph_runner][INFO] - Test, Round 9 : loss => 2.2509679206400377,  accuracy: 0.16196542311191992
[2025-09-17 16:20:27,953][flp2p.graph_runner][INFO] - Train, Round 10 : loss => 2.160813065369924,  accuracy: 0.19033333333333335, gradient_norm : 0.14237519087611356
[2025-09-17 16:20:28,665][flp2p.graph_runner][INFO] - Test, Round 10 : loss => 2.2222699932046943,  accuracy: 0.1798201798201798
[2025-09-17 16:20:31,744][flp2p.graph_runner][INFO] - Train, Round 11 : loss => 2.141223388187813,  accuracy: 0.20515151515151514, gradient_norm : 0.1486347390841458
[2025-09-17 16:20:32,530][flp2p.graph_runner][INFO] - Test, Round 11 : loss => 2.2198210077272855,  accuracy: 0.17509025270758122
[2025-09-17 16:20:35,349][flp2p.graph_runner][INFO] - Train, Round 12 : loss => 2.1249328653017683,  accuracy: 0.20753333333333335, gradient_norm : 0.13749497998489196
[2025-09-17 16:20:36,041][flp2p.graph_runner][INFO] - Test, Round 12 : loss => 2.2263122084387086,  accuracy: 0.17953861584754263
[2025-09-17 16:20:39,066][flp2p.graph_runner][INFO] - Train, Round 13 : loss => 2.1086815880103544,  accuracy: 0.22490909090909095, gradient_norm : 0.15552890075513673
[2025-09-17 16:20:39,852][flp2p.graph_runner][INFO] - Test, Round 13 : loss => 2.2552254003297256,  accuracy: 0.17824497257769653
[2025-09-17 16:20:42,890][flp2p.graph_runner][INFO] - Train, Round 14 : loss => 2.0919187276652367,  accuracy: 0.22496969696969696, gradient_norm : 0.14155746416697798
[2025-09-17 16:20:43,665][flp2p.graph_runner][INFO] - Test, Round 14 : loss => 2.266133998524059,  accuracy: 0.18454545454545454
[2025-09-17 16:20:46,754][flp2p.graph_runner][INFO] - Train, Round 15 : loss => 2.072618209051363,  accuracy: 0.23242424242424245, gradient_norm : 0.15499527086960438
[2025-09-17 16:20:47,560][flp2p.graph_runner][INFO] - Test, Round 15 : loss => 2.27730234565526,  accuracy: 0.17244525547445255
[2025-09-17 16:20:50,664][flp2p.graph_runner][INFO] - Train, Round 16 : loss => 2.041245222543225,  accuracy: 0.2341212121212121, gradient_norm : 0.16332561545452143
[2025-09-17 16:20:51,443][flp2p.graph_runner][INFO] - Test, Round 16 : loss => 2.326011649767558,  accuracy: 0.16621004566210046
[2025-09-17 16:20:54,504][flp2p.graph_runner][INFO] - Train, Round 17 : loss => 2.0055938731088783,  accuracy: 0.24806060606060604, gradient_norm : 0.1641592229290749
[2025-09-17 16:20:55,289][flp2p.graph_runner][INFO] - Test, Round 17 : loss => 2.353863075915454,  accuracy: 0.16485507246376813
[2025-09-17 16:20:58,399][flp2p.graph_runner][INFO] - Train, Round 18 : loss => 1.9520034234632144,  accuracy: 0.2817575757575757, gradient_norm : 0.19725749395801098
[2025-09-17 16:20:59,213][flp2p.graph_runner][INFO] - Test, Round 18 : loss => 2.435056584098122,  accuracy: 0.18454545454545454
[2025-09-17 16:21:02,274][flp2p.graph_runner][INFO] - Train, Round 19 : loss => 1.9567522232731185,  accuracy: 0.27212121212121215, gradient_norm : 0.16740360684058755
[2025-09-17 16:21:03,050][flp2p.graph_runner][INFO] - Test, Round 19 : loss => 2.4797381356073274,  accuracy: 0.1741112123974476
[2025-09-17 16:21:06,159][flp2p.graph_runner][INFO] - Train, Round 20 : loss => 1.931022009953405,  accuracy: 0.2693333333333333, gradient_norm : 0.17897652100287578
[2025-09-17 16:21:06,928][flp2p.graph_runner][INFO] - Test, Round 20 : loss => 2.528837837048133,  accuracy: 0.1621129326047359
[2025-09-17 16:21:10,043][flp2p.graph_runner][INFO] - Train, Round 21 : loss => 1.9026142454734356,  accuracy: 0.3030909090909091, gradient_norm : 0.16654971037253571
[2025-09-17 16:21:10,822][flp2p.graph_runner][INFO] - Test, Round 21 : loss => 2.6296406301891357,  accuracy: 0.1640838650865998
[2025-09-17 16:21:13,898][flp2p.graph_runner][INFO] - Train, Round 22 : loss => 1.8552279652971206,  accuracy: 0.3205454545454545, gradient_norm : 0.16900596455615705
[2025-09-17 16:21:14,690][flp2p.graph_runner][INFO] - Test, Round 22 : loss => 2.640788873598156,  accuracy: 0.19872379216043756
[2025-09-17 16:21:17,744][flp2p.graph_runner][INFO] - Train, Round 23 : loss => 1.883345728246213,  accuracy: 0.29393939393939394, gradient_norm : 0.11268087726920156
[2025-09-17 16:21:18,534][flp2p.graph_runner][INFO] - Test, Round 23 : loss => 2.7609298460050065,  accuracy: 0.17545454545454545
[2025-09-17 16:21:21,610][flp2p.graph_runner][INFO] - Train, Round 24 : loss => 1.8742910789212948,  accuracy: 0.2976969696969696, gradient_norm : 0.10325441455059413
[2025-09-17 16:21:22,376][flp2p.graph_runner][INFO] - Test, Round 24 : loss => 2.8352295411260506,  accuracy: 0.16061705989110708
[2025-09-17 16:21:25,475][flp2p.graph_runner][INFO] - Train, Round 25 : loss => 1.8548591791010096,  accuracy: 0.3185454545454545, gradient_norm : 0.1245963605050907
[2025-09-17 16:21:26,271][flp2p.graph_runner][INFO] - Test, Round 25 : loss => 2.863509225563313,  accuracy: 0.17834394904458598
[2025-09-17 16:21:29,360][flp2p.graph_runner][INFO] - Train, Round 26 : loss => 1.8494896180292761,  accuracy: 0.32296969696969696, gradient_norm : 0.11344782789272863
[2025-09-17 16:21:30,173][flp2p.graph_runner][INFO] - Test, Round 26 : loss => 2.90057738337426,  accuracy: 0.1912964641885766
[2025-09-17 16:21:33,262][flp2p.graph_runner][INFO] - Train, Round 27 : loss => 1.8505152328271472,  accuracy: 0.322, gradient_norm : 0.11917373544972784
[2025-09-17 16:21:34,074][flp2p.graph_runner][INFO] - Test, Round 27 : loss => 2.9265158542888288,  accuracy: 0.19077757685352623
[2025-09-17 16:21:36,895][flp2p.graph_runner][INFO] - Train, Round 28 : loss => 1.8049744272856816,  accuracy: 0.3322, gradient_norm : 0.11983967554558703
[2025-09-17 16:21:37,617][flp2p.graph_runner][INFO] - Test, Round 28 : loss => 3.020108134508133,  accuracy: 0.185
[2025-09-17 16:21:40,720][flp2p.graph_runner][INFO] - Train, Round 29 : loss => 1.868249791150447,  accuracy: 0.3047272727272728, gradient_norm : 0.08604828190058983
[2025-09-17 16:21:41,517][flp2p.graph_runner][INFO] - Test, Round 29 : loss => 2.9898887322416368,  accuracy: 0.18337850045167117
[2025-09-17 16:21:44,647][flp2p.graph_runner][INFO] - Train, Round 30 : loss => 1.8326536222417473,  accuracy: 0.33345454545454545, gradient_norm : 0.13982247643217305
[2025-09-17 16:21:45,462][flp2p.graph_runner][INFO] - Test, Round 30 : loss => 2.991953049866748,  accuracy: 0.18891916439600362
[2025-09-17 16:21:48,603][flp2p.graph_runner][INFO] - Train, Round 31 : loss => 1.8138093437415643,  accuracy: 0.343030303030303, gradient_norm : 0.15738230229315178
[2025-09-17 16:21:49,379][flp2p.graph_runner][INFO] - Test, Round 31 : loss => 2.968287562555929,  accuracy: 0.20598911070780399
[2025-09-17 16:21:52,483][flp2p.graph_runner][INFO] - Train, Round 32 : loss => 1.8116217392036391,  accuracy: 0.3426666666666667, gradient_norm : 0.14831073382342963
[2025-09-17 16:21:53,273][flp2p.graph_runner][INFO] - Test, Round 32 : loss => 3.005110208380795,  accuracy: 0.19764279238440616
[2025-09-17 16:21:56,352][flp2p.graph_runner][INFO] - Train, Round 33 : loss => 1.8366270784304517,  accuracy: 0.3256969696969697, gradient_norm : 0.1267102083320047
[2025-09-17 16:21:57,134][flp2p.graph_runner][INFO] - Test, Round 33 : loss => 3.040942506089698,  accuracy: 0.19616788321167883
[2025-09-17 16:22:00,215][flp2p.graph_runner][INFO] - Train, Round 34 : loss => 1.77147994403618,  accuracy: 0.35387878787878785, gradient_norm : 0.17662047721921556
[2025-09-17 16:22:01,030][flp2p.graph_runner][INFO] - Test, Round 34 : loss => 2.9775371030929985,  accuracy: 0.21834862385321102
[2025-09-17 16:22:04,157][flp2p.graph_runner][INFO] - Train, Round 35 : loss => 1.8203278667378155,  accuracy: 0.3358787878787879, gradient_norm : 0.1434252729243479
[2025-09-17 16:22:04,949][flp2p.graph_runner][INFO] - Test, Round 35 : loss => 3.062963690061003,  accuracy: 0.20273972602739726
[2025-09-17 16:22:08,027][flp2p.graph_runner][INFO] - Train, Round 36 : loss => 1.8289637063324775,  accuracy: 0.3292727272727273, gradient_norm : 0.13458052329671147
[2025-09-17 16:22:08,849][flp2p.graph_runner][INFO] - Test, Round 36 : loss => 3.062072933526379,  accuracy: 0.20649233543733092
[2025-09-17 16:22:12,011][flp2p.graph_runner][INFO] - Train, Round 37 : loss => 1.7771746869945098,  accuracy: 0.3646666666666667, gradient_norm : 0.19371441686468083
[2025-09-17 16:22:12,799][flp2p.graph_runner][INFO] - Test, Round 37 : loss => 3.076683654962403,  accuracy: 0.20670897552130554
[2025-09-17 16:22:15,979][flp2p.graph_runner][INFO] - Train, Round 38 : loss => 1.7553908820978408,  accuracy: 0.36448484848484847, gradient_norm : 0.17970542575435658
[2025-09-17 16:22:16,790][flp2p.graph_runner][INFO] - Test, Round 38 : loss => 3.0335886527103,  accuracy: 0.22413793103448276
[2025-09-17 16:22:19,978][flp2p.graph_runner][INFO] - Train, Round 39 : loss => 1.7682089623748183,  accuracy: 0.35909090909090907, gradient_norm : 0.17486504931558036
[2025-09-17 16:22:20,797][flp2p.graph_runner][INFO] - Test, Round 39 : loss => 3.080614448028759,  accuracy: 0.22354014598540145
[2025-09-17 16:22:23,715][flp2p.graph_runner][INFO] - Train, Round 40 : loss => 1.7888859077138477,  accuracy: 0.35040000000000004, gradient_norm : 0.13118066117623065
[2025-09-17 16:22:24,457][flp2p.graph_runner][INFO] - Test, Round 40 : loss => 3.2142682403983,  accuracy: 0.187374749498998
[2025-09-17 16:22:27,590][flp2p.graph_runner][INFO] - Train, Round 41 : loss => 1.7417234419792125,  accuracy: 0.36793939393939395, gradient_norm : 0.1828102761498226
[2025-09-17 16:22:28,403][flp2p.graph_runner][INFO] - Test, Round 41 : loss => 3.0590392332710494,  accuracy: 0.20655141037306643
[2025-09-17 16:22:31,612][flp2p.graph_runner][INFO] - Train, Round 42 : loss => 1.8349823375213996,  accuracy: 0.32551515151515154, gradient_norm : 0.12250753438040954
[2025-09-17 16:22:32,406][flp2p.graph_runner][INFO] - Test, Round 42 : loss => 3.137400117287269,  accuracy: 0.19728506787330316
[2025-09-17 16:22:35,588][flp2p.graph_runner][INFO] - Train, Round 43 : loss => 1.7849517653473108,  accuracy: 0.3536363636363636, gradient_norm : 0.1380133615280638
[2025-09-17 16:22:36,408][flp2p.graph_runner][INFO] - Test, Round 43 : loss => 3.1122399645712338,  accuracy: 0.2096627164995442
[2025-09-17 16:22:39,549][flp2p.graph_runner][INFO] - Train, Round 44 : loss => 1.7818846490880593,  accuracy: 0.3526666666666667, gradient_norm : 0.1766392883113202
[2025-09-17 16:22:40,368][flp2p.graph_runner][INFO] - Test, Round 44 : loss => 3.109742024562497,  accuracy: 0.213768115942029
[2025-09-17 16:22:43,274][flp2p.graph_runner][INFO] - Train, Round 45 : loss => 1.7390027934243943,  accuracy: 0.3659333333333334, gradient_norm : 0.16305235964432738
[2025-09-17 16:22:44,018][flp2p.graph_runner][INFO] - Test, Round 45 : loss => 3.2298657836510696,  accuracy: 0.20241691842900303
[2025-09-17 16:22:47,234][flp2p.graph_runner][INFO] - Train, Round 46 : loss => 1.7710133809322899,  accuracy: 0.3661818181818181, gradient_norm : 0.15404375249711919
[2025-09-17 16:22:48,061][flp2p.graph_runner][INFO] - Test, Round 46 : loss => 3.147194639609678,  accuracy: 0.21350364963503649
[2025-09-17 16:22:51,200][flp2p.graph_runner][INFO] - Train, Round 47 : loss => 1.711130604757328,  accuracy: 0.38836363636363636, gradient_norm : 0.20187470062639964
[2025-09-17 16:22:52,003][flp2p.graph_runner][INFO] - Test, Round 47 : loss => 3.1143615049651907,  accuracy: 0.23688969258589512
[2025-09-17 16:22:55,167][flp2p.graph_runner][INFO] - Train, Round 48 : loss => 1.7630934712867727,  accuracy: 0.3668484848484848, gradient_norm : 0.15995790561635184
[2025-09-17 16:22:55,975][flp2p.graph_runner][INFO] - Test, Round 48 : loss => 3.1496376591365123,  accuracy: 0.2303473491773309
[2025-09-17 16:22:59,157][flp2p.graph_runner][INFO] - Train, Round 49 : loss => 1.6738782642518564,  accuracy: 0.39890909090909094, gradient_norm : 0.20765319171761518
[2025-09-17 16:22:59,973][flp2p.graph_runner][INFO] - Test, Round 49 : loss => 3.1032472491264342,  accuracy: 0.25388127853881276
[2025-09-17 16:23:03,124][flp2p.graph_runner][INFO] - Train, Round 50 : loss => 1.7495469934041135,  accuracy: 0.36612121212121207, gradient_norm : 0.16161627716670257
[2025-09-17 16:23:03,922][flp2p.graph_runner][INFO] - Test, Round 50 : loss => 3.18357347238053,  accuracy: 0.20291173794358508
[2025-09-17 16:23:07,153][flp2p.graph_runner][INFO] - Train, Round 51 : loss => 1.7138176976573463,  accuracy: 0.376, gradient_norm : 0.19977877070881386
[2025-09-17 16:23:07,969][flp2p.graph_runner][INFO] - Test, Round 51 : loss => 3.1164722883183025,  accuracy: 0.25
[2025-09-17 16:23:11,153][flp2p.graph_runner][INFO] - Train, Round 52 : loss => 1.7472659402214268,  accuracy: 0.3651515151515151, gradient_norm : 0.1742687518636251
[2025-09-17 16:23:12,008][flp2p.graph_runner][INFO] - Test, Round 52 : loss => 3.1785011083023735,  accuracy: 0.2191780821917808
[2025-09-17 16:23:15,183][flp2p.graph_runner][INFO] - Train, Round 53 : loss => 1.7411421843897905,  accuracy: 0.36696969696969695, gradient_norm : 0.17061330773387867
[2025-09-17 16:23:16,003][flp2p.graph_runner][INFO] - Test, Round 53 : loss => 3.2004936834988436,  accuracy: 0.22524977293369663
[2025-09-17 16:23:19,176][flp2p.graph_runner][INFO] - Train, Round 54 : loss => 1.6969605961079872,  accuracy: 0.392969696969697, gradient_norm : 0.195854748545083
[2025-09-17 16:23:20,001][flp2p.graph_runner][INFO] - Test, Round 54 : loss => 3.1725356791508337,  accuracy: 0.24932249322493225
[2025-09-17 16:23:23,183][flp2p.graph_runner][INFO] - Train, Round 55 : loss => 1.614747406065748,  accuracy: 0.41836363636363644, gradient_norm : 0.2242642344141588
[2025-09-17 16:23:24,007][flp2p.graph_runner][INFO] - Test, Round 55 : loss => 3.1167884642811496,  accuracy: 0.26067211625794734
[2025-09-17 16:23:27,149][flp2p.graph_runner][INFO] - Train, Round 56 : loss => 1.5696749244604937,  accuracy: 0.44048484848484853, gradient_norm : 0.2406941669926842
[2025-09-17 16:23:27,991][flp2p.graph_runner][INFO] - Test, Round 56 : loss => 3.1223703830866842,  accuracy: 0.2883046237533998
[2025-09-17 16:23:31,179][flp2p.graph_runner][INFO] - Train, Round 57 : loss => 1.682371541379711,  accuracy: 0.3927878787878788, gradient_norm : 0.18505911572573738
[2025-09-17 16:23:31,983][flp2p.graph_runner][INFO] - Test, Round 57 : loss => 3.2335885231061416,  accuracy: 0.24454545454545454
[2025-09-17 16:23:35,130][flp2p.graph_runner][INFO] - Train, Round 58 : loss => 1.7041588147112634,  accuracy: 0.39630303030303027, gradient_norm : 0.1989212675214975
[2025-09-17 16:23:35,924][flp2p.graph_runner][INFO] - Test, Round 58 : loss => 3.2207514594723707,  accuracy: 0.24087591240875914
[2025-09-17 16:23:39,153][flp2p.graph_runner][INFO] - Train, Round 59 : loss => 1.689022864430929,  accuracy: 0.4042424242424243, gradient_norm : 0.19994201949118964
[2025-09-17 16:23:39,983][flp2p.graph_runner][INFO] - Test, Round 59 : loss => 3.232922084974658,  accuracy: 0.23614895549500453
[2025-09-17 16:23:43,185][flp2p.graph_runner][INFO] - Train, Round 60 : loss => 1.6326472944735237,  accuracy: 0.41557575757575765, gradient_norm : 0.20388376047532153
[2025-09-17 16:23:44,008][flp2p.graph_runner][INFO] - Test, Round 60 : loss => 3.212635102057262,  accuracy: 0.2875341219290264
[2025-09-17 16:23:47,196][flp2p.graph_runner][INFO] - Train, Round 61 : loss => 1.7447912259325897,  accuracy: 0.3761212121212121, gradient_norm : 0.18220395282620183
[2025-09-17 16:23:48,005][flp2p.graph_runner][INFO] - Test, Round 61 : loss => 3.2773954218820385,  accuracy: 0.22554347826086957
[2025-09-17 16:23:51,140][flp2p.graph_runner][INFO] - Train, Round 62 : loss => 1.7571798204237097,  accuracy: 0.36654545454545456, gradient_norm : 0.17827090083163713
[2025-09-17 16:23:51,960][flp2p.graph_runner][INFO] - Test, Round 62 : loss => 3.3315109242714853,  accuracy: 0.2096627164995442
[2025-09-17 16:23:55,180][flp2p.graph_runner][INFO] - Train, Round 63 : loss => 1.6280352124919195,  accuracy: 0.4150303030303031, gradient_norm : 0.22537384851752693
[2025-09-17 16:23:56,013][flp2p.graph_runner][INFO] - Test, Round 63 : loss => 3.2844386478683214,  accuracy: 0.25709057639524246
[2025-09-17 16:23:58,945][flp2p.graph_runner][INFO] - Train, Round 64 : loss => 1.6265669195146377,  accuracy: 0.4072666666666667, gradient_norm : 0.2006583545071666
[2025-09-17 16:23:59,691][flp2p.graph_runner][INFO] - Test, Round 64 : loss => 3.374430661504572,  accuracy: 0.24024024024024024
[2025-09-17 16:24:02,909][flp2p.graph_runner][INFO] - Train, Round 65 : loss => 1.6301712982115562,  accuracy: 0.410909090909091, gradient_norm : 0.24220271954299913
[2025-09-17 16:24:03,716][flp2p.graph_runner][INFO] - Test, Round 65 : loss => 3.2613924703377544,  accuracy: 0.2611060743427017
[2025-09-17 16:24:06,974][flp2p.graph_runner][INFO] - Train, Round 66 : loss => 1.6795959179435158,  accuracy: 0.3866060606060606, gradient_norm : 0.21527496467000348
[2025-09-17 16:24:07,791][flp2p.graph_runner][INFO] - Test, Round 66 : loss => 3.3301137988408986,  accuracy: 0.22020018198362148
[2025-09-17 16:24:10,949][flp2p.graph_runner][INFO] - Train, Round 67 : loss => 1.6189112539250345,  accuracy: 0.41430303030303034, gradient_norm : 0.22567477449709852
[2025-09-17 16:24:11,755][flp2p.graph_runner][INFO] - Test, Round 67 : loss => 3.292368296821948,  accuracy: 0.2484076433121019
[2025-09-17 16:24:14,917][flp2p.graph_runner][INFO] - Train, Round 68 : loss => 1.6968288117081323,  accuracy: 0.3887878787878788, gradient_norm : 0.19030362196368508
[2025-09-17 16:24:15,736][flp2p.graph_runner][INFO] - Test, Round 68 : loss => 3.317535830177132,  accuracy: 0.23222322232223222
[2025-09-17 16:24:18,984][flp2p.graph_runner][INFO] - Train, Round 69 : loss => 1.6867259835048711,  accuracy: 0.3920606060606061, gradient_norm : 0.217947969762996
[2025-09-17 16:24:19,791][flp2p.graph_runner][INFO] - Test, Round 69 : loss => 3.337592721311418,  accuracy: 0.21969006381039197
[2025-09-17 16:24:23,040][flp2p.graph_runner][INFO] - Train, Round 70 : loss => 1.592398884874097,  accuracy: 0.43224242424242426, gradient_norm : 0.24409681261095664
[2025-09-17 16:24:23,848][flp2p.graph_runner][INFO] - Test, Round 70 : loss => 3.3179053256414597,  accuracy: 0.25520361990950224
[2025-09-17 16:24:27,020][flp2p.graph_runner][INFO] - Train, Round 71 : loss => 1.6195184238825855,  accuracy: 0.41921212121212126, gradient_norm : 0.22705262034534857
[2025-09-17 16:24:27,845][flp2p.graph_runner][INFO] - Test, Round 71 : loss => 3.3167572778698284,  accuracy: 0.23556370302474794
[2025-09-17 16:24:31,006][flp2p.graph_runner][INFO] - Train, Round 72 : loss => 1.592812699877179,  accuracy: 0.4258181818181818, gradient_norm : 0.23265616930931438
[2025-09-17 16:24:31,828][flp2p.graph_runner][INFO] - Test, Round 72 : loss => 3.2958765257075564,  accuracy: 0.2631578947368421
[2025-09-17 16:24:34,979][flp2p.graph_runner][INFO] - Train, Round 73 : loss => 1.6381310579606427,  accuracy: 0.4062424242424243, gradient_norm : 0.21487526371295684
[2025-09-17 16:24:35,809][flp2p.graph_runner][INFO] - Test, Round 73 : loss => 3.3309528207670063,  accuracy: 0.2365296803652968
[2025-09-17 16:24:39,006][flp2p.graph_runner][INFO] - Train, Round 74 : loss => 1.5071547680086177,  accuracy: 0.4579393939393939, gradient_norm : 0.23436379653748632
[2025-09-17 16:24:39,809][flp2p.graph_runner][INFO] - Test, Round 74 : loss => 3.3417222942128,  accuracy: 0.27529626253418416
[2025-09-17 16:24:42,952][flp2p.graph_runner][INFO] - Train, Round 75 : loss => 1.538004625458901,  accuracy: 0.4415757575757576, gradient_norm : 0.23116136455069783
[2025-09-17 16:24:43,768][flp2p.graph_runner][INFO] - Test, Round 75 : loss => 3.2705070105515324,  accuracy: 0.2877959927140255
[2025-09-17 16:24:46,908][flp2p.graph_runner][INFO] - Train, Round 76 : loss => 1.4878173037566629,  accuracy: 0.45836363636363636, gradient_norm : 0.23815244089866655
[2025-09-17 16:24:47,740][flp2p.graph_runner][INFO] - Test, Round 76 : loss => 3.326032230358141,  accuracy: 0.298819255222525
[2025-09-17 16:24:50,904][flp2p.graph_runner][INFO] - Train, Round 77 : loss => 1.5487793520534814,  accuracy: 0.4475151515151515, gradient_norm : 0.2385118915213713
[2025-09-17 16:24:51,731][flp2p.graph_runner][INFO] - Test, Round 77 : loss => 3.4246253610958934,  accuracy: 0.2529572338489536
[2025-09-17 16:24:54,909][flp2p.graph_runner][INFO] - Train, Round 78 : loss => 1.6317269431344883,  accuracy: 0.40812121212121216, gradient_norm : 0.21267285280800366
[2025-09-17 16:24:55,735][flp2p.graph_runner][INFO] - Test, Round 78 : loss => 3.396543168296173,  accuracy: 0.23342415985467757
[2025-09-17 16:24:58,963][flp2p.graph_runner][INFO] - Train, Round 79 : loss => 1.4689613957834808,  accuracy: 0.47345454545454546, gradient_norm : 0.24318700611017272
[2025-09-17 16:24:59,795][flp2p.graph_runner][INFO] - Test, Round 79 : loss => 3.430880512440041,  accuracy: 0.2639269406392694
[2025-09-17 16:25:02,948][flp2p.graph_runner][INFO] - Train, Round 80 : loss => 1.6104729620258722,  accuracy: 0.4223030303030303, gradient_norm : 0.21789498201882954
[2025-09-17 16:25:03,766][flp2p.graph_runner][INFO] - Test, Round 80 : loss => 3.401242573207075,  accuracy: 0.2636363636363636
[2025-09-17 16:25:06,920][flp2p.graph_runner][INFO] - Train, Round 81 : loss => 1.5502457445280455,  accuracy: 0.44072727272727275, gradient_norm : 0.22627530484157987
[2025-09-17 16:25:07,769][flp2p.graph_runner][INFO] - Test, Round 81 : loss => 3.4581425958785457,  accuracy: 0.259963768115942
[2025-09-17 16:25:10,889][flp2p.graph_runner][INFO] - Train, Round 82 : loss => 1.5269181327502972,  accuracy: 0.44424242424242416, gradient_norm : 0.23326839706697947
[2025-09-17 16:25:11,717][flp2p.graph_runner][INFO] - Test, Round 82 : loss => 3.4186077867328684,  accuracy: 0.2718978102189781
[2025-09-17 16:25:14,623][flp2p.graph_runner][INFO] - Train, Round 83 : loss => 1.5129770231641788,  accuracy: 0.4547333333333334, gradient_norm : 0.2199189263276778
[2025-09-17 16:25:15,377][flp2p.graph_runner][INFO] - Test, Round 83 : loss => 3.536385251363277,  accuracy: 0.2741774675972084
[2025-09-17 16:25:18,610][flp2p.graph_runner][INFO] - Train, Round 84 : loss => 1.540817320890214,  accuracy: 0.4438787878787878, gradient_norm : 0.23707350054013715
[2025-09-17 16:25:19,458][flp2p.graph_runner][INFO] - Test, Round 84 : loss => 3.3508764775283972,  accuracy: 0.3030852994555354
[2025-09-17 16:25:22,667][flp2p.graph_runner][INFO] - Train, Round 85 : loss => 1.5191472121538636,  accuracy: 0.45254545454545464, gradient_norm : 0.24009326840315975
[2025-09-17 16:25:23,468][flp2p.graph_runner][INFO] - Test, Round 85 : loss => 3.4577815981894515,  accuracy: 0.2737226277372263
[2025-09-17 16:25:26,616][flp2p.graph_runner][INFO] - Train, Round 86 : loss => 1.444566764345919,  accuracy: 0.4804848484848485, gradient_norm : 0.2434954521402415
[2025-09-17 16:25:27,451][flp2p.graph_runner][INFO] - Test, Round 86 : loss => 3.418223975029859,  accuracy: 0.26454545454545453
[2025-09-17 16:25:30,593][flp2p.graph_runner][INFO] - Train, Round 87 : loss => 1.4537627504009971,  accuracy: 0.4699393939393938, gradient_norm : 0.2571018892145942
[2025-09-17 16:25:31,418][flp2p.graph_runner][INFO] - Test, Round 87 : loss => 3.4662868842864647,  accuracy: 0.2683150183150183
[2025-09-17 16:25:34,631][flp2p.graph_runner][INFO] - Train, Round 88 : loss => 1.4324831749174445,  accuracy: 0.48581818181818176, gradient_norm : 0.26477017368461636
[2025-09-17 16:25:35,464][flp2p.graph_runner][INFO] - Test, Round 88 : loss => 3.394104290387847,  accuracy: 0.2772727272727273
[2025-09-17 16:25:38,608][flp2p.graph_runner][INFO] - Train, Round 89 : loss => 1.518863021055288,  accuracy: 0.4515757575757576, gradient_norm : 0.23445588254284247
[2025-09-17 16:25:39,444][flp2p.graph_runner][INFO] - Test, Round 89 : loss => 3.460650393537655,  accuracy: 0.28844404003639673
[2025-09-17 16:25:42,595][flp2p.graph_runner][INFO] - Train, Round 90 : loss => 1.4737407249363472,  accuracy: 0.46993939393939393, gradient_norm : 0.25118302594188596
[2025-09-17 16:25:43,411][flp2p.graph_runner][INFO] - Test, Round 90 : loss => 3.445942180752754,  accuracy: 0.2772727272727273
[2025-09-17 16:25:46,651][flp2p.graph_runner][INFO] - Train, Round 91 : loss => 1.488239163185316,  accuracy: 0.46927272727272734, gradient_norm : 0.23925343549607814
[2025-09-17 16:25:47,483][flp2p.graph_runner][INFO] - Test, Round 91 : loss => 3.5031123758973304,  accuracy: 0.2594594594594595
[2025-09-17 16:25:50,663][flp2p.graph_runner][INFO] - Train, Round 92 : loss => 1.5088642049229912,  accuracy: 0.457090909090909, gradient_norm : 0.2297326280452693
[2025-09-17 16:25:51,488][flp2p.graph_runner][INFO] - Test, Round 92 : loss => 3.4858743148691516,  accuracy: 0.26153846153846155
[2025-09-17 16:25:54,696][flp2p.graph_runner][INFO] - Train, Round 93 : loss => 1.2964287733209472,  accuracy: 0.5362424242424243, gradient_norm : 0.27322807647291963
[2025-09-17 16:25:55,513][flp2p.graph_runner][INFO] - Test, Round 93 : loss => 3.5994555635659777,  accuracy: 0.28377153218495016
[2025-09-17 16:25:58,766][flp2p.graph_runner][INFO] - Train, Round 94 : loss => 1.4284989536731836,  accuracy: 0.483939393939394, gradient_norm : 0.27709720572106344
[2025-09-17 16:25:59,581][flp2p.graph_runner][INFO] - Test, Round 94 : loss => 3.465995687045652,  accuracy: 0.3057324840764331
[2025-09-17 16:26:02,708][flp2p.graph_runner][INFO] - Train, Round 95 : loss => 1.4781867841819334,  accuracy: 0.4652727272727273, gradient_norm : 0.23657582175144423
[2025-09-17 16:26:03,548][flp2p.graph_runner][INFO] - Test, Round 95 : loss => 3.473171131325028,  accuracy: 0.2742960944595822
[2025-09-17 16:26:06,388][flp2p.graph_runner][INFO] - Train, Round 96 : loss => 1.4893103946275004,  accuracy: 0.4616, gradient_norm : 0.22525101079512075
[2025-09-17 16:26:07,140][flp2p.graph_runner][INFO] - Test, Round 96 : loss => 3.6569965372739683,  accuracy: 0.27037773359840955
[2025-09-17 16:26:10,365][flp2p.graph_runner][INFO] - Train, Round 97 : loss => 1.3384466099746395,  accuracy: 0.5124242424242424, gradient_norm : 0.2600334027032737
[2025-09-17 16:26:11,174][flp2p.graph_runner][INFO] - Test, Round 97 : loss => 3.538282307194754,  accuracy: 0.28701180744777477
[2025-09-17 16:26:14,399][flp2p.graph_runner][INFO] - Train, Round 98 : loss => 1.5390296833831434,  accuracy: 0.45248484848484843, gradient_norm : 0.23187405077382708
[2025-09-17 16:26:15,219][flp2p.graph_runner][INFO] - Test, Round 98 : loss => 3.55315126535671,  accuracy: 0.2604735883424408
[2025-09-17 16:26:18,392][flp2p.graph_runner][INFO] - Train, Round 99 : loss => 1.4127257602438918,  accuracy: 0.489030303030303, gradient_norm : 0.27246108353831455
[2025-09-17 16:26:19,203][flp2p.graph_runner][INFO] - Test, Round 99 : loss => 3.511670955985703,  accuracy: 0.292616226071103
[2025-09-17 16:26:22,348][flp2p.graph_runner][INFO] - Train, Round 100 : loss => 1.5039776884667928,  accuracy: 0.4516969696969697, gradient_norm : 0.2503044886055105
[2025-09-17 16:26:23,181][flp2p.graph_runner][INFO] - Test, Round 100 : loss => 3.5140551458823888,  accuracy: 0.2629663330300273
[2025-09-17 16:26:26,365][flp2p.graph_runner][INFO] - Train, Round 101 : loss => 1.3948084172708066,  accuracy: 0.5010909090909091, gradient_norm : 0.26995469509004477
[2025-09-17 16:26:27,185][flp2p.graph_runner][INFO] - Test, Round 101 : loss => 3.557760224057022,  accuracy: 0.2747053490480508
[2025-09-17 16:26:30,021][flp2p.graph_runner][INFO] - Train, Round 102 : loss => 1.5981885081171034,  accuracy: 0.4213333333333333, gradient_norm : 0.2847392053296691
[2025-09-17 16:26:30,759][flp2p.graph_runner][INFO] - Test, Round 102 : loss => 2.905824106646724,  accuracy: 0.26195219123505975
[2025-09-17 16:26:33,941][flp2p.graph_runner][INFO] - Train, Round 103 : loss => 1.287804765134228,  accuracy: 0.5415757575757576, gradient_norm : 0.274902223828865
[2025-09-17 16:26:34,779][flp2p.graph_runner][INFO] - Test, Round 103 : loss => 3.6049650384145395,  accuracy: 0.2984531392174704
[2025-09-17 16:26:37,989][flp2p.graph_runner][INFO] - Train, Round 104 : loss => 1.3104657895146927,  accuracy: 0.5281212121212122, gradient_norm : 0.26189663846803707
[2025-09-17 16:26:38,805][flp2p.graph_runner][INFO] - Test, Round 104 : loss => 3.61900593278196,  accuracy: 0.28765880217785844
[2025-09-17 16:26:41,981][flp2p.graph_runner][INFO] - Train, Round 105 : loss => 1.5085126464747383,  accuracy: 0.4566666666666667, gradient_norm : 0.2348590707397684
[2025-09-17 16:26:42,792][flp2p.graph_runner][INFO] - Test, Round 105 : loss => 3.5615446938094815,  accuracy: 0.27347310847766637
[2025-09-17 16:26:45,984][flp2p.graph_runner][INFO] - Train, Round 106 : loss => 1.504374977545536,  accuracy: 0.4556363636363636, gradient_norm : 0.24454891204600718
[2025-09-17 16:26:46,825][flp2p.graph_runner][INFO] - Test, Round 106 : loss => 3.610418668218479,  accuracy: 0.27661510464058237
[2025-09-17 16:26:50,025][flp2p.graph_runner][INFO] - Train, Round 107 : loss => 1.413778017197811,  accuracy: 0.4918787878787878, gradient_norm : 0.25566953291232003
[2025-09-17 16:26:50,851][flp2p.graph_runner][INFO] - Test, Round 107 : loss => 3.556695726917916,  accuracy: 0.2893539581437671
[2025-09-17 16:26:54,098][flp2p.graph_runner][INFO] - Train, Round 108 : loss => 1.364852474417217,  accuracy: 0.5042424242424243, gradient_norm : 0.26461109728629695
[2025-09-17 16:26:54,912][flp2p.graph_runner][INFO] - Test, Round 108 : loss => 3.621642898690365,  accuracy: 0.2888283378746594
[2025-09-17 16:26:58,046][flp2p.graph_runner][INFO] - Train, Round 109 : loss => 1.395277814852913,  accuracy: 0.5073939393939394, gradient_norm : 0.26325652688995843
[2025-09-17 16:26:58,876][flp2p.graph_runner][INFO] - Test, Round 109 : loss => 3.6371171991933475,  accuracy: 0.2881818181818182
[2025-09-17 16:27:02,020][flp2p.graph_runner][INFO] - Train, Round 110 : loss => 1.3620767826727336,  accuracy: 0.5076969696969698, gradient_norm : 0.25903200569823975
[2025-09-17 16:27:02,819][flp2p.graph_runner][INFO] - Test, Round 110 : loss => 3.6019804891627807,  accuracy: 0.28649138712601996
[2025-09-17 16:27:06,042][flp2p.graph_runner][INFO] - Train, Round 111 : loss => 1.2526013549061397,  accuracy: 0.5492727272727271, gradient_norm : 0.29527168730753134
[2025-09-17 16:27:06,865][flp2p.graph_runner][INFO] - Test, Round 111 : loss => 3.645670288850005,  accuracy: 0.2902638762511374
[2025-09-17 16:27:09,777][flp2p.graph_runner][INFO] - Train, Round 112 : loss => 1.1159832890837609,  accuracy: 0.6020666666666667, gradient_norm : 0.2742565933975503
[2025-09-17 16:27:10,525][flp2p.graph_runner][INFO] - Test, Round 112 : loss => 4.027476150707069,  accuracy: 0.28413654618473894
[2025-09-17 16:27:13,716][flp2p.graph_runner][INFO] - Train, Round 113 : loss => 1.2982136496239587,  accuracy: 0.5258181818181819, gradient_norm : 0.27426074424241814
[2025-09-17 16:27:14,536][flp2p.graph_runner][INFO] - Test, Round 113 : loss => 3.5692745720119756,  accuracy: 0.3235831809872029
[2025-09-17 16:27:17,741][flp2p.graph_runner][INFO] - Train, Round 114 : loss => 1.3224174718377595,  accuracy: 0.5268484848484848, gradient_norm : 0.2572758484701017
[2025-09-17 16:27:18,571][flp2p.graph_runner][INFO] - Test, Round 114 : loss => 3.7036548641978504,  accuracy: 0.2676950998185118
[2025-09-17 16:27:21,758][flp2p.graph_runner][INFO] - Train, Round 115 : loss => 1.3833587710645534,  accuracy: 0.49678787878787883, gradient_norm : 0.26818432614485305
[2025-09-17 16:27:22,578][flp2p.graph_runner][INFO] - Test, Round 115 : loss => 3.6821549668703994,  accuracy: 0.2958904109589041
[2025-09-17 16:27:25,706][flp2p.graph_runner][INFO] - Train, Round 116 : loss => 1.3131526287392756,  accuracy: 0.5324848484848484, gradient_norm : 0.28919679576991164
[2025-09-17 16:27:26,524][flp2p.graph_runner][INFO] - Test, Round 116 : loss => 3.6351748939535833,  accuracy: 0.29545454545454547
[2025-09-17 16:27:29,444][flp2p.graph_runner][INFO] - Train, Round 117 : loss => 1.1134440875509426,  accuracy: 0.6009999999999999, gradient_norm : 0.2779378942921301
[2025-09-17 16:27:30,208][flp2p.graph_runner][INFO] - Test, Round 117 : loss => 3.951620715988209,  accuracy: 0.29518072289156627
[2025-09-17 16:27:33,461][flp2p.graph_runner][INFO] - Train, Round 118 : loss => 1.2334762292042467,  accuracy: 0.5627878787878788, gradient_norm : 0.280531328078598
[2025-09-17 16:27:34,290][flp2p.graph_runner][INFO] - Test, Round 118 : loss => 3.6761804949153554,  accuracy: 0.30272727272727273
[2025-09-17 16:27:37,451][flp2p.graph_runner][INFO] - Train, Round 119 : loss => 1.2319247447954749,  accuracy: 0.5595757575757576, gradient_norm : 0.29185483403219625
[2025-09-17 16:27:38,265][flp2p.graph_runner][INFO] - Test, Round 119 : loss => 3.6502320614865864,  accuracy: 0.3060853769300636
[2025-09-17 16:27:41,143][flp2p.graph_runner][INFO] - Train, Round 120 : loss => 1.3269802171899145,  accuracy: 0.5240666666666667, gradient_norm : 0.3103950766133329
[2025-09-17 16:27:41,891][flp2p.graph_runner][INFO] - Test, Round 120 : loss => 2.900898815099015,  accuracy: 0.32831325301204817
[2025-09-17 16:27:45,109][flp2p.graph_runner][INFO] - Train, Round 121 : loss => 1.3039346900358753,  accuracy: 0.5283636363636364, gradient_norm : 0.27682988583741097
[2025-09-17 16:27:45,930][flp2p.graph_runner][INFO] - Test, Round 121 : loss => 3.7008736979734023,  accuracy: 0.29336966394187103
[2025-09-17 16:27:49,117][flp2p.graph_runner][INFO] - Train, Round 122 : loss => 1.2549089530930084,  accuracy: 0.5476363636363636, gradient_norm : 0.28656470988403354
[2025-09-17 16:27:49,958][flp2p.graph_runner][INFO] - Test, Round 122 : loss => 3.735913591469572,  accuracy: 0.28688524590163933
[2025-09-17 16:27:53,131][flp2p.graph_runner][INFO] - Train, Round 123 : loss => 1.255657617933001,  accuracy: 0.5456969696969698, gradient_norm : 0.29389398996903676
[2025-09-17 16:27:53,958][flp2p.graph_runner][INFO] - Test, Round 123 : loss => 3.7200598033595846,  accuracy: 0.29863013698630136
[2025-09-17 16:27:57,131][flp2p.graph_runner][INFO] - Train, Round 124 : loss => 1.2221543559320653,  accuracy: 0.5596363636363636, gradient_norm : 0.28554358613383096
[2025-09-17 16:27:57,932][flp2p.graph_runner][INFO] - Test, Round 124 : loss => 3.748672621405643,  accuracy: 0.2916666666666667
[2025-09-17 16:28:01,186][flp2p.graph_runner][INFO] - Train, Round 125 : loss => 1.2483756603856049,  accuracy: 0.552, gradient_norm : 0.2815630937900794
[2025-09-17 16:28:02,019][flp2p.graph_runner][INFO] - Test, Round 125 : loss => 3.638517030307534,  accuracy: 0.3369663941871026
[2025-09-17 16:28:05,162][flp2p.graph_runner][INFO] - Train, Round 126 : loss => 1.1493447846842175,  accuracy: 0.5978181818181818, gradient_norm : 0.3008502004035634
[2025-09-17 16:28:05,983][flp2p.graph_runner][INFO] - Test, Round 126 : loss => 3.7609166231225517,  accuracy: 0.31893382352941174
[2025-09-17 16:28:09,205][flp2p.graph_runner][INFO] - Train, Round 127 : loss => 1.2911280327496182,  accuracy: 0.5373939393939393, gradient_norm : 0.27541704795076966
[2025-09-17 16:28:10,045][flp2p.graph_runner][INFO] - Test, Round 127 : loss => 3.77171904554993,  accuracy: 0.3017319963536919
[2025-09-17 16:28:13,200][flp2p.graph_runner][INFO] - Train, Round 128 : loss => 1.269844402382715,  accuracy: 0.5432121212121211, gradient_norm : 0.2828432535216991
[2025-09-17 16:28:14,017][flp2p.graph_runner][INFO] - Test, Round 128 : loss => 3.7389444711538395,  accuracy: 0.30783242258652094
[2025-09-17 16:28:17,242][flp2p.graph_runner][INFO] - Train, Round 129 : loss => 1.1573333523718963,  accuracy: 0.5843636363636364, gradient_norm : 0.29595537660538995
[2025-09-17 16:28:18,067][flp2p.graph_runner][INFO] - Test, Round 129 : loss => 3.7991242806297523,  accuracy: 0.31574158325750684
[2025-09-17 16:28:21,275][flp2p.graph_runner][INFO] - Train, Round 130 : loss => 1.1664129528062819,  accuracy: 0.5806060606060607, gradient_norm : 0.2898293633246217
[2025-09-17 16:28:22,108][flp2p.graph_runner][INFO] - Test, Round 130 : loss => 3.7291780964511894,  accuracy: 0.3088768115942029
[2025-09-17 16:28:25,026][flp2p.graph_runner][INFO] - Train, Round 131 : loss => 1.0612622094756867,  accuracy: 0.6216666666666668, gradient_norm : 0.3183622894901049
[2025-09-17 16:28:25,800][flp2p.graph_runner][INFO] - Test, Round 131 : loss => 4.065186589896319,  accuracy: 0.31388329979879276
[2025-09-17 16:28:28,998][flp2p.graph_runner][INFO] - Train, Round 132 : loss => 1.2061864720816713,  accuracy: 0.566969696969697, gradient_norm : 0.2945140084339645
[2025-09-17 16:28:29,825][flp2p.graph_runner][INFO] - Test, Round 132 : loss => 3.819705934823915,  accuracy: 0.30118289353958144
[2025-09-17 16:28:32,990][flp2p.graph_runner][INFO] - Train, Round 133 : loss => 1.0205642821143472,  accuracy: 0.6404242424242423, gradient_norm : 0.3030005454639791
[2025-09-17 16:28:33,807][flp2p.graph_runner][INFO] - Test, Round 133 : loss => 3.9133033043556456,  accuracy: 0.32155797101449274
[2025-09-17 16:28:36,957][flp2p.graph_runner][INFO] - Train, Round 134 : loss => 1.2562798444875458,  accuracy: 0.5523636363636364, gradient_norm : 0.27941803297328166
[2025-09-17 16:28:37,787][flp2p.graph_runner][INFO] - Test, Round 134 : loss => 3.8158292760614487,  accuracy: 0.30601092896174864
[2025-09-17 16:28:40,946][flp2p.graph_runner][INFO] - Train, Round 135 : loss => 1.1705694954490686,  accuracy: 0.5844242424242424, gradient_norm : 0.2812937016099309
[2025-09-17 16:28:41,771][flp2p.graph_runner][INFO] - Test, Round 135 : loss => 3.8271004746770774,  accuracy: 0.29927667269439423
[2025-09-17 16:28:44,962][flp2p.graph_runner][INFO] - Train, Round 136 : loss => 0.9933566172661895,  accuracy: 0.647939393939394, gradient_norm : 0.3075417741185454
[2025-09-17 16:28:45,752][flp2p.graph_runner][INFO] - Test, Round 136 : loss => 4.061704682556745,  accuracy: 0.31607629427792916
[2025-09-17 16:28:48,998][flp2p.graph_runner][INFO] - Train, Round 137 : loss => 1.104115055580168,  accuracy: 0.6051515151515152, gradient_norm : 0.2968862334195694
[2025-09-17 16:28:49,828][flp2p.graph_runner][INFO] - Test, Round 137 : loss => 3.8903096994445643,  accuracy: 0.2933212996389892
[2025-09-17 16:28:52,998][flp2p.graph_runner][INFO] - Train, Round 138 : loss => 1.129905837264673,  accuracy: 0.596121212121212, gradient_norm : 0.2905363452029937
[2025-09-17 16:28:53,803][flp2p.graph_runner][INFO] - Test, Round 138 : loss => 3.8340571840424342,  accuracy: 0.302262443438914
[2025-09-17 16:28:56,987][flp2p.graph_runner][INFO] - Train, Round 139 : loss => 1.1539098148962308,  accuracy: 0.5847878787878787, gradient_norm : 0.30581275325768936
[2025-09-17 16:28:57,785][flp2p.graph_runner][INFO] - Test, Round 139 : loss => 3.85638803260027,  accuracy: 0.307974335472044
[2025-09-17 16:29:00,965][flp2p.graph_runner][INFO] - Train, Round 140 : loss => 1.0653002492643446,  accuracy: 0.6211515151515151, gradient_norm : 0.30238362904436444
[2025-09-17 16:29:01,782][flp2p.graph_runner][INFO] - Test, Round 140 : loss => 4.060349138628491,  accuracy: 0.3099361896080219
[2025-09-17 16:29:05,030][flp2p.graph_runner][INFO] - Train, Round 141 : loss => 1.0645223802417105,  accuracy: 0.6226060606060606, gradient_norm : 0.3122088644206881
[2025-09-17 16:29:05,852][flp2p.graph_runner][INFO] - Test, Round 141 : loss => 4.0109715247849085,  accuracy: 0.2941712204007286
[2025-09-17 16:29:08,759][flp2p.graph_runner][INFO] - Train, Round 142 : loss => 1.1053573903764269,  accuracy: 0.6024, gradient_norm : 0.27532828836945844
[2025-09-17 16:29:09,504][flp2p.graph_runner][INFO] - Test, Round 142 : loss => 4.056812461391388,  accuracy: 0.32270916334661354
[2025-09-17 16:29:12,386][flp2p.graph_runner][INFO] - Train, Round 143 : loss => 1.0547721065029387,  accuracy: 0.6226666666666666, gradient_norm : 0.2950093875375626
[2025-09-17 16:29:13,128][flp2p.graph_runner][INFO] - Test, Round 143 : loss => 4.048295312317978,  accuracy: 0.32429718875502006
[2025-09-17 16:29:16,377][flp2p.graph_runner][INFO] - Train, Round 144 : loss => 1.0394160456284032,  accuracy: 0.6241212121212122, gradient_norm : 0.31417217492548244
[2025-09-17 16:29:17,176][flp2p.graph_runner][INFO] - Test, Round 144 : loss => 4.03281655649072,  accuracy: 0.2831050228310502
[2025-09-17 16:29:20,392][flp2p.graph_runner][INFO] - Train, Round 145 : loss => 0.9575798499554092,  accuracy: 0.6620606060606061, gradient_norm : 0.30981223629756716
[2025-09-17 16:29:21,227][flp2p.graph_runner][INFO] - Test, Round 145 : loss => 4.109242817618116,  accuracy: 0.3147810218978102
[2025-09-17 16:29:24,446][flp2p.graph_runner][INFO] - Train, Round 146 : loss => 1.0564103161330864,  accuracy: 0.6199999999999999, gradient_norm : 0.30993896154246314
[2025-09-17 16:29:25,247][flp2p.graph_runner][INFO] - Test, Round 146 : loss => 4.132889319080001,  accuracy: 0.3053297199638663
[2025-09-17 16:29:28,402][flp2p.graph_runner][INFO] - Train, Round 147 : loss => 1.0086028989276477,  accuracy: 0.6428484848484849, gradient_norm : 0.31732701554949494
[2025-09-17 16:29:29,221][flp2p.graph_runner][INFO] - Test, Round 147 : loss => 4.084959634036234,  accuracy: 0.3013698630136986
[2025-09-17 16:29:32,351][flp2p.graph_runner][INFO] - Train, Round 148 : loss => 1.2386861169292331,  accuracy: 0.5538787878787879, gradient_norm : 0.28642704112003553
[2025-09-17 16:29:33,171][flp2p.graph_runner][INFO] - Test, Round 148 : loss => 3.948948858207061,  accuracy: 0.2961608775137112
[2025-09-17 16:29:36,391][flp2p.graph_runner][INFO] - Train, Round 149 : loss => 0.8413643376764853,  accuracy: 0.7034545454545454, gradient_norm : 0.31966384202618997
[2025-09-17 16:29:37,211][flp2p.graph_runner][INFO] - Test, Round 149 : loss => 4.245718998413016,  accuracy: 0.29927007299270075
[2025-09-17 16:29:40,378][flp2p.graph_runner][INFO] - Train, Round 150 : loss => 0.9632823708552885,  accuracy: 0.6571515151515153, gradient_norm : 0.31978012552507534
[2025-09-17 16:29:41,201][flp2p.graph_runner][INFO] - Test, Round 150 : loss => 4.102177427561311,  accuracy: 0.3075523202911738
[2025-09-17 16:29:44,412][flp2p.graph_runner][INFO] - Train, Round 151 : loss => 1.013137458843079,  accuracy: 0.6353939393939394, gradient_norm : 0.3227221643594755
[2025-09-17 16:29:45,235][flp2p.graph_runner][INFO] - Test, Round 151 : loss => 4.088109723791696,  accuracy: 0.29981884057971014
[2025-09-17 16:29:48,382][flp2p.graph_runner][INFO] - Train, Round 152 : loss => 0.9534169032261602,  accuracy: 0.6625454545454545, gradient_norm : 0.30411537983684506
[2025-09-17 16:29:49,205][flp2p.graph_runner][INFO] - Test, Round 152 : loss => 4.3300566654838795,  accuracy: 0.2929936305732484
[2025-09-17 16:29:52,142][flp2p.graph_runner][INFO] - Train, Round 153 : loss => 1.0178084886525993,  accuracy: 0.6344666666666666, gradient_norm : 0.3086861179436547
[2025-09-17 16:29:52,889][flp2p.graph_runner][INFO] - Test, Round 153 : loss => 4.2761762813925746,  accuracy: 0.317
[2025-09-17 16:29:56,078][flp2p.graph_runner][INFO] - Train, Round 154 : loss => 0.9189253474904063,  accuracy: 0.6690909090909091, gradient_norm : 0.32930634405632647
[2025-09-17 16:29:56,926][flp2p.graph_runner][INFO] - Test, Round 154 : loss => 4.065741787800321,  accuracy: 0.3235563703024748
[2025-09-17 16:29:59,744][flp2p.graph_runner][INFO] - Train, Round 155 : loss => 0.7411245818008563,  accuracy: 0.7359333333333333, gradient_norm : 0.3380354417380572
[2025-09-17 16:30:00,484][flp2p.graph_runner][INFO] - Test, Round 155 : loss => 4.557988567395253,  accuracy: 0.3123123123123123
[2025-09-17 16:30:03,758][flp2p.graph_runner][INFO] - Train, Round 156 : loss => 0.7068590744465149,  accuracy: 0.7520606060606062, gradient_norm : 0.3473841496154772
[2025-09-17 16:30:04,580][flp2p.graph_runner][INFO] - Test, Round 156 : loss => 4.48602599595608,  accuracy: 0.31996353691886964
[2025-09-17 16:30:07,745][flp2p.graph_runner][INFO] - Train, Round 157 : loss => 0.9665924117363115,  accuracy: 0.6554545454545454, gradient_norm : 0.3318249899625505
[2025-09-17 16:30:08,588][flp2p.graph_runner][INFO] - Test, Round 157 : loss => 4.054193829465236,  accuracy: 0.3212669683257919
[2025-09-17 16:30:11,817][flp2p.graph_runner][INFO] - Train, Round 158 : loss => 0.8966667238581163,  accuracy: 0.6847878787878788, gradient_norm : 0.3258295002414489
[2025-09-17 16:30:12,641][flp2p.graph_runner][INFO] - Test, Round 158 : loss => 4.353826211981048,  accuracy: 0.3081130355515041
[2025-09-17 16:30:15,813][flp2p.graph_runner][INFO] - Train, Round 159 : loss => 0.8552437859095169,  accuracy: 0.6966666666666667, gradient_norm : 0.30374668625951406
[2025-09-17 16:30:16,620][flp2p.graph_runner][INFO] - Test, Round 159 : loss => 4.316577818028737,  accuracy: 0.32065217391304346
[2025-09-17 16:30:19,809][flp2p.graph_runner][INFO] - Train, Round 160 : loss => 0.630748908967193,  accuracy: 0.779939393939394, gradient_norm : 0.33099393899388985
[2025-09-17 16:30:20,634][flp2p.graph_runner][INFO] - Test, Round 160 : loss => 4.392279420463433,  accuracy: 0.329981718464351
[2025-09-17 16:30:23,523][flp2p.graph_runner][INFO] - Train, Round 161 : loss => 0.6958118316740183,  accuracy: 0.7564, gradient_norm : 0.3448799270225679
[2025-09-17 16:30:24,256][flp2p.graph_runner][INFO] - Test, Round 161 : loss => 4.409744401812673,  accuracy: 0.3370110330992979
[2025-09-17 16:30:27,127][flp2p.graph_runner][INFO] - Train, Round 162 : loss => 0.8543473621619417,  accuracy: 0.7004666666666665, gradient_norm : 0.32717435680694334
[2025-09-17 16:30:27,885][flp2p.graph_runner][INFO] - Test, Round 162 : loss => 4.499951376218501,  accuracy: 0.32103688933200397
[2025-09-17 16:30:31,084][flp2p.graph_runner][INFO] - Train, Round 163 : loss => 0.7016712124234827,  accuracy: 0.7486666666666666, gradient_norm : 0.3095644853791406
[2025-09-17 16:30:31,892][flp2p.graph_runner][INFO] - Test, Round 163 : loss => 4.527014986652395,  accuracy: 0.31521739130434784
[2025-09-17 16:30:35,105][flp2p.graph_runner][INFO] - Train, Round 164 : loss => 0.7421593570389617,  accuracy: 0.7403636363636363, gradient_norm : 0.3265886317212729
[2025-09-17 16:30:35,922][flp2p.graph_runner][INFO] - Test, Round 164 : loss => 4.500217709523934,  accuracy: 0.29535095715587967
[2025-09-17 16:30:39,084][flp2p.graph_runner][INFO] - Train, Round 165 : loss => 1.0371323507098704,  accuracy: 0.6324242424242424, gradient_norm : 0.3203370432001047
[2025-09-17 16:30:39,895][flp2p.graph_runner][INFO] - Test, Round 165 : loss => 4.321832361716842,  accuracy: 0.2907930720145852
[2025-09-17 16:30:43,116][flp2p.graph_runner][INFO] - Train, Round 166 : loss => 0.5345345048664251,  accuracy: 0.8153333333333332, gradient_norm : 0.3182223418956918
[2025-09-17 16:30:43,939][flp2p.graph_runner][INFO] - Test, Round 166 : loss => 4.6751793042207375,  accuracy: 0.33516483516483514
[2025-09-17 16:30:47,158][flp2p.graph_runner][INFO] - Train, Round 167 : loss => 0.7310202437770523,  accuracy: 0.7472121212121211, gradient_norm : 0.3042416255675606
[2025-09-17 16:30:47,997][flp2p.graph_runner][INFO] - Test, Round 167 : loss => 4.628139461034831,  accuracy: 0.3190127970749543
[2025-09-17 16:30:51,129][flp2p.graph_runner][INFO] - Train, Round 168 : loss => 0.9045949162607346,  accuracy: 0.6806666666666666, gradient_norm : 0.33230009351626827
[2025-09-17 16:30:51,973][flp2p.graph_runner][INFO] - Test, Round 168 : loss => 4.2644605984561865,  accuracy: 0.31785063752276865
[2025-09-17 16:30:55,163][flp2p.graph_runner][INFO] - Train, Round 169 : loss => 1.0024201296327275,  accuracy: 0.6458181818181818, gradient_norm : 0.3290270844651799
[2025-09-17 16:30:55,994][flp2p.graph_runner][INFO] - Test, Round 169 : loss => 4.179669791810653,  accuracy: 0.3167420814479638
[2025-09-17 16:30:59,183][flp2p.graph_runner][INFO] - Train, Round 170 : loss => 0.7863549720336778,  accuracy: 0.724121212121212, gradient_norm : 0.3098122997798594
[2025-09-17 16:31:00,007][flp2p.graph_runner][INFO] - Test, Round 170 : loss => 4.549626715970735,  accuracy: 0.333029197080292
[2025-09-17 16:31:03,175][flp2p.graph_runner][INFO] - Train, Round 171 : loss => 0.8018798317482448,  accuracy: 0.721939393939394, gradient_norm : 0.2766551284312355
[2025-09-17 16:31:04,010][flp2p.graph_runner][INFO] - Test, Round 171 : loss => 4.793611174989983,  accuracy: 0.3184713375796178
[2025-09-17 16:31:07,154][flp2p.graph_runner][INFO] - Train, Round 172 : loss => 0.854816821510116,  accuracy: 0.6987878787878787, gradient_norm : 0.3440183422706203
[2025-09-17 16:31:07,983][flp2p.graph_runner][INFO] - Test, Round 172 : loss => 4.301440121916716,  accuracy: 0.31814038286235186
[2025-09-17 16:31:11,225][flp2p.graph_runner][INFO] - Train, Round 173 : loss => 0.8557170968650262,  accuracy: 0.7056969696969696, gradient_norm : 0.33237771441945374
[2025-09-17 16:31:12,033][flp2p.graph_runner][INFO] - Test, Round 173 : loss => 4.399412976201259,  accuracy: 0.31473010064043916
[2025-09-17 16:31:15,251][flp2p.graph_runner][INFO] - Train, Round 174 : loss => 0.7449714323731079,  accuracy: 0.742909090909091, gradient_norm : 0.3499511391603949
[2025-09-17 16:31:16,090][flp2p.graph_runner][INFO] - Test, Round 174 : loss => 4.45069079213624,  accuracy: 0.3257506824385805
[2025-09-17 16:31:19,300][flp2p.graph_runner][INFO] - Train, Round 175 : loss => 0.821736632131626,  accuracy: 0.7126666666666667, gradient_norm : 0.3349834232981808
[2025-09-17 16:31:20,120][flp2p.graph_runner][INFO] - Test, Round 175 : loss => 4.421746953397337,  accuracy: 0.32572992700729925
[2025-09-17 16:31:23,302][flp2p.graph_runner][INFO] - Train, Round 176 : loss => 0.9425392060771685,  accuracy: 0.6691515151515152, gradient_norm : 0.33603217039499933
[2025-09-17 16:31:24,114][flp2p.graph_runner][INFO] - Test, Round 176 : loss => 4.245821125914145,  accuracy: 0.33485401459854014
[2025-09-17 16:31:27,341][flp2p.graph_runner][INFO] - Train, Round 177 : loss => 0.5931295283539219,  accuracy: 0.7994545454545454, gradient_norm : 0.3264842604808255
[2025-09-17 16:31:28,169][flp2p.graph_runner][INFO] - Test, Round 177 : loss => 4.911117279910606,  accuracy: 0.30684931506849317
[2025-09-17 16:31:31,357][flp2p.graph_runner][INFO] - Train, Round 178 : loss => 0.6627768824777323,  accuracy: 0.7736969696969698, gradient_norm : 0.32887636723479385
[2025-09-17 16:31:32,220][flp2p.graph_runner][INFO] - Test, Round 178 : loss => 4.616882756352425,  accuracy: 0.34545454545454546
[2025-09-17 16:31:35,426][flp2p.graph_runner][INFO] - Train, Round 179 : loss => 0.8925281814148768,  accuracy: 0.6826060606060607, gradient_norm : 0.3443634447862538
[2025-09-17 16:31:36,245][flp2p.graph_runner][INFO] - Test, Round 179 : loss => 4.287015282403339,  accuracy: 0.34545454545454546
[2025-09-17 16:31:39,388][flp2p.graph_runner][INFO] - Train, Round 180 : loss => 0.795013541107375,  accuracy: 0.7194545454545456, gradient_norm : 0.33594652821172105
[2025-09-17 16:31:40,218][flp2p.graph_runner][INFO] - Test, Round 180 : loss => 4.431693247071019,  accuracy: 0.32429990966576333
[2025-09-17 16:31:43,457][flp2p.graph_runner][INFO] - Train, Round 181 : loss => 0.6325223764717699,  accuracy: 0.7764242424242426, gradient_norm : 0.36124469599896125
[2025-09-17 16:31:44,275][flp2p.graph_runner][INFO] - Test, Round 181 : loss => 4.5914683591550745,  accuracy: 0.3333333333333333
[2025-09-17 16:31:47,495][flp2p.graph_runner][INFO] - Train, Round 182 : loss => 0.8269869726863445,  accuracy: 0.7058181818181818, gradient_norm : 0.2778697387704371
[2025-09-17 16:31:48,318][flp2p.graph_runner][INFO] - Test, Round 182 : loss => 4.539507690334148,  accuracy: 0.32456935630099726
[2025-09-17 16:31:51,513][flp2p.graph_runner][INFO] - Train, Round 183 : loss => 0.7682921326084149,  accuracy: 0.7264242424242424, gradient_norm : 0.3272060503811793
[2025-09-17 16:31:52,319][flp2p.graph_runner][INFO] - Test, Round 183 : loss => 4.591791972531501,  accuracy: 0.3060853769300636
[2025-09-17 16:31:55,495][flp2p.graph_runner][INFO] - Train, Round 184 : loss => 0.7178451885865315,  accuracy: 0.7484848484848484, gradient_norm : 0.37007480476475896
[2025-09-17 16:31:56,309][flp2p.graph_runner][INFO] - Test, Round 184 : loss => 4.651372685132782,  accuracy: 0.31876138433515483
[2025-09-17 16:31:59,480][flp2p.graph_runner][INFO] - Train, Round 185 : loss => 0.7051264630137156,  accuracy: 0.7543030303030304, gradient_norm : 0.3357152523479774
[2025-09-17 16:32:00,317][flp2p.graph_runner][INFO] - Test, Round 185 : loss => 4.614253636796906,  accuracy: 0.30987202925045704
[2025-09-17 16:32:03,529][flp2p.graph_runner][INFO] - Train, Round 186 : loss => 0.7632607561993154,  accuracy: 0.7338181818181817, gradient_norm : 0.3214786350765573
[2025-09-17 16:32:04,348][flp2p.graph_runner][INFO] - Test, Round 186 : loss => 4.636285834709077,  accuracy: 0.33089579524680074
[2025-09-17 16:32:07,586][flp2p.graph_runner][INFO] - Train, Round 187 : loss => 0.7332785124670574,  accuracy: 0.7386060606060606, gradient_norm : 0.26984119521321026
[2025-09-17 16:32:08,405][flp2p.graph_runner][INFO] - Test, Round 187 : loss => 4.716181643259925,  accuracy: 0.3227561196736174
[2025-09-17 16:32:11,275][flp2p.graph_runner][INFO] - Train, Round 188 : loss => 0.8836912439020239,  accuracy: 0.6854666666666667, gradient_norm : 0.31523990780331423
[2025-09-17 16:32:12,045][flp2p.graph_runner][INFO] - Test, Round 188 : loss => 4.652535653493514,  accuracy: 0.3290258449304175
[2025-09-17 16:32:15,220][flp2p.graph_runner][INFO] - Train, Round 189 : loss => 0.41533373353781555,  accuracy: 0.8626666666666667, gradient_norm : 0.3190767987437149
[2025-09-17 16:32:16,022][flp2p.graph_runner][INFO] - Test, Round 189 : loss => 5.085459981832827,  accuracy: 0.31473010064043916
[2025-09-17 16:32:19,216][flp2p.graph_runner][INFO] - Train, Round 190 : loss => 0.6611850560320062,  accuracy: 0.7707272727272727, gradient_norm : 0.3508444446953247
[2025-09-17 16:32:20,045][flp2p.graph_runner][INFO] - Test, Round 190 : loss => 4.685581207112833,  accuracy: 0.33
[2025-09-17 16:32:23,286][flp2p.graph_runner][INFO] - Train, Round 191 : loss => 0.8116178648961068,  accuracy: 0.7095757575757575, gradient_norm : 0.33538644445376076
[2025-09-17 16:32:24,106][flp2p.graph_runner][INFO] - Test, Round 191 : loss => 4.519647213654085,  accuracy: 0.31727272727272726
[2025-09-17 16:32:27,288][flp2p.graph_runner][INFO] - Train, Round 192 : loss => 0.4841707985666172,  accuracy: 0.8363636363636363, gradient_norm : 0.32585068892204866
[2025-09-17 16:32:28,128][flp2p.graph_runner][INFO] - Test, Round 192 : loss => 4.987851605998917,  accuracy: 0.32302092811646954
[2025-09-17 16:32:31,025][flp2p.graph_runner][INFO] - Train, Round 193 : loss => 0.5516288376761205,  accuracy: 0.8087333333333333, gradient_norm : 0.29774213826765195
[2025-09-17 16:32:31,766][flp2p.graph_runner][INFO] - Test, Round 193 : loss => 5.192272820834029,  accuracy: 0.3240279162512463
[2025-09-17 16:32:34,912][flp2p.graph_runner][INFO] - Train, Round 194 : loss => 0.42059181086562025,  accuracy: 0.8629696969696969, gradient_norm : 0.2869387984572059
[2025-09-17 16:32:35,720][flp2p.graph_runner][INFO] - Test, Round 194 : loss => 5.229029216722811,  accuracy: 0.31689497716894977
[2025-09-17 16:32:38,978][flp2p.graph_runner][INFO] - Train, Round 195 : loss => 0.46861571392687584,  accuracy: 0.8437575757575756, gradient_norm : 0.33241456407079706
[2025-09-17 16:32:39,812][flp2p.graph_runner][INFO] - Test, Round 195 : loss => 5.141930474948101,  accuracy: 0.3217866909753874
[2025-09-17 16:32:42,985][flp2p.graph_runner][INFO] - Train, Round 196 : loss => 0.5298482921629171,  accuracy: 0.8176969696969698, gradient_norm : 0.2923962356983673
[2025-09-17 16:32:43,814][flp2p.graph_runner][INFO] - Test, Round 196 : loss => 5.113258108483156,  accuracy: 0.314596554850408
[2025-09-17 16:32:47,018][flp2p.graph_runner][INFO] - Train, Round 197 : loss => 0.5776603797665985,  accuracy: 0.8030909090909091, gradient_norm : 0.3462045152199884
[2025-09-17 16:32:47,848][flp2p.graph_runner][INFO] - Test, Round 197 : loss => 4.784267785131767,  accuracy: 0.34188817598533455
[2025-09-17 16:32:51,045][flp2p.graph_runner][INFO] - Train, Round 198 : loss => 0.43083604531646513,  accuracy: 0.8560606060606062, gradient_norm : 0.32585972544514724
[2025-09-17 16:32:51,875][flp2p.graph_runner][INFO] - Test, Round 198 : loss => 5.234927398447329,  accuracy: 0.3118766999093382
[2025-09-17 16:32:55,057][flp2p.graph_runner][INFO] - Train, Round 199 : loss => 0.31611430680397296,  accuracy: 0.8958181818181816, gradient_norm : 0.2984886024262988
[2025-09-17 16:32:55,890][flp2p.graph_runner][INFO] - Test, Round 199 : loss => 5.367008716519413,  accuracy: 0.33697632058287796
[2025-09-17 16:32:58,847][flp2p.graph_runner][INFO] - Train, Round 200 : loss => 0.3977676165881273,  accuracy: 0.8704666666666666, gradient_norm : 0.34873830158055863
[2025-09-17 16:32:59,581][flp2p.graph_runner][INFO] - Test, Round 200 : loss => 5.388322791260086,  accuracy: 0.3286573146292585
[2025-09-17 16:33:02,796][flp2p.graph_runner][INFO] - Train, Round 201 : loss => 0.33177131695409073,  accuracy: 0.8937575757575759, gradient_norm : 0.32169840692914536
[2025-09-17 16:33:03,620][flp2p.graph_runner][INFO] - Test, Round 201 : loss => 5.50747530340604,  accuracy: 0.291324200913242
[2025-09-17 16:33:06,547][flp2p.graph_runner][INFO] - Train, Round 202 : loss => 0.6657838574191715,  accuracy: 0.7649999999999999, gradient_norm : 0.31456050345704456
[2025-09-17 16:33:07,294][flp2p.graph_runner][INFO] - Test, Round 202 : loss => 5.0131733756322605,  accuracy: 0.3166833166833167
[2025-09-17 16:33:10,392][flp2p.graph_runner][INFO] - Train, Round 203 : loss => 0.4963434636464554,  accuracy: 0.8273939393939392, gradient_norm : 0.30216441104096187
[2025-09-17 16:33:11,219][flp2p.graph_runner][INFO] - Test, Round 203 : loss => 5.262641955007739,  accuracy: 0.3202911737943585
[2025-09-17 16:33:14,422][flp2p.graph_runner][INFO] - Train, Round 204 : loss => 0.4525151734361026,  accuracy: 0.8447272727272729, gradient_norm : 0.3162523964862555
[2025-09-17 16:33:15,258][flp2p.graph_runner][INFO] - Test, Round 204 : loss => 5.1202948823178716,  accuracy: 0.3405994550408719
[2025-09-17 16:33:18,491][flp2p.graph_runner][INFO] - Train, Round 205 : loss => 0.4354459479756398,  accuracy: 0.8535757575757577, gradient_norm : 0.33642631319540073
[2025-09-17 16:33:19,331][flp2p.graph_runner][INFO] - Test, Round 205 : loss => 5.09600473244985,  accuracy: 0.3278538812785388
[2025-09-17 16:33:22,531][flp2p.graph_runner][INFO] - Train, Round 206 : loss => 0.6114034501370663,  accuracy: 0.7924848484848486, gradient_norm : 0.3690550885500921
[2025-09-17 16:33:23,359][flp2p.graph_runner][INFO] - Test, Round 206 : loss => 4.804438526137945,  accuracy: 0.3348498635122839
[2025-09-17 16:33:26,562][flp2p.graph_runner][INFO] - Train, Round 207 : loss => 0.32735204532243783,  accuracy: 0.8951515151515153, gradient_norm : 0.2932496222915537
[2025-09-17 16:33:27,403][flp2p.graph_runner][INFO] - Test, Round 207 : loss => 5.4418814097132,  accuracy: 0.3282097649186257
[2025-09-17 16:33:30,524][flp2p.graph_runner][INFO] - Train, Round 208 : loss => 0.4454336671557258,  accuracy: 0.8496363636363635, gradient_norm : 0.26403821771791897
[2025-09-17 16:33:31,331][flp2p.graph_runner][INFO] - Test, Round 208 : loss => 5.341944737093789,  accuracy: 0.33001808318264014
[2025-09-17 16:33:34,522][flp2p.graph_runner][INFO] - Train, Round 209 : loss => 0.5299760705481387,  accuracy: 0.8196969696969698, gradient_norm : 0.3749355452674514
[2025-09-17 16:33:35,351][flp2p.graph_runner][INFO] - Test, Round 209 : loss => 4.938045138701635,  accuracy: 0.3312274368231047
[2025-09-17 16:33:38,590][flp2p.graph_runner][INFO] - Train, Round 210 : loss => 0.3576678991348465,  accuracy: 0.8793939393939394, gradient_norm : 0.3381305020940822
[2025-09-17 16:33:39,397][flp2p.graph_runner][INFO] - Test, Round 210 : loss => 5.459231813929792,  accuracy: 0.3381950774840474
[2025-09-17 16:33:42,634][flp2p.graph_runner][INFO] - Train, Round 211 : loss => 0.6227431879779068,  accuracy: 0.7871515151515153, gradient_norm : 0.33542995795204583
[2025-09-17 16:33:43,450][flp2p.graph_runner][INFO] - Test, Round 211 : loss => 5.077764223878009,  accuracy: 0.31938125568698816
[2025-09-17 16:33:46,680][flp2p.graph_runner][INFO] - Train, Round 212 : loss => 0.2540707273077728,  accuracy: 0.9223030303030302, gradient_norm : 0.30944471776877
[2025-09-17 16:33:47,495][flp2p.graph_runner][INFO] - Test, Round 212 : loss => 6.020026065958317,  accuracy: 0.30300272975432213
[2025-09-17 16:33:50,643][flp2p.graph_runner][INFO] - Train, Round 213 : loss => 0.43123921328012305,  accuracy: 0.8578181818181818, gradient_norm : 0.3175595758257554
[2025-09-17 16:33:51,465][flp2p.graph_runner][INFO] - Test, Round 213 : loss => 5.49060027776303,  accuracy: 0.3151183970856102
[2025-09-17 16:33:54,647][flp2p.graph_runner][INFO] - Train, Round 214 : loss => 0.4020166712227027,  accuracy: 0.868060606060606, gradient_norm : 0.2778340989852122
[2025-09-17 16:33:55,449][flp2p.graph_runner][INFO] - Test, Round 214 : loss => 5.623849003080215,  accuracy: 0.31616982836495033
[2025-09-17 16:33:58,623][flp2p.graph_runner][INFO] - Train, Round 215 : loss => 0.2718337970491913,  accuracy: 0.9183636363636364, gradient_norm : 0.25174529521003
[2025-09-17 16:33:59,443][flp2p.graph_runner][INFO] - Test, Round 215 : loss => 5.774150650031344,  accuracy: 0.3242506811989101
[2025-09-17 16:34:02,644][flp2p.graph_runner][INFO] - Train, Round 216 : loss => 0.38297175718748533,  accuracy: 0.8695151515151516, gradient_norm : 0.24361235872158404
[2025-09-17 16:34:03,454][flp2p.graph_runner][INFO] - Test, Round 216 : loss => 5.50503673332789,  accuracy: 0.3430127041742287
[2025-09-17 16:34:06,614][flp2p.graph_runner][INFO] - Train, Round 217 : loss => 0.2837308555802159,  accuracy: 0.9123030303030302, gradient_norm : 0.2810419380943913
[2025-09-17 16:34:07,440][flp2p.graph_runner][INFO] - Test, Round 217 : loss => 5.583405766904408,  accuracy: 0.33728350045578853
[2025-09-17 16:34:10,530][flp2p.graph_runner][INFO] - Train, Round 218 : loss => 0.29353064691488756,  accuracy: 0.9075757575757575, gradient_norm : 0.29320826488137985
[2025-09-17 16:34:11,362][flp2p.graph_runner][INFO] - Test, Round 218 : loss => 5.845086855524567,  accuracy: 0.3151680290644868
[2025-09-17 16:34:14,613][flp2p.graph_runner][INFO] - Train, Round 219 : loss => 0.3622852972206736,  accuracy: 0.8804242424242426, gradient_norm : 0.28611889757434833
[2025-09-17 16:34:15,446][flp2p.graph_runner][INFO] - Test, Round 219 : loss => 5.435731446482444,  accuracy: 0.3360655737704918
[2025-09-17 16:34:18,359][flp2p.graph_runner][INFO] - Train, Round 220 : loss => 0.3553504929221428,  accuracy: 0.8818, gradient_norm : 0.2928921538849481
[2025-09-17 16:34:19,089][flp2p.graph_runner][INFO] - Test, Round 220 : loss => 5.700736182916212,  accuracy: 0.3336653386454183
[2025-09-17 16:34:22,254][flp2p.graph_runner][INFO] - Train, Round 221 : loss => 0.322430289304965,  accuracy: 0.893939393939394, gradient_norm : 0.2735766417171162
[2025-09-17 16:34:23,066][flp2p.graph_runner][INFO] - Test, Round 221 : loss => 5.991573650369626,  accuracy: 0.3194192377495463
[2025-09-17 16:34:26,247][flp2p.graph_runner][INFO] - Train, Round 222 : loss => 0.280057517026881,  accuracy: 0.9137575757575758, gradient_norm : 0.2680445670511527
[2025-09-17 16:34:27,057][flp2p.graph_runner][INFO] - Test, Round 222 : loss => 5.727791482030856,  accuracy: 0.35095715587967186
[2025-09-17 16:34:30,183][flp2p.graph_runner][INFO] - Train, Round 223 : loss => 0.3137453338876819,  accuracy: 0.8987272727272727, gradient_norm : 0.28958969567892345
[2025-09-17 16:34:30,988][flp2p.graph_runner][INFO] - Test, Round 223 : loss => 5.664541946112239,  accuracy: 0.3312043795620438
[2025-09-17 16:34:34,219][flp2p.graph_runner][INFO] - Train, Round 224 : loss => 0.2305738100760629,  accuracy: 0.9249696969696969, gradient_norm : 0.2128887558499539
[2025-09-17 16:34:35,019][flp2p.graph_runner][INFO] - Test, Round 224 : loss => 5.753566630508589,  accuracy: 0.3532608695652174
[2025-09-17 16:34:38,253][flp2p.graph_runner][INFO] - Train, Round 225 : loss => 0.26505527643115384,  accuracy: 0.9123636363636364, gradient_norm : 0.21660780866245363
[2025-09-17 16:34:39,058][flp2p.graph_runner][INFO] - Test, Round 225 : loss => 5.938912510168109,  accuracy: 0.33969118982742963
[2025-09-17 16:34:42,307][flp2p.graph_runner][INFO] - Train, Round 226 : loss => 0.16795846460399327,  accuracy: 0.9507878787878787, gradient_norm : 0.21742980997904854
[2025-09-17 16:34:43,130][flp2p.graph_runner][INFO] - Test, Round 226 : loss => 5.957227955189619,  accuracy: 0.33636363636363636
[2025-09-17 16:34:46,246][flp2p.graph_runner][INFO] - Train, Round 227 : loss => 0.4412070597239893,  accuracy: 0.8495151515151514, gradient_norm : 0.2878941778126461
[2025-09-17 16:34:47,043][flp2p.graph_runner][INFO] - Test, Round 227 : loss => 5.485058255534354,  accuracy: 0.3342440801457195
[2025-09-17 16:34:50,182][flp2p.graph_runner][INFO] - Train, Round 228 : loss => 0.44683364362807954,  accuracy: 0.8512121212121211, gradient_norm : 0.3210258200120409
[2025-09-17 16:34:51,018][flp2p.graph_runner][INFO] - Test, Round 228 : loss => 5.315922446220926,  accuracy: 0.3198562443845463
[2025-09-17 16:34:54,186][flp2p.graph_runner][INFO] - Train, Round 229 : loss => 0.32128800916242056,  accuracy: 0.8969696969696969, gradient_norm : 0.2625135578281217
[2025-09-17 16:34:54,989][flp2p.graph_runner][INFO] - Test, Round 229 : loss => 5.6351367517149065,  accuracy: 0.32511415525114157
[2025-09-17 16:34:58,242][flp2p.graph_runner][INFO] - Train, Round 230 : loss => 0.48159054064528084,  accuracy: 0.8342424242424243, gradient_norm : 0.27324868351991893
[2025-09-17 16:34:59,048][flp2p.graph_runner][INFO] - Test, Round 230 : loss => 5.565603008612098,  accuracy: 0.3366606170598911
[2025-09-17 16:35:02,301][flp2p.graph_runner][INFO] - Train, Round 231 : loss => 0.40143787248280727,  accuracy: 0.8676969696969696, gradient_norm : 0.31600642091356407
[2025-09-17 16:35:03,126][flp2p.graph_runner][INFO] - Test, Round 231 : loss => 5.213753545586067,  accuracy: 0.3318264014466546
[2025-09-17 16:35:06,241][flp2p.graph_runner][INFO] - Train, Round 232 : loss => 0.22121888283877214,  accuracy: 0.9318787878787879, gradient_norm : 0.2718991301958002
[2025-09-17 16:35:07,067][flp2p.graph_runner][INFO] - Test, Round 232 : loss => 6.060986847825389,  accuracy: 0.31420765027322406
[2025-09-17 16:35:10,282][flp2p.graph_runner][INFO] - Train, Round 233 : loss => 0.35309271331816244,  accuracy: 0.8806666666666666, gradient_norm : 0.22249489946785628
[2025-09-17 16:35:11,106][flp2p.graph_runner][INFO] - Test, Round 233 : loss => 5.8741417812169905,  accuracy: 0.33394160583941607
[2025-09-17 16:35:14,312][flp2p.graph_runner][INFO] - Train, Round 234 : loss => 0.2019155419674183,  accuracy: 0.9383636363636364, gradient_norm : 0.2835046737107621
[2025-09-17 16:35:15,133][flp2p.graph_runner][INFO] - Test, Round 234 : loss => 5.76740495118896,  accuracy: 0.34814143245693563
[2025-09-17 16:35:18,328][flp2p.graph_runner][INFO] - Train, Round 235 : loss => 0.24268764265232876,  accuracy: 0.9209090909090909, gradient_norm : 0.23486917867727805
[2025-09-17 16:35:19,157][flp2p.graph_runner][INFO] - Test, Round 235 : loss => 5.904825708857497,  accuracy: 0.3471971066907776
[2025-09-17 16:35:22,307][flp2p.graph_runner][INFO] - Train, Round 236 : loss => 0.22084260240027878,  accuracy: 0.9283636363636365, gradient_norm : 0.2093680534941145
[2025-09-17 16:35:23,111][flp2p.graph_runner][INFO] - Test, Round 236 : loss => 6.089879811133442,  accuracy: 0.33815551537070526
[2025-09-17 16:35:26,278][flp2p.graph_runner][INFO] - Train, Round 237 : loss => 0.2432651501588149,  accuracy: 0.9250303030303031, gradient_norm : 0.24391543028948376
[2025-09-17 16:35:27,105][flp2p.graph_runner][INFO] - Test, Round 237 : loss => 6.014427757566901,  accuracy: 0.32848043676069155
[2025-09-17 16:35:30,257][flp2p.graph_runner][INFO] - Train, Round 238 : loss => 0.310236658671253,  accuracy: 0.8990303030303031, gradient_norm : 0.24946409159327698
[2025-09-17 16:35:31,075][flp2p.graph_runner][INFO] - Test, Round 238 : loss => 5.960836906374263,  accuracy: 0.32541133455210236
[2025-09-17 16:35:34,345][flp2p.graph_runner][INFO] - Train, Round 239 : loss => 0.27439930095615256,  accuracy: 0.9124848484848483, gradient_norm : 0.2351958435871163
[2025-09-17 16:35:35,182][flp2p.graph_runner][INFO] - Test, Round 239 : loss => 5.881864838902228,  accuracy: 0.32850678733031674
[2025-09-17 16:35:38,381][flp2p.graph_runner][INFO] - Train, Round 240 : loss => 0.30773613101021885,  accuracy: 0.8972727272727272, gradient_norm : 0.23251736542191395
[2025-09-17 16:35:39,190][flp2p.graph_runner][INFO] - Test, Round 240 : loss => 6.007386782673104,  accuracy: 0.33515482695810567
[2025-09-17 16:35:42,088][flp2p.graph_runner][INFO] - Train, Round 241 : loss => 0.2583160706273001,  accuracy: 0.9253333333333333, gradient_norm : 0.30403702646329506
[2025-09-17 16:35:42,833][flp2p.graph_runner][INFO] - Test, Round 241 : loss => 5.992087109907492,  accuracy: 0.33233233233233234
[2025-09-17 16:35:45,988][flp2p.graph_runner][INFO] - Train, Round 242 : loss => 0.19842921968577018,  accuracy: 0.9396363636363635, gradient_norm : 0.22495251356325408
[2025-09-17 16:35:46,824][flp2p.graph_runner][INFO] - Test, Round 242 : loss => 6.257559699656099,  accuracy: 0.30510018214936246
[2025-09-17 16:35:50,067][flp2p.graph_runner][INFO] - Train, Round 243 : loss => 0.39159940140098115,  accuracy: 0.8697575757575758, gradient_norm : 0.2578074964027675
[2025-09-17 16:35:50,894][flp2p.graph_runner][INFO] - Test, Round 243 : loss => 5.827599725915223,  accuracy: 0.3266239707227813
[2025-09-17 16:35:54,103][flp2p.graph_runner][INFO] - Train, Round 244 : loss => 0.11665357857364107,  accuracy: 0.966848484848485, gradient_norm : 0.15121583892809798
[2025-09-17 16:35:54,937][flp2p.graph_runner][INFO] - Test, Round 244 : loss => 6.377854412507708,  accuracy: 0.3617407071622847
[2025-09-17 16:35:58,186][flp2p.graph_runner][INFO] - Train, Round 245 : loss => 0.1456823943777346,  accuracy: 0.9610909090909091, gradient_norm : 0.23777181711464124
[2025-09-17 16:35:59,008][flp2p.graph_runner][INFO] - Test, Round 245 : loss => 6.230487450014187,  accuracy: 0.32459312839059673
[2025-09-17 16:36:02,153][flp2p.graph_runner][INFO] - Train, Round 246 : loss => 0.20519638340713334,  accuracy: 0.9356969696969698, gradient_norm : 0.2479291633215211
[2025-09-17 16:36:02,976][flp2p.graph_runner][INFO] - Test, Round 246 : loss => 6.154398250667087,  accuracy: 0.32570905763952424
[2025-09-17 16:36:06,153][flp2p.graph_runner][INFO] - Train, Round 247 : loss => 0.2779556557098404,  accuracy: 0.9096363636363635, gradient_norm : 0.2910265954005496
[2025-09-17 16:36:06,965][flp2p.graph_runner][INFO] - Test, Round 247 : loss => 6.107449365848869,  accuracy: 0.3330316742081448
[2025-09-17 16:36:10,208][flp2p.graph_runner][INFO] - Train, Round 248 : loss => 0.32249379281214857,  accuracy: 0.8923636363636364, gradient_norm : 0.21015573450672162
[2025-09-17 16:36:11,041][flp2p.graph_runner][INFO] - Test, Round 248 : loss => 5.908835146427155,  accuracy: 0.3372727272727273
[2025-09-17 16:36:14,236][flp2p.graph_runner][INFO] - Train, Round 249 : loss => 0.09419120671952896,  accuracy: 0.9767272727272728, gradient_norm : 0.17854434005983488
[2025-09-17 16:36:15,050][flp2p.graph_runner][INFO] - Test, Round 249 : loss => 6.435987703940448,  accuracy: 0.3167420814479638
[2025-09-17 16:36:18,275][flp2p.graph_runner][INFO] - Train, Round 250 : loss => 0.1858273799924298,  accuracy: 0.9452121212121213, gradient_norm : 0.23885540384708417
[2025-09-17 16:36:19,080][flp2p.graph_runner][INFO] - Test, Round 250 : loss => 6.101712912510569,  accuracy: 0.3469573115349682
[2025-09-17 16:36:22,242][flp2p.graph_runner][INFO] - Train, Round 251 : loss => 0.1371169518089053,  accuracy: 0.9596363636363636, gradient_norm : 0.20084146600237354
[2025-09-17 16:36:23,042][flp2p.graph_runner][INFO] - Test, Round 251 : loss => 6.346498451202455,  accuracy: 0.3531021897810219
[2025-09-17 16:36:26,228][flp2p.graph_runner][INFO] - Train, Round 252 : loss => 0.24458599683128246,  accuracy: 0.9212121212121213, gradient_norm : 0.24493533057942427
[2025-09-17 16:36:27,043][flp2p.graph_runner][INFO] - Test, Round 252 : loss => 6.2125996772044365,  accuracy: 0.3294010889292196
[2025-09-17 16:36:29,981][flp2p.graph_runner][INFO] - Train, Round 253 : loss => 0.1112141829875706,  accuracy: 0.9673333333333333, gradient_norm : 0.10707740801373022
[2025-09-17 16:36:30,734][flp2p.graph_runner][INFO] - Test, Round 253 : loss => 6.8947373516559605,  accuracy: 0.338
[2025-09-17 16:36:33,968][flp2p.graph_runner][INFO] - Train, Round 254 : loss => 0.10794933829684837,  accuracy: 0.9675757575757574, gradient_norm : 0.12726861324963032
[2025-09-17 16:36:34,790][flp2p.graph_runner][INFO] - Test, Round 254 : loss => 6.5374825415828015,  accuracy: 0.3509090909090909
[2025-09-17 16:36:38,009][flp2p.graph_runner][INFO] - Train, Round 255 : loss => 0.06408363720225907,  accuracy: 0.9844848484848484, gradient_norm : 0.14148276090276826
[2025-09-17 16:36:38,825][flp2p.graph_runner][INFO] - Test, Round 255 : loss => 6.600868192350227,  accuracy: 0.33363553943789664
[2025-09-17 16:36:41,934][flp2p.graph_runner][INFO] - Train, Round 256 : loss => 0.2079787407946613,  accuracy: 0.9341818181818182, gradient_norm : 0.19599084764040972
[2025-09-17 16:36:42,772][flp2p.graph_runner][INFO] - Test, Round 256 : loss => 6.2347404166797995,  accuracy: 0.34275296262534183
[2025-09-17 16:36:45,976][flp2p.graph_runner][INFO] - Train, Round 257 : loss => 0.21007691429118172,  accuracy: 0.9359393939393938, gradient_norm : 0.2390266877253538
[2025-09-17 16:36:46,785][flp2p.graph_runner][INFO] - Test, Round 257 : loss => 6.245385519735414,  accuracy: 0.3167420814479638
[2025-09-17 16:36:49,954][flp2p.graph_runner][INFO] - Train, Round 258 : loss => 0.1170905875818269,  accuracy: 0.9710909090909091, gradient_norm : 0.24052812082646408
[2025-09-17 16:36:50,765][flp2p.graph_runner][INFO] - Test, Round 258 : loss => 6.555511031042446,  accuracy: 0.32636363636363636
[2025-09-17 16:36:54,002][flp2p.graph_runner][INFO] - Train, Round 259 : loss => 0.25347343857328386,  accuracy: 0.920121212121212, gradient_norm : 0.20568393812451322
[2025-09-17 16:36:54,824][flp2p.graph_runner][INFO] - Test, Round 259 : loss => 6.1416080189486815,  accuracy: 0.3393829401088929
[2025-09-17 16:36:57,983][flp2p.graph_runner][INFO] - Train, Round 260 : loss => 0.12268193739620094,  accuracy: 0.9678181818181818, gradient_norm : 0.18326802642756324
[2025-09-17 16:36:58,816][flp2p.graph_runner][INFO] - Test, Round 260 : loss => 6.267139597008382,  accuracy: 0.35149863760217986
[2025-09-17 16:37:01,990][flp2p.graph_runner][INFO] - Train, Round 261 : loss => 0.11291980510202221,  accuracy: 0.9653333333333335, gradient_norm : 0.11607071539177105
[2025-09-17 16:37:02,807][flp2p.graph_runner][INFO] - Test, Round 261 : loss => 6.905359162104141,  accuracy: 0.3166515013648772
[2025-09-17 16:37:05,751][flp2p.graph_runner][INFO] - Train, Round 262 : loss => 0.11339021686687072,  accuracy: 0.9667333333333332, gradient_norm : 0.13569311159265202
[2025-09-17 16:37:06,510][flp2p.graph_runner][INFO] - Test, Round 262 : loss => 6.887488786705988,  accuracy: 0.31763527054108215
[2025-09-17 16:37:09,673][flp2p.graph_runner][INFO] - Train, Round 263 : loss => 0.09474234682844072,  accuracy: 0.9747878787878788, gradient_norm : 0.1429407574703805
[2025-09-17 16:37:10,491][flp2p.graph_runner][INFO] - Test, Round 263 : loss => 6.481163985979611,  accuracy: 0.3406392694063927
[2025-09-17 16:37:13,420][flp2p.graph_runner][INFO] - Train, Round 264 : loss => 0.18038243008356858,  accuracy: 0.9469333333333334, gradient_norm : 0.18008876431477924
[2025-09-17 16:37:14,188][flp2p.graph_runner][INFO] - Test, Round 264 : loss => 6.5802147924190475,  accuracy: 0.3115079365079365
[2025-09-17 16:37:17,350][flp2p.graph_runner][INFO] - Train, Round 265 : loss => 0.0854109143346453,  accuracy: 0.9803030303030303, gradient_norm : 0.16137338534334503
[2025-09-17 16:37:18,187][flp2p.graph_runner][INFO] - Test, Round 265 : loss => 6.487914242714211,  accuracy: 0.338768115942029
[2025-09-17 16:37:21,046][flp2p.graph_runner][INFO] - Train, Round 266 : loss => 0.09529282749340003,  accuracy: 0.9735999999999998, gradient_norm : 0.1322069093968948
[2025-09-17 16:37:21,791][flp2p.graph_runner][INFO] - Test, Round 266 : loss => 6.653449572430979,  accuracy: 0.33762376237623765
[2025-09-17 16:37:25,046][flp2p.graph_runner][INFO] - Train, Round 267 : loss => 0.17709684696734485,  accuracy: 0.9469090909090907, gradient_norm : 0.25782320111847545
[2025-09-17 16:37:25,872][flp2p.graph_runner][INFO] - Test, Round 267 : loss => 6.059591808793772,  accuracy: 0.334841628959276
[2025-09-17 16:37:29,087][flp2p.graph_runner][INFO] - Train, Round 268 : loss => 0.07763945448139833,  accuracy: 0.9811515151515152, gradient_norm : 0.14743442120640898
[2025-09-17 16:37:29,921][flp2p.graph_runner][INFO] - Test, Round 268 : loss => 6.733336257458166,  accuracy: 0.3460490463215259
[2025-09-17 16:37:33,108][flp2p.graph_runner][INFO] - Train, Round 269 : loss => 0.027390610858779408,  accuracy: 0.9963636363636363, gradient_norm : 0.09344180786297979
[2025-09-17 16:37:33,921][flp2p.graph_runner][INFO] - Test, Round 269 : loss => 7.11342806237985,  accuracy: 0.33454876937101186
[2025-09-17 16:37:37,102][flp2p.graph_runner][INFO] - Train, Round 270 : loss => 0.2894830610671978,  accuracy: 0.9043636363636365, gradient_norm : 0.17725443693980963
[2025-09-17 16:37:37,927][flp2p.graph_runner][INFO] - Test, Round 270 : loss => 6.387562858459612,  accuracy: 0.3415525114155251
[2025-09-17 16:37:41,100][flp2p.graph_runner][INFO] - Train, Round 271 : loss => 0.08212268408204243,  accuracy: 0.9754545454545455, gradient_norm : 0.09371459089270963
[2025-09-17 16:37:41,912][flp2p.graph_runner][INFO] - Test, Round 271 : loss => 6.882313621841762,  accuracy: 0.35171790235081374
[2025-09-17 16:37:45,056][flp2p.graph_runner][INFO] - Train, Round 272 : loss => 0.1107878634230124,  accuracy: 0.9670909090909089, gradient_norm : 0.17835071399090066
[2025-09-17 16:37:45,867][flp2p.graph_runner][INFO] - Test, Round 272 : loss => 6.676910713762451,  accuracy: 0.3221110100090992
[2025-09-17 16:37:48,768][flp2p.graph_runner][INFO] - Train, Round 273 : loss => 0.03992374781112555,  accuracy: 0.9904, gradient_norm : 0.08097309232875056
[2025-09-17 16:37:49,514][flp2p.graph_runner][INFO] - Test, Round 273 : loss => 7.401430866739771,  accuracy: 0.3303303303303303
[2025-09-17 16:37:52,731][flp2p.graph_runner][INFO] - Train, Round 274 : loss => 0.15349776731418208,  accuracy: 0.9534545454545453, gradient_norm : 0.1338865471750828
[2025-09-17 16:37:53,543][flp2p.graph_runner][INFO] - Test, Round 274 : loss => 6.5327801312099805,  accuracy: 0.33545454545454545
[2025-09-17 16:37:56,410][flp2p.graph_runner][INFO] - Train, Round 275 : loss => 0.0848667570325887,  accuracy: 0.9768000000000002, gradient_norm : 0.12005358976397881
[2025-09-17 16:37:57,153][flp2p.graph_runner][INFO] - Test, Round 275 : loss => 6.867670957055337,  accuracy: 0.337361530715005
[2025-09-17 16:38:00,338][flp2p.graph_runner][INFO] - Train, Round 276 : loss => 0.07297905844388947,  accuracy: 0.979939393939394, gradient_norm : 0.13384914883791496
[2025-09-17 16:38:01,148][flp2p.graph_runner][INFO] - Test, Round 276 : loss => 6.630307005965002,  accuracy: 0.35342465753424657
[2025-09-17 16:38:04,315][flp2p.graph_runner][INFO] - Train, Round 277 : loss => 0.2118650482186376,  accuracy: 0.9326666666666665, gradient_norm : 0.20963070684697663
[2025-09-17 16:38:05,154][flp2p.graph_runner][INFO] - Test, Round 277 : loss => 6.498764151237404,  accuracy: 0.31851179673321234
[2025-09-17 16:38:08,406][flp2p.graph_runner][INFO] - Train, Round 278 : loss => 0.026301698763942893,  accuracy: 0.9956969696969696, gradient_norm : 0.08038296837534903
[2025-09-17 16:38:09,216][flp2p.graph_runner][INFO] - Test, Round 278 : loss => 6.967112900784416,  accuracy: 0.333029197080292
[2025-09-17 16:38:12,408][flp2p.graph_runner][INFO] - Train, Round 279 : loss => 0.01396672106289481,  accuracy: 0.9995757575757575, gradient_norm : 0.0553441873968406
[2025-09-17 16:38:13,237][flp2p.graph_runner][INFO] - Test, Round 279 : loss => 7.066274112039577,  accuracy: 0.34972677595628415
[2025-09-17 16:38:16,446][flp2p.graph_runner][INFO] - Train, Round 280 : loss => 0.0938830503976991,  accuracy: 0.9782424242424244, gradient_norm : 0.16212747774175781
[2025-09-17 16:38:17,278][flp2p.graph_runner][INFO] - Test, Round 280 : loss => 6.59894746596163,  accuracy: 0.3509090909090909
[2025-09-17 16:38:20,484][flp2p.graph_runner][INFO] - Train, Round 281 : loss => 0.04978009918650542,  accuracy: 0.9901818181818182, gradient_norm : 0.11773462159869004
[2025-09-17 16:38:21,305][flp2p.graph_runner][INFO] - Test, Round 281 : loss => 6.849310735194983,  accuracy: 0.3381686310063463
[2025-09-17 16:38:24,486][flp2p.graph_runner][INFO] - Train, Round 282 : loss => 0.0820591241125681,  accuracy: 0.9770909090909091, gradient_norm : 0.09691006721257601
[2025-09-17 16:38:25,321][flp2p.graph_runner][INFO] - Test, Round 282 : loss => 6.765474529135717,  accuracy: 0.3360730593607306
[2025-09-17 16:38:28,544][flp2p.graph_runner][INFO] - Train, Round 283 : loss => 0.24252044546337045,  accuracy: 0.9195757575757576, gradient_norm : 0.17010322902051087
[2025-09-17 16:38:29,375][flp2p.graph_runner][INFO] - Test, Round 283 : loss => 6.28774517326061,  accuracy: 0.35086128739800543
[2025-09-17 16:38:32,553][flp2p.graph_runner][INFO] - Train, Round 284 : loss => 0.2016355707430932,  accuracy: 0.9369696969696971, gradient_norm : 0.20810324896945367
[2025-09-17 16:38:33,356][flp2p.graph_runner][INFO] - Test, Round 284 : loss => 6.424929329728605,  accuracy: 0.33091568449682685
[2025-09-17 16:38:36,472][flp2p.graph_runner][INFO] - Train, Round 285 : loss => 0.055041671526817965,  accuracy: 0.985939393939394, gradient_norm : 0.10260494723232418
[2025-09-17 16:38:37,312][flp2p.graph_runner][INFO] - Test, Round 285 : loss => 6.854066265665966,  accuracy: 0.35597826086956524
[2025-09-17 16:38:40,242][flp2p.graph_runner][INFO] - Train, Round 286 : loss => 0.04894052401504515,  accuracy: 0.9868666666666668, gradient_norm : 0.06581809804968398
[2025-09-17 16:38:40,994][flp2p.graph_runner][INFO] - Test, Round 286 : loss => 7.279375304658729,  accuracy: 0.3473895582329317
[2025-09-17 16:38:44,196][flp2p.graph_runner][INFO] - Train, Round 287 : loss => 0.03767302377840848,  accuracy: 0.9942424242424241, gradient_norm : 0.10763897525978562
[2025-09-17 16:38:45,021][flp2p.graph_runner][INFO] - Test, Round 287 : loss => 6.992605563405424,  accuracy: 0.335149863760218
[2025-09-17 16:38:48,262][flp2p.graph_runner][INFO] - Train, Round 288 : loss => 0.024266786454152256,  accuracy: 0.994, gradient_norm : 0.05355016101550816
[2025-09-17 16:38:49,092][flp2p.graph_runner][INFO] - Test, Round 288 : loss => 7.053973792577179,  accuracy: 0.3484573502722323
[2025-09-17 16:38:52,278][flp2p.graph_runner][INFO] - Train, Round 289 : loss => 0.006941911213668949,  accuracy: 1.0, gradient_norm : 0.030704472348208542
[2025-09-17 16:38:53,102][flp2p.graph_runner][INFO] - Test, Round 289 : loss => 7.2723428891154995,  accuracy: 0.33546034639927075
[2025-09-17 16:38:56,233][flp2p.graph_runner][INFO] - Train, Round 290 : loss => 0.11014507773181278,  accuracy: 0.9623030303030302, gradient_norm : 0.06275504925642962
[2025-09-17 16:38:57,037][flp2p.graph_runner][INFO] - Test, Round 290 : loss => 7.085010750492055,  accuracy: 0.35171790235081374
[2025-09-17 16:39:00,260][flp2p.graph_runner][INFO] - Train, Round 291 : loss => 0.03667481093751149,  accuracy: 0.9944242424242425, gradient_norm : 0.10750351914998829
[2025-09-17 16:39:01,082][flp2p.graph_runner][INFO] - Test, Round 291 : loss => 6.892553202231653,  accuracy: 0.3494085532302093
[2025-09-17 16:39:04,289][flp2p.graph_runner][INFO] - Train, Round 292 : loss => 0.0747997874540002,  accuracy: 0.9787878787878789, gradient_norm : 0.09122937422334815
[2025-09-17 16:39:05,122][flp2p.graph_runner][INFO] - Test, Round 292 : loss => 7.164782609939575,  accuracy: 0.3381818181818182
[2025-09-17 16:39:08,256][flp2p.graph_runner][INFO] - Train, Round 293 : loss => 0.022046900157910228,  accuracy: 0.9967878787878788, gradient_norm : 0.08063704737768944
[2025-09-17 16:39:09,101][flp2p.graph_runner][INFO] - Test, Round 293 : loss => 7.197250417130024,  accuracy: 0.34153005464480873
[2025-09-17 16:39:12,332][flp2p.graph_runner][INFO] - Train, Round 294 : loss => 0.1317982896730815,  accuracy: 0.9643030303030303, gradient_norm : 0.16936882784327267
[2025-09-17 16:39:13,150][flp2p.graph_runner][INFO] - Test, Round 294 : loss => 6.556985624336636,  accuracy: 0.33634719710669075
[2025-09-17 16:39:16,352][flp2p.graph_runner][INFO] - Train, Round 295 : loss => 0.06230736457576635,  accuracy: 0.9844848484848485, gradient_norm : 0.09507703526872863
[2025-09-17 16:39:17,186][flp2p.graph_runner][INFO] - Test, Round 295 : loss => 6.895692079606717,  accuracy: 0.354014598540146
[2025-09-17 16:39:20,391][flp2p.graph_runner][INFO] - Train, Round 296 : loss => 0.01283346004745247,  accuracy: 0.9993333333333333, gradient_norm : 0.04818350067972202
[2025-09-17 16:39:21,207][flp2p.graph_runner][INFO] - Test, Round 296 : loss => 7.354054705722489,  accuracy: 0.32572992700729925
[2025-09-17 16:39:24,391][flp2p.graph_runner][INFO] - Train, Round 297 : loss => 0.013489353967370887,  accuracy: 0.9982424242424244, gradient_norm : 0.04693474160062286
[2025-09-17 16:39:25,204][flp2p.graph_runner][INFO] - Test, Round 297 : loss => 7.373883636785269,  accuracy: 0.34394904458598724
[2025-09-17 16:39:28,316][flp2p.graph_runner][INFO] - Train, Round 298 : loss => 0.03586964055257151,  accuracy: 0.9932121212121211, gradient_norm : 0.09464729137505823
[2025-09-17 16:39:29,147][flp2p.graph_runner][INFO] - Test, Round 298 : loss => 7.196476923985915,  accuracy: 0.3390909090909091
[2025-09-17 16:39:32,342][flp2p.graph_runner][INFO] - Train, Round 299 : loss => 0.18914364246880536,  accuracy: 0.9344242424242425, gradient_norm : 0.10788146157872046
[2025-09-17 16:39:33,155][flp2p.graph_runner][INFO] - Test, Round 299 : loss => 7.042996026081567,  accuracy: 0.329700272479564
[2025-09-17 16:39:33,156][__main__][INFO] - Train, Round 001: loss=2.3005, accuracy=0.1170, gradient_norm=0.0798, 
[2025-09-17 16:39:33,156][__main__][INFO] - Train, Round 002: loss=2.2978, accuracy=0.1232, gradient_norm=0.0840, 
[2025-09-17 16:39:33,156][__main__][INFO] - Train, Round 003: loss=2.2943, accuracy=0.1275, gradient_norm=0.0866, 
[2025-09-17 16:39:33,156][__main__][INFO] - Train, Round 004: loss=2.2887, accuracy=0.1281, gradient_norm=0.0975, 
[2025-09-17 16:39:33,156][__main__][INFO] - Train, Round 005: loss=2.2783, accuracy=0.1454, gradient_norm=0.1087, 
[2025-09-17 16:39:33,156][__main__][INFO] - Train, Round 006: loss=2.2634, accuracy=0.1468, gradient_norm=0.1217, 
[2025-09-17 16:39:33,156][__main__][INFO] - Train, Round 007: loss=2.2486, accuracy=0.1535, gradient_norm=0.1285, 
[2025-09-17 16:39:33,156][__main__][INFO] - Train, Round 008: loss=2.2226, accuracy=0.1725, gradient_norm=0.1258, 
[2025-09-17 16:39:33,156][__main__][INFO] - Train, Round 009: loss=2.2095, accuracy=0.1876, gradient_norm=0.1301, 
[2025-09-17 16:39:33,156][__main__][INFO] - Train, Round 010: loss=2.1910, accuracy=0.1826, gradient_norm=0.1268, 
[2025-09-17 16:39:33,156][__main__][INFO] - Train, Round 011: loss=2.1608, accuracy=0.1903, gradient_norm=0.1424, 
[2025-09-17 16:39:33,156][__main__][INFO] - Train, Round 012: loss=2.1412, accuracy=0.2052, gradient_norm=0.1486, 
[2025-09-17 16:39:33,156][__main__][INFO] - Train, Round 013: loss=2.1249, accuracy=0.2075, gradient_norm=0.1375, 
[2025-09-17 16:39:33,156][__main__][INFO] - Train, Round 014: loss=2.1087, accuracy=0.2249, gradient_norm=0.1555, 
[2025-09-17 16:39:33,156][__main__][INFO] - Train, Round 015: loss=2.0919, accuracy=0.2250, gradient_norm=0.1416, 
[2025-09-17 16:39:33,156][__main__][INFO] - Train, Round 016: loss=2.0726, accuracy=0.2324, gradient_norm=0.1550, 
[2025-09-17 16:39:33,156][__main__][INFO] - Train, Round 017: loss=2.0412, accuracy=0.2341, gradient_norm=0.1633, 
[2025-09-17 16:39:33,156][__main__][INFO] - Train, Round 018: loss=2.0056, accuracy=0.2481, gradient_norm=0.1642, 
[2025-09-17 16:39:33,156][__main__][INFO] - Train, Round 019: loss=1.9520, accuracy=0.2818, gradient_norm=0.1973, 
[2025-09-17 16:39:33,156][__main__][INFO] - Train, Round 020: loss=1.9568, accuracy=0.2721, gradient_norm=0.1674, 
[2025-09-17 16:39:33,156][__main__][INFO] - Train, Round 021: loss=1.9310, accuracy=0.2693, gradient_norm=0.1790, 
[2025-09-17 16:39:33,156][__main__][INFO] - Train, Round 022: loss=1.9026, accuracy=0.3031, gradient_norm=0.1665, 
[2025-09-17 16:39:33,156][__main__][INFO] - Train, Round 023: loss=1.8552, accuracy=0.3205, gradient_norm=0.1690, 
[2025-09-17 16:39:33,156][__main__][INFO] - Train, Round 024: loss=1.8833, accuracy=0.2939, gradient_norm=0.1127, 
[2025-09-17 16:39:33,156][__main__][INFO] - Train, Round 025: loss=1.8743, accuracy=0.2977, gradient_norm=0.1033, 
[2025-09-17 16:39:33,156][__main__][INFO] - Train, Round 026: loss=1.8549, accuracy=0.3185, gradient_norm=0.1246, 
[2025-09-17 16:39:33,156][__main__][INFO] - Train, Round 027: loss=1.8495, accuracy=0.3230, gradient_norm=0.1134, 
[2025-09-17 16:39:33,156][__main__][INFO] - Train, Round 028: loss=1.8505, accuracy=0.3220, gradient_norm=0.1192, 
[2025-09-17 16:39:33,156][__main__][INFO] - Train, Round 029: loss=1.8050, accuracy=0.3322, gradient_norm=0.1198, 
[2025-09-17 16:39:33,156][__main__][INFO] - Train, Round 030: loss=1.8682, accuracy=0.3047, gradient_norm=0.0860, 
[2025-09-17 16:39:33,156][__main__][INFO] - Train, Round 031: loss=1.8327, accuracy=0.3335, gradient_norm=0.1398, 
[2025-09-17 16:39:33,156][__main__][INFO] - Train, Round 032: loss=1.8138, accuracy=0.3430, gradient_norm=0.1574, 
[2025-09-17 16:39:33,156][__main__][INFO] - Train, Round 033: loss=1.8116, accuracy=0.3427, gradient_norm=0.1483, 
[2025-09-17 16:39:33,156][__main__][INFO] - Train, Round 034: loss=1.8366, accuracy=0.3257, gradient_norm=0.1267, 
[2025-09-17 16:39:33,156][__main__][INFO] - Train, Round 035: loss=1.7715, accuracy=0.3539, gradient_norm=0.1766, 
[2025-09-17 16:39:33,156][__main__][INFO] - Train, Round 036: loss=1.8203, accuracy=0.3359, gradient_norm=0.1434, 
[2025-09-17 16:39:33,156][__main__][INFO] - Train, Round 037: loss=1.8290, accuracy=0.3293, gradient_norm=0.1346, 
[2025-09-17 16:39:33,156][__main__][INFO] - Train, Round 038: loss=1.7772, accuracy=0.3647, gradient_norm=0.1937, 
[2025-09-17 16:39:33,156][__main__][INFO] - Train, Round 039: loss=1.7554, accuracy=0.3645, gradient_norm=0.1797, 
[2025-09-17 16:39:33,156][__main__][INFO] - Train, Round 040: loss=1.7682, accuracy=0.3591, gradient_norm=0.1749, 
[2025-09-17 16:39:33,156][__main__][INFO] - Train, Round 041: loss=1.7889, accuracy=0.3504, gradient_norm=0.1312, 
[2025-09-17 16:39:33,156][__main__][INFO] - Train, Round 042: loss=1.7417, accuracy=0.3679, gradient_norm=0.1828, 
[2025-09-17 16:39:33,156][__main__][INFO] - Train, Round 043: loss=1.8350, accuracy=0.3255, gradient_norm=0.1225, 
[2025-09-17 16:39:33,156][__main__][INFO] - Train, Round 044: loss=1.7850, accuracy=0.3536, gradient_norm=0.1380, 
[2025-09-17 16:39:33,156][__main__][INFO] - Train, Round 045: loss=1.7819, accuracy=0.3527, gradient_norm=0.1766, 
[2025-09-17 16:39:33,156][__main__][INFO] - Train, Round 046: loss=1.7390, accuracy=0.3659, gradient_norm=0.1631, 
[2025-09-17 16:39:33,156][__main__][INFO] - Train, Round 047: loss=1.7710, accuracy=0.3662, gradient_norm=0.1540, 
[2025-09-17 16:39:33,156][__main__][INFO] - Train, Round 048: loss=1.7111, accuracy=0.3884, gradient_norm=0.2019, 
[2025-09-17 16:39:33,156][__main__][INFO] - Train, Round 049: loss=1.7631, accuracy=0.3668, gradient_norm=0.1600, 
[2025-09-17 16:39:33,156][__main__][INFO] - Train, Round 050: loss=1.6739, accuracy=0.3989, gradient_norm=0.2077, 
[2025-09-17 16:39:33,156][__main__][INFO] - Train, Round 051: loss=1.7495, accuracy=0.3661, gradient_norm=0.1616, 
[2025-09-17 16:39:33,156][__main__][INFO] - Train, Round 052: loss=1.7138, accuracy=0.3760, gradient_norm=0.1998, 
[2025-09-17 16:39:33,156][__main__][INFO] - Train, Round 053: loss=1.7473, accuracy=0.3652, gradient_norm=0.1743, 
[2025-09-17 16:39:33,156][__main__][INFO] - Train, Round 054: loss=1.7411, accuracy=0.3670, gradient_norm=0.1706, 
[2025-09-17 16:39:33,156][__main__][INFO] - Train, Round 055: loss=1.6970, accuracy=0.3930, gradient_norm=0.1959, 
[2025-09-17 16:39:33,157][__main__][INFO] - Train, Round 056: loss=1.6147, accuracy=0.4184, gradient_norm=0.2243, 
[2025-09-17 16:39:33,157][__main__][INFO] - Train, Round 057: loss=1.5697, accuracy=0.4405, gradient_norm=0.2407, 
[2025-09-17 16:39:33,157][__main__][INFO] - Train, Round 058: loss=1.6824, accuracy=0.3928, gradient_norm=0.1851, 
[2025-09-17 16:39:33,157][__main__][INFO] - Train, Round 059: loss=1.7042, accuracy=0.3963, gradient_norm=0.1989, 
[2025-09-17 16:39:33,157][__main__][INFO] - Train, Round 060: loss=1.6890, accuracy=0.4042, gradient_norm=0.1999, 
[2025-09-17 16:39:33,157][__main__][INFO] - Train, Round 061: loss=1.6326, accuracy=0.4156, gradient_norm=0.2039, 
[2025-09-17 16:39:33,157][__main__][INFO] - Train, Round 062: loss=1.7448, accuracy=0.3761, gradient_norm=0.1822, 
[2025-09-17 16:39:33,157][__main__][INFO] - Train, Round 063: loss=1.7572, accuracy=0.3665, gradient_norm=0.1783, 
[2025-09-17 16:39:33,157][__main__][INFO] - Train, Round 064: loss=1.6280, accuracy=0.4150, gradient_norm=0.2254, 
[2025-09-17 16:39:33,157][__main__][INFO] - Train, Round 065: loss=1.6266, accuracy=0.4073, gradient_norm=0.2007, 
[2025-09-17 16:39:33,157][__main__][INFO] - Train, Round 066: loss=1.6302, accuracy=0.4109, gradient_norm=0.2422, 
[2025-09-17 16:39:33,157][__main__][INFO] - Train, Round 067: loss=1.6796, accuracy=0.3866, gradient_norm=0.2153, 
[2025-09-17 16:39:33,157][__main__][INFO] - Train, Round 068: loss=1.6189, accuracy=0.4143, gradient_norm=0.2257, 
[2025-09-17 16:39:33,157][__main__][INFO] - Train, Round 069: loss=1.6968, accuracy=0.3888, gradient_norm=0.1903, 
[2025-09-17 16:39:33,157][__main__][INFO] - Train, Round 070: loss=1.6867, accuracy=0.3921, gradient_norm=0.2179, 
[2025-09-17 16:39:33,157][__main__][INFO] - Train, Round 071: loss=1.5924, accuracy=0.4322, gradient_norm=0.2441, 
[2025-09-17 16:39:33,157][__main__][INFO] - Train, Round 072: loss=1.6195, accuracy=0.4192, gradient_norm=0.2271, 
[2025-09-17 16:39:33,157][__main__][INFO] - Train, Round 073: loss=1.5928, accuracy=0.4258, gradient_norm=0.2327, 
[2025-09-17 16:39:33,157][__main__][INFO] - Train, Round 074: loss=1.6381, accuracy=0.4062, gradient_norm=0.2149, 
[2025-09-17 16:39:33,157][__main__][INFO] - Train, Round 075: loss=1.5072, accuracy=0.4579, gradient_norm=0.2344, 
[2025-09-17 16:39:33,157][__main__][INFO] - Train, Round 076: loss=1.5380, accuracy=0.4416, gradient_norm=0.2312, 
[2025-09-17 16:39:33,157][__main__][INFO] - Train, Round 077: loss=1.4878, accuracy=0.4584, gradient_norm=0.2382, 
[2025-09-17 16:39:33,157][__main__][INFO] - Train, Round 078: loss=1.5488, accuracy=0.4475, gradient_norm=0.2385, 
[2025-09-17 16:39:33,157][__main__][INFO] - Train, Round 079: loss=1.6317, accuracy=0.4081, gradient_norm=0.2127, 
[2025-09-17 16:39:33,157][__main__][INFO] - Train, Round 080: loss=1.4690, accuracy=0.4735, gradient_norm=0.2432, 
[2025-09-17 16:39:33,157][__main__][INFO] - Train, Round 081: loss=1.6105, accuracy=0.4223, gradient_norm=0.2179, 
[2025-09-17 16:39:33,157][__main__][INFO] - Train, Round 082: loss=1.5502, accuracy=0.4407, gradient_norm=0.2263, 
[2025-09-17 16:39:33,157][__main__][INFO] - Train, Round 083: loss=1.5269, accuracy=0.4442, gradient_norm=0.2333, 
[2025-09-17 16:39:33,157][__main__][INFO] - Train, Round 084: loss=1.5130, accuracy=0.4547, gradient_norm=0.2199, 
[2025-09-17 16:39:33,157][__main__][INFO] - Train, Round 085: loss=1.5408, accuracy=0.4439, gradient_norm=0.2371, 
[2025-09-17 16:39:33,157][__main__][INFO] - Train, Round 086: loss=1.5191, accuracy=0.4525, gradient_norm=0.2401, 
[2025-09-17 16:39:33,157][__main__][INFO] - Train, Round 087: loss=1.4446, accuracy=0.4805, gradient_norm=0.2435, 
[2025-09-17 16:39:33,157][__main__][INFO] - Train, Round 088: loss=1.4538, accuracy=0.4699, gradient_norm=0.2571, 
[2025-09-17 16:39:33,157][__main__][INFO] - Train, Round 089: loss=1.4325, accuracy=0.4858, gradient_norm=0.2648, 
[2025-09-17 16:39:33,157][__main__][INFO] - Train, Round 090: loss=1.5189, accuracy=0.4516, gradient_norm=0.2345, 
[2025-09-17 16:39:33,157][__main__][INFO] - Train, Round 091: loss=1.4737, accuracy=0.4699, gradient_norm=0.2512, 
[2025-09-17 16:39:33,157][__main__][INFO] - Train, Round 092: loss=1.4882, accuracy=0.4693, gradient_norm=0.2393, 
[2025-09-17 16:39:33,157][__main__][INFO] - Train, Round 093: loss=1.5089, accuracy=0.4571, gradient_norm=0.2297, 
[2025-09-17 16:39:33,157][__main__][INFO] - Train, Round 094: loss=1.2964, accuracy=0.5362, gradient_norm=0.2732, 
[2025-09-17 16:39:33,157][__main__][INFO] - Train, Round 095: loss=1.4285, accuracy=0.4839, gradient_norm=0.2771, 
[2025-09-17 16:39:33,157][__main__][INFO] - Train, Round 096: loss=1.4782, accuracy=0.4653, gradient_norm=0.2366, 
[2025-09-17 16:39:33,157][__main__][INFO] - Train, Round 097: loss=1.4893, accuracy=0.4616, gradient_norm=0.2253, 
[2025-09-17 16:39:33,157][__main__][INFO] - Train, Round 098: loss=1.3384, accuracy=0.5124, gradient_norm=0.2600, 
[2025-09-17 16:39:33,157][__main__][INFO] - Train, Round 099: loss=1.5390, accuracy=0.4525, gradient_norm=0.2319, 
[2025-09-17 16:39:33,157][__main__][INFO] - Train, Round 100: loss=1.4127, accuracy=0.4890, gradient_norm=0.2725, 
[2025-09-17 16:39:33,157][__main__][INFO] - Train, Round 101: loss=1.5040, accuracy=0.4517, gradient_norm=0.2503, 
[2025-09-17 16:39:33,157][__main__][INFO] - Train, Round 102: loss=1.3948, accuracy=0.5011, gradient_norm=0.2700, 
[2025-09-17 16:39:33,157][__main__][INFO] - Train, Round 103: loss=1.5982, accuracy=0.4213, gradient_norm=0.2847, 
[2025-09-17 16:39:33,157][__main__][INFO] - Train, Round 104: loss=1.2878, accuracy=0.5416, gradient_norm=0.2749, 
[2025-09-17 16:39:33,157][__main__][INFO] - Train, Round 105: loss=1.3105, accuracy=0.5281, gradient_norm=0.2619, 
[2025-09-17 16:39:33,157][__main__][INFO] - Train, Round 106: loss=1.5085, accuracy=0.4567, gradient_norm=0.2349, 
[2025-09-17 16:39:33,157][__main__][INFO] - Train, Round 107: loss=1.5044, accuracy=0.4556, gradient_norm=0.2445, 
[2025-09-17 16:39:33,157][__main__][INFO] - Train, Round 108: loss=1.4138, accuracy=0.4919, gradient_norm=0.2557, 
[2025-09-17 16:39:33,157][__main__][INFO] - Train, Round 109: loss=1.3649, accuracy=0.5042, gradient_norm=0.2646, 
[2025-09-17 16:39:33,157][__main__][INFO] - Train, Round 110: loss=1.3953, accuracy=0.5074, gradient_norm=0.2633, 
[2025-09-17 16:39:33,157][__main__][INFO] - Train, Round 111: loss=1.3621, accuracy=0.5077, gradient_norm=0.2590, 
[2025-09-17 16:39:33,157][__main__][INFO] - Train, Round 112: loss=1.2526, accuracy=0.5493, gradient_norm=0.2953, 
[2025-09-17 16:39:33,157][__main__][INFO] - Train, Round 113: loss=1.1160, accuracy=0.6021, gradient_norm=0.2743, 
[2025-09-17 16:39:33,157][__main__][INFO] - Train, Round 114: loss=1.2982, accuracy=0.5258, gradient_norm=0.2743, 
[2025-09-17 16:39:33,157][__main__][INFO] - Train, Round 115: loss=1.3224, accuracy=0.5268, gradient_norm=0.2573, 
[2025-09-17 16:39:33,157][__main__][INFO] - Train, Round 116: loss=1.3834, accuracy=0.4968, gradient_norm=0.2682, 
[2025-09-17 16:39:33,157][__main__][INFO] - Train, Round 117: loss=1.3132, accuracy=0.5325, gradient_norm=0.2892, 
[2025-09-17 16:39:33,157][__main__][INFO] - Train, Round 118: loss=1.1134, accuracy=0.6010, gradient_norm=0.2779, 
[2025-09-17 16:39:33,157][__main__][INFO] - Train, Round 119: loss=1.2335, accuracy=0.5628, gradient_norm=0.2805, 
[2025-09-17 16:39:33,157][__main__][INFO] - Train, Round 120: loss=1.2319, accuracy=0.5596, gradient_norm=0.2919, 
[2025-09-17 16:39:33,157][__main__][INFO] - Train, Round 121: loss=1.3270, accuracy=0.5241, gradient_norm=0.3104, 
[2025-09-17 16:39:33,157][__main__][INFO] - Train, Round 122: loss=1.3039, accuracy=0.5284, gradient_norm=0.2768, 
[2025-09-17 16:39:33,157][__main__][INFO] - Train, Round 123: loss=1.2549, accuracy=0.5476, gradient_norm=0.2866, 
[2025-09-17 16:39:33,157][__main__][INFO] - Train, Round 124: loss=1.2557, accuracy=0.5457, gradient_norm=0.2939, 
[2025-09-17 16:39:33,157][__main__][INFO] - Train, Round 125: loss=1.2222, accuracy=0.5596, gradient_norm=0.2855, 
[2025-09-17 16:39:33,158][__main__][INFO] - Train, Round 126: loss=1.2484, accuracy=0.5520, gradient_norm=0.2816, 
[2025-09-17 16:39:33,158][__main__][INFO] - Train, Round 127: loss=1.1493, accuracy=0.5978, gradient_norm=0.3009, 
[2025-09-17 16:39:33,158][__main__][INFO] - Train, Round 128: loss=1.2911, accuracy=0.5374, gradient_norm=0.2754, 
[2025-09-17 16:39:33,158][__main__][INFO] - Train, Round 129: loss=1.2698, accuracy=0.5432, gradient_norm=0.2828, 
[2025-09-17 16:39:33,158][__main__][INFO] - Train, Round 130: loss=1.1573, accuracy=0.5844, gradient_norm=0.2960, 
[2025-09-17 16:39:33,158][__main__][INFO] - Train, Round 131: loss=1.1664, accuracy=0.5806, gradient_norm=0.2898, 
[2025-09-17 16:39:33,158][__main__][INFO] - Train, Round 132: loss=1.0613, accuracy=0.6217, gradient_norm=0.3184, 
[2025-09-17 16:39:33,158][__main__][INFO] - Train, Round 133: loss=1.2062, accuracy=0.5670, gradient_norm=0.2945, 
[2025-09-17 16:39:33,158][__main__][INFO] - Train, Round 134: loss=1.0206, accuracy=0.6404, gradient_norm=0.3030, 
[2025-09-17 16:39:33,158][__main__][INFO] - Train, Round 135: loss=1.2563, accuracy=0.5524, gradient_norm=0.2794, 
[2025-09-17 16:39:33,158][__main__][INFO] - Train, Round 136: loss=1.1706, accuracy=0.5844, gradient_norm=0.2813, 
[2025-09-17 16:39:33,158][__main__][INFO] - Train, Round 137: loss=0.9934, accuracy=0.6479, gradient_norm=0.3075, 
[2025-09-17 16:39:33,158][__main__][INFO] - Train, Round 138: loss=1.1041, accuracy=0.6052, gradient_norm=0.2969, 
[2025-09-17 16:39:33,158][__main__][INFO] - Train, Round 139: loss=1.1299, accuracy=0.5961, gradient_norm=0.2905, 
[2025-09-17 16:39:33,158][__main__][INFO] - Train, Round 140: loss=1.1539, accuracy=0.5848, gradient_norm=0.3058, 
[2025-09-17 16:39:33,158][__main__][INFO] - Train, Round 141: loss=1.0653, accuracy=0.6212, gradient_norm=0.3024, 
[2025-09-17 16:39:33,158][__main__][INFO] - Train, Round 142: loss=1.0645, accuracy=0.6226, gradient_norm=0.3122, 
[2025-09-17 16:39:33,158][__main__][INFO] - Train, Round 143: loss=1.1054, accuracy=0.6024, gradient_norm=0.2753, 
[2025-09-17 16:39:33,158][__main__][INFO] - Train, Round 144: loss=1.0548, accuracy=0.6227, gradient_norm=0.2950, 
[2025-09-17 16:39:33,158][__main__][INFO] - Train, Round 145: loss=1.0394, accuracy=0.6241, gradient_norm=0.3142, 
[2025-09-17 16:39:33,158][__main__][INFO] - Train, Round 146: loss=0.9576, accuracy=0.6621, gradient_norm=0.3098, 
[2025-09-17 16:39:33,158][__main__][INFO] - Train, Round 147: loss=1.0564, accuracy=0.6200, gradient_norm=0.3099, 
[2025-09-17 16:39:33,158][__main__][INFO] - Train, Round 148: loss=1.0086, accuracy=0.6428, gradient_norm=0.3173, 
[2025-09-17 16:39:33,158][__main__][INFO] - Train, Round 149: loss=1.2387, accuracy=0.5539, gradient_norm=0.2864, 
[2025-09-17 16:39:33,158][__main__][INFO] - Train, Round 150: loss=0.8414, accuracy=0.7035, gradient_norm=0.3197, 
[2025-09-17 16:39:33,158][__main__][INFO] - Train, Round 151: loss=0.9633, accuracy=0.6572, gradient_norm=0.3198, 
[2025-09-17 16:39:33,158][__main__][INFO] - Train, Round 152: loss=1.0131, accuracy=0.6354, gradient_norm=0.3227, 
[2025-09-17 16:39:33,158][__main__][INFO] - Train, Round 153: loss=0.9534, accuracy=0.6625, gradient_norm=0.3041, 
[2025-09-17 16:39:33,158][__main__][INFO] - Train, Round 154: loss=1.0178, accuracy=0.6345, gradient_norm=0.3087, 
[2025-09-17 16:39:33,158][__main__][INFO] - Train, Round 155: loss=0.9189, accuracy=0.6691, gradient_norm=0.3293, 
[2025-09-17 16:39:33,158][__main__][INFO] - Train, Round 156: loss=0.7411, accuracy=0.7359, gradient_norm=0.3380, 
[2025-09-17 16:39:33,158][__main__][INFO] - Train, Round 157: loss=0.7069, accuracy=0.7521, gradient_norm=0.3474, 
[2025-09-17 16:39:33,158][__main__][INFO] - Train, Round 158: loss=0.9666, accuracy=0.6555, gradient_norm=0.3318, 
[2025-09-17 16:39:33,158][__main__][INFO] - Train, Round 159: loss=0.8967, accuracy=0.6848, gradient_norm=0.3258, 
[2025-09-17 16:39:33,158][__main__][INFO] - Train, Round 160: loss=0.8552, accuracy=0.6967, gradient_norm=0.3037, 
[2025-09-17 16:39:33,158][__main__][INFO] - Train, Round 161: loss=0.6307, accuracy=0.7799, gradient_norm=0.3310, 
[2025-09-17 16:39:33,158][__main__][INFO] - Train, Round 162: loss=0.6958, accuracy=0.7564, gradient_norm=0.3449, 
[2025-09-17 16:39:33,158][__main__][INFO] - Train, Round 163: loss=0.8543, accuracy=0.7005, gradient_norm=0.3272, 
[2025-09-17 16:39:33,158][__main__][INFO] - Train, Round 164: loss=0.7017, accuracy=0.7487, gradient_norm=0.3096, 
[2025-09-17 16:39:33,158][__main__][INFO] - Train, Round 165: loss=0.7422, accuracy=0.7404, gradient_norm=0.3266, 
[2025-09-17 16:39:33,158][__main__][INFO] - Train, Round 166: loss=1.0371, accuracy=0.6324, gradient_norm=0.3203, 
[2025-09-17 16:39:33,158][__main__][INFO] - Train, Round 167: loss=0.5345, accuracy=0.8153, gradient_norm=0.3182, 
[2025-09-17 16:39:33,158][__main__][INFO] - Train, Round 168: loss=0.7310, accuracy=0.7472, gradient_norm=0.3042, 
[2025-09-17 16:39:33,158][__main__][INFO] - Train, Round 169: loss=0.9046, accuracy=0.6807, gradient_norm=0.3323, 
[2025-09-17 16:39:33,158][__main__][INFO] - Train, Round 170: loss=1.0024, accuracy=0.6458, gradient_norm=0.3290, 
[2025-09-17 16:39:33,158][__main__][INFO] - Train, Round 171: loss=0.7864, accuracy=0.7241, gradient_norm=0.3098, 
[2025-09-17 16:39:33,158][__main__][INFO] - Train, Round 172: loss=0.8019, accuracy=0.7219, gradient_norm=0.2767, 
[2025-09-17 16:39:33,158][__main__][INFO] - Train, Round 173: loss=0.8548, accuracy=0.6988, gradient_norm=0.3440, 
[2025-09-17 16:39:33,158][__main__][INFO] - Train, Round 174: loss=0.8557, accuracy=0.7057, gradient_norm=0.3324, 
[2025-09-17 16:39:33,158][__main__][INFO] - Train, Round 175: loss=0.7450, accuracy=0.7429, gradient_norm=0.3500, 
[2025-09-17 16:39:33,158][__main__][INFO] - Train, Round 176: loss=0.8217, accuracy=0.7127, gradient_norm=0.3350, 
[2025-09-17 16:39:33,158][__main__][INFO] - Train, Round 177: loss=0.9425, accuracy=0.6692, gradient_norm=0.3360, 
[2025-09-17 16:39:33,158][__main__][INFO] - Train, Round 178: loss=0.5931, accuracy=0.7995, gradient_norm=0.3265, 
[2025-09-17 16:39:33,158][__main__][INFO] - Train, Round 179: loss=0.6628, accuracy=0.7737, gradient_norm=0.3289, 
[2025-09-17 16:39:33,158][__main__][INFO] - Train, Round 180: loss=0.8925, accuracy=0.6826, gradient_norm=0.3444, 
[2025-09-17 16:39:33,158][__main__][INFO] - Train, Round 181: loss=0.7950, accuracy=0.7195, gradient_norm=0.3359, 
[2025-09-17 16:39:33,158][__main__][INFO] - Train, Round 182: loss=0.6325, accuracy=0.7764, gradient_norm=0.3612, 
[2025-09-17 16:39:33,158][__main__][INFO] - Train, Round 183: loss=0.8270, accuracy=0.7058, gradient_norm=0.2779, 
[2025-09-17 16:39:33,158][__main__][INFO] - Train, Round 184: loss=0.7683, accuracy=0.7264, gradient_norm=0.3272, 
[2025-09-17 16:39:33,158][__main__][INFO] - Train, Round 185: loss=0.7178, accuracy=0.7485, gradient_norm=0.3701, 
[2025-09-17 16:39:33,158][__main__][INFO] - Train, Round 186: loss=0.7051, accuracy=0.7543, gradient_norm=0.3357, 
[2025-09-17 16:39:33,158][__main__][INFO] - Train, Round 187: loss=0.7633, accuracy=0.7338, gradient_norm=0.3215, 
[2025-09-17 16:39:33,158][__main__][INFO] - Train, Round 188: loss=0.7333, accuracy=0.7386, gradient_norm=0.2698, 
[2025-09-17 16:39:33,158][__main__][INFO] - Train, Round 189: loss=0.8837, accuracy=0.6855, gradient_norm=0.3152, 
[2025-09-17 16:39:33,158][__main__][INFO] - Train, Round 190: loss=0.4153, accuracy=0.8627, gradient_norm=0.3191, 
[2025-09-17 16:39:33,158][__main__][INFO] - Train, Round 191: loss=0.6612, accuracy=0.7707, gradient_norm=0.3508, 
[2025-09-17 16:39:33,158][__main__][INFO] - Train, Round 192: loss=0.8116, accuracy=0.7096, gradient_norm=0.3354, 
[2025-09-17 16:39:33,158][__main__][INFO] - Train, Round 193: loss=0.4842, accuracy=0.8364, gradient_norm=0.3259, 
[2025-09-17 16:39:33,158][__main__][INFO] - Train, Round 194: loss=0.5516, accuracy=0.8087, gradient_norm=0.2977, 
[2025-09-17 16:39:33,158][__main__][INFO] - Train, Round 195: loss=0.4206, accuracy=0.8630, gradient_norm=0.2869, 
[2025-09-17 16:39:33,158][__main__][INFO] - Train, Round 196: loss=0.4686, accuracy=0.8438, gradient_norm=0.3324, 
[2025-09-17 16:39:33,159][__main__][INFO] - Train, Round 197: loss=0.5298, accuracy=0.8177, gradient_norm=0.2924, 
[2025-09-17 16:39:33,159][__main__][INFO] - Train, Round 198: loss=0.5777, accuracy=0.8031, gradient_norm=0.3462, 
[2025-09-17 16:39:33,159][__main__][INFO] - Train, Round 199: loss=0.4308, accuracy=0.8561, gradient_norm=0.3259, 
[2025-09-17 16:39:33,159][__main__][INFO] - Train, Round 200: loss=0.3161, accuracy=0.8958, gradient_norm=0.2985, 
[2025-09-17 16:39:33,159][__main__][INFO] - Train, Round 201: loss=0.3978, accuracy=0.8705, gradient_norm=0.3487, 
[2025-09-17 16:39:33,159][__main__][INFO] - Train, Round 202: loss=0.3318, accuracy=0.8938, gradient_norm=0.3217, 
[2025-09-17 16:39:33,159][__main__][INFO] - Train, Round 203: loss=0.6658, accuracy=0.7650, gradient_norm=0.3146, 
[2025-09-17 16:39:33,159][__main__][INFO] - Train, Round 204: loss=0.4963, accuracy=0.8274, gradient_norm=0.3022, 
[2025-09-17 16:39:33,159][__main__][INFO] - Train, Round 205: loss=0.4525, accuracy=0.8447, gradient_norm=0.3163, 
[2025-09-17 16:39:33,159][__main__][INFO] - Train, Round 206: loss=0.4354, accuracy=0.8536, gradient_norm=0.3364, 
[2025-09-17 16:39:33,159][__main__][INFO] - Train, Round 207: loss=0.6114, accuracy=0.7925, gradient_norm=0.3691, 
[2025-09-17 16:39:33,159][__main__][INFO] - Train, Round 208: loss=0.3274, accuracy=0.8952, gradient_norm=0.2932, 
[2025-09-17 16:39:33,159][__main__][INFO] - Train, Round 209: loss=0.4454, accuracy=0.8496, gradient_norm=0.2640, 
[2025-09-17 16:39:33,159][__main__][INFO] - Train, Round 210: loss=0.5300, accuracy=0.8197, gradient_norm=0.3749, 
[2025-09-17 16:39:33,159][__main__][INFO] - Train, Round 211: loss=0.3577, accuracy=0.8794, gradient_norm=0.3381, 
[2025-09-17 16:39:33,159][__main__][INFO] - Train, Round 212: loss=0.6227, accuracy=0.7872, gradient_norm=0.3354, 
[2025-09-17 16:39:33,159][__main__][INFO] - Train, Round 213: loss=0.2541, accuracy=0.9223, gradient_norm=0.3094, 
[2025-09-17 16:39:33,159][__main__][INFO] - Train, Round 214: loss=0.4312, accuracy=0.8578, gradient_norm=0.3176, 
[2025-09-17 16:39:33,159][__main__][INFO] - Train, Round 215: loss=0.4020, accuracy=0.8681, gradient_norm=0.2778, 
[2025-09-17 16:39:33,159][__main__][INFO] - Train, Round 216: loss=0.2718, accuracy=0.9184, gradient_norm=0.2517, 
[2025-09-17 16:39:33,159][__main__][INFO] - Train, Round 217: loss=0.3830, accuracy=0.8695, gradient_norm=0.2436, 
[2025-09-17 16:39:33,159][__main__][INFO] - Train, Round 218: loss=0.2837, accuracy=0.9123, gradient_norm=0.2810, 
[2025-09-17 16:39:33,159][__main__][INFO] - Train, Round 219: loss=0.2935, accuracy=0.9076, gradient_norm=0.2932, 
[2025-09-17 16:39:33,159][__main__][INFO] - Train, Round 220: loss=0.3623, accuracy=0.8804, gradient_norm=0.2861, 
[2025-09-17 16:39:33,159][__main__][INFO] - Train, Round 221: loss=0.3554, accuracy=0.8818, gradient_norm=0.2929, 
[2025-09-17 16:39:33,159][__main__][INFO] - Train, Round 222: loss=0.3224, accuracy=0.8939, gradient_norm=0.2736, 
[2025-09-17 16:39:33,159][__main__][INFO] - Train, Round 223: loss=0.2801, accuracy=0.9138, gradient_norm=0.2680, 
[2025-09-17 16:39:33,159][__main__][INFO] - Train, Round 224: loss=0.3137, accuracy=0.8987, gradient_norm=0.2896, 
[2025-09-17 16:39:33,159][__main__][INFO] - Train, Round 225: loss=0.2306, accuracy=0.9250, gradient_norm=0.2129, 
[2025-09-17 16:39:33,159][__main__][INFO] - Train, Round 226: loss=0.2651, accuracy=0.9124, gradient_norm=0.2166, 
[2025-09-17 16:39:33,159][__main__][INFO] - Train, Round 227: loss=0.1680, accuracy=0.9508, gradient_norm=0.2174, 
[2025-09-17 16:39:33,159][__main__][INFO] - Train, Round 228: loss=0.4412, accuracy=0.8495, gradient_norm=0.2879, 
[2025-09-17 16:39:33,159][__main__][INFO] - Train, Round 229: loss=0.4468, accuracy=0.8512, gradient_norm=0.3210, 
[2025-09-17 16:39:33,159][__main__][INFO] - Train, Round 230: loss=0.3213, accuracy=0.8970, gradient_norm=0.2625, 
[2025-09-17 16:39:33,159][__main__][INFO] - Train, Round 231: loss=0.4816, accuracy=0.8342, gradient_norm=0.2732, 
[2025-09-17 16:39:33,159][__main__][INFO] - Train, Round 232: loss=0.4014, accuracy=0.8677, gradient_norm=0.3160, 
[2025-09-17 16:39:33,159][__main__][INFO] - Train, Round 233: loss=0.2212, accuracy=0.9319, gradient_norm=0.2719, 
[2025-09-17 16:39:33,159][__main__][INFO] - Train, Round 234: loss=0.3531, accuracy=0.8807, gradient_norm=0.2225, 
[2025-09-17 16:39:33,159][__main__][INFO] - Train, Round 235: loss=0.2019, accuracy=0.9384, gradient_norm=0.2835, 
[2025-09-17 16:39:33,159][__main__][INFO] - Train, Round 236: loss=0.2427, accuracy=0.9209, gradient_norm=0.2349, 
[2025-09-17 16:39:33,159][__main__][INFO] - Train, Round 237: loss=0.2208, accuracy=0.9284, gradient_norm=0.2094, 
[2025-09-17 16:39:33,159][__main__][INFO] - Train, Round 238: loss=0.2433, accuracy=0.9250, gradient_norm=0.2439, 
[2025-09-17 16:39:33,159][__main__][INFO] - Train, Round 239: loss=0.3102, accuracy=0.8990, gradient_norm=0.2495, 
[2025-09-17 16:39:33,159][__main__][INFO] - Train, Round 240: loss=0.2744, accuracy=0.9125, gradient_norm=0.2352, 
[2025-09-17 16:39:33,159][__main__][INFO] - Train, Round 241: loss=0.3077, accuracy=0.8973, gradient_norm=0.2325, 
[2025-09-17 16:39:33,159][__main__][INFO] - Train, Round 242: loss=0.2583, accuracy=0.9253, gradient_norm=0.3040, 
[2025-09-17 16:39:33,159][__main__][INFO] - Train, Round 243: loss=0.1984, accuracy=0.9396, gradient_norm=0.2250, 
[2025-09-17 16:39:33,159][__main__][INFO] - Train, Round 244: loss=0.3916, accuracy=0.8698, gradient_norm=0.2578, 
[2025-09-17 16:39:33,159][__main__][INFO] - Train, Round 245: loss=0.1167, accuracy=0.9668, gradient_norm=0.1512, 
[2025-09-17 16:39:33,159][__main__][INFO] - Train, Round 246: loss=0.1457, accuracy=0.9611, gradient_norm=0.2378, 
[2025-09-17 16:39:33,159][__main__][INFO] - Train, Round 247: loss=0.2052, accuracy=0.9357, gradient_norm=0.2479, 
[2025-09-17 16:39:33,159][__main__][INFO] - Train, Round 248: loss=0.2780, accuracy=0.9096, gradient_norm=0.2910, 
[2025-09-17 16:39:33,159][__main__][INFO] - Train, Round 249: loss=0.3225, accuracy=0.8924, gradient_norm=0.2102, 
[2025-09-17 16:39:33,159][__main__][INFO] - Train, Round 250: loss=0.0942, accuracy=0.9767, gradient_norm=0.1785, 
[2025-09-17 16:39:33,159][__main__][INFO] - Train, Round 251: loss=0.1858, accuracy=0.9452, gradient_norm=0.2389, 
[2025-09-17 16:39:33,159][__main__][INFO] - Train, Round 252: loss=0.1371, accuracy=0.9596, gradient_norm=0.2008, 
[2025-09-17 16:39:33,159][__main__][INFO] - Train, Round 253: loss=0.2446, accuracy=0.9212, gradient_norm=0.2449, 
[2025-09-17 16:39:33,159][__main__][INFO] - Train, Round 254: loss=0.1112, accuracy=0.9673, gradient_norm=0.1071, 
[2025-09-17 16:39:33,159][__main__][INFO] - Train, Round 255: loss=0.1079, accuracy=0.9676, gradient_norm=0.1273, 
[2025-09-17 16:39:33,159][__main__][INFO] - Train, Round 256: loss=0.0641, accuracy=0.9845, gradient_norm=0.1415, 
[2025-09-17 16:39:33,159][__main__][INFO] - Train, Round 257: loss=0.2080, accuracy=0.9342, gradient_norm=0.1960, 
[2025-09-17 16:39:33,159][__main__][INFO] - Train, Round 258: loss=0.2101, accuracy=0.9359, gradient_norm=0.2390, 
[2025-09-17 16:39:33,159][__main__][INFO] - Train, Round 259: loss=0.1171, accuracy=0.9711, gradient_norm=0.2405, 
[2025-09-17 16:39:33,159][__main__][INFO] - Train, Round 260: loss=0.2535, accuracy=0.9201, gradient_norm=0.2057, 
[2025-09-17 16:39:33,160][__main__][INFO] - Train, Round 261: loss=0.1227, accuracy=0.9678, gradient_norm=0.1833, 
[2025-09-17 16:39:33,160][__main__][INFO] - Train, Round 262: loss=0.1129, accuracy=0.9653, gradient_norm=0.1161, 
[2025-09-17 16:39:33,160][__main__][INFO] - Train, Round 263: loss=0.1134, accuracy=0.9667, gradient_norm=0.1357, 
[2025-09-17 16:39:33,160][__main__][INFO] - Train, Round 264: loss=0.0947, accuracy=0.9748, gradient_norm=0.1429, 
[2025-09-17 16:39:33,160][__main__][INFO] - Train, Round 265: loss=0.1804, accuracy=0.9469, gradient_norm=0.1801, 
[2025-09-17 16:39:33,160][__main__][INFO] - Train, Round 266: loss=0.0854, accuracy=0.9803, gradient_norm=0.1614, 
[2025-09-17 16:39:33,160][__main__][INFO] - Train, Round 267: loss=0.0953, accuracy=0.9736, gradient_norm=0.1322, 
[2025-09-17 16:39:33,160][__main__][INFO] - Train, Round 268: loss=0.1771, accuracy=0.9469, gradient_norm=0.2578, 
[2025-09-17 16:39:33,160][__main__][INFO] - Train, Round 269: loss=0.0776, accuracy=0.9812, gradient_norm=0.1474, 
[2025-09-17 16:39:33,160][__main__][INFO] - Train, Round 270: loss=0.0274, accuracy=0.9964, gradient_norm=0.0934, 
[2025-09-17 16:39:33,160][__main__][INFO] - Train, Round 271: loss=0.2895, accuracy=0.9044, gradient_norm=0.1773, 
[2025-09-17 16:39:33,160][__main__][INFO] - Train, Round 272: loss=0.0821, accuracy=0.9755, gradient_norm=0.0937, 
[2025-09-17 16:39:33,160][__main__][INFO] - Train, Round 273: loss=0.1108, accuracy=0.9671, gradient_norm=0.1784, 
[2025-09-17 16:39:33,160][__main__][INFO] - Train, Round 274: loss=0.0399, accuracy=0.9904, gradient_norm=0.0810, 
[2025-09-17 16:39:33,160][__main__][INFO] - Train, Round 275: loss=0.1535, accuracy=0.9535, gradient_norm=0.1339, 
[2025-09-17 16:39:33,160][__main__][INFO] - Train, Round 276: loss=0.0849, accuracy=0.9768, gradient_norm=0.1201, 
[2025-09-17 16:39:33,160][__main__][INFO] - Train, Round 277: loss=0.0730, accuracy=0.9799, gradient_norm=0.1338, 
[2025-09-17 16:39:33,160][__main__][INFO] - Train, Round 278: loss=0.2119, accuracy=0.9327, gradient_norm=0.2096, 
[2025-09-17 16:39:33,160][__main__][INFO] - Train, Round 279: loss=0.0263, accuracy=0.9957, gradient_norm=0.0804, 
[2025-09-17 16:39:33,160][__main__][INFO] - Train, Round 280: loss=0.0140, accuracy=0.9996, gradient_norm=0.0553, 
[2025-09-17 16:39:33,160][__main__][INFO] - Train, Round 281: loss=0.0939, accuracy=0.9782, gradient_norm=0.1621, 
[2025-09-17 16:39:33,160][__main__][INFO] - Train, Round 282: loss=0.0498, accuracy=0.9902, gradient_norm=0.1177, 
[2025-09-17 16:39:33,160][__main__][INFO] - Train, Round 283: loss=0.0821, accuracy=0.9771, gradient_norm=0.0969, 
[2025-09-17 16:39:33,160][__main__][INFO] - Train, Round 284: loss=0.2425, accuracy=0.9196, gradient_norm=0.1701, 
[2025-09-17 16:39:33,160][__main__][INFO] - Train, Round 285: loss=0.2016, accuracy=0.9370, gradient_norm=0.2081, 
[2025-09-17 16:39:33,160][__main__][INFO] - Train, Round 286: loss=0.0550, accuracy=0.9859, gradient_norm=0.1026, 
[2025-09-17 16:39:33,160][__main__][INFO] - Train, Round 287: loss=0.0489, accuracy=0.9869, gradient_norm=0.0658, 
[2025-09-17 16:39:33,160][__main__][INFO] - Train, Round 288: loss=0.0377, accuracy=0.9942, gradient_norm=0.1076, 
[2025-09-17 16:39:33,160][__main__][INFO] - Train, Round 289: loss=0.0243, accuracy=0.9940, gradient_norm=0.0536, 
[2025-09-17 16:39:33,160][__main__][INFO] - Train, Round 290: loss=0.0069, accuracy=1.0000, gradient_norm=0.0307, 
[2025-09-17 16:39:33,160][__main__][INFO] - Train, Round 291: loss=0.1101, accuracy=0.9623, gradient_norm=0.0628, 
[2025-09-17 16:39:33,160][__main__][INFO] - Train, Round 292: loss=0.0367, accuracy=0.9944, gradient_norm=0.1075, 
[2025-09-17 16:39:33,160][__main__][INFO] - Train, Round 293: loss=0.0748, accuracy=0.9788, gradient_norm=0.0912, 
[2025-09-17 16:39:33,160][__main__][INFO] - Train, Round 294: loss=0.0220, accuracy=0.9968, gradient_norm=0.0806, 
[2025-09-17 16:39:33,160][__main__][INFO] - Train, Round 295: loss=0.1318, accuracy=0.9643, gradient_norm=0.1694, 
[2025-09-17 16:39:33,160][__main__][INFO] - Train, Round 296: loss=0.0623, accuracy=0.9845, gradient_norm=0.0951, 
[2025-09-17 16:39:33,160][__main__][INFO] - Train, Round 297: loss=0.0128, accuracy=0.9993, gradient_norm=0.0482, 
[2025-09-17 16:39:33,160][__main__][INFO] - Train, Round 298: loss=0.0135, accuracy=0.9982, gradient_norm=0.0469, 
[2025-09-17 16:39:33,160][__main__][INFO] - Train, Round 299: loss=0.0359, accuracy=0.9932, gradient_norm=0.0946, 
[2025-09-17 16:39:33,160][__main__][INFO] - Train, Round 300: loss=0.1891, accuracy=0.9344, gradient_norm=0.1079, 
[2025-09-17 16:39:33,160][__main__][INFO] - Test, Round 001: loss=2.2937, accuracy=0.1486, 
[2025-09-17 16:39:33,160][__main__][INFO] - Test, Round 002: loss=2.2922, accuracy=0.1294, 
[2025-09-17 16:39:33,160][__main__][INFO] - Test, Round 003: loss=2.2886, accuracy=0.1410, 
[2025-09-17 16:39:33,160][__main__][INFO] - Test, Round 004: loss=2.2824, accuracy=0.1526, 
[2025-09-17 16:39:33,161][__main__][INFO] - Test, Round 005: loss=2.2742, accuracy=0.1532, 
[2025-09-17 16:39:33,161][__main__][INFO] - Test, Round 006: loss=2.2690, accuracy=0.1453, 
[2025-09-17 16:39:33,161][__main__][INFO] - Test, Round 007: loss=2.2620, accuracy=0.1477, 
[2025-09-17 16:39:33,161][__main__][INFO] - Test, Round 008: loss=2.2540, accuracy=0.1583, 
[2025-09-17 16:39:33,161][__main__][INFO] - Test, Round 009: loss=2.2440, accuracy=0.1765, 
[2025-09-17 16:39:33,161][__main__][INFO] - Test, Round 010: loss=2.2510, accuracy=0.1620, 
[2025-09-17 16:39:33,161][__main__][INFO] - Test, Round 011: loss=2.2223, accuracy=0.1798, 
[2025-09-17 16:39:33,161][__main__][INFO] - Test, Round 012: loss=2.2198, accuracy=0.1751, 
[2025-09-17 16:39:33,161][__main__][INFO] - Test, Round 013: loss=2.2263, accuracy=0.1795, 
[2025-09-17 16:39:33,161][__main__][INFO] - Test, Round 014: loss=2.2552, accuracy=0.1782, 
[2025-09-17 16:39:33,161][__main__][INFO] - Test, Round 015: loss=2.2661, accuracy=0.1845, 
[2025-09-17 16:39:33,161][__main__][INFO] - Test, Round 016: loss=2.2773, accuracy=0.1724, 
[2025-09-17 16:39:33,161][__main__][INFO] - Test, Round 017: loss=2.3260, accuracy=0.1662, 
[2025-09-17 16:39:33,161][__main__][INFO] - Test, Round 018: loss=2.3539, accuracy=0.1649, 
[2025-09-17 16:39:33,161][__main__][INFO] - Test, Round 019: loss=2.4351, accuracy=0.1845, 
[2025-09-17 16:39:33,161][__main__][INFO] - Test, Round 020: loss=2.4797, accuracy=0.1741, 
[2025-09-17 16:39:33,161][__main__][INFO] - Test, Round 021: loss=2.5288, accuracy=0.1621, 
[2025-09-17 16:39:33,161][__main__][INFO] - Test, Round 022: loss=2.6296, accuracy=0.1641, 
[2025-09-17 16:39:33,161][__main__][INFO] - Test, Round 023: loss=2.6408, accuracy=0.1987, 
[2025-09-17 16:39:33,161][__main__][INFO] - Test, Round 024: loss=2.7609, accuracy=0.1755, 
[2025-09-17 16:39:33,161][__main__][INFO] - Test, Round 025: loss=2.8352, accuracy=0.1606, 
[2025-09-17 16:39:33,161][__main__][INFO] - Test, Round 026: loss=2.8635, accuracy=0.1783, 
[2025-09-17 16:39:33,161][__main__][INFO] - Test, Round 027: loss=2.9006, accuracy=0.1913, 
[2025-09-17 16:39:33,161][__main__][INFO] - Test, Round 028: loss=2.9265, accuracy=0.1908, 
[2025-09-17 16:39:33,161][__main__][INFO] - Test, Round 029: loss=3.0201, accuracy=0.1850, 
[2025-09-17 16:39:33,161][__main__][INFO] - Test, Round 030: loss=2.9899, accuracy=0.1834, 
[2025-09-17 16:39:33,161][__main__][INFO] - Test, Round 031: loss=2.9920, accuracy=0.1889, 
[2025-09-17 16:39:33,161][__main__][INFO] - Test, Round 032: loss=2.9683, accuracy=0.2060, 
[2025-09-17 16:39:33,161][__main__][INFO] - Test, Round 033: loss=3.0051, accuracy=0.1976, 
[2025-09-17 16:39:33,161][__main__][INFO] - Test, Round 034: loss=3.0409, accuracy=0.1962, 
[2025-09-17 16:39:33,161][__main__][INFO] - Test, Round 035: loss=2.9775, accuracy=0.2183, 
[2025-09-17 16:39:33,161][__main__][INFO] - Test, Round 036: loss=3.0630, accuracy=0.2027, 
[2025-09-17 16:39:33,161][__main__][INFO] - Test, Round 037: loss=3.0621, accuracy=0.2065, 
[2025-09-17 16:39:33,161][__main__][INFO] - Test, Round 038: loss=3.0767, accuracy=0.2067, 
[2025-09-17 16:39:33,161][__main__][INFO] - Test, Round 039: loss=3.0336, accuracy=0.2241, 
[2025-09-17 16:39:33,161][__main__][INFO] - Test, Round 040: loss=3.0806, accuracy=0.2235, 
[2025-09-17 16:39:33,161][__main__][INFO] - Test, Round 041: loss=3.2143, accuracy=0.1874, 
[2025-09-17 16:39:33,161][__main__][INFO] - Test, Round 042: loss=3.0590, accuracy=0.2066, 
[2025-09-17 16:39:33,161][__main__][INFO] - Test, Round 043: loss=3.1374, accuracy=0.1973, 
[2025-09-17 16:39:33,161][__main__][INFO] - Test, Round 044: loss=3.1122, accuracy=0.2097, 
[2025-09-17 16:39:33,161][__main__][INFO] - Test, Round 045: loss=3.1097, accuracy=0.2138, 
[2025-09-17 16:39:33,161][__main__][INFO] - Test, Round 046: loss=3.2299, accuracy=0.2024, 
[2025-09-17 16:39:33,161][__main__][INFO] - Test, Round 047: loss=3.1472, accuracy=0.2135, 
[2025-09-17 16:39:33,162][__main__][INFO] - Test, Round 048: loss=3.1144, accuracy=0.2369, 
[2025-09-17 16:39:33,162][__main__][INFO] - Test, Round 049: loss=3.1496, accuracy=0.2303, 
[2025-09-17 16:39:33,162][__main__][INFO] - Test, Round 050: loss=3.1032, accuracy=0.2539, 
[2025-09-17 16:39:33,162][__main__][INFO] - Test, Round 051: loss=3.1836, accuracy=0.2029, 
[2025-09-17 16:39:33,162][__main__][INFO] - Test, Round 052: loss=3.1165, accuracy=0.2500, 
[2025-09-17 16:39:33,162][__main__][INFO] - Test, Round 053: loss=3.1785, accuracy=0.2192, 
[2025-09-17 16:39:33,162][__main__][INFO] - Test, Round 054: loss=3.2005, accuracy=0.2252, 
[2025-09-17 16:39:33,162][__main__][INFO] - Test, Round 055: loss=3.1725, accuracy=0.2493, 
[2025-09-17 16:39:33,162][__main__][INFO] - Test, Round 056: loss=3.1168, accuracy=0.2607, 
[2025-09-17 16:39:33,162][__main__][INFO] - Test, Round 057: loss=3.1224, accuracy=0.2883, 
[2025-09-17 16:39:33,162][__main__][INFO] - Test, Round 058: loss=3.2336, accuracy=0.2445, 
[2025-09-17 16:39:33,162][__main__][INFO] - Test, Round 059: loss=3.2208, accuracy=0.2409, 
[2025-09-17 16:39:33,162][__main__][INFO] - Test, Round 060: loss=3.2329, accuracy=0.2361, 
[2025-09-17 16:39:33,162][__main__][INFO] - Test, Round 061: loss=3.2126, accuracy=0.2875, 
[2025-09-17 16:39:33,162][__main__][INFO] - Test, Round 062: loss=3.2774, accuracy=0.2255, 
[2025-09-17 16:39:33,162][__main__][INFO] - Test, Round 063: loss=3.3315, accuracy=0.2097, 
[2025-09-17 16:39:33,162][__main__][INFO] - Test, Round 064: loss=3.2844, accuracy=0.2571, 
[2025-09-17 16:39:33,162][__main__][INFO] - Test, Round 065: loss=3.3744, accuracy=0.2402, 
[2025-09-17 16:39:33,162][__main__][INFO] - Test, Round 066: loss=3.2614, accuracy=0.2611, 
[2025-09-17 16:39:33,162][__main__][INFO] - Test, Round 067: loss=3.3301, accuracy=0.2202, 
[2025-09-17 16:39:33,162][__main__][INFO] - Test, Round 068: loss=3.2924, accuracy=0.2484, 
[2025-09-17 16:39:33,162][__main__][INFO] - Test, Round 069: loss=3.3175, accuracy=0.2322, 
[2025-09-17 16:39:33,162][__main__][INFO] - Test, Round 070: loss=3.3376, accuracy=0.2197, 
[2025-09-17 16:39:33,162][__main__][INFO] - Test, Round 071: loss=3.3179, accuracy=0.2552, 
[2025-09-17 16:39:33,162][__main__][INFO] - Test, Round 072: loss=3.3168, accuracy=0.2356, 
[2025-09-17 16:39:33,162][__main__][INFO] - Test, Round 073: loss=3.2959, accuracy=0.2632, 
[2025-09-17 16:39:33,162][__main__][INFO] - Test, Round 074: loss=3.3310, accuracy=0.2365, 
[2025-09-17 16:39:33,162][__main__][INFO] - Test, Round 075: loss=3.3417, accuracy=0.2753, 
[2025-09-17 16:39:33,162][__main__][INFO] - Test, Round 076: loss=3.2705, accuracy=0.2878, 
[2025-09-17 16:39:33,162][__main__][INFO] - Test, Round 077: loss=3.3260, accuracy=0.2988, 
[2025-09-17 16:39:33,162][__main__][INFO] - Test, Round 078: loss=3.4246, accuracy=0.2530, 
[2025-09-17 16:39:33,162][__main__][INFO] - Test, Round 079: loss=3.3965, accuracy=0.2334, 
[2025-09-17 16:39:33,162][__main__][INFO] - Test, Round 080: loss=3.4309, accuracy=0.2639, 
[2025-09-17 16:39:33,162][__main__][INFO] - Test, Round 081: loss=3.4012, accuracy=0.2636, 
[2025-09-17 16:39:33,162][__main__][INFO] - Test, Round 082: loss=3.4581, accuracy=0.2600, 
[2025-09-17 16:39:33,162][__main__][INFO] - Test, Round 083: loss=3.4186, accuracy=0.2719, 
[2025-09-17 16:39:33,162][__main__][INFO] - Test, Round 084: loss=3.5364, accuracy=0.2742, 
[2025-09-17 16:39:33,162][__main__][INFO] - Test, Round 085: loss=3.3509, accuracy=0.3031, 
[2025-09-17 16:39:33,162][__main__][INFO] - Test, Round 086: loss=3.4578, accuracy=0.2737, 
[2025-09-17 16:39:33,162][__main__][INFO] - Test, Round 087: loss=3.4182, accuracy=0.2645, 
[2025-09-17 16:39:33,162][__main__][INFO] - Test, Round 088: loss=3.4663, accuracy=0.2683, 
[2025-09-17 16:39:33,162][__main__][INFO] - Test, Round 089: loss=3.3941, accuracy=0.2773, 
[2025-09-17 16:39:33,162][__main__][INFO] - Test, Round 090: loss=3.4607, accuracy=0.2884, 
[2025-09-17 16:39:33,163][__main__][INFO] - Test, Round 091: loss=3.4459, accuracy=0.2773, 
[2025-09-17 16:39:33,163][__main__][INFO] - Test, Round 092: loss=3.5031, accuracy=0.2595, 
[2025-09-17 16:39:33,163][__main__][INFO] - Test, Round 093: loss=3.4859, accuracy=0.2615, 
[2025-09-17 16:39:33,163][__main__][INFO] - Test, Round 094: loss=3.5995, accuracy=0.2838, 
[2025-09-17 16:39:33,163][__main__][INFO] - Test, Round 095: loss=3.4660, accuracy=0.3057, 
[2025-09-17 16:39:33,163][__main__][INFO] - Test, Round 096: loss=3.4732, accuracy=0.2743, 
[2025-09-17 16:39:33,163][__main__][INFO] - Test, Round 097: loss=3.6570, accuracy=0.2704, 
[2025-09-17 16:39:33,163][__main__][INFO] - Test, Round 098: loss=3.5383, accuracy=0.2870, 
[2025-09-17 16:39:33,163][__main__][INFO] - Test, Round 099: loss=3.5532, accuracy=0.2605, 
[2025-09-17 16:39:33,163][__main__][INFO] - Test, Round 100: loss=3.5117, accuracy=0.2926, 
[2025-09-17 16:39:33,163][__main__][INFO] - Test, Round 101: loss=3.5141, accuracy=0.2630, 
[2025-09-17 16:39:33,163][__main__][INFO] - Test, Round 102: loss=3.5578, accuracy=0.2747, 
[2025-09-17 16:39:33,163][__main__][INFO] - Test, Round 103: loss=2.9058, accuracy=0.2620, 
[2025-09-17 16:39:33,163][__main__][INFO] - Test, Round 104: loss=3.6050, accuracy=0.2985, 
[2025-09-17 16:39:33,163][__main__][INFO] - Test, Round 105: loss=3.6190, accuracy=0.2877, 
[2025-09-17 16:39:33,163][__main__][INFO] - Test, Round 106: loss=3.5615, accuracy=0.2735, 
[2025-09-17 16:39:33,163][__main__][INFO] - Test, Round 107: loss=3.6104, accuracy=0.2766, 
[2025-09-17 16:39:33,163][__main__][INFO] - Test, Round 108: loss=3.5567, accuracy=0.2894, 
[2025-09-17 16:39:33,163][__main__][INFO] - Test, Round 109: loss=3.6216, accuracy=0.2888, 
[2025-09-17 16:39:33,163][__main__][INFO] - Test, Round 110: loss=3.6371, accuracy=0.2882, 
[2025-09-17 16:39:33,163][__main__][INFO] - Test, Round 111: loss=3.6020, accuracy=0.2865, 
[2025-09-17 16:39:33,163][__main__][INFO] - Test, Round 112: loss=3.6457, accuracy=0.2903, 
[2025-09-17 16:39:33,163][__main__][INFO] - Test, Round 113: loss=4.0275, accuracy=0.2841, 
[2025-09-17 16:39:33,163][__main__][INFO] - Test, Round 114: loss=3.5693, accuracy=0.3236, 
[2025-09-17 16:39:33,163][__main__][INFO] - Test, Round 115: loss=3.7037, accuracy=0.2677, 
[2025-09-17 16:39:33,163][__main__][INFO] - Test, Round 116: loss=3.6822, accuracy=0.2959, 
[2025-09-17 16:39:33,163][__main__][INFO] - Test, Round 117: loss=3.6352, accuracy=0.2955, 
[2025-09-17 16:39:33,163][__main__][INFO] - Test, Round 118: loss=3.9516, accuracy=0.2952, 
[2025-09-17 16:39:33,163][__main__][INFO] - Test, Round 119: loss=3.6762, accuracy=0.3027, 
[2025-09-17 16:39:33,163][__main__][INFO] - Test, Round 120: loss=3.6502, accuracy=0.3061, 
[2025-09-17 16:39:33,163][__main__][INFO] - Test, Round 121: loss=2.9009, accuracy=0.3283, 
[2025-09-17 16:39:33,163][__main__][INFO] - Test, Round 122: loss=3.7009, accuracy=0.2934, 
[2025-09-17 16:39:33,163][__main__][INFO] - Test, Round 123: loss=3.7359, accuracy=0.2869, 
[2025-09-17 16:39:33,163][__main__][INFO] - Test, Round 124: loss=3.7201, accuracy=0.2986, 
[2025-09-17 16:39:33,163][__main__][INFO] - Test, Round 125: loss=3.7487, accuracy=0.2917, 
[2025-09-17 16:39:33,163][__main__][INFO] - Test, Round 126: loss=3.6385, accuracy=0.3370, 
[2025-09-17 16:39:33,163][__main__][INFO] - Test, Round 127: loss=3.7609, accuracy=0.3189, 
[2025-09-17 16:39:33,163][__main__][INFO] - Test, Round 128: loss=3.7717, accuracy=0.3017, 
[2025-09-17 16:39:33,163][__main__][INFO] - Test, Round 129: loss=3.7389, accuracy=0.3078, 
[2025-09-17 16:39:33,163][__main__][INFO] - Test, Round 130: loss=3.7991, accuracy=0.3157, 
[2025-09-17 16:39:33,163][__main__][INFO] - Test, Round 131: loss=3.7292, accuracy=0.3089, 
[2025-09-17 16:39:33,163][__main__][INFO] - Test, Round 132: loss=4.0652, accuracy=0.3139, 
[2025-09-17 16:39:33,163][__main__][INFO] - Test, Round 133: loss=3.8197, accuracy=0.3012, 
[2025-09-17 16:39:33,163][__main__][INFO] - Test, Round 134: loss=3.9133, accuracy=0.3216, 
[2025-09-17 16:39:33,163][__main__][INFO] - Test, Round 135: loss=3.8158, accuracy=0.3060, 
[2025-09-17 16:39:33,163][__main__][INFO] - Test, Round 136: loss=3.8271, accuracy=0.2993, 
[2025-09-17 16:39:33,163][__main__][INFO] - Test, Round 137: loss=4.0617, accuracy=0.3161, 
[2025-09-17 16:39:33,163][__main__][INFO] - Test, Round 138: loss=3.8903, accuracy=0.2933, 
[2025-09-17 16:39:33,163][__main__][INFO] - Test, Round 139: loss=3.8341, accuracy=0.3023, 
[2025-09-17 16:39:33,163][__main__][INFO] - Test, Round 140: loss=3.8564, accuracy=0.3080, 
[2025-09-17 16:39:33,163][__main__][INFO] - Test, Round 141: loss=4.0603, accuracy=0.3099, 
[2025-09-17 16:39:33,163][__main__][INFO] - Test, Round 142: loss=4.0110, accuracy=0.2942, 
[2025-09-17 16:39:33,163][__main__][INFO] - Test, Round 143: loss=4.0568, accuracy=0.3227, 
[2025-09-17 16:39:33,163][__main__][INFO] - Test, Round 144: loss=4.0483, accuracy=0.3243, 
[2025-09-17 16:39:33,163][__main__][INFO] - Test, Round 145: loss=4.0328, accuracy=0.2831, 
[2025-09-17 16:39:33,163][__main__][INFO] - Test, Round 146: loss=4.1092, accuracy=0.3148, 
[2025-09-17 16:39:33,163][__main__][INFO] - Test, Round 147: loss=4.1329, accuracy=0.3053, 
[2025-09-17 16:39:33,164][__main__][INFO] - Test, Round 148: loss=4.0850, accuracy=0.3014, 
[2025-09-17 16:39:33,164][__main__][INFO] - Test, Round 149: loss=3.9489, accuracy=0.2962, 
[2025-09-17 16:39:33,164][__main__][INFO] - Test, Round 150: loss=4.2457, accuracy=0.2993, 
[2025-09-17 16:39:33,164][__main__][INFO] - Test, Round 151: loss=4.1022, accuracy=0.3076, 
[2025-09-17 16:39:33,164][__main__][INFO] - Test, Round 152: loss=4.0881, accuracy=0.2998, 
[2025-09-17 16:39:33,164][__main__][INFO] - Test, Round 153: loss=4.3301, accuracy=0.2930, 
[2025-09-17 16:39:33,164][__main__][INFO] - Test, Round 154: loss=4.2762, accuracy=0.3170, 
[2025-09-17 16:39:33,164][__main__][INFO] - Test, Round 155: loss=4.0657, accuracy=0.3236, 
[2025-09-17 16:39:33,164][__main__][INFO] - Test, Round 156: loss=4.5580, accuracy=0.3123, 
[2025-09-17 16:39:33,164][__main__][INFO] - Test, Round 157: loss=4.4860, accuracy=0.3200, 
[2025-09-17 16:39:33,164][__main__][INFO] - Test, Round 158: loss=4.0542, accuracy=0.3213, 
[2025-09-17 16:39:33,164][__main__][INFO] - Test, Round 159: loss=4.3538, accuracy=0.3081, 
[2025-09-17 16:39:33,164][__main__][INFO] - Test, Round 160: loss=4.3166, accuracy=0.3207, 
[2025-09-17 16:39:33,164][__main__][INFO] - Test, Round 161: loss=4.3923, accuracy=0.3300, 
[2025-09-17 16:39:33,164][__main__][INFO] - Test, Round 162: loss=4.4097, accuracy=0.3370, 
[2025-09-17 16:39:33,164][__main__][INFO] - Test, Round 163: loss=4.5000, accuracy=0.3210, 
[2025-09-17 16:39:33,164][__main__][INFO] - Test, Round 164: loss=4.5270, accuracy=0.3152, 
[2025-09-17 16:39:33,164][__main__][INFO] - Test, Round 165: loss=4.5002, accuracy=0.2954, 
[2025-09-17 16:39:33,164][__main__][INFO] - Test, Round 166: loss=4.3218, accuracy=0.2908, 
[2025-09-17 16:39:33,164][__main__][INFO] - Test, Round 167: loss=4.6752, accuracy=0.3352, 
[2025-09-17 16:39:33,164][__main__][INFO] - Test, Round 168: loss=4.6281, accuracy=0.3190, 
[2025-09-17 16:39:33,164][__main__][INFO] - Test, Round 169: loss=4.2645, accuracy=0.3179, 
[2025-09-17 16:39:33,164][__main__][INFO] - Test, Round 170: loss=4.1797, accuracy=0.3167, 
[2025-09-17 16:39:33,164][__main__][INFO] - Test, Round 171: loss=4.5496, accuracy=0.3330, 
[2025-09-17 16:39:33,164][__main__][INFO] - Test, Round 172: loss=4.7936, accuracy=0.3185, 
[2025-09-17 16:39:33,164][__main__][INFO] - Test, Round 173: loss=4.3014, accuracy=0.3181, 
[2025-09-17 16:39:33,164][__main__][INFO] - Test, Round 174: loss=4.3994, accuracy=0.3147, 
[2025-09-17 16:39:33,164][__main__][INFO] - Test, Round 175: loss=4.4507, accuracy=0.3258, 
[2025-09-17 16:39:33,164][__main__][INFO] - Test, Round 176: loss=4.4217, accuracy=0.3257, 
[2025-09-17 16:39:33,164][__main__][INFO] - Test, Round 177: loss=4.2458, accuracy=0.3349, 
[2025-09-17 16:39:33,164][__main__][INFO] - Test, Round 178: loss=4.9111, accuracy=0.3068, 
[2025-09-17 16:39:33,164][__main__][INFO] - Test, Round 179: loss=4.6169, accuracy=0.3455, 
[2025-09-17 16:39:33,164][__main__][INFO] - Test, Round 180: loss=4.2870, accuracy=0.3455, 
[2025-09-17 16:39:33,164][__main__][INFO] - Test, Round 181: loss=4.4317, accuracy=0.3243, 
[2025-09-17 16:39:33,164][__main__][INFO] - Test, Round 182: loss=4.5915, accuracy=0.3333, 
[2025-09-17 16:39:33,164][__main__][INFO] - Test, Round 183: loss=4.5395, accuracy=0.3246, 
[2025-09-17 16:39:33,164][__main__][INFO] - Test, Round 184: loss=4.5918, accuracy=0.3061, 
[2025-09-17 16:39:33,165][__main__][INFO] - Test, Round 185: loss=4.6514, accuracy=0.3188, 
[2025-09-17 16:39:33,165][__main__][INFO] - Test, Round 186: loss=4.6143, accuracy=0.3099, 
[2025-09-17 16:39:33,165][__main__][INFO] - Test, Round 187: loss=4.6363, accuracy=0.3309, 
[2025-09-17 16:39:33,165][__main__][INFO] - Test, Round 188: loss=4.7162, accuracy=0.3228, 
[2025-09-17 16:39:33,165][__main__][INFO] - Test, Round 189: loss=4.6525, accuracy=0.3290, 
[2025-09-17 16:39:33,165][__main__][INFO] - Test, Round 190: loss=5.0855, accuracy=0.3147, 
[2025-09-17 16:39:33,165][__main__][INFO] - Test, Round 191: loss=4.6856, accuracy=0.3300, 
[2025-09-17 16:39:33,165][__main__][INFO] - Test, Round 192: loss=4.5196, accuracy=0.3173, 
[2025-09-17 16:39:33,165][__main__][INFO] - Test, Round 193: loss=4.9879, accuracy=0.3230, 
[2025-09-17 16:39:33,165][__main__][INFO] - Test, Round 194: loss=5.1923, accuracy=0.3240, 
[2025-09-17 16:39:33,165][__main__][INFO] - Test, Round 195: loss=5.2290, accuracy=0.3169, 
[2025-09-17 16:39:33,165][__main__][INFO] - Test, Round 196: loss=5.1419, accuracy=0.3218, 
[2025-09-17 16:39:33,165][__main__][INFO] - Test, Round 197: loss=5.1133, accuracy=0.3146, 
[2025-09-17 16:39:33,165][__main__][INFO] - Test, Round 198: loss=4.7843, accuracy=0.3419, 
[2025-09-17 16:39:33,165][__main__][INFO] - Test, Round 199: loss=5.2349, accuracy=0.3119, 
[2025-09-17 16:39:33,165][__main__][INFO] - Test, Round 200: loss=5.3670, accuracy=0.3370, 
[2025-09-17 16:39:33,165][__main__][INFO] - Test, Round 201: loss=5.3883, accuracy=0.3287, 
[2025-09-17 16:39:33,165][__main__][INFO] - Test, Round 202: loss=5.5075, accuracy=0.2913, 
[2025-09-17 16:39:33,165][__main__][INFO] - Test, Round 203: loss=5.0132, accuracy=0.3167, 
[2025-09-17 16:39:33,165][__main__][INFO] - Test, Round 204: loss=5.2626, accuracy=0.3203, 
[2025-09-17 16:39:33,165][__main__][INFO] - Test, Round 205: loss=5.1203, accuracy=0.3406, 
[2025-09-17 16:39:33,165][__main__][INFO] - Test, Round 206: loss=5.0960, accuracy=0.3279, 
[2025-09-17 16:39:33,165][__main__][INFO] - Test, Round 207: loss=4.8044, accuracy=0.3348, 
[2025-09-17 16:39:33,165][__main__][INFO] - Test, Round 208: loss=5.4419, accuracy=0.3282, 
[2025-09-17 16:39:33,165][__main__][INFO] - Test, Round 209: loss=5.3419, accuracy=0.3300, 
[2025-09-17 16:39:33,165][__main__][INFO] - Test, Round 210: loss=4.9380, accuracy=0.3312, 
[2025-09-17 16:39:33,165][__main__][INFO] - Test, Round 211: loss=5.4592, accuracy=0.3382, 
[2025-09-17 16:39:33,165][__main__][INFO] - Test, Round 212: loss=5.0778, accuracy=0.3194, 
[2025-09-17 16:39:33,165][__main__][INFO] - Test, Round 213: loss=6.0200, accuracy=0.3030, 
[2025-09-17 16:39:33,165][__main__][INFO] - Test, Round 214: loss=5.4906, accuracy=0.3151, 
[2025-09-17 16:39:33,165][__main__][INFO] - Test, Round 215: loss=5.6238, accuracy=0.3162, 
[2025-09-17 16:39:33,165][__main__][INFO] - Test, Round 216: loss=5.7742, accuracy=0.3243, 
[2025-09-17 16:39:33,165][__main__][INFO] - Test, Round 217: loss=5.5050, accuracy=0.3430, 
[2025-09-17 16:39:33,165][__main__][INFO] - Test, Round 218: loss=5.5834, accuracy=0.3373, 
[2025-09-17 16:39:33,165][__main__][INFO] - Test, Round 219: loss=5.8451, accuracy=0.3152, 
[2025-09-17 16:39:33,166][__main__][INFO] - Test, Round 220: loss=5.4357, accuracy=0.3361, 
[2025-09-17 16:39:33,166][__main__][INFO] - Test, Round 221: loss=5.7007, accuracy=0.3337, 
[2025-09-17 16:39:33,166][__main__][INFO] - Test, Round 222: loss=5.9916, accuracy=0.3194, 
[2025-09-17 16:39:33,166][__main__][INFO] - Test, Round 223: loss=5.7278, accuracy=0.3510, 
[2025-09-17 16:39:33,166][__main__][INFO] - Test, Round 224: loss=5.6645, accuracy=0.3312, 
[2025-09-17 16:39:33,166][__main__][INFO] - Test, Round 225: loss=5.7536, accuracy=0.3533, 
[2025-09-17 16:39:33,166][__main__][INFO] - Test, Round 226: loss=5.9389, accuracy=0.3397, 
[2025-09-17 16:39:33,166][__main__][INFO] - Test, Round 227: loss=5.9572, accuracy=0.3364, 
[2025-09-17 16:39:33,166][__main__][INFO] - Test, Round 228: loss=5.4851, accuracy=0.3342, 
[2025-09-17 16:39:33,166][__main__][INFO] - Test, Round 229: loss=5.3159, accuracy=0.3199, 
[2025-09-17 16:39:33,166][__main__][INFO] - Test, Round 230: loss=5.6351, accuracy=0.3251, 
[2025-09-17 16:39:33,166][__main__][INFO] - Test, Round 231: loss=5.5656, accuracy=0.3367, 
[2025-09-17 16:39:33,166][__main__][INFO] - Test, Round 232: loss=5.2138, accuracy=0.3318, 
[2025-09-17 16:39:33,166][__main__][INFO] - Test, Round 233: loss=6.0610, accuracy=0.3142, 
[2025-09-17 16:39:33,166][__main__][INFO] - Test, Round 234: loss=5.8741, accuracy=0.3339, 
[2025-09-17 16:39:33,166][__main__][INFO] - Test, Round 235: loss=5.7674, accuracy=0.3481, 
[2025-09-17 16:39:33,166][__main__][INFO] - Test, Round 236: loss=5.9048, accuracy=0.3472, 
[2025-09-17 16:39:33,166][__main__][INFO] - Test, Round 237: loss=6.0899, accuracy=0.3382, 
[2025-09-17 16:39:33,166][__main__][INFO] - Test, Round 238: loss=6.0144, accuracy=0.3285, 
[2025-09-17 16:39:33,166][__main__][INFO] - Test, Round 239: loss=5.9608, accuracy=0.3254, 
[2025-09-17 16:39:33,166][__main__][INFO] - Test, Round 240: loss=5.8819, accuracy=0.3285, 
[2025-09-17 16:39:33,166][__main__][INFO] - Test, Round 241: loss=6.0074, accuracy=0.3352, 
[2025-09-17 16:39:33,166][__main__][INFO] - Test, Round 242: loss=5.9921, accuracy=0.3323, 
[2025-09-17 16:39:33,166][__main__][INFO] - Test, Round 243: loss=6.2576, accuracy=0.3051, 
[2025-09-17 16:39:33,166][__main__][INFO] - Test, Round 244: loss=5.8276, accuracy=0.3266, 
[2025-09-17 16:39:33,166][__main__][INFO] - Test, Round 245: loss=6.3779, accuracy=0.3617, 
[2025-09-17 16:39:33,166][__main__][INFO] - Test, Round 246: loss=6.2305, accuracy=0.3246, 
[2025-09-17 16:39:33,166][__main__][INFO] - Test, Round 247: loss=6.1544, accuracy=0.3257, 
[2025-09-17 16:39:33,166][__main__][INFO] - Test, Round 248: loss=6.1074, accuracy=0.3330, 
[2025-09-17 16:39:33,166][__main__][INFO] - Test, Round 249: loss=5.9088, accuracy=0.3373, 
[2025-09-17 16:39:33,166][__main__][INFO] - Test, Round 250: loss=6.4360, accuracy=0.3167, 
[2025-09-17 16:39:33,166][__main__][INFO] - Test, Round 251: loss=6.1017, accuracy=0.3470, 
[2025-09-17 16:39:33,166][__main__][INFO] - Test, Round 252: loss=6.3465, accuracy=0.3531, 
[2025-09-17 16:39:33,166][__main__][INFO] - Test, Round 253: loss=6.2126, accuracy=0.3294, 
[2025-09-17 16:39:33,166][__main__][INFO] - Test, Round 254: loss=6.8947, accuracy=0.3380, 
[2025-09-17 16:39:33,166][__main__][INFO] - Test, Round 255: loss=6.5375, accuracy=0.3509, 
[2025-09-17 16:39:33,167][__main__][INFO] - Test, Round 256: loss=6.6009, accuracy=0.3336, 
[2025-09-17 16:39:33,167][__main__][INFO] - Test, Round 257: loss=6.2347, accuracy=0.3428, 
[2025-09-17 16:39:33,167][__main__][INFO] - Test, Round 258: loss=6.2454, accuracy=0.3167, 
[2025-09-17 16:39:33,167][__main__][INFO] - Test, Round 259: loss=6.5555, accuracy=0.3264, 
[2025-09-17 16:39:33,167][__main__][INFO] - Test, Round 260: loss=6.1416, accuracy=0.3394, 
[2025-09-17 16:39:33,167][__main__][INFO] - Test, Round 261: loss=6.2671, accuracy=0.3515, 
[2025-09-17 16:39:33,167][__main__][INFO] - Test, Round 262: loss=6.9054, accuracy=0.3167, 
[2025-09-17 16:39:33,167][__main__][INFO] - Test, Round 263: loss=6.8875, accuracy=0.3176, 
[2025-09-17 16:39:33,167][__main__][INFO] - Test, Round 264: loss=6.4812, accuracy=0.3406, 
[2025-09-17 16:39:33,167][__main__][INFO] - Test, Round 265: loss=6.5802, accuracy=0.3115, 
[2025-09-17 16:39:33,167][__main__][INFO] - Test, Round 266: loss=6.4879, accuracy=0.3388, 
[2025-09-17 16:39:33,167][__main__][INFO] - Test, Round 267: loss=6.6534, accuracy=0.3376, 
[2025-09-17 16:39:33,167][__main__][INFO] - Test, Round 268: loss=6.0596, accuracy=0.3348, 
[2025-09-17 16:39:33,167][__main__][INFO] - Test, Round 269: loss=6.7333, accuracy=0.3460, 
[2025-09-17 16:39:33,167][__main__][INFO] - Test, Round 270: loss=7.1134, accuracy=0.3345, 
[2025-09-17 16:39:33,167][__main__][INFO] - Test, Round 271: loss=6.3876, accuracy=0.3416, 
[2025-09-17 16:39:33,167][__main__][INFO] - Test, Round 272: loss=6.8823, accuracy=0.3517, 
[2025-09-17 16:39:33,167][__main__][INFO] - Test, Round 273: loss=6.6769, accuracy=0.3221, 
[2025-09-17 16:39:33,167][__main__][INFO] - Test, Round 274: loss=7.4014, accuracy=0.3303, 
[2025-09-17 16:39:33,167][__main__][INFO] - Test, Round 275: loss=6.5328, accuracy=0.3355, 
[2025-09-17 16:39:33,167][__main__][INFO] - Test, Round 276: loss=6.8677, accuracy=0.3374, 
[2025-09-17 16:39:33,167][__main__][INFO] - Test, Round 277: loss=6.6303, accuracy=0.3534, 
[2025-09-17 16:39:33,167][__main__][INFO] - Test, Round 278: loss=6.4988, accuracy=0.3185, 
[2025-09-17 16:39:33,167][__main__][INFO] - Test, Round 279: loss=6.9671, accuracy=0.3330, 
[2025-09-17 16:39:33,167][__main__][INFO] - Test, Round 280: loss=7.0663, accuracy=0.3497, 
[2025-09-17 16:39:33,167][__main__][INFO] - Test, Round 281: loss=6.5989, accuracy=0.3509, 
[2025-09-17 16:39:33,167][__main__][INFO] - Test, Round 282: loss=6.8493, accuracy=0.3382, 
[2025-09-17 16:39:33,167][__main__][INFO] - Test, Round 283: loss=6.7655, accuracy=0.3361, 
[2025-09-17 16:39:33,167][__main__][INFO] - Test, Round 284: loss=6.2877, accuracy=0.3509, 
[2025-09-17 16:39:33,167][__main__][INFO] - Test, Round 285: loss=6.4249, accuracy=0.3309, 
[2025-09-17 16:39:33,167][__main__][INFO] - Test, Round 286: loss=6.8541, accuracy=0.3560, 
[2025-09-17 16:39:33,167][__main__][INFO] - Test, Round 287: loss=7.2794, accuracy=0.3474, 
[2025-09-17 16:39:33,167][__main__][INFO] - Test, Round 288: loss=6.9926, accuracy=0.3351, 
[2025-09-17 16:39:33,167][__main__][INFO] - Test, Round 289: loss=7.0540, accuracy=0.3485, 
[2025-09-17 16:39:33,167][__main__][INFO] - Test, Round 290: loss=7.2723, accuracy=0.3355, 
[2025-09-17 16:39:33,168][__main__][INFO] - Test, Round 291: loss=7.0850, accuracy=0.3517, 
[2025-09-17 16:39:33,168][__main__][INFO] - Test, Round 292: loss=6.8926, accuracy=0.3494, 
[2025-09-17 16:39:33,168][__main__][INFO] - Test, Round 293: loss=7.1648, accuracy=0.3382, 
[2025-09-17 16:39:33,168][__main__][INFO] - Test, Round 294: loss=7.1973, accuracy=0.3415, 
[2025-09-17 16:39:33,168][__main__][INFO] - Test, Round 295: loss=6.5570, accuracy=0.3363, 
[2025-09-17 16:39:33,168][__main__][INFO] - Test, Round 296: loss=6.8957, accuracy=0.3540, 
[2025-09-17 16:39:33,168][__main__][INFO] - Test, Round 297: loss=7.3541, accuracy=0.3257, 
[2025-09-17 16:39:33,168][__main__][INFO] - Test, Round 298: loss=7.3739, accuracy=0.3439, 
[2025-09-17 16:39:33,168][__main__][INFO] - Test, Round 299: loss=7.1965, accuracy=0.3391, 
[2025-09-17 16:39:33,168][__main__][INFO] - Test, Round 300: loss=7.0430, accuracy=0.3297, 
