[2025-09-17 16:22:34,081][flp2p.graph_runner][INFO] - Train, Round 0 : loss => 2.259156232992808,  accuracy: 0.19324444444444447, gradient_norm : 0.2499771963117239
[2025-09-17 16:22:37,888][flp2p.graph_runner][INFO] - Test, Round 0 : loss => 2.139335916758014,  accuracy: 0.26007619811509924
[2025-09-17 16:22:52,482][flp2p.graph_runner][INFO] - Train, Round 1 : loss => 2.028600671728452,  accuracy: 0.30358888888888885, gradient_norm : 0.4552354982161855
[2025-09-17 16:22:56,207][flp2p.graph_runner][INFO] - Test, Round 1 : loss => 1.6874782800793886,  accuracy: 0.3981963927855711
[2025-09-17 16:23:10,881][flp2p.graph_runner][INFO] - Train, Round 2 : loss => 1.7683475338220596,  accuracy: 0.37585555555555555, gradient_norm : 0.49125296413189035
[2025-09-17 16:23:14,744][flp2p.graph_runner][INFO] - Test, Round 2 : loss => 1.4519196215743078,  accuracy: 0.4798637547585654
[2025-09-17 16:23:29,294][flp2p.graph_runner][INFO] - Train, Round 3 : loss => 1.471197478055954,  accuracy: 0.4735000000000001, gradient_norm : 0.47031356582826417
[2025-09-17 16:23:33,093][flp2p.graph_runner][INFO] - Test, Round 3 : loss => 1.2796355688261365,  accuracy: 0.530060120240481
[2025-09-17 16:23:48,044][flp2p.graph_runner][INFO] - Train, Round 4 : loss => 1.3100476859052197,  accuracy: 0.5276470588235296, gradient_norm : 0.4658989457616731
[2025-09-17 16:23:51,975][flp2p.graph_runner][INFO] - Test, Round 4 : loss => 1.14501815271429,  accuracy: 0.5686543742644175
[2025-09-17 16:24:07,079][flp2p.graph_runner][INFO] - Train, Round 5 : loss => 1.0926425129954331,  accuracy: 0.5898148148148148, gradient_norm : 0.41398947899309074
[2025-09-17 16:24:10,996][flp2p.graph_runner][INFO] - Test, Round 5 : loss => 1.059852039624179,  accuracy: 0.6072341262040495
[2025-09-17 16:24:25,757][flp2p.graph_runner][INFO] - Train, Round 6 : loss => 1.0196045755346617,  accuracy: 0.6165666666666668, gradient_norm : 0.3609813740964604
[2025-09-17 16:24:29,626][flp2p.graph_runner][INFO] - Test, Round 6 : loss => 1.0039033347801816,  accuracy: 0.6191047162270183
[2025-09-17 16:24:44,477][flp2p.graph_runner][INFO] - Train, Round 7 : loss => 0.8642033944527309,  accuracy: 0.670021786492375, gradient_norm : 0.3245239807542525
[2025-09-17 16:24:48,458][flp2p.graph_runner][INFO] - Test, Round 7 : loss => 0.9266353331817607,  accuracy: 0.6535216794192662
[2025-09-17 16:25:03,133][flp2p.graph_runner][INFO] - Train, Round 8 : loss => 0.8456457983752091,  accuracy: 0.6749666666666667, gradient_norm : 0.30095103665306894
[2025-09-17 16:25:06,996][flp2p.graph_runner][INFO] - Test, Round 8 : loss => 0.9246713260347992,  accuracy: 0.6559182855998398
[2025-09-17 16:25:21,992][flp2p.graph_runner][INFO] - Train, Round 9 : loss => 0.7776929334682576,  accuracy: 0.698202614379085, gradient_norm : 0.2888199992153252
[2025-09-17 16:25:25,898][flp2p.graph_runner][INFO] - Test, Round 9 : loss => 0.9005189451499882,  accuracy: 0.6652284761717984
[2025-09-17 16:25:40,697][flp2p.graph_runner][INFO] - Train, Round 10 : loss => 0.73180131945014,  accuracy: 0.7154777777777778, gradient_norm : 0.2670638926095948
[2025-09-17 16:25:44,573][flp2p.graph_runner][INFO] - Test, Round 10 : loss => 0.8519110453788179,  accuracy: 0.6845047923322684
[2025-09-17 16:25:59,539][flp2p.graph_runner][INFO] - Train, Round 11 : loss => 0.7204884269560864,  accuracy: 0.7188888888888888, gradient_norm : 0.26705943158260753
[2025-09-17 16:26:03,482][flp2p.graph_runner][INFO] - Test, Round 11 : loss => 0.8315951795444431,  accuracy: 0.6856357927786499
[2025-09-17 16:26:18,104][flp2p.graph_runner][INFO] - Train, Round 12 : loss => 0.6433208877195915,  accuracy: 0.7482888888888888, gradient_norm : 0.2534822793935292
[2025-09-17 16:26:21,960][flp2p.graph_runner][INFO] - Test, Round 12 : loss => 0.8254136521808849,  accuracy: 0.6952514526147064
[2025-09-17 16:26:36,843][flp2p.graph_runner][INFO] - Train, Round 13 : loss => 0.640500069521611,  accuracy: 0.7499019607843135, gradient_norm : 0.24153202365165452
[2025-09-17 16:26:40,772][flp2p.graph_runner][INFO] - Test, Round 13 : loss => 0.823280012276567,  accuracy: 0.695150567070786
[2025-09-17 16:26:55,876][flp2p.graph_runner][INFO] - Train, Round 14 : loss => 0.6089324556729373,  accuracy: 0.7611002178649239, gradient_norm : 0.2364471520434475
[2025-09-17 16:26:59,809][flp2p.graph_runner][INFO] - Test, Round 14 : loss => 0.7935132856852356,  accuracy: 0.7085131424087877
[2025-09-17 16:27:14,860][flp2p.graph_runner][INFO] - Train, Round 15 : loss => 0.5666763027996019,  accuracy: 0.7783877995642702, gradient_norm : 0.22265453695139828
[2025-09-17 16:27:18,804][flp2p.graph_runner][INFO] - Test, Round 15 : loss => 0.7766549984224176,  accuracy: 0.7150706436420722
[2025-09-17 16:27:33,645][flp2p.graph_runner][INFO] - Train, Round 16 : loss => 0.5475871424078942,  accuracy: 0.7833777777777777, gradient_norm : 0.2245990196924369
[2025-09-17 16:27:37,513][flp2p.graph_runner][INFO] - Test, Round 16 : loss => 0.7774373256632543,  accuracy: 0.7220108151411977
[2025-09-17 16:27:52,587][flp2p.graph_runner][INFO] - Train, Round 17 : loss => 0.5449075873332475,  accuracy: 0.7841285403050109, gradient_norm : 0.21596174683779085
[2025-09-17 16:27:56,522][flp2p.graph_runner][INFO] - Test, Round 17 : loss => 0.779331255498097,  accuracy: 0.7271479011377011
[2025-09-17 16:28:11,301][flp2p.graph_runner][INFO] - Train, Round 18 : loss => 0.5091795103351275,  accuracy: 0.7998000000000003, gradient_norm : 0.21621214409274295
[2025-09-17 16:28:15,197][flp2p.graph_runner][INFO] - Test, Round 18 : loss => 0.7963753396236861,  accuracy: 0.7193193193193194
[2025-09-17 16:28:30,270][flp2p.graph_runner][INFO] - Train, Round 19 : loss => 0.5040941536962207,  accuracy: 0.8011437908496731, gradient_norm : 0.21735460967204193
[2025-09-17 16:28:34,194][flp2p.graph_runner][INFO] - Test, Round 19 : loss => 0.767408665187121,  accuracy: 0.7231040564373897
[2025-09-17 16:28:49,203][flp2p.graph_runner][INFO] - Train, Round 20 : loss => 0.48799604348887027,  accuracy: 0.8079520697167757, gradient_norm : 0.21591683269451106
[2025-09-17 16:28:53,162][flp2p.graph_runner][INFO] - Test, Round 20 : loss => 0.7507479246490161,  accuracy: 0.7202275848538355
[2025-09-17 16:29:08,200][flp2p.graph_runner][INFO] - Train, Round 21 : loss => 0.4664948380460926,  accuracy: 0.8156644880174294, gradient_norm : 0.2211325554993162
[2025-09-17 16:29:12,085][flp2p.graph_runner][INFO] - Test, Round 21 : loss => 0.7645188867198818,  accuracy: 0.7285378283026264
[2025-09-17 16:29:27,065][flp2p.graph_runner][INFO] - Train, Round 22 : loss => 0.4525261054923332,  accuracy: 0.8226470588235296, gradient_norm : 0.2183406003266419
[2025-09-17 16:29:30,977][flp2p.graph_runner][INFO] - Test, Round 22 : loss => 0.7461358680299717,  accuracy: 0.7434390912651783
[2025-09-17 16:29:45,994][flp2p.graph_runner][INFO] - Train, Round 23 : loss => 0.44963504107169855,  accuracy: 0.8228867102396514, gradient_norm : 0.21570913213588946
[2025-09-17 16:29:49,961][flp2p.graph_runner][INFO] - Test, Round 23 : loss => 0.772373530619938,  accuracy: 0.7234751912139635
[2025-09-17 16:30:04,727][flp2p.graph_runner][INFO] - Train, Round 24 : loss => 0.40066502018024525,  accuracy: 0.8416777777777776, gradient_norm : 0.2103589027146743
[2025-09-17 16:30:08,562][flp2p.graph_runner][INFO] - Test, Round 24 : loss => 0.7622065198939053,  accuracy: 0.7382953181272509
[2025-09-17 16:30:23,221][flp2p.graph_runner][INFO] - Train, Round 25 : loss => 0.4033677294701338,  accuracy: 0.8409888888888891, gradient_norm : 0.20315061593046338
[2025-09-17 16:30:27,090][flp2p.graph_runner][INFO] - Test, Round 25 : loss => 0.7575304247700043,  accuracy: 0.7406368916483076
[2025-09-17 16:30:41,764][flp2p.graph_runner][INFO] - Train, Round 26 : loss => 0.3823083639939626,  accuracy: 0.8513111111111116, gradient_norm : 0.20433776785091656
[2025-09-17 16:30:45,581][flp2p.graph_runner][INFO] - Test, Round 26 : loss => 0.7226113876462436,  accuracy: 0.7532519511707024
[2025-09-17 16:31:00,355][flp2p.graph_runner][INFO] - Train, Round 27 : loss => 0.3726266467000047,  accuracy: 0.8566333333333335, gradient_norm : 0.2006770962932454
[2025-09-17 16:31:04,191][flp2p.graph_runner][INFO] - Test, Round 27 : loss => 0.7515208929544548,  accuracy: 0.7496497898739244
[2025-09-17 16:31:19,310][flp2p.graph_runner][INFO] - Train, Round 28 : loss => 0.3596261539161692,  accuracy: 0.8609803921568626, gradient_norm : 0.20146192927586148
[2025-09-17 16:31:23,189][flp2p.graph_runner][INFO] - Test, Round 28 : loss => 0.7714772189670523,  accuracy: 0.7497059976479812
[2025-09-17 16:31:37,909][flp2p.graph_runner][INFO] - Train, Round 29 : loss => 0.33304739380131165,  accuracy: 0.8696555555555556, gradient_norm : 0.1990733890599982
[2025-09-17 16:31:41,759][flp2p.graph_runner][INFO] - Test, Round 29 : loss => 0.7551914786359705,  accuracy: 0.7586551931158695
[2025-09-17 16:31:56,483][flp2p.graph_runner][INFO] - Train, Round 30 : loss => 0.330959961189578,  accuracy: 0.8731666666666666, gradient_norm : 0.20378991201614813
[2025-09-17 16:32:00,248][flp2p.graph_runner][INFO] - Test, Round 30 : loss => 0.7853389704326431,  accuracy: 0.7472989195678271
[2025-09-17 16:32:15,033][flp2p.graph_runner][INFO] - Train, Round 31 : loss => 0.31646636076644064,  accuracy: 0.8791111111111111, gradient_norm : 0.20426508936298354
[2025-09-17 16:32:18,884][flp2p.graph_runner][INFO] - Test, Round 31 : loss => 0.7900776394707761,  accuracy: 0.7471977582065652
[2025-09-17 16:32:33,957][flp2p.graph_runner][INFO] - Train, Round 32 : loss => 0.3042389674139593,  accuracy: 0.8822984749455339, gradient_norm : 0.2056793654618218
[2025-09-17 16:32:37,861][flp2p.graph_runner][INFO] - Test, Round 32 : loss => 0.7965405350375288,  accuracy: 0.75
[2025-09-17 16:32:52,592][flp2p.graph_runner][INFO] - Train, Round 33 : loss => 0.2975439586689075,  accuracy: 0.8852444444444445, gradient_norm : 0.19859749566845436
[2025-09-17 16:32:56,469][flp2p.graph_runner][INFO] - Test, Round 33 : loss => 0.8047828620972454,  accuracy: 0.7491505096941835
[2025-09-17 16:33:11,525][flp2p.graph_runner][INFO] - Train, Round 34 : loss => 0.2686906614846069,  accuracy: 0.8975490196078431, gradient_norm : 0.18949058227565524
[2025-09-17 16:33:15,457][flp2p.graph_runner][INFO] - Test, Round 34 : loss => 0.7992952506161073,  accuracy: 0.7637470542026709
[2025-09-17 16:33:30,150][flp2p.graph_runner][INFO] - Train, Round 35 : loss => 0.2641005798947687,  accuracy: 0.9004222222222221, gradient_norm : 0.19177911163340575
[2025-09-17 16:33:34,015][flp2p.graph_runner][INFO] - Test, Round 35 : loss => 0.8036249425204122,  accuracy: 0.757297081167533
[2025-09-17 16:33:49,122][flp2p.graph_runner][INFO] - Train, Round 36 : loss => 0.25393016118802786,  accuracy: 0.9049019607843137, gradient_norm : 0.19298969777698705
[2025-09-17 16:33:53,067][flp2p.graph_runner][INFO] - Test, Round 36 : loss => 0.8222447389571228,  accuracy: 0.7533843437316068
[2025-09-17 16:34:07,787][flp2p.graph_runner][INFO] - Train, Round 37 : loss => 0.24710937265741328,  accuracy: 0.9058666666666668, gradient_norm : 0.19068247121604934
[2025-09-17 16:34:11,667][flp2p.graph_runner][INFO] - Test, Round 37 : loss => 0.8124142707397962,  accuracy: 0.7604250200481155
[2025-09-17 16:34:26,440][flp2p.graph_runner][INFO] - Train, Round 38 : loss => 0.22743963118207952,  accuracy: 0.916222222222222, gradient_norm : 0.19043628841973098
[2025-09-17 16:34:30,250][flp2p.graph_runner][INFO] - Test, Round 38 : loss => 0.808102986837402,  accuracy: 0.7614568741244747
[2025-09-17 16:34:45,030][flp2p.graph_runner][INFO] - Train, Round 39 : loss => 0.22596857093585035,  accuracy: 0.9164111111111113, gradient_norm : 0.19001778658760418
[2025-09-17 16:34:48,896][flp2p.graph_runner][INFO] - Test, Round 39 : loss => 0.816487053398813,  accuracy: 0.7553870710295292
[2025-09-17 16:35:03,643][flp2p.graph_runner][INFO] - Train, Round 40 : loss => 0.21079626723813513,  accuracy: 0.920322222222222, gradient_norm : 0.187450199774877
[2025-09-17 16:35:07,523][flp2p.graph_runner][INFO] - Test, Round 40 : loss => 0.8503904693761095,  accuracy: 0.7556977209116353
[2025-09-17 16:35:22,526][flp2p.graph_runner][INFO] - Train, Round 41 : loss => 0.2029033756116405,  accuracy: 0.9261873638344227, gradient_norm : 0.18482013005021705
[2025-09-17 16:35:26,444][flp2p.graph_runner][INFO] - Test, Round 41 : loss => 0.8503245791625356,  accuracy: 0.7640031335683509
[2025-09-17 16:35:41,424][flp2p.graph_runner][INFO] - Train, Round 42 : loss => 0.17871949484745298,  accuracy: 0.9353485838779958, gradient_norm : 0.18013588820460336
[2025-09-17 16:35:45,358][flp2p.graph_runner][INFO] - Test, Round 42 : loss => 0.8773497323541928,  accuracy: 0.7672092567170034
[2025-09-17 16:35:59,995][flp2p.graph_runner][INFO] - Train, Round 43 : loss => 0.18404147040704266,  accuracy: 0.933288888888889, gradient_norm : 0.19080650667167567
[2025-09-17 16:36:03,787][flp2p.graph_runner][INFO] - Test, Round 43 : loss => 0.8739137219594155,  accuracy: 0.7605915267785771
[2025-09-17 16:36:18,570][flp2p.graph_runner][INFO] - Train, Round 44 : loss => 0.15781407081126236,  accuracy: 0.9441222222222219, gradient_norm : 0.16655319570263416
[2025-09-17 16:36:22,399][flp2p.graph_runner][INFO] - Test, Round 44 : loss => 0.9210563801502183,  accuracy: 0.7721899418954118
[2025-09-17 16:36:37,428][flp2p.graph_runner][INFO] - Train, Round 45 : loss => 0.16176150832580116,  accuracy: 0.9423638344226581, gradient_norm : 0.17511565945287919
[2025-09-17 16:36:41,290][flp2p.graph_runner][INFO] - Test, Round 45 : loss => 0.901285882379938,  accuracy: 0.7621292476920055
[2025-09-17 16:36:55,975][flp2p.graph_runner][INFO] - Train, Round 46 : loss => 0.14762533772333214,  accuracy: 0.9474888888888887, gradient_norm : 0.18209969304837384
[2025-09-17 16:36:59,823][flp2p.graph_runner][INFO] - Test, Round 46 : loss => 0.9213523998254297,  accuracy: 0.7647058823529411
[2025-09-17 16:37:14,800][flp2p.graph_runner][INFO] - Train, Round 47 : loss => 0.14614217540072702,  accuracy: 0.9491830065359476, gradient_norm : 0.1737144274448352
[2025-09-17 16:37:18,752][flp2p.graph_runner][INFO] - Test, Round 47 : loss => 0.9059724671537672,  accuracy: 0.7638861629048086
[2025-09-17 16:37:33,500][flp2p.graph_runner][INFO] - Train, Round 48 : loss => 0.1380445662830801,  accuracy: 0.9527222222222224, gradient_norm : 0.17860706320941172
[2025-09-17 16:37:37,347][flp2p.graph_runner][INFO] - Test, Round 48 : loss => 0.9599992820838311,  accuracy: 0.7671671671671672
[2025-09-17 16:37:52,359][flp2p.graph_runner][INFO] - Train, Round 49 : loss => 0.13230527861449362,  accuracy: 0.9551633986928108, gradient_norm : 0.1673148397078603
[2025-09-17 16:37:56,321][flp2p.graph_runner][INFO] - Test, Round 49 : loss => 0.9631132481614608,  accuracy: 0.7681159420289855
[2025-09-17 16:38:11,161][flp2p.graph_runner][INFO] - Train, Round 50 : loss => 0.1161601299768469,  accuracy: 0.9608888888888888, gradient_norm : 0.1610401583678329
[2025-09-17 16:38:15,010][flp2p.graph_runner][INFO] - Test, Round 50 : loss => 0.9619946684725688,  accuracy: 0.7727637384677096
[2025-09-17 16:38:29,675][flp2p.graph_runner][INFO] - Train, Round 51 : loss => 0.1068196888147698,  accuracy: 0.9642, gradient_norm : 0.1500153370387683
[2025-09-17 16:38:33,514][flp2p.graph_runner][INFO] - Test, Round 51 : loss => 0.9755827451281653,  accuracy: 0.7685537107421484
[2025-09-17 16:38:48,189][flp2p.graph_runner][INFO] - Train, Round 52 : loss => 0.10597288759353493,  accuracy: 0.9642222222222225, gradient_norm : 0.15325330467309592
[2025-09-17 16:38:52,041][flp2p.graph_runner][INFO] - Test, Round 52 : loss => 0.9755609427183867,  accuracy: 0.7748
[2025-09-17 16:39:06,729][flp2p.graph_runner][INFO] - Train, Round 53 : loss => 0.09337489913861888,  accuracy: 0.9694222222222222, gradient_norm : 0.1511244625255348
[2025-09-17 16:39:10,549][flp2p.graph_runner][INFO] - Test, Round 53 : loss => 1.0067705629389359,  accuracy: 0.7670602361416851
[2025-09-17 16:39:25,545][flp2p.graph_runner][INFO] - Train, Round 54 : loss => 0.08827865141340428,  accuracy: 0.9714814814814814, gradient_norm : 0.14520488883666668
[2025-09-17 16:39:29,481][flp2p.graph_runner][INFO] - Test, Round 54 : loss => 1.050362533721672,  accuracy: 0.7706457925636008
[2025-09-17 16:39:44,232][flp2p.graph_runner][INFO] - Train, Round 55 : loss => 0.0925517877344319,  accuracy: 0.9698366013071895, gradient_norm : 0.14783790712175288
[2025-09-17 16:39:48,066][flp2p.graph_runner][INFO] - Test, Round 55 : loss => 1.020163260186669,  accuracy: 0.7804399057344855
[2025-09-17 16:40:02,569][flp2p.graph_runner][INFO] - Train, Round 56 : loss => 0.08201426587269331,  accuracy: 0.9738777777777775, gradient_norm : 0.14213220339394916
[2025-09-17 16:40:06,301][flp2p.graph_runner][INFO] - Test, Round 56 : loss => 1.0427727298950973,  accuracy: 0.7698778289605448
[2025-09-17 16:40:20,801][flp2p.graph_runner][INFO] - Train, Round 57 : loss => 0.0708354528018972,  accuracy: 0.9789555555555556, gradient_norm : 0.13448477762860753
[2025-09-17 16:40:24,497][flp2p.graph_runner][INFO] - Test, Round 57 : loss => 1.0135887916025437,  accuracy: 0.7785328802718369
[2025-09-17 16:40:38,843][flp2p.graph_runner][INFO] - Train, Round 58 : loss => 0.06221965167737411,  accuracy: 0.9814333333333335, gradient_norm : 0.12445743995296749
[2025-09-17 16:40:42,614][flp2p.graph_runner][INFO] - Test, Round 58 : loss => 1.0630113123878837,  accuracy: 0.7786
[2025-09-17 16:40:57,239][flp2p.graph_runner][INFO] - Train, Round 59 : loss => 0.07897852845415793,  accuracy: 0.9750871459694987, gradient_norm : 0.1401024645678384
[2025-09-17 16:41:01,052][flp2p.graph_runner][INFO] - Test, Round 59 : loss => 1.0281684385112588,  accuracy: 0.7766875981161695
[2025-09-17 16:41:15,533][flp2p.graph_runner][INFO] - Train, Round 60 : loss => 0.06340314342196993,  accuracy: 0.9813888888888886, gradient_norm : 0.13101376725541306
[2025-09-17 16:41:19,275][flp2p.graph_runner][INFO] - Test, Round 60 : loss => 1.113053526724135,  accuracy: 0.7738261738261738
[2025-09-17 16:41:33,601][flp2p.graph_runner][INFO] - Train, Round 61 : loss => 0.04937241527001606,  accuracy: 0.9862555555555554, gradient_norm : 0.10759409303803859
[2025-09-17 16:41:37,328][flp2p.graph_runner][INFO] - Test, Round 61 : loss => 1.170495782557538,  accuracy: 0.7718174539631706
[2025-09-17 16:41:51,698][flp2p.graph_runner][INFO] - Train, Round 62 : loss => 0.05116996591216109,  accuracy: 0.9853444444444444, gradient_norm : 0.10814213856674497
[2025-09-17 16:41:55,453][flp2p.graph_runner][INFO] - Test, Round 62 : loss => 1.0743982723236372,  accuracy: 0.7828353719671145
[2025-09-17 16:42:10,133][flp2p.graph_runner][INFO] - Train, Round 63 : loss => 0.05448116014178559,  accuracy: 0.9842047930283223, gradient_norm : 0.11327096997740271
[2025-09-17 16:42:13,939][flp2p.graph_runner][INFO] - Test, Round 63 : loss => 1.0984524553499406,  accuracy: 0.77222875048962
[2025-09-17 16:42:28,381][flp2p.graph_runner][INFO] - Train, Round 64 : loss => 0.04721366857801331,  accuracy: 0.9872888888888885, gradient_norm : 0.1041104727024485
[2025-09-17 16:42:32,080][flp2p.graph_runner][INFO] - Test, Round 64 : loss => 1.0549818322217588,  accuracy: 0.7800240192153723
[2025-09-17 16:42:46,747][flp2p.graph_runner][INFO] - Train, Round 65 : loss => 0.044307678308337164,  accuracy: 0.9877559912854031, gradient_norm : 0.09982400460230245
[2025-09-17 16:42:50,542][flp2p.graph_runner][INFO] - Test, Round 65 : loss => 1.1610741575593102,  accuracy: 0.7815505397448479
[2025-09-17 16:43:05,190][flp2p.graph_runner][INFO] - Train, Round 66 : loss => 0.0318279019167557,  accuracy: 0.9916884531590413, gradient_norm : 0.08806025336260342
[2025-09-17 16:43:09,013][flp2p.graph_runner][INFO] - Test, Round 66 : loss => 1.2281732721470273,  accuracy: 0.7752742946708464
[2025-09-17 16:43:23,446][flp2p.graph_runner][INFO] - Train, Round 67 : loss => 0.03811891362414462,  accuracy: 0.9898333333333336, gradient_norm : 0.09261540079741915
[2025-09-17 16:43:27,206][flp2p.graph_runner][INFO] - Test, Round 67 : loss => 1.1690521055973027,  accuracy: 0.781099518459069
[2025-09-17 16:43:41,783][flp2p.graph_runner][INFO] - Train, Round 68 : loss => 0.024973514723116715,  accuracy: 0.9944553376906321, gradient_norm : 0.07530222057291304
[2025-09-17 16:43:45,658][flp2p.graph_runner][INFO] - Test, Round 68 : loss => 1.2415397620610162,  accuracy: 0.7784313725490196
[2025-09-17 16:44:00,366][flp2p.graph_runner][INFO] - Train, Round 69 : loss => 0.0257086664116774,  accuracy: 0.9942592592592596, gradient_norm : 0.07298914718035586
[2025-09-17 16:44:04,199][flp2p.graph_runner][INFO] - Test, Round 69 : loss => 1.2067462518208862,  accuracy: 0.7739403453689168
[2025-09-17 16:44:18,955][flp2p.graph_runner][INFO] - Train, Round 70 : loss => 0.027948030796882795,  accuracy: 0.993169934640523, gradient_norm : 0.08452779558798967
[2025-09-17 16:44:22,764][flp2p.graph_runner][INFO] - Test, Round 70 : loss => 1.2030588805054747,  accuracy: 0.7814894871291019
[2025-09-17 16:44:37,351][flp2p.graph_runner][INFO] - Train, Round 71 : loss => 0.026048653179344423,  accuracy: 0.993464052287582, gradient_norm : 0.07507557725100365
[2025-09-17 16:44:41,116][flp2p.graph_runner][INFO] - Test, Round 71 : loss => 1.2639275986090726,  accuracy: 0.7750736015701668
[2025-09-17 16:44:55,445][flp2p.graph_runner][INFO] - Train, Round 72 : loss => 0.026960631968511735,  accuracy: 0.9933888888888889, gradient_norm : 0.07745123857475351
[2025-09-17 16:44:59,215][flp2p.graph_runner][INFO] - Test, Round 72 : loss => 1.2037397795612872,  accuracy: 0.7822435512897421
[2025-09-17 16:45:13,625][flp2p.graph_runner][INFO] - Train, Round 73 : loss => 0.024927520120778354,  accuracy: 0.9941666666666666, gradient_norm : 0.07384675268559653
[2025-09-17 16:45:17,331][flp2p.graph_runner][INFO] - Test, Round 73 : loss => 1.2199274635355384,  accuracy: 0.7808820594691678
[2025-09-17 16:45:32,129][flp2p.graph_runner][INFO] - Train, Round 74 : loss => 0.01781984703764441,  accuracy: 0.9966448801742919, gradient_norm : 0.06097532681137175
[2025-09-17 16:45:35,918][flp2p.graph_runner][INFO] - Test, Round 74 : loss => 1.2670530169391268,  accuracy: 0.783476898981989
[2025-09-17 16:45:50,351][flp2p.graph_runner][INFO] - Train, Round 75 : loss => 0.01764740480851227,  accuracy: 0.9962444444444445, gradient_norm : 0.06677941977371511
[2025-09-17 16:45:54,093][flp2p.graph_runner][INFO] - Test, Round 75 : loss => 1.3065265003731186,  accuracy: 0.7772663598158895
[2025-09-17 16:46:08,529][flp2p.graph_runner][INFO] - Train, Round 76 : loss => 0.017462272268816983,  accuracy: 0.9966444444444447, gradient_norm : 0.05599818095702149
[2025-09-17 16:46:12,178][flp2p.graph_runner][INFO] - Test, Round 76 : loss => 1.2439381995709344,  accuracy: 0.788334335538184
[2025-09-17 16:46:26,905][flp2p.graph_runner][INFO] - Train, Round 77 : loss => 0.01666417654582415,  accuracy: 0.9963398692810458, gradient_norm : 0.05598285450987356
[2025-09-17 16:46:30,726][flp2p.graph_runner][INFO] - Test, Round 77 : loss => 1.2310302955091317,  accuracy: 0.7892359065016696
[2025-09-17 16:46:45,170][flp2p.graph_runner][INFO] - Train, Round 78 : loss => 0.018479933010045593,  accuracy: 0.9959666666666666, gradient_norm : 0.06251566405335357
[2025-09-17 16:46:48,899][flp2p.graph_runner][INFO] - Test, Round 78 : loss => 1.3176440962117177,  accuracy: 0.7797523961661342
[2025-09-17 16:47:03,365][flp2p.graph_runner][INFO] - Train, Round 79 : loss => 0.01270397143206598,  accuracy: 0.9979111111111111, gradient_norm : 0.05125419162227782
[2025-09-17 16:47:07,063][flp2p.graph_runner][INFO] - Test, Round 79 : loss => 1.3657979403084126,  accuracy: 0.7781789009225832
[2025-09-17 16:47:21,600][flp2p.graph_runner][INFO] - Train, Round 80 : loss => 0.014409567651241862,  accuracy: 0.9971999999999999, gradient_norm : 0.049680626914309335
[2025-09-17 16:47:25,437][flp2p.graph_runner][INFO] - Test, Round 80 : loss => 1.2697393695374468,  accuracy: 0.7854145854145854
[2025-09-17 16:47:40,095][flp2p.graph_runner][INFO] - Train, Round 81 : loss => 0.009042990407699132,  accuracy: 0.9988453159041394, gradient_norm : 0.03522375097266543
[2025-09-17 16:47:43,900][flp2p.graph_runner][INFO] - Test, Round 81 : loss => 1.293661300141866,  accuracy: 0.7851401685943933
[2025-09-17 16:47:58,331][flp2p.graph_runner][INFO] - Train, Round 82 : loss => 0.013340105423497638,  accuracy: 0.9971777777777777, gradient_norm : 0.04463541606996854
[2025-09-17 16:48:02,065][flp2p.graph_runner][INFO] - Test, Round 82 : loss => 1.3397038804321029,  accuracy: 0.7835731414868106
[2025-09-17 16:48:16,433][flp2p.graph_runner][INFO] - Train, Round 83 : loss => 0.012887273442640434,  accuracy: 0.9972333333333333, gradient_norm : 0.04526490027976973
[2025-09-17 16:48:20,187][flp2p.graph_runner][INFO] - Test, Round 83 : loss => 1.3244518101424396,  accuracy: 0.7835671342685371
[2025-09-17 16:48:34,591][flp2p.graph_runner][INFO] - Train, Round 84 : loss => 0.00991937970847357,  accuracy: 0.9983777777777778, gradient_norm : 0.03596268968965205
[2025-09-17 16:48:38,326][flp2p.graph_runner][INFO] - Test, Round 84 : loss => 1.370429859222427,  accuracy: 0.7785127280016035
[2025-09-17 16:48:53,080][flp2p.graph_runner][INFO] - Train, Round 85 : loss => 0.009459979931467815,  accuracy: 0.99838779956427, gradient_norm : 0.035655821351563304
[2025-09-17 16:48:56,895][flp2p.graph_runner][INFO] - Test, Round 85 : loss => 1.3357159084527839,  accuracy: 0.781305114638448
[2025-09-17 16:49:11,272][flp2p.graph_runner][INFO] - Train, Round 86 : loss => 0.008838254411452605,  accuracy: 0.9987444444444444, gradient_norm : 0.03524352759945094
[2025-09-17 16:49:14,994][flp2p.graph_runner][INFO] - Test, Round 86 : loss => 1.3750590931318087,  accuracy: 0.7805611222444889
[2025-09-17 16:49:29,352][flp2p.graph_runner][INFO] - Train, Round 87 : loss => 0.010024051144137047,  accuracy: 0.9979555555555557, gradient_norm : 0.037451246144818554
[2025-09-17 16:49:33,123][flp2p.graph_runner][INFO] - Test, Round 87 : loss => 1.3546371515027853,  accuracy: 0.7889155662264906
[2025-09-17 16:49:47,788][flp2p.graph_runner][INFO] - Train, Round 88 : loss => 0.006618128485846017,  accuracy: 0.9991830065359477, gradient_norm : 0.02809094368976682
[2025-09-17 16:49:51,671][flp2p.graph_runner][INFO] - Test, Round 88 : loss => 1.3571590219608833,  accuracy: 0.7831537708129285
[2025-09-17 16:50:06,176][flp2p.graph_runner][INFO] - Train, Round 89 : loss => 0.0054777945971582085,  accuracy: 0.9995444444444446, gradient_norm : 0.024241136060883895
[2025-09-17 16:50:09,939][flp2p.graph_runner][INFO] - Test, Round 89 : loss => 1.376698993532128,  accuracy: 0.7845292824305417
[2025-09-17 16:50:24,237][flp2p.graph_runner][INFO] - Train, Round 90 : loss => 0.005391961720306426,  accuracy: 0.9993444444444445, gradient_norm : 0.024738005471946566
[2025-09-17 16:50:27,958][flp2p.graph_runner][INFO] - Test, Round 90 : loss => 1.3937735160033067,  accuracy: 0.7864135864135864
[2025-09-17 16:50:42,599][flp2p.graph_runner][INFO] - Train, Round 91 : loss => 0.004222981097053788,  accuracy: 0.9996949891067538, gradient_norm : 0.02141450729967282
[2025-09-17 16:50:46,439][flp2p.graph_runner][INFO] - Test, Round 91 : loss => 1.363573693278187,  accuracy: 0.7919199843106491
[2025-09-17 16:51:01,186][flp2p.graph_runner][INFO] - Train, Round 92 : loss => 0.006135810462990395,  accuracy: 0.9991830065359476, gradient_norm : 0.026094627223624618
[2025-09-17 16:51:05,049][flp2p.graph_runner][INFO] - Test, Round 92 : loss => 1.3744748253178147,  accuracy: 0.7926303410427283
[2025-09-17 16:51:19,794][flp2p.graph_runner][INFO] - Train, Round 93 : loss => 0.004260191260521251,  accuracy: 0.9997058823529413, gradient_norm : 0.018209093389041135
[2025-09-17 16:51:23,650][flp2p.graph_runner][INFO] - Test, Round 93 : loss => 1.3933661048984116,  accuracy: 0.786189358372457
[2025-09-17 16:51:48,641][flp2p.graph_runner][INFO] - Train, Round 94 : loss => 0.0035297660130308942,  accuracy: 0.9998444444444443, gradient_norm : 0.01721061629547543
[2025-09-17 16:51:52,407][flp2p.graph_runner][INFO] - Test, Round 94 : loss => 1.4019526027858258,  accuracy: 0.7846
[2025-09-17 16:52:07,293][flp2p.graph_runner][INFO] - Train, Round 95 : loss => 0.003955292723273243,  accuracy: 0.9997712418300654, gradient_norm : 0.019212128965989277
[2025-09-17 16:52:11,141][flp2p.graph_runner][INFO] - Test, Round 95 : loss => 1.4041531216952146,  accuracy: 0.7874509803921569
[2025-09-17 16:52:25,593][flp2p.graph_runner][INFO] - Train, Round 96 : loss => 0.0037526347016100772,  accuracy: 0.9998444444444445, gradient_norm : 0.01813339510067864
[2025-09-17 16:52:29,417][flp2p.graph_runner][INFO] - Test, Round 96 : loss => 1.3974067919247144,  accuracy: 0.786986986986987
[2025-09-17 16:52:44,248][flp2p.graph_runner][INFO] - Train, Round 97 : loss => 0.002618506652157711,  accuracy: 1.0, gradient_norm : 0.012336946363829022
[2025-09-17 16:52:48,099][flp2p.graph_runner][INFO] - Test, Round 97 : loss => 1.44820660353,  accuracy: 0.7885523210070811
[2025-09-17 16:53:02,971][flp2p.graph_runner][INFO] - Train, Round 98 : loss => 0.00303609294708806,  accuracy: 0.9999019607843137, gradient_norm : 0.01458310646505688
[2025-09-17 16:53:06,794][flp2p.graph_runner][INFO] - Test, Round 98 : loss => 1.4692994314630436,  accuracy: 0.7818003913894325
[2025-09-17 16:53:21,607][flp2p.graph_runner][INFO] - Train, Round 99 : loss => 0.002771480708963959,  accuracy: 0.9999237472766885, gradient_norm : 0.013394967412651176
[2025-09-17 16:53:25,419][flp2p.graph_runner][INFO] - Test, Round 99 : loss => 1.4720646087058336,  accuracy: 0.782438259506076
[2025-09-17 16:53:40,001][flp2p.graph_runner][INFO] - Train, Round 100 : loss => 0.00348797503701644,  accuracy: 0.9996444444444444, gradient_norm : 0.016775533176933925
[2025-09-17 16:53:43,755][flp2p.graph_runner][INFO] - Test, Round 100 : loss => 1.4344927973937815,  accuracy: 0.789863782051282
[2025-09-17 16:53:58,618][flp2p.graph_runner][INFO] - Train, Round 101 : loss => 0.0029233103536801365,  accuracy: 0.9998039215686273, gradient_norm : 0.01319156942133203
[2025-09-17 16:54:02,424][flp2p.graph_runner][INFO] - Test, Round 101 : loss => 1.449389441301123,  accuracy: 0.7878312070657507
[2025-09-17 16:54:17,067][flp2p.graph_runner][INFO] - Train, Round 102 : loss => 0.0024254137456882745,  accuracy: 0.9999777777777777, gradient_norm : 0.012144573655980056
[2025-09-17 16:54:20,844][flp2p.graph_runner][INFO] - Test, Round 102 : loss => 1.487937678154014,  accuracy: 0.7887154861944778
[2025-09-17 16:54:35,569][flp2p.graph_runner][INFO] - Train, Round 103 : loss => 0.0024870823041590697,  accuracy: 0.9999237472766884, gradient_norm : 0.011822709944840465
[2025-09-17 16:54:39,463][flp2p.graph_runner][INFO] - Test, Round 103 : loss => 1.4279330826241425,  accuracy: 0.7914543316346531
[2025-09-17 16:54:53,982][flp2p.graph_runner][INFO] - Train, Round 104 : loss => 0.002215549446370763,  accuracy: 1.0, gradient_norm : 0.011345908364378297
[2025-09-17 16:54:57,759][flp2p.graph_runner][INFO] - Test, Round 104 : loss => 1.3983501043496476,  accuracy: 0.7884730838503102
[2025-09-17 16:55:12,153][flp2p.graph_runner][INFO] - Train, Round 105 : loss => 0.0020494943002704533,  accuracy: 0.9999888888888888, gradient_norm : 0.010353514391585246
[2025-09-17 16:55:15,945][flp2p.graph_runner][INFO] - Test, Round 105 : loss => 1.5354138463228535,  accuracy: 0.7824259407526021
[2025-09-17 16:55:30,806][flp2p.graph_runner][INFO] - Train, Round 106 : loss => 0.002159910030481162,  accuracy: 0.9999673202614379, gradient_norm : 0.010833592563966656
[2025-09-17 16:55:34,598][flp2p.graph_runner][INFO] - Test, Round 106 : loss => 1.4313951737517367,  accuracy: 0.7884917517674784
[2025-09-17 16:55:49,123][flp2p.graph_runner][INFO] - Train, Round 107 : loss => 0.0020092145366361366,  accuracy: 0.9999888888888889, gradient_norm : 0.010201855575914994
[2025-09-17 16:55:52,902][flp2p.graph_runner][INFO] - Test, Round 107 : loss => 1.4946308723746073,  accuracy: 0.7843568713742749
[2025-09-17 16:56:07,385][flp2p.graph_runner][INFO] - Train, Round 108 : loss => 0.001998000600996117,  accuracy: 0.9999888888888889, gradient_norm : 0.010106199831441185
[2025-09-17 16:56:11,164][flp2p.graph_runner][INFO] - Test, Round 108 : loss => 1.5248173647912202,  accuracy: 0.7812312011229195
[2025-09-17 16:56:25,641][flp2p.graph_runner][INFO] - Train, Round 109 : loss => 0.0019626564385932096,  accuracy: 0.9999888888888889, gradient_norm : 0.009249208153579522
[2025-09-17 16:56:29,404][flp2p.graph_runner][INFO] - Test, Round 109 : loss => 1.4725449901372194,  accuracy: 0.7862
[2025-09-17 16:56:43,929][flp2p.graph_runner][INFO] - Train, Round 110 : loss => 0.0019552699718090785,  accuracy: 0.9999777777777777, gradient_norm : 0.00965188568289702
[2025-09-17 16:56:47,694][flp2p.graph_runner][INFO] - Test, Round 110 : loss => 1.5357648697740387,  accuracy: 0.7824435112977405
[2025-09-17 16:57:02,492][flp2p.graph_runner][INFO] - Train, Round 111 : loss => 0.001749161580907336,  accuracy: 1.0, gradient_norm : 0.009123738754658325
[2025-09-17 16:57:06,388][flp2p.graph_runner][INFO] - Test, Round 111 : loss => 1.493523230079253,  accuracy: 0.7872006281900275
[2025-09-17 16:57:20,994][flp2p.graph_runner][INFO] - Train, Round 112 : loss => 0.0016646856772810376,  accuracy: 1.0, gradient_norm : 0.008233764292528745
[2025-09-17 16:57:24,750][flp2p.graph_runner][INFO] - Test, Round 112 : loss => 1.5624383997805051,  accuracy: 0.7860716429857915
[2025-09-17 16:57:39,255][flp2p.graph_runner][INFO] - Train, Round 113 : loss => 0.00169488613968133,  accuracy: 1.0, gradient_norm : 0.008560802784396492
[2025-09-17 16:57:43,034][flp2p.graph_runner][INFO] - Test, Round 113 : loss => 1.5601388411909343,  accuracy: 0.784
[2025-09-17 16:57:57,612][flp2p.graph_runner][INFO] - Train, Round 114 : loss => 0.0016917255548469256,  accuracy: 1.0, gradient_norm : 0.008526704507508986
[2025-09-17 16:58:01,365][flp2p.graph_runner][INFO] - Test, Round 114 : loss => 1.5444912671744824,  accuracy: 0.7842
[2025-09-17 16:58:15,916][flp2p.graph_runner][INFO] - Train, Round 115 : loss => 0.0016028457267287498,  accuracy: 1.0, gradient_norm : 0.007873497017132754
[2025-09-17 16:58:19,707][flp2p.graph_runner][INFO] - Test, Round 115 : loss => 1.5144386400628271,  accuracy: 0.7848581701957651
[2025-09-17 16:58:34,216][flp2p.graph_runner][INFO] - Train, Round 116 : loss => 0.0015425465982795382,  accuracy: 1.0, gradient_norm : 0.007232298922613594
[2025-09-17 16:58:37,953][flp2p.graph_runner][INFO] - Test, Round 116 : loss => 1.5293962053647268,  accuracy: 0.7871445734881858
[2025-09-17 16:58:52,877][flp2p.graph_runner][INFO] - Train, Round 117 : loss => 0.001525069901981624,  accuracy: 0.9999891067538126, gradient_norm : 0.007394653166149908
[2025-09-17 16:58:56,713][flp2p.graph_runner][INFO] - Test, Round 117 : loss => 1.4796737269778444,  accuracy: 0.7919686581782566
[2025-09-17 16:59:11,710][flp2p.graph_runner][INFO] - Train, Round 118 : loss => 0.0014901438799973584,  accuracy: 1.0, gradient_norm : 0.007643831816359889
[2025-09-17 16:59:15,545][flp2p.graph_runner][INFO] - Test, Round 118 : loss => 1.5075967561509946,  accuracy: 0.7840440165061898
[2025-09-17 16:59:30,263][flp2p.graph_runner][INFO] - Train, Round 119 : loss => 0.0014748477875303554,  accuracy: 0.9999888888888888, gradient_norm : 0.007846687187773064
[2025-09-17 16:59:34,000][flp2p.graph_runner][INFO] - Test, Round 119 : loss => 1.5569123511833336,  accuracy: 0.7839407288746496
[2025-09-17 16:59:48,588][flp2p.graph_runner][INFO] - Train, Round 120 : loss => 0.0014490991221052052,  accuracy: 1.0, gradient_norm : 0.00691722509037181
[2025-09-17 16:59:52,383][flp2p.graph_runner][INFO] - Test, Round 120 : loss => 1.5543303315574872,  accuracy: 0.7863572714542909
[2025-09-17 17:00:06,970][flp2p.graph_runner][INFO] - Train, Round 121 : loss => 0.0014124732534110081,  accuracy: 1.0, gradient_norm : 0.0070360174272113985
[2025-09-17 17:00:10,747][flp2p.graph_runner][INFO] - Test, Round 121 : loss => 1.4978035670359038,  accuracy: 0.7864718831298779
[2025-09-17 17:00:25,505][flp2p.graph_runner][INFO] - Train, Round 122 : loss => 0.0013289371499673043,  accuracy: 1.0, gradient_norm : 0.00657660090116777
[2025-09-17 17:00:29,324][flp2p.graph_runner][INFO] - Test, Round 122 : loss => 1.5769574106735935,  accuracy: 0.7785644051130777
[2025-09-17 17:00:43,943][flp2p.graph_runner][INFO] - Train, Round 123 : loss => 0.0013318333582040699,  accuracy: 1.0, gradient_norm : 0.006489812327842432
[2025-09-17 17:00:47,721][flp2p.graph_runner][INFO] - Test, Round 123 : loss => 1.624586435869619,  accuracy: 0.7834165834165834
[2025-09-17 17:01:02,241][flp2p.graph_runner][INFO] - Train, Round 124 : loss => 0.0012794506582043444,  accuracy: 1.0, gradient_norm : 0.006445064411851158
[2025-09-17 17:01:06,034][flp2p.graph_runner][INFO] - Test, Round 124 : loss => 1.5803949905823484,  accuracy: 0.7870463204331262
[2025-09-17 17:01:20,953][flp2p.graph_runner][INFO] - Train, Round 125 : loss => 0.001314658722853565,  accuracy: 1.0, gradient_norm : 0.006520785154690965
[2025-09-17 17:01:24,793][flp2p.graph_runner][INFO] - Test, Round 125 : loss => 1.5281112311601872,  accuracy: 0.7888015717092338
[2025-09-17 17:01:39,356][flp2p.graph_runner][INFO] - Train, Round 126 : loss => 0.0012731363280278553,  accuracy: 1.0, gradient_norm : 0.006059801945946273
[2025-09-17 17:01:43,150][flp2p.graph_runner][INFO] - Test, Round 126 : loss => 1.5604162046784478,  accuracy: 0.7867174959871589
[2025-09-17 17:01:57,649][flp2p.graph_runner][INFO] - Train, Round 127 : loss => 0.0011984057669324102,  accuracy: 1.0, gradient_norm : 0.005948983004993206
[2025-09-17 17:02:01,387][flp2p.graph_runner][INFO] - Test, Round 127 : loss => 1.5863146304893607,  accuracy: 0.7869278432940235
[2025-09-17 17:02:16,203][flp2p.graph_runner][INFO] - Train, Round 128 : loss => 0.0011969858374190974,  accuracy: 1.0, gradient_norm : 0.005927837233811265
[2025-09-17 17:02:20,042][flp2p.graph_runner][INFO] - Test, Round 128 : loss => 1.601345125532524,  accuracy: 0.7817825661116552
[2025-09-17 17:02:34,856][flp2p.graph_runner][INFO] - Train, Round 129 : loss => 0.0012508773944312874,  accuracy: 1.0, gradient_norm : 0.005929254248053644
[2025-09-17 17:02:38,689][flp2p.graph_runner][INFO] - Test, Round 129 : loss => 1.5162880281486943,  accuracy: 0.7936290795387922
[2025-09-17 17:02:53,265][flp2p.graph_runner][INFO] - Train, Round 130 : loss => 0.0011870195327404265,  accuracy: 1.0, gradient_norm : 0.005728541522867078
[2025-09-17 17:02:57,101][flp2p.graph_runner][INFO] - Test, Round 130 : loss => 1.6223913008665347,  accuracy: 0.7814185814185814
[2025-09-17 17:03:11,830][flp2p.graph_runner][INFO] - Train, Round 131 : loss => 0.0011265097563653375,  accuracy: 1.0, gradient_norm : 0.0055427016675432435
[2025-09-17 17:03:15,687][flp2p.graph_runner][INFO] - Test, Round 131 : loss => 1.5846723567954875,  accuracy: 0.7873134328358209
[2025-09-17 17:03:30,185][flp2p.graph_runner][INFO] - Train, Round 132 : loss => 0.0011290273904257144,  accuracy: 1.0, gradient_norm : 0.005769290622345231
[2025-09-17 17:03:33,980][flp2p.graph_runner][INFO] - Test, Round 132 : loss => 1.5553363546016954,  accuracy: 0.7876424715056989
[2025-09-17 17:03:48,702][flp2p.graph_runner][INFO] - Train, Round 133 : loss => 0.0011273735337145017,  accuracy: 1.0, gradient_norm : 0.005659295873920462
[2025-09-17 17:03:52,514][flp2p.graph_runner][INFO] - Test, Round 133 : loss => 1.6471042447814754,  accuracy: 0.7817647058823529
[2025-09-17 17:04:06,992][flp2p.graph_runner][INFO] - Train, Round 134 : loss => 0.0011331459935609019,  accuracy: 1.0, gradient_norm : 0.005756466421358923
[2025-09-17 17:04:10,766][flp2p.graph_runner][INFO] - Test, Round 134 : loss => 1.6325822021337044,  accuracy: 0.7864854058376649
[2025-09-17 17:04:25,572][flp2p.graph_runner][INFO] - Train, Round 135 : loss => 0.0010712195815061206,  accuracy: 1.0, gradient_norm : 0.00510306616322787
[2025-09-17 17:04:29,387][flp2p.graph_runner][INFO] - Test, Round 135 : loss => 1.5243878769394927,  accuracy: 0.7921491658488714
[2025-09-17 17:04:44,223][flp2p.graph_runner][INFO] - Train, Round 136 : loss => 0.001013031180727743,  accuracy: 1.0, gradient_norm : 0.004794950866753242
[2025-09-17 17:04:48,012][flp2p.graph_runner][INFO] - Test, Round 136 : loss => 1.5562760928157633,  accuracy: 0.7869173521347435
[2025-09-17 17:05:02,944][flp2p.graph_runner][INFO] - Train, Round 137 : loss => 0.0010724876451099084,  accuracy: 1.0, gradient_norm : 0.005293563537904416
[2025-09-17 17:05:06,827][flp2p.graph_runner][INFO] - Test, Round 137 : loss => 1.5799053884352583,  accuracy: 0.7882653061224489
[2025-09-17 17:05:21,345][flp2p.graph_runner][INFO] - Train, Round 138 : loss => 0.001036560146031358,  accuracy: 1.0, gradient_norm : 0.005214706650749135
[2025-09-17 17:05:25,078][flp2p.graph_runner][INFO] - Test, Round 138 : loss => 1.62249534993439,  accuracy: 0.7877877877877878
[2025-09-17 17:05:39,993][flp2p.graph_runner][INFO] - Train, Round 139 : loss => 0.0010082465411697247,  accuracy: 1.0, gradient_norm : 0.005042994806967482
[2025-09-17 17:05:43,877][flp2p.graph_runner][INFO] - Test, Round 139 : loss => 1.6200990947531242,  accuracy: 0.7839639286414428
[2025-09-17 17:05:58,387][flp2p.graph_runner][INFO] - Train, Round 140 : loss => 0.0009571657113168233,  accuracy: 1.0, gradient_norm : 0.004908342267510039
[2025-09-17 17:06:02,164][flp2p.graph_runner][INFO] - Test, Round 140 : loss => 1.588770984107555,  accuracy: 0.7818436312737452
[2025-09-17 17:06:16,994][flp2p.graph_runner][INFO] - Train, Round 141 : loss => 0.0009866322172646642,  accuracy: 1.0, gradient_norm : 0.004779565891733783
[2025-09-17 17:06:20,865][flp2p.graph_runner][INFO] - Test, Round 141 : loss => 1.5721118451187424,  accuracy: 0.7868627450980392
[2025-09-17 17:06:35,405][flp2p.graph_runner][INFO] - Train, Round 142 : loss => 0.0009601317843480502,  accuracy: 1.0, gradient_norm : 0.004735941381964332
[2025-09-17 17:06:39,140][flp2p.graph_runner][INFO] - Test, Round 142 : loss => 1.6602749997245312,  accuracy: 0.7889798362946696
[2025-09-17 17:06:53,859][flp2p.graph_runner][INFO] - Train, Round 143 : loss => 0.0009461447197275869,  accuracy: 1.0, gradient_norm : 0.004627746624999472
[2025-09-17 17:06:57,708][flp2p.graph_runner][INFO] - Test, Round 143 : loss => 1.6009090996140427,  accuracy: 0.790743282996666
[2025-09-17 17:07:12,238][flp2p.graph_runner][INFO] - Train, Round 144 : loss => 0.0009476758333524534,  accuracy: 1.0, gradient_norm : 0.004687006514888792
[2025-09-17 17:07:16,023][flp2p.graph_runner][INFO] - Test, Round 144 : loss => 1.6306556140283903,  accuracy: 0.7851437699680511
[2025-09-17 17:07:30,568][flp2p.graph_runner][INFO] - Train, Round 145 : loss => 0.0009337351268671531,  accuracy: 1.0, gradient_norm : 0.004603282181587452
[2025-09-17 17:07:34,302][flp2p.graph_runner][INFO] - Test, Round 145 : loss => 1.6261620461279418,  accuracy: 0.7854858056777289
[2025-09-17 17:07:48,832][flp2p.graph_runner][INFO] - Train, Round 146 : loss => 0.0008981561163188114,  accuracy: 1.0, gradient_norm : 0.004352884644039167
[2025-09-17 17:07:52,568][flp2p.graph_runner][INFO] - Test, Round 146 : loss => 1.5958948110287743,  accuracy: 0.7845016976233273
[2025-09-17 17:08:07,346][flp2p.graph_runner][INFO] - Train, Round 147 : loss => 0.0009267106933977296,  accuracy: 1.0, gradient_norm : 0.004719458624743756
[2025-09-17 17:08:11,185][flp2p.graph_runner][INFO] - Test, Round 147 : loss => 1.5999729507231062,  accuracy: 0.7887821141400274
[2025-09-17 17:08:25,685][flp2p.graph_runner][INFO] - Train, Round 148 : loss => 0.0008989568839703376,  accuracy: 1.0, gradient_norm : 0.004594782709650246
[2025-09-17 17:08:29,432][flp2p.graph_runner][INFO] - Test, Round 148 : loss => 1.6470903349254398,  accuracy: 0.7864291433146517
[2025-09-17 17:08:44,329][flp2p.graph_runner][INFO] - Train, Round 149 : loss => 0.0008722592221150324,  accuracy: 1.0, gradient_norm : 0.004272482809548608
[2025-09-17 17:08:48,157][flp2p.graph_runner][INFO] - Test, Round 149 : loss => 1.6090184183541782,  accuracy: 0.7917238674249852
[2025-09-17 17:09:03,021][flp2p.graph_runner][INFO] - Train, Round 150 : loss => 0.000879745310074665,  accuracy: 1.0, gradient_norm : 0.004820374079726391
[2025-09-17 17:09:06,824][flp2p.graph_runner][INFO] - Test, Round 150 : loss => 1.6266354273452128,  accuracy: 0.7897205393785421
[2025-09-17 17:09:21,685][flp2p.graph_runner][INFO] - Train, Round 151 : loss => 0.0008535109843268954,  accuracy: 1.0, gradient_norm : 0.0044808853428808275
[2025-09-17 17:09:25,548][flp2p.graph_runner][INFO] - Test, Round 151 : loss => 1.6551100933178164,  accuracy: 0.7919686581782566
[2025-09-17 17:09:40,428][flp2p.graph_runner][INFO] - Train, Round 152 : loss => 0.0008590047724972619,  accuracy: 1.0, gradient_norm : 0.004262610195303072
[2025-09-17 17:09:44,233][flp2p.graph_runner][INFO] - Test, Round 152 : loss => 1.6346659465336506,  accuracy: 0.7889476778365667
[2025-09-17 17:09:59,043][flp2p.graph_runner][INFO] - Train, Round 153 : loss => 0.0008401191080718764,  accuracy: 1.0, gradient_norm : 0.004361543900431802
[2025-09-17 17:10:02,883][flp2p.graph_runner][INFO] - Test, Round 153 : loss => 1.65366260796201,  accuracy: 0.7905882352941176
[2025-09-17 17:10:17,401][flp2p.graph_runner][INFO] - Train, Round 154 : loss => 0.0008219248328096001,  accuracy: 1.0, gradient_norm : 0.003972057641774895
[2025-09-17 17:10:21,187][flp2p.graph_runner][INFO] - Test, Round 154 : loss => 1.6405973973617756,  accuracy: 0.784572342126299
[2025-09-17 17:10:35,688][flp2p.graph_runner][INFO] - Train, Round 155 : loss => 0.0007760597694529376,  accuracy: 1.0, gradient_norm : 0.0037763806585448313
[2025-09-17 17:10:39,403][flp2p.graph_runner][INFO] - Test, Round 155 : loss => 1.599923335229159,  accuracy: 0.786501101542159
[2025-09-17 17:10:54,283][flp2p.graph_runner][INFO] - Train, Round 156 : loss => 0.0007959483249198183,  accuracy: 1.0, gradient_norm : 0.004095708262390323
[2025-09-17 17:10:58,124][flp2p.graph_runner][INFO] - Test, Round 156 : loss => 1.588058192431051,  accuracy: 0.7909892262487757
[2025-09-17 17:11:12,970][flp2p.graph_runner][INFO] - Train, Round 157 : loss => 0.0008137114843598253,  accuracy: 1.0, gradient_norm : 0.004334259218327501
[2025-09-17 17:11:16,803][flp2p.graph_runner][INFO] - Test, Round 157 : loss => 1.6896747118020885,  accuracy: 0.7834607093866354
[2025-09-17 17:11:31,666][flp2p.graph_runner][INFO] - Train, Round 158 : loss => 0.0007896074662258958,  accuracy: 1.0, gradient_norm : 0.003887314847975311
[2025-09-17 17:11:35,478][flp2p.graph_runner][INFO] - Test, Round 158 : loss => 1.6769982809140584,  accuracy: 0.786933490288405
[2025-09-17 17:11:50,319][flp2p.graph_runner][INFO] - Train, Round 159 : loss => 0.000786245327199021,  accuracy: 1.0, gradient_norm : 0.0038524908682241073
[2025-09-17 17:11:54,097][flp2p.graph_runner][INFO] - Test, Round 159 : loss => 1.6655774867251363,  accuracy: 0.7843676355066771
[2025-09-17 17:12:08,642][flp2p.graph_runner][INFO] - Train, Round 160 : loss => 0.0007780369633692317,  accuracy: 1.0, gradient_norm : 0.0038208953860523896
[2025-09-17 17:12:12,368][flp2p.graph_runner][INFO] - Test, Round 160 : loss => 1.635604311148611,  accuracy: 0.7851585876720527
[2025-09-17 17:12:27,196][flp2p.graph_runner][INFO] - Train, Round 161 : loss => 0.0007686784567174067,  accuracy: 1.0, gradient_norm : 0.003891822145720025
[2025-09-17 17:12:31,020][flp2p.graph_runner][INFO] - Test, Round 161 : loss => 1.7517754469874796,  accuracy: 0.7819430814524043
[2025-09-17 17:12:45,681][flp2p.graph_runner][INFO] - Train, Round 162 : loss => 0.0007360797902899018,  accuracy: 1.0, gradient_norm : 0.0036194395345333485
[2025-09-17 17:12:49,390][flp2p.graph_runner][INFO] - Test, Round 162 : loss => 1.7210028950929641,  accuracy: 0.7862
[2025-09-17 17:13:04,282][flp2p.graph_runner][INFO] - Train, Round 163 : loss => 0.0007669860515283159,  accuracy: 1.0, gradient_norm : 0.0038939720786777166
[2025-09-17 17:13:08,161][flp2p.graph_runner][INFO] - Test, Round 163 : loss => 1.6187427906687357,  accuracy: 0.794474921630094
[2025-09-17 17:13:22,639][flp2p.graph_runner][INFO] - Train, Round 164 : loss => 0.000760780219968486,  accuracy: 1.0, gradient_norm : 0.0037254548829170646
[2025-09-17 17:13:26,414][flp2p.graph_runner][INFO] - Test, Round 164 : loss => 1.6171813183351351,  accuracy: 0.7806684010406244
[2025-09-17 17:13:41,125][flp2p.graph_runner][INFO] - Train, Round 165 : loss => 0.0007495596333052769,  accuracy: 1.0, gradient_norm : 0.0037857340808301813
[2025-09-17 17:13:44,883][flp2p.graph_runner][INFO] - Test, Round 165 : loss => 1.5784382198044689,  accuracy: 0.7942353883106485
[2025-09-17 17:13:59,430][flp2p.graph_runner][INFO] - Train, Round 166 : loss => 0.000732195434650445,  accuracy: 1.0, gradient_norm : 0.003737418618988098
[2025-09-17 17:14:03,207][flp2p.graph_runner][INFO] - Test, Round 166 : loss => 1.678105742484182,  accuracy: 0.789242151569686
[2025-09-17 17:14:17,807][flp2p.graph_runner][INFO] - Train, Round 167 : loss => 0.0007286365534819196,  accuracy: 1.0, gradient_norm : 0.0036111681407270213
[2025-09-17 17:14:21,634][flp2p.graph_runner][INFO] - Test, Round 167 : loss => 1.6753861830190155,  accuracy: 0.7884730838503102
[2025-09-17 17:14:36,447][flp2p.graph_runner][INFO] - Train, Round 168 : loss => 0.0007124322135567519,  accuracy: 1.0, gradient_norm : 0.0037748574506837984
[2025-09-17 17:14:40,316][flp2p.graph_runner][INFO] - Test, Round 168 : loss => 1.6199141793540988,  accuracy: 0.7885747938751473
[2025-09-17 17:14:55,223][flp2p.graph_runner][INFO] - Train, Round 169 : loss => 0.0006893050247229624,  accuracy: 1.0, gradient_norm : 0.0034942379302840103
[2025-09-17 17:14:59,062][flp2p.graph_runner][INFO] - Test, Round 169 : loss => 1.7171955107177659,  accuracy: 0.7880456154148643
[2025-09-17 17:15:13,610][flp2p.graph_runner][INFO] - Train, Round 170 : loss => 0.0006901162879366893,  accuracy: 1.0, gradient_norm : 0.003538145620045652
[2025-09-17 17:15:17,345][flp2p.graph_runner][INFO] - Test, Round 170 : loss => 1.6637809579561134,  accuracy: 0.7857715430861724
[2025-09-17 17:15:32,131][flp2p.graph_runner][INFO] - Train, Round 171 : loss => 0.0006843611856858213,  accuracy: 1.0, gradient_norm : 0.0034342254788223957
[2025-09-17 17:15:35,997][flp2p.graph_runner][INFO] - Test, Round 171 : loss => 1.6456002979154558,  accuracy: 0.7846908734052993
[2025-09-17 17:15:50,598][flp2p.graph_runner][INFO] - Train, Round 172 : loss => 0.0006743308564279383,  accuracy: 1.0, gradient_norm : 0.0033270525064671265
[2025-09-17 17:15:54,370][flp2p.graph_runner][INFO] - Test, Round 172 : loss => 1.6955479024469853,  accuracy: 0.7838
[2025-09-17 17:16:09,209][flp2p.graph_runner][INFO] - Train, Round 173 : loss => 0.0006818199546412668,  accuracy: 1.0, gradient_norm : 0.003673250966576289
[2025-09-17 17:16:13,113][flp2p.graph_runner][INFO] - Test, Round 173 : loss => 1.7019730535005653,  accuracy: 0.784844915586965
[2025-09-17 17:16:27,601][flp2p.graph_runner][INFO] - Train, Round 174 : loss => 0.0006478637633166121,  accuracy: 1.0, gradient_norm : 0.0032034302699028073
[2025-09-17 17:16:31,325][flp2p.graph_runner][INFO] - Test, Round 174 : loss => 1.6531009387403726,  accuracy: 0.7978
[2025-09-17 17:16:46,102][flp2p.graph_runner][INFO] - Train, Round 175 : loss => 0.0006592840915021762,  accuracy: 1.0, gradient_norm : 0.0032917563536790637
[2025-09-17 17:16:49,898][flp2p.graph_runner][INFO] - Test, Round 175 : loss => 1.6497496325351937,  accuracy: 0.7878965922444183
[2025-09-17 17:17:04,404][flp2p.graph_runner][INFO] - Train, Round 176 : loss => 0.0006530782948927177,  accuracy: 1.0, gradient_norm : 0.0033255431533092085
[2025-09-17 17:17:08,180][flp2p.graph_runner][INFO] - Test, Round 176 : loss => 1.708893826063112,  accuracy: 0.7827737809752199
[2025-09-17 17:17:22,697][flp2p.graph_runner][INFO] - Train, Round 177 : loss => 0.0006458909264905135,  accuracy: 1.0, gradient_norm : 0.003228057686190274
[2025-09-17 17:17:26,460][flp2p.graph_runner][INFO] - Test, Round 177 : loss => 1.7033867294089293,  accuracy: 0.7891047466453034
[2025-09-17 17:17:40,942][flp2p.graph_runner][INFO] - Train, Round 178 : loss => 0.0006590320272613705,  accuracy: 1.0, gradient_norm : 0.003312242841328211
[2025-09-17 17:17:44,725][flp2p.graph_runner][INFO] - Test, Round 178 : loss => 1.6619617390353567,  accuracy: 0.790516206482593
[2025-09-17 17:17:59,204][flp2p.graph_runner][INFO] - Train, Round 179 : loss => 0.0006320564457322083,  accuracy: 1.0, gradient_norm : 0.0032548184245385714
[2025-09-17 17:18:03,002][flp2p.graph_runner][INFO] - Test, Round 179 : loss => 1.6494747776299714,  accuracy: 0.7904
[2025-09-17 17:18:17,392][flp2p.graph_runner][INFO] - Train, Round 180 : loss => 0.0006407651525538919,  accuracy: 1.0, gradient_norm : 0.003203046027889581
[2025-09-17 17:18:21,197][flp2p.graph_runner][INFO] - Test, Round 180 : loss => 1.7070222561776638,  accuracy: 0.7806
[2025-09-17 17:18:35,639][flp2p.graph_runner][INFO] - Train, Round 181 : loss => 0.000656985582842026,  accuracy: 1.0, gradient_norm : 0.003358059846166754
[2025-09-17 17:18:39,417][flp2p.graph_runner][INFO] - Test, Round 181 : loss => 1.654175663608927,  accuracy: 0.7903903903903904
[2025-09-17 17:18:54,202][flp2p.graph_runner][INFO] - Train, Round 182 : loss => 0.0006180858488911398,  accuracy: 1.0, gradient_norm : 0.003155444798764848
[2025-09-17 17:18:58,053][flp2p.graph_runner][INFO] - Test, Round 182 : loss => 1.6721614598293113,  accuracy: 0.789453048421878
[2025-09-17 17:19:12,699][flp2p.graph_runner][INFO] - Train, Round 183 : loss => 0.000618507381661961,  accuracy: 1.0, gradient_norm : 0.0031617404171417674
[2025-09-17 17:19:16,532][flp2p.graph_runner][INFO] - Test, Round 183 : loss => 1.719428854228096,  accuracy: 0.7906931081877087
[2025-09-17 17:19:31,075][flp2p.graph_runner][INFO] - Train, Round 184 : loss => 0.0006173791410862274,  accuracy: 1.0, gradient_norm : 0.003080264591900391
[2025-09-17 17:19:34,874][flp2p.graph_runner][INFO] - Test, Round 184 : loss => 1.656694884313358,  accuracy: 0.7957957957957958
[2025-09-17 17:19:49,380][flp2p.graph_runner][INFO] - Train, Round 185 : loss => 0.0006112523896978625,  accuracy: 1.0, gradient_norm : 0.00307339236437344
[2025-09-17 17:19:53,161][flp2p.graph_runner][INFO] - Test, Round 185 : loss => 1.7245937280559702,  accuracy: 0.7912329863891113
[2025-09-17 17:20:08,019][flp2p.graph_runner][INFO] - Train, Round 186 : loss => 0.000596757396661678,  accuracy: 1.0, gradient_norm : 0.0029772722841859456
[2025-09-17 17:20:11,909][flp2p.graph_runner][INFO] - Test, Round 186 : loss => 1.7587328909129665,  accuracy: 0.7856581532416503
[2025-09-17 17:20:26,454][flp2p.graph_runner][INFO] - Train, Round 187 : loss => 0.0005734592409426114,  accuracy: 1.0, gradient_norm : 0.003015613754090255
[2025-09-17 17:20:30,219][flp2p.graph_runner][INFO] - Test, Round 187 : loss => 1.650242691620497,  accuracy: 0.7875823517668197
[2025-09-17 17:20:44,959][flp2p.graph_runner][INFO] - Train, Round 188 : loss => 0.0005766811510612671,  accuracy: 1.0, gradient_norm : 0.0029541912966529983
[2025-09-17 17:20:48,829][flp2p.graph_runner][INFO] - Test, Round 188 : loss => 1.6841202384492777,  accuracy: 0.7858122672937488
[2025-09-17 17:21:03,454][flp2p.graph_runner][INFO] - Train, Round 189 : loss => 0.0005815984693423767,  accuracy: 1.0, gradient_norm : 0.0029577884793369836
[2025-09-17 17:21:07,201][flp2p.graph_runner][INFO] - Test, Round 189 : loss => 1.6772318216457751,  accuracy: 0.7855002995805872
[2025-09-17 17:21:22,062][flp2p.graph_runner][INFO] - Train, Round 190 : loss => 0.0005905598793117926,  accuracy: 1.0, gradient_norm : 0.0029714287536636553
[2025-09-17 17:21:25,941][flp2p.graph_runner][INFO] - Test, Round 190 : loss => 1.7233253873972332,  accuracy: 0.7852941176470588
[2025-09-17 17:21:40,500][flp2p.graph_runner][INFO] - Train, Round 191 : loss => 0.0005817846912735453,  accuracy: 1.0, gradient_norm : 0.0029378303481878785
[2025-09-17 17:21:44,191][flp2p.graph_runner][INFO] - Test, Round 191 : loss => 1.6789299940171776,  accuracy: 0.7873276034379373
[2025-09-17 17:21:58,667][flp2p.graph_runner][INFO] - Train, Round 192 : loss => 0.0005898290491556206,  accuracy: 1.0, gradient_norm : 0.0029511131041313533
[2025-09-17 17:22:02,468][flp2p.graph_runner][INFO] - Test, Round 192 : loss => 1.720494362694025,  accuracy: 0.7826
[2025-09-17 17:22:17,012][flp2p.graph_runner][INFO] - Train, Round 193 : loss => 0.0005752370219512766,  accuracy: 1.0, gradient_norm : 0.0029040334417582054
[2025-09-17 17:22:20,773][flp2p.graph_runner][INFO] - Test, Round 193 : loss => 1.683628811683502,  accuracy: 0.7937937937937938
[2025-09-17 17:22:35,573][flp2p.graph_runner][INFO] - Train, Round 194 : loss => 0.0005668954420091015,  accuracy: 1.0, gradient_norm : 0.0028660465163320797
[2025-09-17 17:22:39,378][flp2p.graph_runner][INFO] - Test, Round 194 : loss => 1.6843629598097576,  accuracy: 0.7880807684767692
[2025-09-17 17:22:54,301][flp2p.graph_runner][INFO] - Train, Round 195 : loss => 0.00056880124525705,  accuracy: 1.0, gradient_norm : 0.0028375496461612664
[2025-09-17 17:22:58,109][flp2p.graph_runner][INFO] - Test, Round 195 : loss => 1.725865651713602,  accuracy: 0.7857563272513243
[2025-09-17 17:23:12,770][flp2p.graph_runner][INFO] - Train, Round 196 : loss => 0.0005611332341989813,  accuracy: 1.0, gradient_norm : 0.0028582869907640834
[2025-09-17 17:23:16,528][flp2p.graph_runner][INFO] - Test, Round 196 : loss => 1.7070504043903052,  accuracy: 0.7899277688603531
[2025-09-17 17:23:31,052][flp2p.graph_runner][INFO] - Train, Round 197 : loss => 0.0005592138523061295,  accuracy: 1.0, gradient_norm : 0.002903372391078226
[2025-09-17 17:23:34,806][flp2p.graph_runner][INFO] - Test, Round 197 : loss => 1.740263664857338,  accuracy: 0.7792910074103745
[2025-09-17 17:23:49,356][flp2p.graph_runner][INFO] - Train, Round 198 : loss => 0.0005509529768508703,  accuracy: 1.0, gradient_norm : 0.0028014240005042133
[2025-09-17 17:23:53,098][flp2p.graph_runner][INFO] - Test, Round 198 : loss => 1.7062362393472208,  accuracy: 0.7853277209861695
[2025-09-17 17:24:07,990][flp2p.graph_runner][INFO] - Train, Round 199 : loss => 0.00054009507967909,  accuracy: 1.0, gradient_norm : 0.0027471210828947613
[2025-09-17 17:24:11,837][flp2p.graph_runner][INFO] - Test, Round 199 : loss => 1.719131631465884,  accuracy: 0.7866117000392618
[2025-09-17 17:24:11,839][__main__][INFO] - Train, Round 001: loss=2.2592, accuracy=0.1932, gradient_norm=0.2500, 
[2025-09-17 17:24:11,839][__main__][INFO] - Train, Round 002: loss=2.0286, accuracy=0.3036, gradient_norm=0.4552, 
[2025-09-17 17:24:11,839][__main__][INFO] - Train, Round 003: loss=1.7683, accuracy=0.3759, gradient_norm=0.4913, 
[2025-09-17 17:24:11,839][__main__][INFO] - Train, Round 004: loss=1.4712, accuracy=0.4735, gradient_norm=0.4703, 
[2025-09-17 17:24:11,840][__main__][INFO] - Train, Round 005: loss=1.3100, accuracy=0.5276, gradient_norm=0.4659, 
[2025-09-17 17:24:11,840][__main__][INFO] - Train, Round 006: loss=1.0926, accuracy=0.5898, gradient_norm=0.4140, 
[2025-09-17 17:24:11,840][__main__][INFO] - Train, Round 007: loss=1.0196, accuracy=0.6166, gradient_norm=0.3610, 
[2025-09-17 17:24:11,840][__main__][INFO] - Train, Round 008: loss=0.8642, accuracy=0.6700, gradient_norm=0.3245, 
[2025-09-17 17:24:11,840][__main__][INFO] - Train, Round 009: loss=0.8456, accuracy=0.6750, gradient_norm=0.3010, 
[2025-09-17 17:24:11,840][__main__][INFO] - Train, Round 010: loss=0.7777, accuracy=0.6982, gradient_norm=0.2888, 
[2025-09-17 17:24:11,840][__main__][INFO] - Train, Round 011: loss=0.7318, accuracy=0.7155, gradient_norm=0.2671, 
[2025-09-17 17:24:11,840][__main__][INFO] - Train, Round 012: loss=0.7205, accuracy=0.7189, gradient_norm=0.2671, 
[2025-09-17 17:24:11,840][__main__][INFO] - Train, Round 013: loss=0.6433, accuracy=0.7483, gradient_norm=0.2535, 
[2025-09-17 17:24:11,840][__main__][INFO] - Train, Round 014: loss=0.6405, accuracy=0.7499, gradient_norm=0.2415, 
[2025-09-17 17:24:11,840][__main__][INFO] - Train, Round 015: loss=0.6089, accuracy=0.7611, gradient_norm=0.2364, 
[2025-09-17 17:24:11,840][__main__][INFO] - Train, Round 016: loss=0.5667, accuracy=0.7784, gradient_norm=0.2227, 
[2025-09-17 17:24:11,840][__main__][INFO] - Train, Round 017: loss=0.5476, accuracy=0.7834, gradient_norm=0.2246, 
[2025-09-17 17:24:11,840][__main__][INFO] - Train, Round 018: loss=0.5449, accuracy=0.7841, gradient_norm=0.2160, 
[2025-09-17 17:24:11,840][__main__][INFO] - Train, Round 019: loss=0.5092, accuracy=0.7998, gradient_norm=0.2162, 
[2025-09-17 17:24:11,840][__main__][INFO] - Train, Round 020: loss=0.5041, accuracy=0.8011, gradient_norm=0.2174, 
[2025-09-17 17:24:11,840][__main__][INFO] - Train, Round 021: loss=0.4880, accuracy=0.8080, gradient_norm=0.2159, 
[2025-09-17 17:24:11,840][__main__][INFO] - Train, Round 022: loss=0.4665, accuracy=0.8157, gradient_norm=0.2211, 
[2025-09-17 17:24:11,840][__main__][INFO] - Train, Round 023: loss=0.4525, accuracy=0.8226, gradient_norm=0.2183, 
[2025-09-17 17:24:11,840][__main__][INFO] - Train, Round 024: loss=0.4496, accuracy=0.8229, gradient_norm=0.2157, 
[2025-09-17 17:24:11,840][__main__][INFO] - Train, Round 025: loss=0.4007, accuracy=0.8417, gradient_norm=0.2104, 
[2025-09-17 17:24:11,840][__main__][INFO] - Train, Round 026: loss=0.4034, accuracy=0.8410, gradient_norm=0.2032, 
[2025-09-17 17:24:11,840][__main__][INFO] - Train, Round 027: loss=0.3823, accuracy=0.8513, gradient_norm=0.2043, 
[2025-09-17 17:24:11,840][__main__][INFO] - Train, Round 028: loss=0.3726, accuracy=0.8566, gradient_norm=0.2007, 
[2025-09-17 17:24:11,840][__main__][INFO] - Train, Round 029: loss=0.3596, accuracy=0.8610, gradient_norm=0.2015, 
[2025-09-17 17:24:11,840][__main__][INFO] - Train, Round 030: loss=0.3330, accuracy=0.8697, gradient_norm=0.1991, 
[2025-09-17 17:24:11,840][__main__][INFO] - Train, Round 031: loss=0.3310, accuracy=0.8732, gradient_norm=0.2038, 
[2025-09-17 17:24:11,840][__main__][INFO] - Train, Round 032: loss=0.3165, accuracy=0.8791, gradient_norm=0.2043, 
[2025-09-17 17:24:11,840][__main__][INFO] - Train, Round 033: loss=0.3042, accuracy=0.8823, gradient_norm=0.2057, 
[2025-09-17 17:24:11,840][__main__][INFO] - Train, Round 034: loss=0.2975, accuracy=0.8852, gradient_norm=0.1986, 
[2025-09-17 17:24:11,840][__main__][INFO] - Train, Round 035: loss=0.2687, accuracy=0.8975, gradient_norm=0.1895, 
[2025-09-17 17:24:11,840][__main__][INFO] - Train, Round 036: loss=0.2641, accuracy=0.9004, gradient_norm=0.1918, 
[2025-09-17 17:24:11,840][__main__][INFO] - Train, Round 037: loss=0.2539, accuracy=0.9049, gradient_norm=0.1930, 
[2025-09-17 17:24:11,840][__main__][INFO] - Train, Round 038: loss=0.2471, accuracy=0.9059, gradient_norm=0.1907, 
[2025-09-17 17:24:11,840][__main__][INFO] - Train, Round 039: loss=0.2274, accuracy=0.9162, gradient_norm=0.1904, 
[2025-09-17 17:24:11,840][__main__][INFO] - Train, Round 040: loss=0.2260, accuracy=0.9164, gradient_norm=0.1900, 
[2025-09-17 17:24:11,840][__main__][INFO] - Train, Round 041: loss=0.2108, accuracy=0.9203, gradient_norm=0.1875, 
[2025-09-17 17:24:11,840][__main__][INFO] - Train, Round 042: loss=0.2029, accuracy=0.9262, gradient_norm=0.1848, 
[2025-09-17 17:24:11,840][__main__][INFO] - Train, Round 043: loss=0.1787, accuracy=0.9353, gradient_norm=0.1801, 
[2025-09-17 17:24:11,840][__main__][INFO] - Train, Round 044: loss=0.1840, accuracy=0.9333, gradient_norm=0.1908, 
[2025-09-17 17:24:11,840][__main__][INFO] - Train, Round 045: loss=0.1578, accuracy=0.9441, gradient_norm=0.1666, 
[2025-09-17 17:24:11,840][__main__][INFO] - Train, Round 046: loss=0.1618, accuracy=0.9424, gradient_norm=0.1751, 
[2025-09-17 17:24:11,840][__main__][INFO] - Train, Round 047: loss=0.1476, accuracy=0.9475, gradient_norm=0.1821, 
[2025-09-17 17:24:11,840][__main__][INFO] - Train, Round 048: loss=0.1461, accuracy=0.9492, gradient_norm=0.1737, 
[2025-09-17 17:24:11,840][__main__][INFO] - Train, Round 049: loss=0.1380, accuracy=0.9527, gradient_norm=0.1786, 
[2025-09-17 17:24:11,840][__main__][INFO] - Train, Round 050: loss=0.1323, accuracy=0.9552, gradient_norm=0.1673, 
[2025-09-17 17:24:11,840][__main__][INFO] - Train, Round 051: loss=0.1162, accuracy=0.9609, gradient_norm=0.1610, 
[2025-09-17 17:24:11,840][__main__][INFO] - Train, Round 052: loss=0.1068, accuracy=0.9642, gradient_norm=0.1500, 
[2025-09-17 17:24:11,840][__main__][INFO] - Train, Round 053: loss=0.1060, accuracy=0.9642, gradient_norm=0.1533, 
[2025-09-17 17:24:11,840][__main__][INFO] - Train, Round 054: loss=0.0934, accuracy=0.9694, gradient_norm=0.1511, 
[2025-09-17 17:24:11,840][__main__][INFO] - Train, Round 055: loss=0.0883, accuracy=0.9715, gradient_norm=0.1452, 
[2025-09-17 17:24:11,840][__main__][INFO] - Train, Round 056: loss=0.0926, accuracy=0.9698, gradient_norm=0.1478, 
[2025-09-17 17:24:11,840][__main__][INFO] - Train, Round 057: loss=0.0820, accuracy=0.9739, gradient_norm=0.1421, 
[2025-09-17 17:24:11,841][__main__][INFO] - Train, Round 058: loss=0.0708, accuracy=0.9790, gradient_norm=0.1345, 
[2025-09-17 17:24:11,841][__main__][INFO] - Train, Round 059: loss=0.0622, accuracy=0.9814, gradient_norm=0.1245, 
[2025-09-17 17:24:11,841][__main__][INFO] - Train, Round 060: loss=0.0790, accuracy=0.9751, gradient_norm=0.1401, 
[2025-09-17 17:24:11,841][__main__][INFO] - Train, Round 061: loss=0.0634, accuracy=0.9814, gradient_norm=0.1310, 
[2025-09-17 17:24:11,841][__main__][INFO] - Train, Round 062: loss=0.0494, accuracy=0.9863, gradient_norm=0.1076, 
[2025-09-17 17:24:11,841][__main__][INFO] - Train, Round 063: loss=0.0512, accuracy=0.9853, gradient_norm=0.1081, 
[2025-09-17 17:24:11,841][__main__][INFO] - Train, Round 064: loss=0.0545, accuracy=0.9842, gradient_norm=0.1133, 
[2025-09-17 17:24:11,841][__main__][INFO] - Train, Round 065: loss=0.0472, accuracy=0.9873, gradient_norm=0.1041, 
[2025-09-17 17:24:11,841][__main__][INFO] - Train, Round 066: loss=0.0443, accuracy=0.9878, gradient_norm=0.0998, 
[2025-09-17 17:24:11,841][__main__][INFO] - Train, Round 067: loss=0.0318, accuracy=0.9917, gradient_norm=0.0881, 
[2025-09-17 17:24:11,841][__main__][INFO] - Train, Round 068: loss=0.0381, accuracy=0.9898, gradient_norm=0.0926, 
[2025-09-17 17:24:11,841][__main__][INFO] - Train, Round 069: loss=0.0250, accuracy=0.9945, gradient_norm=0.0753, 
[2025-09-17 17:24:11,841][__main__][INFO] - Train, Round 070: loss=0.0257, accuracy=0.9943, gradient_norm=0.0730, 
[2025-09-17 17:24:11,841][__main__][INFO] - Train, Round 071: loss=0.0279, accuracy=0.9932, gradient_norm=0.0845, 
[2025-09-17 17:24:11,841][__main__][INFO] - Train, Round 072: loss=0.0260, accuracy=0.9935, gradient_norm=0.0751, 
[2025-09-17 17:24:11,841][__main__][INFO] - Train, Round 073: loss=0.0270, accuracy=0.9934, gradient_norm=0.0775, 
[2025-09-17 17:24:11,841][__main__][INFO] - Train, Round 074: loss=0.0249, accuracy=0.9942, gradient_norm=0.0738, 
[2025-09-17 17:24:11,841][__main__][INFO] - Train, Round 075: loss=0.0178, accuracy=0.9966, gradient_norm=0.0610, 
[2025-09-17 17:24:11,841][__main__][INFO] - Train, Round 076: loss=0.0176, accuracy=0.9962, gradient_norm=0.0668, 
[2025-09-17 17:24:11,841][__main__][INFO] - Train, Round 077: loss=0.0175, accuracy=0.9966, gradient_norm=0.0560, 
[2025-09-17 17:24:11,841][__main__][INFO] - Train, Round 078: loss=0.0167, accuracy=0.9963, gradient_norm=0.0560, 
[2025-09-17 17:24:11,841][__main__][INFO] - Train, Round 079: loss=0.0185, accuracy=0.9960, gradient_norm=0.0625, 
[2025-09-17 17:24:11,841][__main__][INFO] - Train, Round 080: loss=0.0127, accuracy=0.9979, gradient_norm=0.0513, 
[2025-09-17 17:24:11,841][__main__][INFO] - Train, Round 081: loss=0.0144, accuracy=0.9972, gradient_norm=0.0497, 
[2025-09-17 17:24:11,841][__main__][INFO] - Train, Round 082: loss=0.0090, accuracy=0.9988, gradient_norm=0.0352, 
[2025-09-17 17:24:11,841][__main__][INFO] - Train, Round 083: loss=0.0133, accuracy=0.9972, gradient_norm=0.0446, 
[2025-09-17 17:24:11,841][__main__][INFO] - Train, Round 084: loss=0.0129, accuracy=0.9972, gradient_norm=0.0453, 
[2025-09-17 17:24:11,841][__main__][INFO] - Train, Round 085: loss=0.0099, accuracy=0.9984, gradient_norm=0.0360, 
[2025-09-17 17:24:11,841][__main__][INFO] - Train, Round 086: loss=0.0095, accuracy=0.9984, gradient_norm=0.0357, 
[2025-09-17 17:24:11,841][__main__][INFO] - Train, Round 087: loss=0.0088, accuracy=0.9987, gradient_norm=0.0352, 
[2025-09-17 17:24:11,841][__main__][INFO] - Train, Round 088: loss=0.0100, accuracy=0.9980, gradient_norm=0.0375, 
[2025-09-17 17:24:11,841][__main__][INFO] - Train, Round 089: loss=0.0066, accuracy=0.9992, gradient_norm=0.0281, 
[2025-09-17 17:24:11,841][__main__][INFO] - Train, Round 090: loss=0.0055, accuracy=0.9995, gradient_norm=0.0242, 
[2025-09-17 17:24:11,841][__main__][INFO] - Train, Round 091: loss=0.0054, accuracy=0.9993, gradient_norm=0.0247, 
[2025-09-17 17:24:11,841][__main__][INFO] - Train, Round 092: loss=0.0042, accuracy=0.9997, gradient_norm=0.0214, 
[2025-09-17 17:24:11,841][__main__][INFO] - Train, Round 093: loss=0.0061, accuracy=0.9992, gradient_norm=0.0261, 
[2025-09-17 17:24:11,841][__main__][INFO] - Train, Round 094: loss=0.0043, accuracy=0.9997, gradient_norm=0.0182, 
[2025-09-17 17:24:11,841][__main__][INFO] - Train, Round 095: loss=0.0035, accuracy=0.9998, gradient_norm=0.0172, 
[2025-09-17 17:24:11,841][__main__][INFO] - Train, Round 096: loss=0.0040, accuracy=0.9998, gradient_norm=0.0192, 
[2025-09-17 17:24:11,841][__main__][INFO] - Train, Round 097: loss=0.0038, accuracy=0.9998, gradient_norm=0.0181, 
[2025-09-17 17:24:11,841][__main__][INFO] - Train, Round 098: loss=0.0026, accuracy=1.0000, gradient_norm=0.0123, 
[2025-09-17 17:24:11,841][__main__][INFO] - Train, Round 099: loss=0.0030, accuracy=0.9999, gradient_norm=0.0146, 
[2025-09-17 17:24:11,841][__main__][INFO] - Train, Round 100: loss=0.0028, accuracy=0.9999, gradient_norm=0.0134, 
[2025-09-17 17:24:11,841][__main__][INFO] - Train, Round 101: loss=0.0035, accuracy=0.9996, gradient_norm=0.0168, 
[2025-09-17 17:24:11,841][__main__][INFO] - Train, Round 102: loss=0.0029, accuracy=0.9998, gradient_norm=0.0132, 
[2025-09-17 17:24:11,841][__main__][INFO] - Train, Round 103: loss=0.0024, accuracy=1.0000, gradient_norm=0.0121, 
[2025-09-17 17:24:11,841][__main__][INFO] - Train, Round 104: loss=0.0025, accuracy=0.9999, gradient_norm=0.0118, 
[2025-09-17 17:24:11,841][__main__][INFO] - Train, Round 105: loss=0.0022, accuracy=1.0000, gradient_norm=0.0113, 
[2025-09-17 17:24:11,841][__main__][INFO] - Train, Round 106: loss=0.0020, accuracy=1.0000, gradient_norm=0.0104, 
[2025-09-17 17:24:11,841][__main__][INFO] - Train, Round 107: loss=0.0022, accuracy=1.0000, gradient_norm=0.0108, 
[2025-09-17 17:24:11,841][__main__][INFO] - Train, Round 108: loss=0.0020, accuracy=1.0000, gradient_norm=0.0102, 
[2025-09-17 17:24:11,841][__main__][INFO] - Train, Round 109: loss=0.0020, accuracy=1.0000, gradient_norm=0.0101, 
[2025-09-17 17:24:11,841][__main__][INFO] - Train, Round 110: loss=0.0020, accuracy=1.0000, gradient_norm=0.0092, 
[2025-09-17 17:24:11,841][__main__][INFO] - Train, Round 111: loss=0.0020, accuracy=1.0000, gradient_norm=0.0097, 
[2025-09-17 17:24:11,841][__main__][INFO] - Train, Round 112: loss=0.0017, accuracy=1.0000, gradient_norm=0.0091, 
[2025-09-17 17:24:11,841][__main__][INFO] - Train, Round 113: loss=0.0017, accuracy=1.0000, gradient_norm=0.0082, 
[2025-09-17 17:24:11,841][__main__][INFO] - Train, Round 114: loss=0.0017, accuracy=1.0000, gradient_norm=0.0086, 
[2025-09-17 17:24:11,841][__main__][INFO] - Train, Round 115: loss=0.0017, accuracy=1.0000, gradient_norm=0.0085, 
[2025-09-17 17:24:11,841][__main__][INFO] - Train, Round 116: loss=0.0016, accuracy=1.0000, gradient_norm=0.0079, 
[2025-09-17 17:24:11,841][__main__][INFO] - Train, Round 117: loss=0.0015, accuracy=1.0000, gradient_norm=0.0072, 
[2025-09-17 17:24:11,841][__main__][INFO] - Train, Round 118: loss=0.0015, accuracy=1.0000, gradient_norm=0.0074, 
[2025-09-17 17:24:11,841][__main__][INFO] - Train, Round 119: loss=0.0015, accuracy=1.0000, gradient_norm=0.0076, 
[2025-09-17 17:24:11,842][__main__][INFO] - Train, Round 120: loss=0.0015, accuracy=1.0000, gradient_norm=0.0078, 
[2025-09-17 17:24:11,842][__main__][INFO] - Train, Round 121: loss=0.0014, accuracy=1.0000, gradient_norm=0.0069, 
[2025-09-17 17:24:11,842][__main__][INFO] - Train, Round 122: loss=0.0014, accuracy=1.0000, gradient_norm=0.0070, 
[2025-09-17 17:24:11,842][__main__][INFO] - Train, Round 123: loss=0.0013, accuracy=1.0000, gradient_norm=0.0066, 
[2025-09-17 17:24:11,842][__main__][INFO] - Train, Round 124: loss=0.0013, accuracy=1.0000, gradient_norm=0.0065, 
[2025-09-17 17:24:11,842][__main__][INFO] - Train, Round 125: loss=0.0013, accuracy=1.0000, gradient_norm=0.0064, 
[2025-09-17 17:24:11,842][__main__][INFO] - Train, Round 126: loss=0.0013, accuracy=1.0000, gradient_norm=0.0065, 
[2025-09-17 17:24:11,842][__main__][INFO] - Train, Round 127: loss=0.0013, accuracy=1.0000, gradient_norm=0.0061, 
[2025-09-17 17:24:11,842][__main__][INFO] - Train, Round 128: loss=0.0012, accuracy=1.0000, gradient_norm=0.0059, 
[2025-09-17 17:24:11,842][__main__][INFO] - Train, Round 129: loss=0.0012, accuracy=1.0000, gradient_norm=0.0059, 
[2025-09-17 17:24:11,842][__main__][INFO] - Train, Round 130: loss=0.0013, accuracy=1.0000, gradient_norm=0.0059, 
[2025-09-17 17:24:11,842][__main__][INFO] - Train, Round 131: loss=0.0012, accuracy=1.0000, gradient_norm=0.0057, 
[2025-09-17 17:24:11,842][__main__][INFO] - Train, Round 132: loss=0.0011, accuracy=1.0000, gradient_norm=0.0055, 
[2025-09-17 17:24:11,842][__main__][INFO] - Train, Round 133: loss=0.0011, accuracy=1.0000, gradient_norm=0.0058, 
[2025-09-17 17:24:11,842][__main__][INFO] - Train, Round 134: loss=0.0011, accuracy=1.0000, gradient_norm=0.0057, 
[2025-09-17 17:24:11,842][__main__][INFO] - Train, Round 135: loss=0.0011, accuracy=1.0000, gradient_norm=0.0058, 
[2025-09-17 17:24:11,842][__main__][INFO] - Train, Round 136: loss=0.0011, accuracy=1.0000, gradient_norm=0.0051, 
[2025-09-17 17:24:11,842][__main__][INFO] - Train, Round 137: loss=0.0010, accuracy=1.0000, gradient_norm=0.0048, 
[2025-09-17 17:24:11,842][__main__][INFO] - Train, Round 138: loss=0.0011, accuracy=1.0000, gradient_norm=0.0053, 
[2025-09-17 17:24:11,842][__main__][INFO] - Train, Round 139: loss=0.0010, accuracy=1.0000, gradient_norm=0.0052, 
[2025-09-17 17:24:11,842][__main__][INFO] - Train, Round 140: loss=0.0010, accuracy=1.0000, gradient_norm=0.0050, 
[2025-09-17 17:24:11,842][__main__][INFO] - Train, Round 141: loss=0.0010, accuracy=1.0000, gradient_norm=0.0049, 
[2025-09-17 17:24:11,842][__main__][INFO] - Train, Round 142: loss=0.0010, accuracy=1.0000, gradient_norm=0.0048, 
[2025-09-17 17:24:11,842][__main__][INFO] - Train, Round 143: loss=0.0010, accuracy=1.0000, gradient_norm=0.0047, 
[2025-09-17 17:24:11,842][__main__][INFO] - Train, Round 144: loss=0.0009, accuracy=1.0000, gradient_norm=0.0046, 
[2025-09-17 17:24:11,842][__main__][INFO] - Train, Round 145: loss=0.0009, accuracy=1.0000, gradient_norm=0.0047, 
[2025-09-17 17:24:11,842][__main__][INFO] - Train, Round 146: loss=0.0009, accuracy=1.0000, gradient_norm=0.0046, 
[2025-09-17 17:24:11,842][__main__][INFO] - Train, Round 147: loss=0.0009, accuracy=1.0000, gradient_norm=0.0044, 
[2025-09-17 17:24:11,842][__main__][INFO] - Train, Round 148: loss=0.0009, accuracy=1.0000, gradient_norm=0.0047, 
[2025-09-17 17:24:11,842][__main__][INFO] - Train, Round 149: loss=0.0009, accuracy=1.0000, gradient_norm=0.0046, 
[2025-09-17 17:24:11,842][__main__][INFO] - Train, Round 150: loss=0.0009, accuracy=1.0000, gradient_norm=0.0043, 
[2025-09-17 17:24:11,842][__main__][INFO] - Train, Round 151: loss=0.0009, accuracy=1.0000, gradient_norm=0.0048, 
[2025-09-17 17:24:11,842][__main__][INFO] - Train, Round 152: loss=0.0009, accuracy=1.0000, gradient_norm=0.0045, 
[2025-09-17 17:24:11,842][__main__][INFO] - Train, Round 153: loss=0.0009, accuracy=1.0000, gradient_norm=0.0043, 
[2025-09-17 17:24:11,842][__main__][INFO] - Train, Round 154: loss=0.0008, accuracy=1.0000, gradient_norm=0.0044, 
[2025-09-17 17:24:11,842][__main__][INFO] - Train, Round 155: loss=0.0008, accuracy=1.0000, gradient_norm=0.0040, 
[2025-09-17 17:24:11,842][__main__][INFO] - Train, Round 156: loss=0.0008, accuracy=1.0000, gradient_norm=0.0038, 
[2025-09-17 17:24:11,842][__main__][INFO] - Train, Round 157: loss=0.0008, accuracy=1.0000, gradient_norm=0.0041, 
[2025-09-17 17:24:11,842][__main__][INFO] - Train, Round 158: loss=0.0008, accuracy=1.0000, gradient_norm=0.0043, 
[2025-09-17 17:24:11,842][__main__][INFO] - Train, Round 159: loss=0.0008, accuracy=1.0000, gradient_norm=0.0039, 
[2025-09-17 17:24:11,842][__main__][INFO] - Train, Round 160: loss=0.0008, accuracy=1.0000, gradient_norm=0.0039, 
[2025-09-17 17:24:11,842][__main__][INFO] - Train, Round 161: loss=0.0008, accuracy=1.0000, gradient_norm=0.0038, 
[2025-09-17 17:24:11,842][__main__][INFO] - Train, Round 162: loss=0.0008, accuracy=1.0000, gradient_norm=0.0039, 
[2025-09-17 17:24:11,842][__main__][INFO] - Train, Round 163: loss=0.0007, accuracy=1.0000, gradient_norm=0.0036, 
[2025-09-17 17:24:11,842][__main__][INFO] - Train, Round 164: loss=0.0008, accuracy=1.0000, gradient_norm=0.0039, 
[2025-09-17 17:24:11,842][__main__][INFO] - Train, Round 165: loss=0.0008, accuracy=1.0000, gradient_norm=0.0037, 
[2025-09-17 17:24:11,842][__main__][INFO] - Train, Round 166: loss=0.0007, accuracy=1.0000, gradient_norm=0.0038, 
[2025-09-17 17:24:11,842][__main__][INFO] - Train, Round 167: loss=0.0007, accuracy=1.0000, gradient_norm=0.0037, 
[2025-09-17 17:24:11,842][__main__][INFO] - Train, Round 168: loss=0.0007, accuracy=1.0000, gradient_norm=0.0036, 
[2025-09-17 17:24:11,842][__main__][INFO] - Train, Round 169: loss=0.0007, accuracy=1.0000, gradient_norm=0.0038, 
[2025-09-17 17:24:11,842][__main__][INFO] - Train, Round 170: loss=0.0007, accuracy=1.0000, gradient_norm=0.0035, 
[2025-09-17 17:24:11,842][__main__][INFO] - Train, Round 171: loss=0.0007, accuracy=1.0000, gradient_norm=0.0035, 
[2025-09-17 17:24:11,842][__main__][INFO] - Train, Round 172: loss=0.0007, accuracy=1.0000, gradient_norm=0.0034, 
[2025-09-17 17:24:11,842][__main__][INFO] - Train, Round 173: loss=0.0007, accuracy=1.0000, gradient_norm=0.0033, 
[2025-09-17 17:24:11,842][__main__][INFO] - Train, Round 174: loss=0.0007, accuracy=1.0000, gradient_norm=0.0037, 
[2025-09-17 17:24:11,842][__main__][INFO] - Train, Round 175: loss=0.0006, accuracy=1.0000, gradient_norm=0.0032, 
[2025-09-17 17:24:11,842][__main__][INFO] - Train, Round 176: loss=0.0007, accuracy=1.0000, gradient_norm=0.0033, 
[2025-09-17 17:24:11,842][__main__][INFO] - Train, Round 177: loss=0.0007, accuracy=1.0000, gradient_norm=0.0033, 
[2025-09-17 17:24:11,842][__main__][INFO] - Train, Round 178: loss=0.0006, accuracy=1.0000, gradient_norm=0.0032, 
[2025-09-17 17:24:11,842][__main__][INFO] - Train, Round 179: loss=0.0007, accuracy=1.0000, gradient_norm=0.0033, 
[2025-09-17 17:24:11,842][__main__][INFO] - Train, Round 180: loss=0.0006, accuracy=1.0000, gradient_norm=0.0033, 
[2025-09-17 17:24:11,842][__main__][INFO] - Train, Round 181: loss=0.0006, accuracy=1.0000, gradient_norm=0.0032, 
[2025-09-17 17:24:11,842][__main__][INFO] - Train, Round 182: loss=0.0007, accuracy=1.0000, gradient_norm=0.0034, 
[2025-09-17 17:24:11,842][__main__][INFO] - Train, Round 183: loss=0.0006, accuracy=1.0000, gradient_norm=0.0032, 
[2025-09-17 17:24:11,842][__main__][INFO] - Train, Round 184: loss=0.0006, accuracy=1.0000, gradient_norm=0.0032, 
[2025-09-17 17:24:11,842][__main__][INFO] - Train, Round 185: loss=0.0006, accuracy=1.0000, gradient_norm=0.0031, 
[2025-09-17 17:24:11,842][__main__][INFO] - Train, Round 186: loss=0.0006, accuracy=1.0000, gradient_norm=0.0031, 
[2025-09-17 17:24:11,842][__main__][INFO] - Train, Round 187: loss=0.0006, accuracy=1.0000, gradient_norm=0.0030, 
[2025-09-17 17:24:11,843][__main__][INFO] - Train, Round 188: loss=0.0006, accuracy=1.0000, gradient_norm=0.0030, 
[2025-09-17 17:24:11,843][__main__][INFO] - Train, Round 189: loss=0.0006, accuracy=1.0000, gradient_norm=0.0030, 
[2025-09-17 17:24:11,843][__main__][INFO] - Train, Round 190: loss=0.0006, accuracy=1.0000, gradient_norm=0.0030, 
[2025-09-17 17:24:11,843][__main__][INFO] - Train, Round 191: loss=0.0006, accuracy=1.0000, gradient_norm=0.0030, 
[2025-09-17 17:24:11,843][__main__][INFO] - Train, Round 192: loss=0.0006, accuracy=1.0000, gradient_norm=0.0029, 
[2025-09-17 17:24:11,843][__main__][INFO] - Train, Round 193: loss=0.0006, accuracy=1.0000, gradient_norm=0.0030, 
[2025-09-17 17:24:11,843][__main__][INFO] - Train, Round 194: loss=0.0006, accuracy=1.0000, gradient_norm=0.0029, 
[2025-09-17 17:24:11,843][__main__][INFO] - Train, Round 195: loss=0.0006, accuracy=1.0000, gradient_norm=0.0029, 
[2025-09-17 17:24:11,843][__main__][INFO] - Train, Round 196: loss=0.0006, accuracy=1.0000, gradient_norm=0.0028, 
[2025-09-17 17:24:11,843][__main__][INFO] - Train, Round 197: loss=0.0006, accuracy=1.0000, gradient_norm=0.0029, 
[2025-09-17 17:24:11,843][__main__][INFO] - Train, Round 198: loss=0.0006, accuracy=1.0000, gradient_norm=0.0029, 
[2025-09-17 17:24:11,843][__main__][INFO] - Train, Round 199: loss=0.0006, accuracy=1.0000, gradient_norm=0.0028, 
[2025-09-17 17:24:11,843][__main__][INFO] - Train, Round 200: loss=0.0005, accuracy=1.0000, gradient_norm=0.0027, 
[2025-09-17 17:24:11,843][__main__][INFO] - Test, Round 001: loss=2.1393, accuracy=0.2601, 
[2025-09-17 17:24:11,843][__main__][INFO] - Test, Round 002: loss=1.6875, accuracy=0.3982, 
[2025-09-17 17:24:11,843][__main__][INFO] - Test, Round 003: loss=1.4519, accuracy=0.4799, 
[2025-09-17 17:24:11,843][__main__][INFO] - Test, Round 004: loss=1.2796, accuracy=0.5301, 
[2025-09-17 17:24:11,843][__main__][INFO] - Test, Round 005: loss=1.1450, accuracy=0.5687, 
[2025-09-17 17:24:11,843][__main__][INFO] - Test, Round 006: loss=1.0599, accuracy=0.6072, 
[2025-09-17 17:24:11,843][__main__][INFO] - Test, Round 007: loss=1.0039, accuracy=0.6191, 
[2025-09-17 17:24:11,843][__main__][INFO] - Test, Round 008: loss=0.9266, accuracy=0.6535, 
[2025-09-17 17:24:11,843][__main__][INFO] - Test, Round 009: loss=0.9247, accuracy=0.6559, 
[2025-09-17 17:24:11,843][__main__][INFO] - Test, Round 010: loss=0.9005, accuracy=0.6652, 
[2025-09-17 17:24:11,843][__main__][INFO] - Test, Round 011: loss=0.8519, accuracy=0.6845, 
[2025-09-17 17:24:11,843][__main__][INFO] - Test, Round 012: loss=0.8316, accuracy=0.6856, 
[2025-09-17 17:24:11,843][__main__][INFO] - Test, Round 013: loss=0.8254, accuracy=0.6953, 
[2025-09-17 17:24:11,843][__main__][INFO] - Test, Round 014: loss=0.8233, accuracy=0.6952, 
[2025-09-17 17:24:11,843][__main__][INFO] - Test, Round 015: loss=0.7935, accuracy=0.7085, 
[2025-09-17 17:24:11,843][__main__][INFO] - Test, Round 016: loss=0.7767, accuracy=0.7151, 
[2025-09-17 17:24:11,843][__main__][INFO] - Test, Round 017: loss=0.7774, accuracy=0.7220, 
[2025-09-17 17:24:11,843][__main__][INFO] - Test, Round 018: loss=0.7793, accuracy=0.7271, 
[2025-09-17 17:24:11,843][__main__][INFO] - Test, Round 019: loss=0.7964, accuracy=0.7193, 
[2025-09-17 17:24:11,843][__main__][INFO] - Test, Round 020: loss=0.7674, accuracy=0.7231, 
[2025-09-17 17:24:11,843][__main__][INFO] - Test, Round 021: loss=0.7507, accuracy=0.7202, 
[2025-09-17 17:24:11,843][__main__][INFO] - Test, Round 022: loss=0.7645, accuracy=0.7285, 
[2025-09-17 17:24:11,843][__main__][INFO] - Test, Round 023: loss=0.7461, accuracy=0.7434, 
[2025-09-17 17:24:11,843][__main__][INFO] - Test, Round 024: loss=0.7724, accuracy=0.7235, 
[2025-09-17 17:24:11,843][__main__][INFO] - Test, Round 025: loss=0.7622, accuracy=0.7383, 
[2025-09-17 17:24:11,843][__main__][INFO] - Test, Round 026: loss=0.7575, accuracy=0.7406, 
[2025-09-17 17:24:11,843][__main__][INFO] - Test, Round 027: loss=0.7226, accuracy=0.7533, 
[2025-09-17 17:24:11,843][__main__][INFO] - Test, Round 028: loss=0.7515, accuracy=0.7496, 
[2025-09-17 17:24:11,843][__main__][INFO] - Test, Round 029: loss=0.7715, accuracy=0.7497, 
[2025-09-17 17:24:11,843][__main__][INFO] - Test, Round 030: loss=0.7552, accuracy=0.7587, 
[2025-09-17 17:24:11,843][__main__][INFO] - Test, Round 031: loss=0.7853, accuracy=0.7473, 
[2025-09-17 17:24:11,843][__main__][INFO] - Test, Round 032: loss=0.7901, accuracy=0.7472, 
[2025-09-17 17:24:11,843][__main__][INFO] - Test, Round 033: loss=0.7965, accuracy=0.7500, 
[2025-09-17 17:24:11,843][__main__][INFO] - Test, Round 034: loss=0.8048, accuracy=0.7492, 
[2025-09-17 17:24:11,843][__main__][INFO] - Test, Round 035: loss=0.7993, accuracy=0.7637, 
[2025-09-17 17:24:11,843][__main__][INFO] - Test, Round 036: loss=0.8036, accuracy=0.7573, 
[2025-09-17 17:24:11,843][__main__][INFO] - Test, Round 037: loss=0.8222, accuracy=0.7534, 
[2025-09-17 17:24:11,843][__main__][INFO] - Test, Round 038: loss=0.8124, accuracy=0.7604, 
[2025-09-17 17:24:11,843][__main__][INFO] - Test, Round 039: loss=0.8081, accuracy=0.7615, 
[2025-09-17 17:24:11,843][__main__][INFO] - Test, Round 040: loss=0.8165, accuracy=0.7554, 
[2025-09-17 17:24:11,843][__main__][INFO] - Test, Round 041: loss=0.8504, accuracy=0.7557, 
[2025-09-17 17:24:11,843][__main__][INFO] - Test, Round 042: loss=0.8503, accuracy=0.7640, 
[2025-09-17 17:24:11,843][__main__][INFO] - Test, Round 043: loss=0.8773, accuracy=0.7672, 
[2025-09-17 17:24:11,843][__main__][INFO] - Test, Round 044: loss=0.8739, accuracy=0.7606, 
[2025-09-17 17:24:11,843][__main__][INFO] - Test, Round 045: loss=0.9211, accuracy=0.7722, 
[2025-09-17 17:24:11,843][__main__][INFO] - Test, Round 046: loss=0.9013, accuracy=0.7621, 
[2025-09-17 17:24:11,843][__main__][INFO] - Test, Round 047: loss=0.9214, accuracy=0.7647, 
[2025-09-17 17:24:11,843][__main__][INFO] - Test, Round 048: loss=0.9060, accuracy=0.7639, 
[2025-09-17 17:24:11,843][__main__][INFO] - Test, Round 049: loss=0.9600, accuracy=0.7672, 
[2025-09-17 17:24:11,843][__main__][INFO] - Test, Round 050: loss=0.9631, accuracy=0.7681, 
[2025-09-17 17:24:11,843][__main__][INFO] - Test, Round 051: loss=0.9620, accuracy=0.7728, 
[2025-09-17 17:24:11,843][__main__][INFO] - Test, Round 052: loss=0.9756, accuracy=0.7686, 
[2025-09-17 17:24:11,843][__main__][INFO] - Test, Round 053: loss=0.9756, accuracy=0.7748, 
[2025-09-17 17:24:11,843][__main__][INFO] - Test, Round 054: loss=1.0068, accuracy=0.7671, 
[2025-09-17 17:24:11,844][__main__][INFO] - Test, Round 055: loss=1.0504, accuracy=0.7706, 
[2025-09-17 17:24:11,844][__main__][INFO] - Test, Round 056: loss=1.0202, accuracy=0.7804, 
[2025-09-17 17:24:11,844][__main__][INFO] - Test, Round 057: loss=1.0428, accuracy=0.7699, 
[2025-09-17 17:24:11,844][__main__][INFO] - Test, Round 058: loss=1.0136, accuracy=0.7785, 
[2025-09-17 17:24:11,844][__main__][INFO] - Test, Round 059: loss=1.0630, accuracy=0.7786, 
[2025-09-17 17:24:11,844][__main__][INFO] - Test, Round 060: loss=1.0282, accuracy=0.7767, 
[2025-09-17 17:24:11,844][__main__][INFO] - Test, Round 061: loss=1.1131, accuracy=0.7738, 
[2025-09-17 17:24:11,844][__main__][INFO] - Test, Round 062: loss=1.1705, accuracy=0.7718, 
[2025-09-17 17:24:11,844][__main__][INFO] - Test, Round 063: loss=1.0744, accuracy=0.7828, 
[2025-09-17 17:24:11,844][__main__][INFO] - Test, Round 064: loss=1.0985, accuracy=0.7722, 
[2025-09-17 17:24:11,844][__main__][INFO] - Test, Round 065: loss=1.0550, accuracy=0.7800, 
[2025-09-17 17:24:11,844][__main__][INFO] - Test, Round 066: loss=1.1611, accuracy=0.7816, 
[2025-09-17 17:24:11,844][__main__][INFO] - Test, Round 067: loss=1.2282, accuracy=0.7753, 
[2025-09-17 17:24:11,844][__main__][INFO] - Test, Round 068: loss=1.1691, accuracy=0.7811, 
[2025-09-17 17:24:11,844][__main__][INFO] - Test, Round 069: loss=1.2415, accuracy=0.7784, 
[2025-09-17 17:24:11,844][__main__][INFO] - Test, Round 070: loss=1.2067, accuracy=0.7739, 
[2025-09-17 17:24:11,844][__main__][INFO] - Test, Round 071: loss=1.2031, accuracy=0.7815, 
[2025-09-17 17:24:11,844][__main__][INFO] - Test, Round 072: loss=1.2639, accuracy=0.7751, 
[2025-09-17 17:24:11,844][__main__][INFO] - Test, Round 073: loss=1.2037, accuracy=0.7822, 
[2025-09-17 17:24:11,844][__main__][INFO] - Test, Round 074: loss=1.2199, accuracy=0.7809, 
[2025-09-17 17:24:11,844][__main__][INFO] - Test, Round 075: loss=1.2671, accuracy=0.7835, 
[2025-09-17 17:24:11,844][__main__][INFO] - Test, Round 076: loss=1.3065, accuracy=0.7773, 
[2025-09-17 17:24:11,844][__main__][INFO] - Test, Round 077: loss=1.2439, accuracy=0.7883, 
[2025-09-17 17:24:11,844][__main__][INFO] - Test, Round 078: loss=1.2310, accuracy=0.7892, 
[2025-09-17 17:24:11,844][__main__][INFO] - Test, Round 079: loss=1.3176, accuracy=0.7798, 
[2025-09-17 17:24:11,844][__main__][INFO] - Test, Round 080: loss=1.3658, accuracy=0.7782, 
[2025-09-17 17:24:11,844][__main__][INFO] - Test, Round 081: loss=1.2697, accuracy=0.7854, 
[2025-09-17 17:24:11,844][__main__][INFO] - Test, Round 082: loss=1.2937, accuracy=0.7851, 
[2025-09-17 17:24:11,844][__main__][INFO] - Test, Round 083: loss=1.3397, accuracy=0.7836, 
[2025-09-17 17:24:11,844][__main__][INFO] - Test, Round 084: loss=1.3245, accuracy=0.7836, 
[2025-09-17 17:24:11,844][__main__][INFO] - Test, Round 085: loss=1.3704, accuracy=0.7785, 
[2025-09-17 17:24:11,844][__main__][INFO] - Test, Round 086: loss=1.3357, accuracy=0.7813, 
[2025-09-17 17:24:11,844][__main__][INFO] - Test, Round 087: loss=1.3751, accuracy=0.7806, 
[2025-09-17 17:24:11,844][__main__][INFO] - Test, Round 088: loss=1.3546, accuracy=0.7889, 
[2025-09-17 17:24:11,844][__main__][INFO] - Test, Round 089: loss=1.3572, accuracy=0.7832, 
[2025-09-17 17:24:11,844][__main__][INFO] - Test, Round 090: loss=1.3767, accuracy=0.7845, 
[2025-09-17 17:24:11,844][__main__][INFO] - Test, Round 091: loss=1.3938, accuracy=0.7864, 
[2025-09-17 17:24:11,844][__main__][INFO] - Test, Round 092: loss=1.3636, accuracy=0.7919, 
[2025-09-17 17:24:11,844][__main__][INFO] - Test, Round 093: loss=1.3745, accuracy=0.7926, 
[2025-09-17 17:24:11,844][__main__][INFO] - Test, Round 094: loss=1.3934, accuracy=0.7862, 
[2025-09-17 17:24:11,844][__main__][INFO] - Test, Round 095: loss=1.4020, accuracy=0.7846, 
[2025-09-17 17:24:11,844][__main__][INFO] - Test, Round 096: loss=1.4042, accuracy=0.7875, 
[2025-09-17 17:24:11,844][__main__][INFO] - Test, Round 097: loss=1.3974, accuracy=0.7870, 
[2025-09-17 17:24:11,844][__main__][INFO] - Test, Round 098: loss=1.4482, accuracy=0.7886, 
[2025-09-17 17:24:11,844][__main__][INFO] - Test, Round 099: loss=1.4693, accuracy=0.7818, 
[2025-09-17 17:24:11,844][__main__][INFO] - Test, Round 100: loss=1.4721, accuracy=0.7824, 
[2025-09-17 17:24:11,844][__main__][INFO] - Test, Round 101: loss=1.4345, accuracy=0.7899, 
[2025-09-17 17:24:11,844][__main__][INFO] - Test, Round 102: loss=1.4494, accuracy=0.7878, 
[2025-09-17 17:24:11,844][__main__][INFO] - Test, Round 103: loss=1.4879, accuracy=0.7887, 
[2025-09-17 17:24:11,844][__main__][INFO] - Test, Round 104: loss=1.4279, accuracy=0.7915, 
[2025-09-17 17:24:11,844][__main__][INFO] - Test, Round 105: loss=1.3984, accuracy=0.7885, 
[2025-09-17 17:24:11,844][__main__][INFO] - Test, Round 106: loss=1.5354, accuracy=0.7824, 
[2025-09-17 17:24:11,844][__main__][INFO] - Test, Round 107: loss=1.4314, accuracy=0.7885, 
[2025-09-17 17:24:11,844][__main__][INFO] - Test, Round 108: loss=1.4946, accuracy=0.7844, 
[2025-09-17 17:24:11,844][__main__][INFO] - Test, Round 109: loss=1.5248, accuracy=0.7812, 
[2025-09-17 17:24:11,844][__main__][INFO] - Test, Round 110: loss=1.4725, accuracy=0.7862, 
[2025-09-17 17:24:11,844][__main__][INFO] - Test, Round 111: loss=1.5358, accuracy=0.7824, 
[2025-09-17 17:24:11,844][__main__][INFO] - Test, Round 112: loss=1.4935, accuracy=0.7872, 
[2025-09-17 17:24:11,844][__main__][INFO] - Test, Round 113: loss=1.5624, accuracy=0.7861, 
[2025-09-17 17:24:11,844][__main__][INFO] - Test, Round 114: loss=1.5601, accuracy=0.7840, 
[2025-09-17 17:24:11,844][__main__][INFO] - Test, Round 115: loss=1.5445, accuracy=0.7842, 
[2025-09-17 17:24:11,844][__main__][INFO] - Test, Round 116: loss=1.5144, accuracy=0.7849, 
[2025-09-17 17:24:11,844][__main__][INFO] - Test, Round 117: loss=1.5294, accuracy=0.7871, 
[2025-09-17 17:24:11,844][__main__][INFO] - Test, Round 118: loss=1.4797, accuracy=0.7920, 
[2025-09-17 17:24:11,844][__main__][INFO] - Test, Round 119: loss=1.5076, accuracy=0.7840, 
[2025-09-17 17:24:11,844][__main__][INFO] - Test, Round 120: loss=1.5569, accuracy=0.7839, 
[2025-09-17 17:24:11,844][__main__][INFO] - Test, Round 121: loss=1.5543, accuracy=0.7864, 
[2025-09-17 17:24:11,844][__main__][INFO] - Test, Round 122: loss=1.4978, accuracy=0.7865, 
[2025-09-17 17:24:11,844][__main__][INFO] - Test, Round 123: loss=1.5770, accuracy=0.7786, 
[2025-09-17 17:24:11,844][__main__][INFO] - Test, Round 124: loss=1.6246, accuracy=0.7834, 
[2025-09-17 17:24:11,845][__main__][INFO] - Test, Round 125: loss=1.5804, accuracy=0.7870, 
[2025-09-17 17:24:11,845][__main__][INFO] - Test, Round 126: loss=1.5281, accuracy=0.7888, 
[2025-09-17 17:24:11,845][__main__][INFO] - Test, Round 127: loss=1.5604, accuracy=0.7867, 
[2025-09-17 17:24:11,845][__main__][INFO] - Test, Round 128: loss=1.5863, accuracy=0.7869, 
[2025-09-17 17:24:11,845][__main__][INFO] - Test, Round 129: loss=1.6013, accuracy=0.7818, 
[2025-09-17 17:24:11,845][__main__][INFO] - Test, Round 130: loss=1.5163, accuracy=0.7936, 
[2025-09-17 17:24:11,845][__main__][INFO] - Test, Round 131: loss=1.6224, accuracy=0.7814, 
[2025-09-17 17:24:11,845][__main__][INFO] - Test, Round 132: loss=1.5847, accuracy=0.7873, 
[2025-09-17 17:24:11,845][__main__][INFO] - Test, Round 133: loss=1.5553, accuracy=0.7876, 
[2025-09-17 17:24:11,845][__main__][INFO] - Test, Round 134: loss=1.6471, accuracy=0.7818, 
[2025-09-17 17:24:11,845][__main__][INFO] - Test, Round 135: loss=1.6326, accuracy=0.7865, 
[2025-09-17 17:24:11,845][__main__][INFO] - Test, Round 136: loss=1.5244, accuracy=0.7921, 
[2025-09-17 17:24:11,845][__main__][INFO] - Test, Round 137: loss=1.5563, accuracy=0.7869, 
[2025-09-17 17:24:11,845][__main__][INFO] - Test, Round 138: loss=1.5799, accuracy=0.7883, 
[2025-09-17 17:24:11,845][__main__][INFO] - Test, Round 139: loss=1.6225, accuracy=0.7878, 
[2025-09-17 17:24:11,845][__main__][INFO] - Test, Round 140: loss=1.6201, accuracy=0.7840, 
[2025-09-17 17:24:11,845][__main__][INFO] - Test, Round 141: loss=1.5888, accuracy=0.7818, 
[2025-09-17 17:24:11,845][__main__][INFO] - Test, Round 142: loss=1.5721, accuracy=0.7869, 
[2025-09-17 17:24:11,845][__main__][INFO] - Test, Round 143: loss=1.6603, accuracy=0.7890, 
[2025-09-17 17:24:11,845][__main__][INFO] - Test, Round 144: loss=1.6009, accuracy=0.7907, 
[2025-09-17 17:24:11,845][__main__][INFO] - Test, Round 145: loss=1.6307, accuracy=0.7851, 
[2025-09-17 17:24:11,845][__main__][INFO] - Test, Round 146: loss=1.6262, accuracy=0.7855, 
[2025-09-17 17:24:11,845][__main__][INFO] - Test, Round 147: loss=1.5959, accuracy=0.7845, 
[2025-09-17 17:24:11,845][__main__][INFO] - Test, Round 148: loss=1.6000, accuracy=0.7888, 
[2025-09-17 17:24:11,845][__main__][INFO] - Test, Round 149: loss=1.6471, accuracy=0.7864, 
[2025-09-17 17:24:11,845][__main__][INFO] - Test, Round 150: loss=1.6090, accuracy=0.7917, 
[2025-09-17 17:24:11,845][__main__][INFO] - Test, Round 151: loss=1.6266, accuracy=0.7897, 
[2025-09-17 17:24:11,845][__main__][INFO] - Test, Round 152: loss=1.6551, accuracy=0.7920, 
[2025-09-17 17:24:11,845][__main__][INFO] - Test, Round 153: loss=1.6347, accuracy=0.7889, 
[2025-09-17 17:24:11,845][__main__][INFO] - Test, Round 154: loss=1.6537, accuracy=0.7906, 
[2025-09-17 17:24:11,845][__main__][INFO] - Test, Round 155: loss=1.6406, accuracy=0.7846, 
[2025-09-17 17:24:11,845][__main__][INFO] - Test, Round 156: loss=1.5999, accuracy=0.7865, 
[2025-09-17 17:24:11,845][__main__][INFO] - Test, Round 157: loss=1.5881, accuracy=0.7910, 
[2025-09-17 17:24:11,845][__main__][INFO] - Test, Round 158: loss=1.6897, accuracy=0.7835, 
[2025-09-17 17:24:11,845][__main__][INFO] - Test, Round 159: loss=1.6770, accuracy=0.7869, 
[2025-09-17 17:24:11,845][__main__][INFO] - Test, Round 160: loss=1.6656, accuracy=0.7844, 
[2025-09-17 17:24:11,845][__main__][INFO] - Test, Round 161: loss=1.6356, accuracy=0.7852, 
[2025-09-17 17:24:11,845][__main__][INFO] - Test, Round 162: loss=1.7518, accuracy=0.7819, 
[2025-09-17 17:24:11,845][__main__][INFO] - Test, Round 163: loss=1.7210, accuracy=0.7862, 
[2025-09-17 17:24:11,845][__main__][INFO] - Test, Round 164: loss=1.6187, accuracy=0.7945, 
[2025-09-17 17:24:11,845][__main__][INFO] - Test, Round 165: loss=1.6172, accuracy=0.7807, 
[2025-09-17 17:24:11,845][__main__][INFO] - Test, Round 166: loss=1.5784, accuracy=0.7942, 
[2025-09-17 17:24:11,845][__main__][INFO] - Test, Round 167: loss=1.6781, accuracy=0.7892, 
[2025-09-17 17:24:11,845][__main__][INFO] - Test, Round 168: loss=1.6754, accuracy=0.7885, 
[2025-09-17 17:24:11,845][__main__][INFO] - Test, Round 169: loss=1.6199, accuracy=0.7886, 
[2025-09-17 17:24:11,845][__main__][INFO] - Test, Round 170: loss=1.7172, accuracy=0.7880, 
[2025-09-17 17:24:11,845][__main__][INFO] - Test, Round 171: loss=1.6638, accuracy=0.7858, 
[2025-09-17 17:24:11,845][__main__][INFO] - Test, Round 172: loss=1.6456, accuracy=0.7847, 
[2025-09-17 17:24:11,845][__main__][INFO] - Test, Round 173: loss=1.6955, accuracy=0.7838, 
[2025-09-17 17:24:11,845][__main__][INFO] - Test, Round 174: loss=1.7020, accuracy=0.7848, 
[2025-09-17 17:24:11,845][__main__][INFO] - Test, Round 175: loss=1.6531, accuracy=0.7978, 
[2025-09-17 17:24:11,845][__main__][INFO] - Test, Round 176: loss=1.6497, accuracy=0.7879, 
[2025-09-17 17:24:11,845][__main__][INFO] - Test, Round 177: loss=1.7089, accuracy=0.7828, 
[2025-09-17 17:24:11,845][__main__][INFO] - Test, Round 178: loss=1.7034, accuracy=0.7891, 
[2025-09-17 17:24:11,845][__main__][INFO] - Test, Round 179: loss=1.6620, accuracy=0.7905, 
[2025-09-17 17:24:11,845][__main__][INFO] - Test, Round 180: loss=1.6495, accuracy=0.7904, 
[2025-09-17 17:24:11,845][__main__][INFO] - Test, Round 181: loss=1.7070, accuracy=0.7806, 
[2025-09-17 17:24:11,845][__main__][INFO] - Test, Round 182: loss=1.6542, accuracy=0.7904, 
[2025-09-17 17:24:11,845][__main__][INFO] - Test, Round 183: loss=1.6722, accuracy=0.7895, 
[2025-09-17 17:24:11,845][__main__][INFO] - Test, Round 184: loss=1.7194, accuracy=0.7907, 
[2025-09-17 17:24:11,845][__main__][INFO] - Test, Round 185: loss=1.6567, accuracy=0.7958, 
[2025-09-17 17:24:11,845][__main__][INFO] - Test, Round 186: loss=1.7246, accuracy=0.7912, 
[2025-09-17 17:24:11,845][__main__][INFO] - Test, Round 187: loss=1.7587, accuracy=0.7857, 
[2025-09-17 17:24:11,845][__main__][INFO] - Test, Round 188: loss=1.6502, accuracy=0.7876, 
[2025-09-17 17:24:11,845][__main__][INFO] - Test, Round 189: loss=1.6841, accuracy=0.7858, 
[2025-09-17 17:24:11,845][__main__][INFO] - Test, Round 190: loss=1.6772, accuracy=0.7855, 
[2025-09-17 17:24:11,845][__main__][INFO] - Test, Round 191: loss=1.7233, accuracy=0.7853, 
[2025-09-17 17:24:11,845][__main__][INFO] - Test, Round 192: loss=1.6789, accuracy=0.7873, 
[2025-09-17 17:24:11,845][__main__][INFO] - Test, Round 193: loss=1.7205, accuracy=0.7826, 
[2025-09-17 17:24:11,845][__main__][INFO] - Test, Round 194: loss=1.6836, accuracy=0.7938, 
[2025-09-17 17:24:11,846][__main__][INFO] - Test, Round 195: loss=1.6844, accuracy=0.7881, 
[2025-09-17 17:24:11,846][__main__][INFO] - Test, Round 196: loss=1.7259, accuracy=0.7858, 
[2025-09-17 17:24:11,846][__main__][INFO] - Test, Round 197: loss=1.7071, accuracy=0.7899, 
[2025-09-17 17:24:11,846][__main__][INFO] - Test, Round 198: loss=1.7403, accuracy=0.7793, 
[2025-09-17 17:24:11,846][__main__][INFO] - Test, Round 199: loss=1.7062, accuracy=0.7853, 
[2025-09-17 17:24:11,846][__main__][INFO] - Test, Round 200: loss=1.7191, accuracy=0.7866, 
