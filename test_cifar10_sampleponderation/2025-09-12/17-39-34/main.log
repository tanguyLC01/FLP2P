[2025-09-12 17:39:48,589][flp2p.graph_runner][INFO] - Train, Round 0 : loss => 2.305329055786133,  accuracy: 0.10256, gradient_norm : 0.1797738444931479
[2025-09-12 17:40:18,856][flp2p.graph_runner][INFO] - Test, Round 0 : loss => 2.3057345269322393,  accuracy: 0.1013
[2025-09-12 17:40:29,519][flp2p.graph_runner][INFO] - Train, Round 1 : loss => 2.305315021574497,  accuracy: 0.10252, gradient_norm : 0.1745107321359796
[2025-09-12 17:40:57,717][flp2p.graph_runner][INFO] - Test, Round 1 : loss => 2.3056983355998995,  accuracy: 0.1014
[2025-09-12 17:41:08,585][flp2p.graph_runner][INFO] - Train, Round 2 : loss => 2.3052371504902838,  accuracy: 0.10264, gradient_norm : 0.17351752199452544
[2025-09-12 17:41:26,020][flp2p.graph_runner][INFO] - Test, Round 2 : loss => 2.305662481534481,  accuracy: 0.1016
[2025-09-12 17:41:37,122][flp2p.graph_runner][INFO] - Train, Round 3 : loss => 2.3052013260126114,  accuracy: 0.10274, gradient_norm : 0.17404831596265372
[2025-09-12 17:41:49,605][flp2p.graph_runner][INFO] - Test, Round 3 : loss => 2.3056269570946695,  accuracy: 0.1015
[2025-09-12 17:42:00,478][flp2p.graph_runner][INFO] - Train, Round 4 : loss => 2.305208370089531,  accuracy: 0.10288, gradient_norm : 0.18151909552457382
[2025-09-12 17:42:12,957][flp2p.graph_runner][INFO] - Test, Round 4 : loss => 2.3055910318136217,  accuracy: 0.1011
[2025-09-12 17:42:23,791][flp2p.graph_runner][INFO] - Train, Round 5 : loss => 2.3051278391480445,  accuracy: 0.10314, gradient_norm : 0.1760410616759949
[2025-09-12 17:42:40,843][flp2p.graph_runner][INFO] - Test, Round 5 : loss => 2.3055554508447647,  accuracy: 0.1013
[2025-09-12 17:42:51,572][flp2p.graph_runner][INFO] - Train, Round 6 : loss => 2.3050925838947296,  accuracy: 0.10346, gradient_norm : 0.17524906709554647
[2025-09-12 17:43:20,426][flp2p.graph_runner][INFO] - Test, Round 6 : loss => 2.3055200105547904,  accuracy: 0.1015
[2025-09-12 17:43:31,420][flp2p.graph_runner][INFO] - Train, Round 7 : loss => 2.3050540816783904,  accuracy: 0.10354, gradient_norm : 0.18080813523197278
[2025-09-12 17:44:03,256][flp2p.graph_runner][INFO] - Test, Round 7 : loss => 2.305484515416622,  accuracy: 0.1019
[2025-09-12 17:44:14,191][flp2p.graph_runner][INFO] - Train, Round 8 : loss => 2.3049882248044016,  accuracy: 0.10364, gradient_norm : 0.17177229979033448
[2025-09-12 17:44:42,881][flp2p.graph_runner][INFO] - Test, Round 8 : loss => 2.3054491631865504,  accuracy: 0.1015
[2025-09-12 17:44:53,617][flp2p.graph_runner][INFO] - Train, Round 9 : loss => 2.3049526780843737,  accuracy: 0.1039, gradient_norm : 0.17810513674563366
[2025-09-12 17:45:13,867][flp2p.graph_runner][INFO] - Test, Round 9 : loss => 2.305414183080196,  accuracy: 0.1013
[2025-09-12 17:45:25,080][flp2p.graph_runner][INFO] - Train, Round 10 : loss => 2.3049240550398826,  accuracy: 0.10418, gradient_norm : 0.17516555571354178
[2025-09-12 17:45:49,936][flp2p.graph_runner][INFO] - Test, Round 10 : loss => 2.305379026091099,  accuracy: 0.101
[2025-09-12 17:46:00,921][flp2p.graph_runner][INFO] - Train, Round 11 : loss => 2.3049254846572875,  accuracy: 0.10422, gradient_norm : 0.1762553929714877
[2025-09-12 17:46:31,411][flp2p.graph_runner][INFO] - Test, Round 11 : loss => 2.3053438117027283,  accuracy: 0.1012
[2025-09-12 17:46:42,185][flp2p.graph_runner][INFO] - Train, Round 12 : loss => 2.3048768693208697,  accuracy: 0.10452, gradient_norm : 0.17747307367151025
[2025-09-12 17:47:10,782][flp2p.graph_runner][INFO] - Test, Round 12 : loss => 2.30530861928463,  accuracy: 0.1022
[2025-09-12 17:47:21,618][flp2p.graph_runner][INFO] - Train, Round 13 : loss => 2.304828051626682,  accuracy: 0.10446, gradient_norm : 0.17233430922477364
[2025-09-12 17:47:45,585][flp2p.graph_runner][INFO] - Test, Round 13 : loss => 2.3052736727952956,  accuracy: 0.1021
[2025-09-12 17:47:56,602][flp2p.graph_runner][INFO] - Train, Round 14 : loss => 2.304794357120991,  accuracy: 0.10472, gradient_norm : 0.17291494992608328
[2025-09-12 17:48:09,138][flp2p.graph_runner][INFO] - Test, Round 14 : loss => 2.3052385283231733,  accuracy: 0.1025
[2025-09-12 17:48:19,968][flp2p.graph_runner][INFO] - Train, Round 15 : loss => 2.3047622725367547,  accuracy: 0.10478, gradient_norm : 0.17905528597856674
[2025-09-12 17:48:32,528][flp2p.graph_runner][INFO] - Test, Round 15 : loss => 2.305203538477421,  accuracy: 0.1026
[2025-09-12 17:48:43,498][flp2p.graph_runner][INFO] - Train, Round 16 : loss => 2.3047465020418167,  accuracy: 0.10518, gradient_norm : 0.1749239831238005
[2025-09-12 17:48:55,880][flp2p.graph_runner][INFO] - Test, Round 16 : loss => 2.3051685039281846,  accuracy: 0.1026
[2025-09-12 17:49:06,651][flp2p.graph_runner][INFO] - Train, Round 17 : loss => 2.304673023223877,  accuracy: 0.10538, gradient_norm : 0.17630104671245914
[2025-09-12 17:49:30,592][flp2p.graph_runner][INFO] - Test, Round 17 : loss => 2.3051337626338007,  accuracy: 0.1023
[2025-09-12 17:49:41,431][flp2p.graph_runner][INFO] - Train, Round 18 : loss => 2.304646525979042,  accuracy: 0.10576, gradient_norm : 0.17576591152581333
[2025-09-12 17:50:09,156][flp2p.graph_runner][INFO] - Test, Round 18 : loss => 2.3050990174651145,  accuracy: 0.1022
[2025-09-12 17:50:20,022][flp2p.graph_runner][INFO] - Train, Round 19 : loss => 2.304579198360443,  accuracy: 0.1059, gradient_norm : 0.17603932477454548
[2025-09-12 17:50:40,424][flp2p.graph_runner][INFO] - Test, Round 19 : loss => 2.3050646017551424,  accuracy: 0.1016
[2025-09-12 17:50:51,356][flp2p.graph_runner][INFO] - Train, Round 20 : loss => 2.304545743763447,  accuracy: 0.10586, gradient_norm : 0.17167732587761797
[2025-09-12 17:51:15,002][flp2p.graph_runner][INFO] - Test, Round 20 : loss => 2.3050302170991896,  accuracy: 0.1022
[2025-09-12 17:51:25,769][flp2p.graph_runner][INFO] - Train, Round 21 : loss => 2.304504550695419,  accuracy: 0.10616, gradient_norm : 0.17714528174150174
[2025-09-12 17:51:46,235][flp2p.graph_runner][INFO] - Test, Round 21 : loss => 2.3049959151268005,  accuracy: 0.1025
[2025-09-12 17:51:57,083][flp2p.graph_runner][INFO] - Train, Round 22 : loss => 2.3044241040945055,  accuracy: 0.10672, gradient_norm : 0.1665156777215557
[2025-09-12 17:52:09,646][flp2p.graph_runner][INFO] - Test, Round 22 : loss => 2.304962063360214,  accuracy: 0.1032
[2025-09-12 17:52:20,685][flp2p.graph_runner][INFO] - Train, Round 23 : loss => 2.304449452161789,  accuracy: 0.10664, gradient_norm : 0.17342273427730265
[2025-09-12 17:52:33,294][flp2p.graph_runner][INFO] - Test, Round 23 : loss => 2.3049276840090753,  accuracy: 0.1036
[2025-09-12 17:52:44,088][flp2p.graph_runner][INFO] - Train, Round 24 : loss => 2.304386212527752,  accuracy: 0.10702, gradient_norm : 0.17381756588015818
[2025-09-12 17:52:56,611][flp2p.graph_runner][INFO] - Test, Round 24 : loss => 2.304893420493603,  accuracy: 0.1039
[2025-09-12 17:53:07,242][flp2p.graph_runner][INFO] - Train, Round 25 : loss => 2.3043723315000535,  accuracy: 0.10706, gradient_norm : 0.18012696704819794
[2025-09-12 17:53:32,986][flp2p.graph_runner][INFO] - Test, Round 25 : loss => 2.304859134876728,  accuracy: 0.1042
[2025-09-12 17:53:43,913][flp2p.graph_runner][INFO] - Train, Round 26 : loss => 2.3043461912870407,  accuracy: 0.10756, gradient_norm : 0.17546973733922996
[2025-09-12 17:54:15,939][flp2p.graph_runner][INFO] - Test, Round 26 : loss => 2.3048247262358665,  accuracy: 0.1048
[2025-09-12 17:54:27,006][flp2p.graph_runner][INFO] - Train, Round 27 : loss => 2.304295751154423,  accuracy: 0.10752, gradient_norm : 0.17692953256467792
[2025-09-12 17:55:02,851][flp2p.graph_runner][INFO] - Test, Round 27 : loss => 2.304790590429306,  accuracy: 0.1052
[2025-09-12 17:55:13,636][flp2p.graph_runner][INFO] - Train, Round 28 : loss => 2.3042672431468962,  accuracy: 0.10768, gradient_norm : 0.17796209783901
[2025-09-12 17:55:35,898][flp2p.graph_runner][INFO] - Test, Round 28 : loss => 2.3047564757585524,  accuracy: 0.1053
[2025-09-12 17:55:47,033][flp2p.graph_runner][INFO] - Train, Round 29 : loss => 2.3042365369200706,  accuracy: 0.10776, gradient_norm : 0.176804187955497
[2025-09-12 17:55:59,464][flp2p.graph_runner][INFO] - Test, Round 29 : loss => 2.304722438108921,  accuracy: 0.1057
[2025-09-12 17:56:10,387][flp2p.graph_runner][INFO] - Train, Round 30 : loss => 2.3042130866646766,  accuracy: 0.10782, gradient_norm : 0.17363335211718897
[2025-09-12 17:56:23,493][flp2p.graph_runner][INFO] - Test, Round 30 : loss => 2.3046884632349016,  accuracy: 0.1058
[2025-09-12 17:56:34,343][flp2p.graph_runner][INFO] - Train, Round 31 : loss => 2.304113935530186,  accuracy: 0.10814, gradient_norm : 0.1725128831850605
[2025-09-12 17:56:46,746][flp2p.graph_runner][INFO] - Test, Round 31 : loss => 2.3046549853801728,  accuracy: 0.1064
[2025-09-12 17:56:57,467][flp2p.graph_runner][INFO] - Train, Round 32 : loss => 2.304043157994747,  accuracy: 0.10808, gradient_norm : 0.1736872358251552
[2025-09-12 17:57:20,631][flp2p.graph_runner][INFO] - Test, Round 32 : loss => 2.3046216796875,  accuracy: 0.1068
[2025-09-12 17:57:31,647][flp2p.graph_runner][INFO] - Train, Round 33 : loss => 2.3040400686860085,  accuracy: 0.10828, gradient_norm : 0.1794187396539037
[2025-09-12 17:57:53,047][flp2p.graph_runner][INFO] - Test, Round 33 : loss => 2.3045884287953378,  accuracy: 0.1071
[2025-09-12 17:58:03,774][flp2p.graph_runner][INFO] - Train, Round 34 : loss => 2.3039987340569494,  accuracy: 0.10826, gradient_norm : 0.17454503443362523
[2025-09-12 17:58:24,699][flp2p.graph_runner][INFO] - Test, Round 34 : loss => 2.304555060863495,  accuracy: 0.1074
[2025-09-12 17:58:35,703][flp2p.graph_runner][INFO] - Train, Round 35 : loss => 2.3039923360943795,  accuracy: 0.10846, gradient_norm : 0.17269322961418854
[2025-09-12 17:59:04,696][flp2p.graph_runner][INFO] - Test, Round 35 : loss => 2.3045215071201324,  accuracy: 0.1074
[2025-09-12 17:59:15,613][flp2p.graph_runner][INFO] - Train, Round 36 : loss => 2.3039152163267134,  accuracy: 0.10846, gradient_norm : 0.17352613373467857
[2025-09-12 17:59:42,648][flp2p.graph_runner][INFO] - Test, Round 36 : loss => 2.3044883298754693,  accuracy: 0.1079
[2025-09-12 17:59:53,708][flp2p.graph_runner][INFO] - Train, Round 37 : loss => 2.30391881108284,  accuracy: 0.10872, gradient_norm : 0.17967061727361494
[2025-09-12 18:00:15,986][flp2p.graph_runner][INFO] - Test, Round 37 : loss => 2.3044550852537156,  accuracy: 0.1083
[2025-09-12 18:00:26,970][flp2p.graph_runner][INFO] - Train, Round 38 : loss => 2.3038754883408545,  accuracy: 0.10888, gradient_norm : 0.17198783708430057
[2025-09-12 18:00:39,408][flp2p.graph_runner][INFO] - Test, Round 38 : loss => 2.3044218729496,  accuracy: 0.1091
[2025-09-12 18:00:50,647][flp2p.graph_runner][INFO] - Train, Round 39 : loss => 2.3038521906733513,  accuracy: 0.1092, gradient_norm : 0.17854162237318552
[2025-09-12 18:01:03,513][flp2p.graph_runner][INFO] - Test, Round 39 : loss => 2.3043886368751525,  accuracy: 0.1096
[2025-09-12 18:01:14,267][flp2p.graph_runner][INFO] - Train, Round 40 : loss => 2.303828396499157,  accuracy: 0.10906, gradient_norm : 0.1801933128188438
[2025-09-12 18:01:26,638][flp2p.graph_runner][INFO] - Test, Round 40 : loss => 2.304355446839333,  accuracy: 0.1097
[2025-09-12 18:01:37,351][flp2p.graph_runner][INFO] - Train, Round 41 : loss => 2.30378023147583,  accuracy: 0.10932, gradient_norm : 0.17296510837112508
[2025-09-12 18:01:59,201][flp2p.graph_runner][INFO] - Test, Round 41 : loss => 2.304322361791134,  accuracy: 0.1104
[2025-09-12 18:02:10,082][flp2p.graph_runner][INFO] - Train, Round 42 : loss => 2.30373941808939,  accuracy: 0.10936, gradient_norm : 0.17738493800191882
[2025-09-12 18:02:33,149][flp2p.graph_runner][INFO] - Test, Round 42 : loss => 2.3042893956780435,  accuracy: 0.1107
[2025-09-12 18:02:43,930][flp2p.graph_runner][INFO] - Train, Round 43 : loss => 2.303740651011467,  accuracy: 0.10938, gradient_norm : 0.18006931400335027
[2025-09-12 18:03:03,643][flp2p.graph_runner][INFO] - Test, Round 43 : loss => 2.3042559555411337,  accuracy: 0.1114
[2025-09-12 18:03:14,739][flp2p.graph_runner][INFO] - Train, Round 44 : loss => 2.30366086602211,  accuracy: 0.10964, gradient_norm : 0.17704644349133705
[2025-09-12 18:03:42,109][flp2p.graph_runner][INFO] - Test, Round 44 : loss => 2.30422330840826,  accuracy: 0.1109
[2025-09-12 18:03:53,016][flp2p.graph_runner][INFO] - Train, Round 45 : loss => 2.3036609655618667,  accuracy: 0.10964, gradient_norm : 0.17523822335086903
[2025-09-12 18:04:25,077][flp2p.graph_runner][INFO] - Test, Round 45 : loss => 2.3041901229023933,  accuracy: 0.111
[2025-09-12 18:04:35,901][flp2p.graph_runner][INFO] - Train, Round 46 : loss => 2.3036247679591177,  accuracy: 0.10994, gradient_norm : 0.1791749560335416
[2025-09-12 18:05:04,097][flp2p.graph_runner][INFO] - Test, Round 46 : loss => 2.304157128024101,  accuracy: 0.1109
[2025-09-12 18:05:14,995][flp2p.graph_runner][INFO] - Train, Round 47 : loss => 2.3035717272758482,  accuracy: 0.11012, gradient_norm : 0.17527876597409006
[2025-09-12 18:05:32,780][flp2p.graph_runner][INFO] - Test, Round 47 : loss => 2.304124116563797,  accuracy: 0.1113
[2025-09-12 18:05:44,025][flp2p.graph_runner][INFO] - Train, Round 48 : loss => 2.3035214999318123,  accuracy: 0.11036, gradient_norm : 0.16984096832457063
[2025-09-12 18:05:56,572][flp2p.graph_runner][INFO] - Test, Round 48 : loss => 2.3040914798498155,  accuracy: 0.1118
[2025-09-12 18:06:07,433][flp2p.graph_runner][INFO] - Train, Round 49 : loss => 2.3035065466165543,  accuracy: 0.1102, gradient_norm : 0.17300708423887518
[2025-09-12 18:06:19,887][flp2p.graph_runner][INFO] - Test, Round 49 : loss => 2.304058696103096,  accuracy: 0.1108
[2025-09-12 18:06:30,601][flp2p.graph_runner][INFO] - Train, Round 50 : loss => 2.303421387672424,  accuracy: 0.11036, gradient_norm : 0.16531054202050485
[2025-09-12 18:06:44,574][flp2p.graph_runner][INFO] - Test, Round 50 : loss => 2.3040264761805536,  accuracy: 0.1116
[2025-09-12 18:06:55,281][flp2p.graph_runner][INFO] - Train, Round 51 : loss => 2.3034038850665093,  accuracy: 0.11046, gradient_norm : 0.18101310224988626
[2025-09-12 18:07:16,166][flp2p.graph_runner][INFO] - Test, Round 51 : loss => 2.3039938167095184,  accuracy: 0.1116
[2025-09-12 18:07:26,990][flp2p.graph_runner][INFO] - Train, Round 52 : loss => 2.3034080123901366,  accuracy: 0.11096, gradient_norm : 0.17147684024838944
[2025-09-12 18:07:45,683][flp2p.graph_runner][INFO] - Test, Round 52 : loss => 2.3039611834049225,  accuracy: 0.112
[2025-09-12 18:07:56,740][flp2p.graph_runner][INFO] - Train, Round 53 : loss => 2.303365866243839,  accuracy: 0.11106, gradient_norm : 0.17215992223492266
[2025-09-12 18:08:19,336][flp2p.graph_runner][INFO] - Test, Round 53 : loss => 2.303928736400604,  accuracy: 0.1117
[2025-09-12 18:08:29,941][flp2p.graph_runner][INFO] - Train, Round 54 : loss => 2.303318289220333,  accuracy: 0.11136, gradient_norm : 0.16776459718726605
[2025-09-12 18:08:47,972][flp2p.graph_runner][INFO] - Test, Round 54 : loss => 2.3038962609410287,  accuracy: 0.1113
[2025-09-12 18:08:59,108][flp2p.graph_runner][INFO] - Train, Round 55 : loss => 2.30330429315567,  accuracy: 0.1117, gradient_norm : 0.1729332014061221
[2025-09-12 18:09:19,032][flp2p.graph_runner][INFO] - Test, Round 55 : loss => 2.303863682794571,  accuracy: 0.1108
[2025-09-12 18:09:29,819][flp2p.graph_runner][INFO] - Train, Round 56 : loss => 2.303243997991085,  accuracy: 0.11172, gradient_norm : 0.17574611200472276
[2025-09-12 18:09:56,513][flp2p.graph_runner][INFO] - Test, Round 56 : loss => 2.303831327998638,  accuracy: 0.1109
[2025-09-12 18:10:07,542][flp2p.graph_runner][INFO] - Train, Round 57 : loss => 2.3032050094008447,  accuracy: 0.11208, gradient_norm : 0.17235477610852903
[2025-09-12 18:10:31,653][flp2p.graph_runner][INFO] - Test, Round 57 : loss => 2.3037990837335585,  accuracy: 0.1111
[2025-09-12 18:10:42,784][flp2p.graph_runner][INFO] - Train, Round 58 : loss => 2.3031794184446337,  accuracy: 0.11226, gradient_norm : 0.17996366337675254
[2025-09-12 18:10:55,864][flp2p.graph_runner][INFO] - Test, Round 58 : loss => 2.30376663377285,  accuracy: 0.1107
[2025-09-12 18:11:06,945][flp2p.graph_runner][INFO] - Train, Round 59 : loss => 2.303146314620972,  accuracy: 0.11248, gradient_norm : 0.1737424374677175
[2025-09-12 18:11:19,833][flp2p.graph_runner][INFO] - Test, Round 59 : loss => 2.303734602975845,  accuracy: 0.1104
[2025-09-12 18:11:30,598][flp2p.graph_runner][INFO] - Train, Round 60 : loss => 2.3031158950924873,  accuracy: 0.1126, gradient_norm : 0.1781166992880515
[2025-09-12 18:11:42,974][flp2p.graph_runner][INFO] - Test, Round 60 : loss => 2.303702495265007,  accuracy: 0.1105
[2025-09-12 18:11:53,632][flp2p.graph_runner][INFO] - Train, Round 61 : loss => 2.3030475115776063,  accuracy: 0.11272, gradient_norm : 0.16928437598418575
[2025-09-12 18:12:12,512][flp2p.graph_runner][INFO] - Test, Round 61 : loss => 2.3036706092953683,  accuracy: 0.1107
[2025-09-12 18:12:23,257][flp2p.graph_runner][INFO] - Train, Round 62 : loss => 2.3030401548743247,  accuracy: 0.11254, gradient_norm : 0.17472866464150283
[2025-09-12 18:12:41,744][flp2p.graph_runner][INFO] - Test, Round 62 : loss => 2.3036386394619943,  accuracy: 0.1105
[2025-09-12 18:12:52,450][flp2p.graph_runner][INFO] - Train, Round 63 : loss => 2.3030580952763557,  accuracy: 0.11278, gradient_norm : 0.17808653709685607
[2025-09-12 18:13:09,852][flp2p.graph_runner][INFO] - Test, Round 63 : loss => 2.303606234562397,  accuracy: 0.1108
[2025-09-12 18:13:20,851][flp2p.graph_runner][INFO] - Train, Round 64 : loss => 2.3029794442653655,  accuracy: 0.1133, gradient_norm : 0.17604319976402313
[2025-09-12 18:13:42,160][flp2p.graph_runner][INFO] - Test, Round 64 : loss => 2.3035742340564727,  accuracy: 0.1109
[2025-09-12 18:13:52,897][flp2p.graph_runner][INFO] - Train, Round 65 : loss => 2.302909090220928,  accuracy: 0.11356, gradient_norm : 0.17100081584663815
[2025-09-12 18:14:11,648][flp2p.graph_runner][INFO] - Test, Round 65 : loss => 2.3035424189209937,  accuracy: 0.1109
[2025-09-12 18:14:22,693][flp2p.graph_runner][INFO] - Train, Round 66 : loss => 2.3029229378700258,  accuracy: 0.11352, gradient_norm : 0.17547527575295682
[2025-09-12 18:14:43,667][flp2p.graph_runner][INFO] - Test, Round 66 : loss => 2.3035104518175125,  accuracy: 0.1107
[2025-09-12 18:14:54,634][flp2p.graph_runner][INFO] - Train, Round 67 : loss => 2.302852498590946,  accuracy: 0.11376, gradient_norm : 0.17636683812762616
[2025-09-12 18:15:14,397][flp2p.graph_runner][INFO] - Test, Round 67 : loss => 2.303478844344616,  accuracy: 0.1109
[2025-09-12 18:15:25,441][flp2p.graph_runner][INFO] - Train, Round 68 : loss => 2.3028611144423485,  accuracy: 0.11374, gradient_norm : 0.17518109157111408
[2025-09-12 18:15:37,973][flp2p.graph_runner][INFO] - Test, Round 68 : loss => 2.3034470034360885,  accuracy: 0.1108
[2025-09-12 18:15:49,083][flp2p.graph_runner][INFO] - Train, Round 69 : loss => 2.3028194645047186,  accuracy: 0.11396, gradient_norm : 0.1759802669590493
[2025-09-12 18:16:01,826][flp2p.graph_runner][INFO] - Test, Round 69 : loss => 2.3034150493621826,  accuracy: 0.1109
[2025-09-12 18:16:12,368][flp2p.graph_runner][INFO] - Train, Round 70 : loss => 2.3027825984358787,  accuracy: 0.11424, gradient_norm : 0.16980551217680037
[2025-09-12 18:16:24,720][flp2p.graph_runner][INFO] - Test, Round 70 : loss => 2.3033835450410844,  accuracy: 0.111
[2025-09-12 18:16:35,346][flp2p.graph_runner][INFO] - Train, Round 71 : loss => 2.3027178397774697,  accuracy: 0.11458, gradient_norm : 0.1736684780176486
[2025-09-12 18:16:59,859][flp2p.graph_runner][INFO] - Test, Round 71 : loss => 2.303352081465721,  accuracy: 0.1116
[2025-09-12 18:17:10,530][flp2p.graph_runner][INFO] - Train, Round 72 : loss => 2.302670797109604,  accuracy: 0.11482, gradient_norm : 0.16877504363541743
[2025-09-12 18:17:35,541][flp2p.graph_runner][INFO] - Test, Round 72 : loss => 2.303320678508282,  accuracy: 0.1115
[2025-09-12 18:17:46,411][flp2p.graph_runner][INFO] - Train, Round 73 : loss => 2.3026447424292567,  accuracy: 0.1146, gradient_norm : 0.17176715460919137
[2025-09-12 18:18:07,447][flp2p.graph_runner][INFO] - Test, Round 73 : loss => 2.3032891904354096,  accuracy: 0.1116
[2025-09-12 18:18:18,658][flp2p.graph_runner][INFO] - Train, Round 74 : loss => 2.3026112127304077,  accuracy: 0.11468, gradient_norm : 0.17768350173253677
[2025-09-12 18:18:44,047][flp2p.graph_runner][INFO] - Test, Round 74 : loss => 2.3032576632380484,  accuracy: 0.1121
[2025-09-12 18:18:54,888][flp2p.graph_runner][INFO] - Train, Round 75 : loss => 2.3025742000341416,  accuracy: 0.1145, gradient_norm : 0.1752236547071181
[2025-09-12 18:19:23,525][flp2p.graph_runner][INFO] - Test, Round 75 : loss => 2.3032264953255654,  accuracy: 0.1128
[2025-09-12 18:19:34,493][flp2p.graph_runner][INFO] - Train, Round 76 : loss => 2.302597749531269,  accuracy: 0.11482, gradient_norm : 0.1800407855852731
[2025-09-12 18:19:57,744][flp2p.graph_runner][INFO] - Test, Round 76 : loss => 2.303194810438156,  accuracy: 0.1133
[2025-09-12 18:20:08,711][flp2p.graph_runner][INFO] - Train, Round 77 : loss => 2.302504813969135,  accuracy: 0.11488, gradient_norm : 0.18022962047760507
[2025-09-12 18:20:21,847][flp2p.graph_runner][INFO] - Test, Round 77 : loss => 2.3031636592149733,  accuracy: 0.1133
[2025-09-12 18:20:33,088][flp2p.graph_runner][INFO] - Train, Round 78 : loss => 2.3024787721037865,  accuracy: 0.11466, gradient_norm : 0.17872362277424525
[2025-09-12 18:20:46,102][flp2p.graph_runner][INFO] - Test, Round 78 : loss => 2.3031323673844337,  accuracy: 0.1135
[2025-09-12 18:20:56,913][flp2p.graph_runner][INFO] - Train, Round 79 : loss => 2.302400692105293,  accuracy: 0.11474, gradient_norm : 0.1796628199607153
[2025-09-12 18:21:09,116][flp2p.graph_runner][INFO] - Test, Round 79 : loss => 2.3031014309048654,  accuracy: 0.114
[2025-09-12 18:21:19,946][flp2p.graph_runner][INFO] - Train, Round 80 : loss => 2.3024196603894236,  accuracy: 0.11498, gradient_norm : 0.1753328092773525
[2025-09-12 18:21:39,117][flp2p.graph_runner][INFO] - Test, Round 80 : loss => 2.3030702833652494,  accuracy: 0.114
[2025-09-12 18:21:49,864][flp2p.graph_runner][INFO] - Train, Round 81 : loss => 2.302380585074425,  accuracy: 0.11498, gradient_norm : 0.1719509845487989
[2025-09-12 18:22:14,601][flp2p.graph_runner][INFO] - Test, Round 81 : loss => 2.3030390821456908,  accuracy: 0.1142
[2025-09-12 18:22:25,477][flp2p.graph_runner][INFO] - Train, Round 82 : loss => 2.3023677617311478,  accuracy: 0.11528, gradient_norm : 0.178321112715446
[2025-09-12 18:22:46,037][flp2p.graph_runner][INFO] - Test, Round 82 : loss => 2.3030075905680656,  accuracy: 0.114
[2025-09-12 18:22:56,921][flp2p.graph_runner][INFO] - Train, Round 83 : loss => 2.302324526011944,  accuracy: 0.11554, gradient_norm : 0.1745633196602415
[2025-09-12 18:23:21,956][flp2p.graph_runner][INFO] - Test, Round 83 : loss => 2.302976558077335,  accuracy: 0.1143
[2025-09-12 18:23:32,879][flp2p.graph_runner][INFO] - Train, Round 84 : loss => 2.302304004132748,  accuracy: 0.11572, gradient_norm : 0.17322750609154433
[2025-09-12 18:24:01,379][flp2p.graph_runner][INFO] - Test, Round 84 : loss => 2.3029452504515646,  accuracy: 0.114
[2025-09-12 18:24:12,208][flp2p.graph_runner][INFO] - Train, Round 85 : loss => 2.302233898639679,  accuracy: 0.11578, gradient_norm : 0.17094476540503126
[2025-09-12 18:24:30,468][flp2p.graph_runner][INFO] - Test, Round 85 : loss => 2.3029146842718125,  accuracy: 0.1138
[2025-09-12 18:24:41,422][flp2p.graph_runner][INFO] - Train, Round 86 : loss => 2.302209618091583,  accuracy: 0.11598, gradient_norm : 0.17477431170434554
[2025-09-12 18:24:53,960][flp2p.graph_runner][INFO] - Test, Round 86 : loss => 2.302883686733246,  accuracy: 0.1139
[2025-09-12 18:25:04,908][flp2p.graph_runner][INFO] - Train, Round 87 : loss => 2.302198601067066,  accuracy: 0.1159, gradient_norm : 0.1810239438318903
[2025-09-12 18:25:17,362][flp2p.graph_runner][INFO] - Test, Round 87 : loss => 2.3028530336499213,  accuracy: 0.1142
[2025-09-12 18:25:27,970][flp2p.graph_runner][INFO] - Train, Round 88 : loss => 2.3021597874164583,  accuracy: 0.11586, gradient_norm : 0.17580907623190267
[2025-09-12 18:25:44,014][flp2p.graph_runner][INFO] - Test, Round 88 : loss => 2.3028221808075906,  accuracy: 0.1145
[2025-09-12 18:25:54,667][flp2p.graph_runner][INFO] - Train, Round 89 : loss => 2.3021236714720725,  accuracy: 0.11586, gradient_norm : 0.1755760452627854
[2025-09-12 18:26:20,222][flp2p.graph_runner][INFO] - Test, Round 89 : loss => 2.302791356039047,  accuracy: 0.1145
[2025-09-12 18:26:31,076][flp2p.graph_runner][INFO] - Train, Round 90 : loss => 2.3020888316631316,  accuracy: 0.11568, gradient_norm : 0.1711867554464932
[2025-09-12 18:26:51,748][flp2p.graph_runner][INFO] - Test, Round 90 : loss => 2.302760755968094,  accuracy: 0.1146
[2025-09-12 18:27:02,691][flp2p.graph_runner][INFO] - Train, Round 91 : loss => 2.3020582753419876,  accuracy: 0.11584, gradient_norm : 0.17459921035988826
[2025-09-12 18:27:24,338][flp2p.graph_runner][INFO] - Test, Round 91 : loss => 2.3027301573634147,  accuracy: 0.1148
[2025-09-12 18:27:35,210][flp2p.graph_runner][INFO] - Train, Round 92 : loss => 2.302046785056591,  accuracy: 0.11628, gradient_norm : 0.18115163065956652
[2025-09-12 18:28:06,182][flp2p.graph_runner][INFO] - Test, Round 92 : loss => 2.3026991113901136,  accuracy: 0.1153
[2025-09-12 18:28:17,321][flp2p.graph_runner][INFO] - Train, Round 93 : loss => 2.301974433362484,  accuracy: 0.11636, gradient_norm : 0.18171162317036227
[2025-09-12 18:28:45,175][flp2p.graph_runner][INFO] - Test, Round 93 : loss => 2.302668694126606,  accuracy: 0.1154
[2025-09-12 18:28:56,133][flp2p.graph_runner][INFO] - Train, Round 94 : loss => 2.3019260782003403,  accuracy: 0.11648, gradient_norm : 0.17354720769767262
[2025-09-12 18:29:13,651][flp2p.graph_runner][INFO] - Test, Round 94 : loss => 2.3026382411122324,  accuracy: 0.1154
[2025-09-12 18:29:24,870][flp2p.graph_runner][INFO] - Train, Round 95 : loss => 2.301911378800869,  accuracy: 0.11678, gradient_norm : 0.17324143587567917
[2025-09-12 18:29:37,640][flp2p.graph_runner][INFO] - Test, Round 95 : loss => 2.3026079572558404,  accuracy: 0.1156
[2025-09-12 18:29:48,581][flp2p.graph_runner][INFO] - Train, Round 96 : loss => 2.3019045957922937,  accuracy: 0.11666, gradient_norm : 0.17739902275095318
[2025-09-12 18:30:01,267][flp2p.graph_runner][INFO] - Test, Round 96 : loss => 2.302577219235897,  accuracy: 0.1157
[2025-09-12 18:30:11,905][flp2p.graph_runner][INFO] - Train, Round 97 : loss => 2.301899234354496,  accuracy: 0.11688, gradient_norm : 0.17890098954411623
[2025-09-12 18:30:26,194][flp2p.graph_runner][INFO] - Test, Round 97 : loss => 2.3025464238762856,  accuracy: 0.1158
[2025-09-12 18:30:37,112][flp2p.graph_runner][INFO] - Train, Round 98 : loss => 2.301811118721962,  accuracy: 0.11704, gradient_norm : 0.17026153756814336
[2025-09-12 18:31:01,692][flp2p.graph_runner][INFO] - Test, Round 98 : loss => 2.302516124892235,  accuracy: 0.1161
[2025-09-12 18:31:12,412][flp2p.graph_runner][INFO] - Train, Round 99 : loss => 2.3017679706215857,  accuracy: 0.1169, gradient_norm : 0.17398910764763845
[2025-09-12 18:31:41,007][flp2p.graph_runner][INFO] - Test, Round 99 : loss => 2.30248596252203,  accuracy: 0.1165
[2025-09-12 18:31:51,867][flp2p.graph_runner][INFO] - Train, Round 100 : loss => 2.301758890748024,  accuracy: 0.1171, gradient_norm : 0.17266499326768125
[2025-09-12 18:32:13,201][flp2p.graph_runner][INFO] - Test, Round 100 : loss => 2.302455808866024,  accuracy: 0.1171
[2025-09-12 18:32:24,164][flp2p.graph_runner][INFO] - Train, Round 101 : loss => 2.3017112571001053,  accuracy: 0.11726, gradient_norm : 0.17919250781873683
[2025-09-12 18:32:51,959][flp2p.graph_runner][INFO] - Test, Round 101 : loss => 2.302425451695919,  accuracy: 0.1171
[2025-09-12 18:33:02,919][flp2p.graph_runner][INFO] - Train, Round 102 : loss => 2.30171793371439,  accuracy: 0.11742, gradient_norm : 0.18332041364914267
[2025-09-12 18:33:29,494][flp2p.graph_runner][INFO] - Test, Round 102 : loss => 2.302394980061054,  accuracy: 0.1172
[2025-09-12 18:33:40,525][flp2p.graph_runner][INFO] - Train, Round 103 : loss => 2.3016642212867735,  accuracy: 0.11748, gradient_norm : 0.17902966019107577
[2025-09-12 18:33:52,686][flp2p.graph_runner][INFO] - Test, Round 103 : loss => 2.302364669752121,  accuracy: 0.1173
[2025-09-12 18:34:03,653][flp2p.graph_runner][INFO] - Train, Round 104 : loss => 2.3016074642539026,  accuracy: 0.11772, gradient_norm : 0.17478106147423564
[2025-09-12 18:34:16,304][flp2p.graph_runner][INFO] - Test, Round 104 : loss => 2.302334723055363,  accuracy: 0.1173
[2025-09-12 18:34:27,016][flp2p.graph_runner][INFO] - Train, Round 105 : loss => 2.3015759256482125,  accuracy: 0.11756, gradient_norm : 0.1736986763248079
[2025-09-12 18:34:39,310][flp2p.graph_runner][INFO] - Test, Round 105 : loss => 2.302304608631134,  accuracy: 0.1176
[2025-09-12 18:34:52,136][flp2p.graph_runner][INFO] - Train, Round 106 : loss => 2.301522922217846,  accuracy: 0.1178, gradient_norm : 0.17989736927769484
[2025-09-12 18:35:09,751][flp2p.graph_runner][INFO] - Test, Round 106 : loss => 2.3022745539069174,  accuracy: 0.1178
[2025-09-12 18:35:20,720][flp2p.graph_runner][INFO] - Train, Round 107 : loss => 2.3015004763007165,  accuracy: 0.11806, gradient_norm : 0.1766631005434727
[2025-09-12 18:35:46,197][flp2p.graph_runner][INFO] - Test, Round 107 : loss => 2.3022446377396584,  accuracy: 0.1183
[2025-09-12 18:35:56,990][flp2p.graph_runner][INFO] - Train, Round 108 : loss => 2.301463424861431,  accuracy: 0.11816, gradient_norm : 0.16922614192531565
[2025-09-12 18:36:16,687][flp2p.graph_runner][INFO] - Test, Round 108 : loss => 2.302214773392677,  accuracy: 0.1186
[2025-09-12 18:36:27,632][flp2p.graph_runner][INFO] - Train, Round 109 : loss => 2.3014279586076736,  accuracy: 0.11814, gradient_norm : 0.1733467623728671
[2025-09-12 18:36:50,374][flp2p.graph_runner][INFO] - Test, Round 109 : loss => 2.30218476575613,  accuracy: 0.1186
[2025-09-12 18:37:01,249][flp2p.graph_runner][INFO] - Train, Round 110 : loss => 2.3014070129394533,  accuracy: 0.1183, gradient_norm : 0.17670175173309224
[2025-09-12 18:37:32,683][flp2p.graph_runner][INFO] - Test, Round 110 : loss => 2.302154718208313,  accuracy: 0.1188
[2025-09-12 18:37:43,544][flp2p.graph_runner][INFO] - Train, Round 111 : loss => 2.301348990499973,  accuracy: 0.11848, gradient_norm : 0.17311198659721488
[2025-09-12 18:38:10,151][flp2p.graph_runner][INFO] - Test, Round 111 : loss => 2.302125085556507,  accuracy: 0.1191
[2025-09-12 18:38:20,787][flp2p.graph_runner][INFO] - Train, Round 112 : loss => 2.3013310873508455,  accuracy: 0.1185, gradient_norm : 0.17122647451521542
[2025-09-12 18:38:41,937][flp2p.graph_runner][INFO] - Test, Round 112 : loss => 2.30209530980587,  accuracy: 0.119
[2025-09-12 18:38:52,825][flp2p.graph_runner][INFO] - Train, Round 113 : loss => 2.3013268575072288,  accuracy: 0.1187, gradient_norm : 0.17867175640058508
[2025-09-12 18:39:05,314][flp2p.graph_runner][INFO] - Test, Round 113 : loss => 2.3020651249527933,  accuracy: 0.1194
[2025-09-12 18:39:16,365][flp2p.graph_runner][INFO] - Train, Round 114 : loss => 2.301294939517975,  accuracy: 0.1187, gradient_norm : 0.17956878288281292
[2025-09-12 18:39:29,152][flp2p.graph_runner][INFO] - Test, Round 114 : loss => 2.302035277581215,  accuracy: 0.1201
[2025-09-12 18:39:39,924][flp2p.graph_runner][INFO] - Train, Round 115 : loss => 2.301259492635727,  accuracy: 0.11886, gradient_norm : 0.17689930981486665
[2025-09-12 18:39:52,516][flp2p.graph_runner][INFO] - Test, Round 115 : loss => 2.302005365347862,  accuracy: 0.1203
[2025-09-12 18:40:03,449][flp2p.graph_runner][INFO] - Train, Round 116 : loss => 2.301222304999828,  accuracy: 0.11892, gradient_norm : 0.17097549527216016
[2025-09-12 18:40:15,939][flp2p.graph_runner][INFO] - Test, Round 116 : loss => 2.30197547044754,  accuracy: 0.1205
[2025-09-12 18:40:26,767][flp2p.graph_runner][INFO] - Train, Round 117 : loss => 2.301171765625477,  accuracy: 0.11916, gradient_norm : 0.18389712335417369
[2025-09-12 18:40:39,060][flp2p.graph_runner][INFO] - Test, Round 117 : loss => 2.3019458549261094,  accuracy: 0.1207
[2025-09-12 18:40:49,528][flp2p.graph_runner][INFO] - Train, Round 118 : loss => 2.3011465910077096,  accuracy: 0.1195, gradient_norm : 0.1787147710033346
[2025-09-12 18:41:03,463][flp2p.graph_runner][INFO] - Test, Round 118 : loss => 2.301916017508507,  accuracy: 0.1209
[2025-09-12 18:41:14,208][flp2p.graph_runner][INFO] - Train, Round 119 : loss => 2.3011126539111135,  accuracy: 0.11966, gradient_norm : 0.1756649773043174
[2025-09-12 18:41:27,731][flp2p.graph_runner][INFO] - Test, Round 119 : loss => 2.301886239731312,  accuracy: 0.1214
[2025-09-12 18:41:38,548][flp2p.graph_runner][INFO] - Train, Round 120 : loss => 2.3010869196057318,  accuracy: 0.11984, gradient_norm : 0.17340136153493024
[2025-09-12 18:41:52,237][flp2p.graph_runner][INFO] - Test, Round 120 : loss => 2.301856552219391,  accuracy: 0.1222
[2025-09-12 18:42:02,916][flp2p.graph_runner][INFO] - Train, Round 121 : loss => 2.3010582754015925,  accuracy: 0.12006, gradient_norm : 0.18468697264214357
[2025-09-12 18:42:15,675][flp2p.graph_runner][INFO] - Test, Round 121 : loss => 2.301826739895344,  accuracy: 0.1224
[2025-09-12 18:42:26,260][flp2p.graph_runner][INFO] - Train, Round 122 : loss => 2.30101490855217,  accuracy: 0.12018, gradient_norm : 0.17431318745107
[2025-09-12 18:42:38,920][flp2p.graph_runner][INFO] - Test, Round 122 : loss => 2.3017970596790316,  accuracy: 0.1222
[2025-09-12 18:42:49,600][flp2p.graph_runner][INFO] - Train, Round 123 : loss => 2.300996328294277,  accuracy: 0.12004, gradient_norm : 0.18278897472299627
[2025-09-12 18:43:02,076][flp2p.graph_runner][INFO] - Test, Round 123 : loss => 2.3017672327041625,  accuracy: 0.122
[2025-09-12 18:43:12,848][flp2p.graph_runner][INFO] - Train, Round 124 : loss => 2.3009990376234053,  accuracy: 0.12022, gradient_norm : 0.17899092469926806
[2025-09-12 18:43:25,415][flp2p.graph_runner][INFO] - Test, Round 124 : loss => 2.301737320148945,  accuracy: 0.1219
[2025-09-12 18:43:36,374][flp2p.graph_runner][INFO] - Train, Round 125 : loss => 2.300893705189228,  accuracy: 0.12048, gradient_norm : 0.1765586069398239
[2025-09-12 18:43:48,988][flp2p.graph_runner][INFO] - Test, Round 125 : loss => 2.3017079870462416,  accuracy: 0.1219
[2025-09-12 18:43:59,810][flp2p.graph_runner][INFO] - Train, Round 126 : loss => 2.3008634313941,  accuracy: 0.12052, gradient_norm : 0.1735275876580075
[2025-09-12 18:44:12,438][flp2p.graph_runner][INFO] - Test, Round 126 : loss => 2.301678830742836,  accuracy: 0.1217
[2025-09-12 18:44:23,352][flp2p.graph_runner][INFO] - Train, Round 127 : loss => 2.3008575624227525,  accuracy: 0.12054, gradient_norm : 0.17509586500273744
[2025-09-12 18:44:36,210][flp2p.graph_runner][INFO] - Test, Round 127 : loss => 2.301649313914776,  accuracy: 0.1209
[2025-09-12 18:44:47,021][flp2p.graph_runner][INFO] - Train, Round 128 : loss => 2.300797279179096,  accuracy: 0.12038, gradient_norm : 0.17620970604304062
[2025-09-12 18:44:59,483][flp2p.graph_runner][INFO] - Test, Round 128 : loss => 2.3016200169324876,  accuracy: 0.1207
[2025-09-12 18:45:10,308][flp2p.graph_runner][INFO] - Train, Round 129 : loss => 2.300769878923893,  accuracy: 0.12056, gradient_norm : 0.18036890096664546
[2025-09-12 18:45:22,783][flp2p.graph_runner][INFO] - Test, Round 129 : loss => 2.3015907383084295,  accuracy: 0.1206
[2025-09-12 18:45:33,624][flp2p.graph_runner][INFO] - Train, Round 130 : loss => 2.3007511246204375,  accuracy: 0.1206, gradient_norm : 0.17688599431866073
[2025-09-12 18:45:46,336][flp2p.graph_runner][INFO] - Test, Round 130 : loss => 2.301561311519146,  accuracy: 0.1206
[2025-09-12 18:45:57,106][flp2p.graph_runner][INFO] - Train, Round 131 : loss => 2.30075496584177,  accuracy: 0.12076, gradient_norm : 0.17898009379833418
[2025-09-12 18:46:09,803][flp2p.graph_runner][INFO] - Test, Round 131 : loss => 2.3015318476557733,  accuracy: 0.1205
[2025-09-12 18:46:20,706][flp2p.graph_runner][INFO] - Train, Round 132 : loss => 2.300727237761021,  accuracy: 0.12056, gradient_norm : 0.17953259691379422
[2025-09-12 18:46:33,866][flp2p.graph_runner][INFO] - Test, Round 132 : loss => 2.301502221429348,  accuracy: 0.1208
[2025-09-12 18:46:44,636][flp2p.graph_runner][INFO] - Train, Round 133 : loss => 2.3006655687093733,  accuracy: 0.1208, gradient_norm : 0.17206613420906547
[2025-09-12 18:47:00,859][flp2p.graph_runner][INFO] - Test, Round 133 : loss => 2.301472913777828,  accuracy: 0.1207
[2025-09-12 18:47:12,115][flp2p.graph_runner][INFO] - Train, Round 134 : loss => 2.300608503520489,  accuracy: 0.12074, gradient_norm : 0.17649995606246788
[2025-09-12 18:47:31,502][flp2p.graph_runner][INFO] - Test, Round 134 : loss => 2.301443752360344,  accuracy: 0.1209
[2025-09-12 18:47:42,576][flp2p.graph_runner][INFO] - Train, Round 135 : loss => 2.3005852076411246,  accuracy: 0.12084, gradient_norm : 0.179484313824764
[2025-09-12 18:47:55,026][flp2p.graph_runner][INFO] - Test, Round 135 : loss => 2.301414440095425,  accuracy: 0.121
[2025-09-12 18:48:05,812][flp2p.graph_runner][INFO] - Train, Round 136 : loss => 2.3005706396698953,  accuracy: 0.12088, gradient_norm : 0.17590481777830203
[2025-09-12 18:48:18,598][flp2p.graph_runner][INFO] - Test, Round 136 : loss => 2.30138513866663,  accuracy: 0.1211
[2025-09-12 18:48:29,518][flp2p.graph_runner][INFO] - Train, Round 137 : loss => 2.300512266755104,  accuracy: 0.12098, gradient_norm : 0.1727109268706955
[2025-09-12 18:48:42,102][flp2p.graph_runner][INFO] - Test, Round 137 : loss => 2.301356211400032,  accuracy: 0.1212
[2025-09-12 18:48:52,901][flp2p.graph_runner][INFO] - Train, Round 138 : loss => 2.300482251942158,  accuracy: 0.12122, gradient_norm : 0.1708108061665459
[2025-09-12 18:49:05,350][flp2p.graph_runner][INFO] - Test, Round 138 : loss => 2.3013271894574165,  accuracy: 0.1215
[2025-09-12 18:49:16,130][flp2p.graph_runner][INFO] - Train, Round 139 : loss => 2.3004635253548624,  accuracy: 0.12118, gradient_norm : 0.17501645766616222
[2025-09-12 18:49:35,135][flp2p.graph_runner][INFO] - Test, Round 139 : loss => 2.3012980492949486,  accuracy: 0.1218
[2025-09-12 18:49:45,771][flp2p.graph_runner][INFO] - Train, Round 140 : loss => 2.30042410671711,  accuracy: 0.12132, gradient_norm : 0.17595805755959615
[2025-09-12 18:50:02,681][flp2p.graph_runner][INFO] - Test, Round 140 : loss => 2.3012688398957253,  accuracy: 0.1218
[2025-09-12 18:50:13,366][flp2p.graph_runner][INFO] - Train, Round 141 : loss => 2.3004036277532576,  accuracy: 0.12134, gradient_norm : 0.18128098371314
[2025-09-12 18:50:30,493][flp2p.graph_runner][INFO] - Test, Round 141 : loss => 2.301239704477787,  accuracy: 0.1219
[2025-09-12 18:50:41,605][flp2p.graph_runner][INFO] - Train, Round 142 : loss => 2.3003761464357377,  accuracy: 0.12154, gradient_norm : 0.1737975043011639
[2025-09-12 18:51:02,163][flp2p.graph_runner][INFO] - Test, Round 142 : loss => 2.30121053763628,  accuracy: 0.1217
[2025-09-12 18:51:13,092][flp2p.graph_runner][INFO] - Train, Round 143 : loss => 2.300328989326954,  accuracy: 0.12154, gradient_norm : 0.17413854447898708
[2025-09-12 18:51:31,214][flp2p.graph_runner][INFO] - Test, Round 143 : loss => 2.3011817058205604,  accuracy: 0.1226
[2025-09-12 18:51:42,219][flp2p.graph_runner][INFO] - Train, Round 144 : loss => 2.3003239646553992,  accuracy: 0.12186, gradient_norm : 0.1750710583900718
[2025-09-12 18:52:03,250][flp2p.graph_runner][INFO] - Test, Round 144 : loss => 2.301152542567253,  accuracy: 0.1225
[2025-09-12 18:52:14,091][flp2p.graph_runner][INFO] - Train, Round 145 : loss => 2.3002869465947153,  accuracy: 0.12192, gradient_norm : 0.17857170681795498
[2025-09-12 18:52:35,189][flp2p.graph_runner][INFO] - Test, Round 145 : loss => 2.3011233342885973,  accuracy: 0.1225
[2025-09-12 18:52:45,928][flp2p.graph_runner][INFO] - Train, Round 146 : loss => 2.300262901186943,  accuracy: 0.12212, gradient_norm : 0.17378428112814628
[2025-09-12 18:52:58,279][flp2p.graph_runner][INFO] - Test, Round 146 : loss => 2.3010942784667017,  accuracy: 0.1222
[2025-09-12 18:53:09,311][flp2p.graph_runner][INFO] - Train, Round 147 : loss => 2.3001873967051507,  accuracy: 0.12228, gradient_norm : 0.17032189443744675
[2025-09-12 18:53:22,360][flp2p.graph_runner][INFO] - Test, Round 147 : loss => 2.3010656841754913,  accuracy: 0.122
[2025-09-12 18:53:33,258][flp2p.graph_runner][INFO] - Train, Round 148 : loss => 2.300157243013382,  accuracy: 0.12236, gradient_norm : 0.17599618195411496
[2025-09-12 18:53:45,750][flp2p.graph_runner][INFO] - Test, Round 148 : loss => 2.3010371707081796,  accuracy: 0.1219
[2025-09-12 18:53:56,457][flp2p.graph_runner][INFO] - Train, Round 149 : loss => 2.300149310231209,  accuracy: 0.1225, gradient_norm : 0.1785682089874663
[2025-09-12 18:54:09,090][flp2p.graph_runner][INFO] - Test, Round 149 : loss => 2.301008251643181,  accuracy: 0.1222
[2025-09-12 18:54:19,768][flp2p.graph_runner][INFO] - Train, Round 150 : loss => 2.300079761147499,  accuracy: 0.1227, gradient_norm : 0.1769143399419974
[2025-09-12 18:54:43,196][flp2p.graph_runner][INFO] - Test, Round 150 : loss => 2.3009798396945,  accuracy: 0.1227
[2025-09-12 18:54:53,890][flp2p.graph_runner][INFO] - Train, Round 151 : loss => 2.3000522297620773,  accuracy: 0.12276, gradient_norm : 0.17668913098161979
[2025-09-12 18:55:12,954][flp2p.graph_runner][INFO] - Test, Round 151 : loss => 2.3009511427283287,  accuracy: 0.1226
[2025-09-12 18:55:23,964][flp2p.graph_runner][INFO] - Train, Round 152 : loss => 2.3000171160697938,  accuracy: 0.12276, gradient_norm : 0.17598760591591853
[2025-09-12 18:55:42,866][flp2p.graph_runner][INFO] - Test, Round 152 : loss => 2.3009225987792017,  accuracy: 0.1229
[2025-09-12 18:55:53,857][flp2p.graph_runner][INFO] - Train, Round 153 : loss => 2.2999805107712747,  accuracy: 0.12312, gradient_norm : 0.17696132227804146
[2025-09-12 18:56:20,342][flp2p.graph_runner][INFO] - Test, Round 153 : loss => 2.3008942268013954,  accuracy: 0.1225
[2025-09-12 18:56:31,272][flp2p.graph_runner][INFO] - Train, Round 154 : loss => 2.299953865110874,  accuracy: 0.12298, gradient_norm : 0.1708352766339225
[2025-09-12 18:56:51,776][flp2p.graph_runner][INFO] - Test, Round 154 : loss => 2.3008656440496447,  accuracy: 0.1226
[2025-09-12 18:57:02,710][flp2p.graph_runner][INFO] - Train, Round 155 : loss => 2.299931936562061,  accuracy: 0.12326, gradient_norm : 0.18153115819774698
[2025-09-12 18:57:26,242][flp2p.graph_runner][INFO] - Test, Round 155 : loss => 2.300836788904667,  accuracy: 0.123
[2025-09-12 18:57:37,281][flp2p.graph_runner][INFO] - Train, Round 156 : loss => 2.2998631939291956,  accuracy: 0.12306, gradient_norm : 0.17471649292687239
[2025-09-12 18:58:01,542][flp2p.graph_runner][INFO] - Test, Round 156 : loss => 2.300808468699455,  accuracy: 0.123
[2025-09-12 18:58:12,750][flp2p.graph_runner][INFO] - Train, Round 157 : loss => 2.2998972919583323,  accuracy: 0.123, gradient_norm : 0.17605612943573362
[2025-09-12 18:58:25,812][flp2p.graph_runner][INFO] - Test, Round 157 : loss => 2.300779952979088,  accuracy: 0.1229
[2025-09-12 18:58:37,043][flp2p.graph_runner][INFO] - Train, Round 158 : loss => 2.299844442307949,  accuracy: 0.1234, gradient_norm : 0.18436331555056765
[2025-09-12 18:58:49,761][flp2p.graph_runner][INFO] - Test, Round 158 : loss => 2.300751382493973,  accuracy: 0.1231
[2025-09-12 18:59:00,594][flp2p.graph_runner][INFO] - Train, Round 159 : loss => 2.2998151242733003,  accuracy: 0.1234, gradient_norm : 0.1721621970555098
[2025-09-12 18:59:12,984][flp2p.graph_runner][INFO] - Test, Round 159 : loss => 2.300722957754135,  accuracy: 0.1236
[2025-09-12 18:59:23,789][flp2p.graph_runner][INFO] - Train, Round 160 : loss => 2.299798679351807,  accuracy: 0.1236, gradient_norm : 0.17890888694823356
[2025-09-12 18:59:39,486][flp2p.graph_runner][INFO] - Test, Round 160 : loss => 2.3006942752599717,  accuracy: 0.1234
[2025-09-12 18:59:50,091][flp2p.graph_runner][INFO] - Train, Round 161 : loss => 2.299759419262409,  accuracy: 0.12336, gradient_norm : 0.17610532556682224
[2025-09-12 19:00:10,226][flp2p.graph_runner][INFO] - Test, Round 161 : loss => 2.3006657492876053,  accuracy: 0.1235
[2025-09-12 19:00:21,157][flp2p.graph_runner][INFO] - Train, Round 162 : loss => 2.2997244608402254,  accuracy: 0.12336, gradient_norm : 0.17679863495577977
[2025-09-12 19:00:40,660][flp2p.graph_runner][INFO] - Test, Round 162 : loss => 2.3006372109651565,  accuracy: 0.1233
[2025-09-12 19:00:51,649][flp2p.graph_runner][INFO] - Train, Round 163 : loss => 2.2997084140777586,  accuracy: 0.1233, gradient_norm : 0.17506649959003254
[2025-09-12 19:01:16,575][flp2p.graph_runner][INFO] - Test, Round 163 : loss => 2.3006086418151854,  accuracy: 0.1234
[2025-09-12 19:01:27,388][flp2p.graph_runner][INFO] - Train, Round 164 : loss => 2.2996849343180656,  accuracy: 0.12328, gradient_norm : 0.17386358927722914
[2025-09-12 19:01:49,325][flp2p.graph_runner][INFO] - Test, Round 164 : loss => 2.300580048465729,  accuracy: 0.1233
[2025-09-12 19:02:00,071][flp2p.graph_runner][INFO] - Train, Round 165 : loss => 2.2996221458911896,  accuracy: 0.12346, gradient_norm : 0.17801691115111595
[2025-09-12 19:02:21,092][flp2p.graph_runner][INFO] - Test, Round 165 : loss => 2.300551552307606,  accuracy: 0.1231
[2025-09-12 19:02:32,194][flp2p.graph_runner][INFO] - Train, Round 166 : loss => 2.2995933842658998,  accuracy: 0.1236, gradient_norm : 0.17647670821771996
[2025-09-12 19:02:58,946][flp2p.graph_runner][INFO] - Test, Round 166 : loss => 2.300522968173027,  accuracy: 0.1232
[2025-09-12 19:03:09,925][flp2p.graph_runner][INFO] - Train, Round 167 : loss => 2.299557367861271,  accuracy: 0.12354, gradient_norm : 0.17470899844547488
[2025-09-12 19:03:40,415][flp2p.graph_runner][INFO] - Test, Round 167 : loss => 2.3004946230292322,  accuracy: 0.1233
[2025-09-12 19:03:51,200][flp2p.graph_runner][INFO] - Train, Round 168 : loss => 2.2995151567459104,  accuracy: 0.12356, gradient_norm : 0.17469883502445907
[2025-09-12 19:04:05,897][flp2p.graph_runner][INFO] - Test, Round 168 : loss => 2.300466235470772,  accuracy: 0.1235
[2025-09-12 19:04:16,744][flp2p.graph_runner][INFO] - Train, Round 169 : loss => 2.2994917178153993,  accuracy: 0.12372, gradient_norm : 0.17316042347864358
[2025-09-12 19:04:32,944][flp2p.graph_runner][INFO] - Test, Round 169 : loss => 2.300437830579281,  accuracy: 0.1236
[2025-09-12 19:04:43,671][flp2p.graph_runner][INFO] - Train, Round 170 : loss => 2.2994681972265245,  accuracy: 0.12384, gradient_norm : 0.17506609403854648
[2025-09-12 19:05:01,108][flp2p.graph_runner][INFO] - Test, Round 170 : loss => 2.3004093920350073,  accuracy: 0.1234
[2025-09-12 19:05:12,092][flp2p.graph_runner][INFO] - Train, Round 171 : loss => 2.29944502055645,  accuracy: 0.12374, gradient_norm : 0.1697319123729735
[2025-09-12 19:05:30,916][flp2p.graph_runner][INFO] - Test, Round 171 : loss => 2.3003811340212823,  accuracy: 0.1235
[2025-09-12 19:05:41,642][flp2p.graph_runner][INFO] - Train, Round 172 : loss => 2.2994315829873084,  accuracy: 0.12366, gradient_norm : 0.17824056831281518
[2025-09-12 19:06:00,435][flp2p.graph_runner][INFO] - Test, Round 172 : loss => 2.3003526088118553,  accuracy: 0.1237
[2025-09-12 19:06:11,294][flp2p.graph_runner][INFO] - Train, Round 173 : loss => 2.2993468061089515,  accuracy: 0.12372, gradient_norm : 0.17572500457856435
[2025-09-12 19:06:29,923][flp2p.graph_runner][INFO] - Test, Round 173 : loss => 2.3003243086576464,  accuracy: 0.1239
[2025-09-12 19:06:40,774][flp2p.graph_runner][INFO] - Train, Round 174 : loss => 2.2993189507722853,  accuracy: 0.12374, gradient_norm : 0.17741058699046836
[2025-09-12 19:06:58,944][flp2p.graph_runner][INFO] - Test, Round 174 : loss => 2.3002959052801133,  accuracy: 0.1237
[2025-09-12 19:07:09,813][flp2p.graph_runner][INFO] - Train, Round 175 : loss => 2.2993560299277305,  accuracy: 0.12384, gradient_norm : 0.17282410045540142
[2025-09-12 19:07:26,975][flp2p.graph_runner][INFO] - Test, Round 175 : loss => 2.300267469680309,  accuracy: 0.1236
[2025-09-12 19:07:37,610][flp2p.graph_runner][INFO] - Train, Round 176 : loss => 2.2992713060975074,  accuracy: 0.1237, gradient_norm : 0.17743015829768333
[2025-09-12 19:07:54,427][flp2p.graph_runner][INFO] - Test, Round 176 : loss => 2.300239301800728,  accuracy: 0.1234
[2025-09-12 19:08:05,413][flp2p.graph_runner][INFO] - Train, Round 177 : loss => 2.2992463481426237,  accuracy: 0.12382, gradient_norm : 0.17690298735801896
[2025-09-12 19:08:20,817][flp2p.graph_runner][INFO] - Test, Round 177 : loss => 2.3002111243963244,  accuracy: 0.1235
[2025-09-12 19:08:31,364][flp2p.graph_runner][INFO] - Train, Round 178 : loss => 2.2991889277100563,  accuracy: 0.12374, gradient_norm : 0.1789481952778868
[2025-09-12 19:08:46,784][flp2p.graph_runner][INFO] - Test, Round 178 : loss => 2.3001829166173935,  accuracy: 0.1233
[2025-09-12 19:08:57,462][flp2p.graph_runner][INFO] - Train, Round 179 : loss => 2.299200689792633,  accuracy: 0.12396, gradient_norm : 0.1794553539251388
[2025-09-12 19:09:12,708][flp2p.graph_runner][INFO] - Test, Round 179 : loss => 2.300154662621021,  accuracy: 0.1232
[2025-09-12 19:09:23,491][flp2p.graph_runner][INFO] - Train, Round 180 : loss => 2.2991679754853247,  accuracy: 0.12388, gradient_norm : 0.18193169505664364
[2025-09-12 19:09:38,019][flp2p.graph_runner][INFO] - Test, Round 180 : loss => 2.3001261093616487,  accuracy: 0.1233
[2025-09-12 19:09:48,422][flp2p.graph_runner][INFO] - Train, Round 181 : loss => 2.299150897860527,  accuracy: 0.12384, gradient_norm : 0.17804255617802045
[2025-09-12 19:10:03,381][flp2p.graph_runner][INFO] - Test, Round 181 : loss => 2.3000975879669188,  accuracy: 0.1235
[2025-09-12 19:10:13,987][flp2p.graph_runner][INFO] - Train, Round 182 : loss => 2.299112052321434,  accuracy: 0.1237, gradient_norm : 0.18152675230704038
[2025-09-12 19:10:28,445][flp2p.graph_runner][INFO] - Test, Round 182 : loss => 2.300069275319576,  accuracy: 0.1236
[2025-09-12 19:10:39,186][flp2p.graph_runner][INFO] - Train, Round 183 : loss => 2.2990918293595315,  accuracy: 0.12372, gradient_norm : 0.17508859021122355
[2025-09-12 19:10:51,999][flp2p.graph_runner][INFO] - Test, Round 183 : loss => 2.3000410383939744,  accuracy: 0.1236
[2025-09-12 19:11:02,712][flp2p.graph_runner][INFO] - Train, Round 184 : loss => 2.2989917635917663,  accuracy: 0.1239, gradient_norm : 0.18171162070966682
[2025-09-12 19:11:15,561][flp2p.graph_runner][INFO] - Test, Round 184 : loss => 2.3000127892136573,  accuracy: 0.1237
[2025-09-12 19:11:26,277][flp2p.graph_runner][INFO] - Train, Round 185 : loss => 2.2990294912457467,  accuracy: 0.1238, gradient_norm : 0.18036578678206128
[2025-09-12 19:11:39,298][flp2p.graph_runner][INFO] - Test, Round 185 : loss => 2.2999842687129974,  accuracy: 0.1238
[2025-09-12 19:11:49,858][flp2p.graph_runner][INFO] - Train, Round 186 : loss => 2.2989887845516206,  accuracy: 0.12392, gradient_norm : 0.17735005224634268
[2025-09-12 19:12:02,069][flp2p.graph_runner][INFO] - Test, Round 186 : loss => 2.2999560247540476,  accuracy: 0.1239
[2025-09-12 19:12:12,971][flp2p.graph_runner][INFO] - Train, Round 187 : loss => 2.29890122205019,  accuracy: 0.12396, gradient_norm : 0.16992909869284148
[2025-09-12 19:12:24,970][flp2p.graph_runner][INFO] - Test, Round 187 : loss => 2.299928061056137,  accuracy: 0.1241
[2025-09-12 19:12:35,568][flp2p.graph_runner][INFO] - Train, Round 188 : loss => 2.298946881592274,  accuracy: 0.12398, gradient_norm : 0.17220683262387282
[2025-09-12 19:12:47,526][flp2p.graph_runner][INFO] - Test, Round 188 : loss => 2.2998996421217917,  accuracy: 0.1243
[2025-09-12 19:12:58,106][flp2p.graph_runner][INFO] - Train, Round 189 : loss => 2.2988898438215255,  accuracy: 0.12406, gradient_norm : 0.178579738473721
[2025-09-12 19:13:09,931][flp2p.graph_runner][INFO] - Test, Round 189 : loss => 2.2998712319374084,  accuracy: 0.1245
[2025-09-12 19:13:20,443][flp2p.graph_runner][INFO] - Train, Round 190 : loss => 2.298838546872139,  accuracy: 0.12418, gradient_norm : 0.17784376606675414
[2025-09-12 19:13:32,365][flp2p.graph_runner][INFO] - Test, Round 190 : loss => 2.299842952823639,  accuracy: 0.1243
[2025-09-12 19:13:43,063][flp2p.graph_runner][INFO] - Train, Round 191 : loss => 2.2987783479690553,  accuracy: 0.12418, gradient_norm : 0.1776598983408114
[2025-09-12 19:13:54,938][flp2p.graph_runner][INFO] - Test, Round 191 : loss => 2.2998149602770805,  accuracy: 0.1246
[2025-09-12 19:14:05,433][flp2p.graph_runner][INFO] - Train, Round 192 : loss => 2.2987308380007745,  accuracy: 0.12418, gradient_norm : 0.17545831705340986
[2025-09-12 19:14:17,480][flp2p.graph_runner][INFO] - Test, Round 192 : loss => 2.2997870809674263,  accuracy: 0.1244
[2025-09-12 19:14:28,001][flp2p.graph_runner][INFO] - Train, Round 193 : loss => 2.29876630038023,  accuracy: 0.12436, gradient_norm : 0.17812663712019677
[2025-09-12 19:14:39,874][flp2p.graph_runner][INFO] - Test, Round 193 : loss => 2.299758661258221,  accuracy: 0.124
[2025-09-12 19:14:50,473][flp2p.graph_runner][INFO] - Train, Round 194 : loss => 2.29871512234211,  accuracy: 0.12432, gradient_norm : 0.18081429516384248
[2025-09-12 19:15:02,378][flp2p.graph_runner][INFO] - Test, Round 194 : loss => 2.299730376791954,  accuracy: 0.1244
[2025-09-12 19:15:13,015][flp2p.graph_runner][INFO] - Train, Round 195 : loss => 2.2986676624417304,  accuracy: 0.12446, gradient_norm : 0.17827744844546925
[2025-09-12 19:15:24,755][flp2p.graph_runner][INFO] - Test, Round 195 : loss => 2.299702141070366,  accuracy: 0.1249
[2025-09-12 19:15:35,172][flp2p.graph_runner][INFO] - Train, Round 196 : loss => 2.2986163452267645,  accuracy: 0.1245, gradient_norm : 0.17075259698915146
[2025-09-12 19:15:47,009][flp2p.graph_runner][INFO] - Test, Round 196 : loss => 2.2996742992162704,  accuracy: 0.1248
[2025-09-12 19:15:57,493][flp2p.graph_runner][INFO] - Train, Round 197 : loss => 2.298606349527836,  accuracy: 0.12462, gradient_norm : 0.1762563245885257
[2025-09-12 19:16:09,241][flp2p.graph_runner][INFO] - Test, Round 197 : loss => 2.2996461493849756,  accuracy: 0.125
[2025-09-12 19:16:19,923][flp2p.graph_runner][INFO] - Train, Round 198 : loss => 2.2985712134838105,  accuracy: 0.12452, gradient_norm : 0.18194650685166788
[2025-09-12 19:16:32,493][flp2p.graph_runner][INFO] - Test, Round 198 : loss => 2.299617955958843,  accuracy: 0.1248
[2025-09-12 19:16:43,097][flp2p.graph_runner][INFO] - Train, Round 199 : loss => 2.2985710832476616,  accuracy: 0.1247, gradient_norm : 0.1775556594991503
[2025-09-12 19:16:54,992][flp2p.graph_runner][INFO] - Test, Round 199 : loss => 2.299589661228657,  accuracy: 0.1245
[2025-09-12 19:17:05,641][flp2p.graph_runner][INFO] - Train, Round 200 : loss => 2.298519404232502,  accuracy: 0.12466, gradient_norm : 0.17863361076301476
[2025-09-12 19:17:17,508][flp2p.graph_runner][INFO] - Test, Round 200 : loss => 2.2995614771962165,  accuracy: 0.1242
[2025-09-12 19:17:28,190][flp2p.graph_runner][INFO] - Train, Round 201 : loss => 2.298494300842285,  accuracy: 0.12498, gradient_norm : 0.1854814075229654
[2025-09-12 19:17:40,094][flp2p.graph_runner][INFO] - Test, Round 201 : loss => 2.29953316218853,  accuracy: 0.1244
[2025-09-12 19:17:50,535][flp2p.graph_runner][INFO] - Train, Round 202 : loss => 2.2984927871823313,  accuracy: 0.125, gradient_norm : 0.18049550644726384
[2025-09-12 19:18:02,400][flp2p.graph_runner][INFO] - Test, Round 202 : loss => 2.299504876828194,  accuracy: 0.1243
[2025-09-12 19:18:13,103][flp2p.graph_runner][INFO] - Train, Round 203 : loss => 2.298428236246109,  accuracy: 0.12494, gradient_norm : 0.17870467631351056
[2025-09-12 19:18:25,114][flp2p.graph_runner][INFO] - Test, Round 203 : loss => 2.2994765213131902,  accuracy: 0.1243
[2025-09-12 19:18:35,787][flp2p.graph_runner][INFO] - Train, Round 204 : loss => 2.2983829101920126,  accuracy: 0.12482, gradient_norm : 0.17578002047590113
[2025-09-12 19:18:47,574][flp2p.graph_runner][INFO] - Test, Round 204 : loss => 2.2994485967516898,  accuracy: 0.1244
[2025-09-12 19:18:58,126][flp2p.graph_runner][INFO] - Train, Round 205 : loss => 2.2983303931355477,  accuracy: 0.12494, gradient_norm : 0.1790713805785247
[2025-09-12 19:19:09,971][flp2p.graph_runner][INFO] - Test, Round 205 : loss => 2.299420665502548,  accuracy: 0.1241
[2025-09-12 19:19:20,580][flp2p.graph_runner][INFO] - Train, Round 206 : loss => 2.298317355811596,  accuracy: 0.12482, gradient_norm : 0.17842509640987164
[2025-09-12 19:19:32,494][flp2p.graph_runner][INFO] - Test, Round 206 : loss => 2.299392761063576,  accuracy: 0.1243
[2025-09-12 19:19:43,050][flp2p.graph_runner][INFO] - Train, Round 207 : loss => 2.2983238804340362,  accuracy: 0.12486, gradient_norm : 0.18846555346830343
[2025-09-12 19:19:54,833][flp2p.graph_runner][INFO] - Test, Round 207 : loss => 2.299364229953289,  accuracy: 0.1245
[2025-09-12 19:20:05,469][flp2p.graph_runner][INFO] - Train, Round 208 : loss => 2.2982690861821173,  accuracy: 0.12476, gradient_norm : 0.1836733911311937
[2025-09-12 19:20:17,500][flp2p.graph_runner][INFO] - Test, Round 208 : loss => 2.299335945367813,  accuracy: 0.1249
[2025-09-12 19:20:28,058][flp2p.graph_runner][INFO] - Train, Round 209 : loss => 2.298261756002903,  accuracy: 0.125, gradient_norm : 0.18025178193496402
[2025-09-12 19:20:39,953][flp2p.graph_runner][INFO] - Test, Round 209 : loss => 2.299307701265812,  accuracy: 0.1252
[2025-09-12 19:20:50,516][flp2p.graph_runner][INFO] - Train, Round 210 : loss => 2.298183244466782,  accuracy: 0.12502, gradient_norm : 0.18259312781106055
[2025-09-12 19:21:02,303][flp2p.graph_runner][INFO] - Test, Round 210 : loss => 2.299279611301422,  accuracy: 0.1253
[2025-09-12 19:21:12,877][flp2p.graph_runner][INFO] - Train, Round 211 : loss => 2.2981797045469285,  accuracy: 0.1252, gradient_norm : 0.17641294071617275
[2025-09-12 19:21:24,769][flp2p.graph_runner][INFO] - Test, Round 211 : loss => 2.2992515574216843,  accuracy: 0.1257
[2025-09-12 19:21:35,301][flp2p.graph_runner][INFO] - Train, Round 212 : loss => 2.298148129582405,  accuracy: 0.12514, gradient_norm : 0.18131124607652094
[2025-09-12 19:21:47,130][flp2p.graph_runner][INFO] - Test, Round 212 : loss => 2.2992233496427534,  accuracy: 0.1259
[2025-09-12 19:21:57,772][flp2p.graph_runner][INFO] - Train, Round 213 : loss => 2.2981148996949194,  accuracy: 0.12518, gradient_norm : 0.1839960568829358
[2025-09-12 19:22:09,630][flp2p.graph_runner][INFO] - Test, Round 213 : loss => 2.299195044505596,  accuracy: 0.1259
[2025-09-12 19:22:20,105][flp2p.graph_runner][INFO] - Train, Round 214 : loss => 2.298115013539791,  accuracy: 0.12516, gradient_norm : 0.1770847449478721
[2025-09-12 19:22:32,125][flp2p.graph_runner][INFO] - Test, Round 214 : loss => 2.2991668585538863,  accuracy: 0.1263
[2025-09-12 19:22:42,637][flp2p.graph_runner][INFO] - Train, Round 215 : loss => 2.2980534827709196,  accuracy: 0.12514, gradient_norm : 0.17938182972633135
[2025-09-12 19:22:54,450][flp2p.graph_runner][INFO] - Test, Round 215 : loss => 2.2991385699510576,  accuracy: 0.1263
[2025-09-12 19:23:05,091][flp2p.graph_runner][INFO] - Train, Round 216 : loss => 2.2980045768618584,  accuracy: 0.12514, gradient_norm : 0.18384408804090632
[2025-09-12 19:23:17,033][flp2p.graph_runner][INFO] - Test, Round 216 : loss => 2.299110353219509,  accuracy: 0.1264
[2025-09-12 19:23:27,585][flp2p.graph_runner][INFO] - Train, Round 217 : loss => 2.2979549077153205,  accuracy: 0.12524, gradient_norm : 0.18019684772993239
[2025-09-12 19:23:39,322][flp2p.graph_runner][INFO] - Test, Round 217 : loss => 2.2990822356939318,  accuracy: 0.1263
[2025-09-12 19:23:50,075][flp2p.graph_runner][INFO] - Train, Round 218 : loss => 2.29791549384594,  accuracy: 0.1253, gradient_norm : 0.17720067611944645
[2025-09-12 19:24:01,986][flp2p.graph_runner][INFO] - Test, Round 218 : loss => 2.2990544312477112,  accuracy: 0.1257
[2025-09-12 19:24:12,524][flp2p.graph_runner][INFO] - Train, Round 219 : loss => 2.297913959026337,  accuracy: 0.12538, gradient_norm : 0.17821935093190183
[2025-09-12 19:24:24,530][flp2p.graph_runner][INFO] - Test, Round 219 : loss => 2.2990262109160424,  accuracy: 0.1258
[2025-09-12 19:24:35,061][flp2p.graph_runner][INFO] - Train, Round 220 : loss => 2.2979055744409562,  accuracy: 0.12544, gradient_norm : 0.1772455452741105
[2025-09-12 19:24:46,901][flp2p.graph_runner][INFO] - Test, Round 220 : loss => 2.2989983040213584,  accuracy: 0.1261
[2025-09-12 19:24:57,456][flp2p.graph_runner][INFO] - Train, Round 221 : loss => 2.2978656592965128,  accuracy: 0.12546, gradient_norm : 0.1796220370891094
[2025-09-12 19:25:09,326][flp2p.graph_runner][INFO] - Test, Round 221 : loss => 2.2989702246546746,  accuracy: 0.1259
[2025-09-12 19:25:19,669][flp2p.graph_runner][INFO] - Train, Round 222 : loss => 2.297861829996109,  accuracy: 0.1256, gradient_norm : 0.17667711248022827
[2025-09-12 19:25:31,906][flp2p.graph_runner][INFO] - Test, Round 222 : loss => 2.298942084622383,  accuracy: 0.126
[2025-09-12 19:25:42,663][flp2p.graph_runner][INFO] - Train, Round 223 : loss => 2.297801813185215,  accuracy: 0.12564, gradient_norm : 0.1835978399831324
[2025-09-12 19:25:54,710][flp2p.graph_runner][INFO] - Test, Round 223 : loss => 2.2989143149614333,  accuracy: 0.1261
[2025-09-12 19:26:05,197][flp2p.graph_runner][INFO] - Train, Round 224 : loss => 2.2977384728193284,  accuracy: 0.12586, gradient_norm : 0.1739870372468583
[2025-09-12 19:26:18,052][flp2p.graph_runner][INFO] - Test, Round 224 : loss => 2.298886389386654,  accuracy: 0.1261
[2025-09-12 19:26:28,706][flp2p.graph_runner][INFO] - Train, Round 225 : loss => 2.297716128826141,  accuracy: 0.12598, gradient_norm : 0.17948178788509592
[2025-09-12 19:26:41,307][flp2p.graph_runner][INFO] - Test, Round 225 : loss => 2.2988583934783935,  accuracy: 0.1263
[2025-09-12 19:26:51,811][flp2p.graph_runner][INFO] - Train, Round 226 : loss => 2.29769595593214,  accuracy: 0.12614, gradient_norm : 0.17592583033561282
[2025-09-12 19:27:04,195][flp2p.graph_runner][INFO] - Test, Round 226 : loss => 2.2988301137566567,  accuracy: 0.1264
[2025-09-12 19:27:15,025][flp2p.graph_runner][INFO] - Train, Round 227 : loss => 2.2976728799939155,  accuracy: 0.12618, gradient_norm : 0.1819664033001314
[2025-09-12 19:27:27,219][flp2p.graph_runner][INFO] - Test, Round 227 : loss => 2.2988021290421488,  accuracy: 0.1263
[2025-09-12 19:27:37,877][flp2p.graph_runner][INFO] - Train, Round 228 : loss => 2.297651551961899,  accuracy: 0.12624, gradient_norm : 0.1796082841193451
[2025-09-12 19:27:50,175][flp2p.graph_runner][INFO] - Test, Round 228 : loss => 2.2987741025567057,  accuracy: 0.1264
[2025-09-12 19:28:00,750][flp2p.graph_runner][INFO] - Train, Round 229 : loss => 2.2976044580340385,  accuracy: 0.12638, gradient_norm : 0.17493854853987223
[2025-09-12 19:28:13,067][flp2p.graph_runner][INFO] - Test, Round 229 : loss => 2.298746153151989,  accuracy: 0.1264
[2025-09-12 19:28:23,709][flp2p.graph_runner][INFO] - Train, Round 230 : loss => 2.297595567703247,  accuracy: 0.12644, gradient_norm : 0.1764025386416479
[2025-09-12 19:28:35,638][flp2p.graph_runner][INFO] - Test, Round 230 : loss => 2.298718022620678,  accuracy: 0.1263
[2025-09-12 19:28:46,401][flp2p.graph_runner][INFO] - Train, Round 231 : loss => 2.297523906826973,  accuracy: 0.12652, gradient_norm : 0.1760190437332101
[2025-09-12 19:28:58,284][flp2p.graph_runner][INFO] - Test, Round 231 : loss => 2.298690200674534,  accuracy: 0.1264
[2025-09-12 19:29:08,867][flp2p.graph_runner][INFO] - Train, Round 232 : loss => 2.297501500248909,  accuracy: 0.12664, gradient_norm : 0.1789441817556842
[2025-09-12 19:29:20,700][flp2p.graph_runner][INFO] - Test, Round 232 : loss => 2.2986619707107545,  accuracy: 0.1265
[2025-09-12 19:29:31,429][flp2p.graph_runner][INFO] - Train, Round 233 : loss => 2.297455016374588,  accuracy: 0.12646, gradient_norm : 0.17429077403302334
[2025-09-12 19:29:43,194][flp2p.graph_runner][INFO] - Test, Round 233 : loss => 2.2986341022729873,  accuracy: 0.1266
[2025-09-12 19:29:53,702][flp2p.graph_runner][INFO] - Train, Round 234 : loss => 2.2974454602599144,  accuracy: 0.12648, gradient_norm : 0.18321800856250023
[2025-09-12 19:30:05,817][flp2p.graph_runner][INFO] - Test, Round 234 : loss => 2.298605812728405,  accuracy: 0.1267
[2025-09-12 19:30:16,534][flp2p.graph_runner][INFO] - Train, Round 235 : loss => 2.297397326231003,  accuracy: 0.12676, gradient_norm : 0.17466225181204334
[2025-09-12 19:30:28,428][flp2p.graph_runner][INFO] - Test, Round 235 : loss => 2.2985776300907137,  accuracy: 0.1272
[2025-09-12 19:30:39,265][flp2p.graph_runner][INFO] - Train, Round 236 : loss => 2.297431256175041,  accuracy: 0.12674, gradient_norm : 0.1742406819127292
[2025-09-12 19:30:51,273][flp2p.graph_runner][INFO] - Test, Round 236 : loss => 2.298549274420738,  accuracy: 0.1274
[2025-09-12 19:31:01,772][flp2p.graph_runner][INFO] - Train, Round 237 : loss => 2.2973561137914658,  accuracy: 0.12676, gradient_norm : 0.17407271087528717
[2025-09-12 19:31:13,513][flp2p.graph_runner][INFO] - Test, Round 237 : loss => 2.298521140742302,  accuracy: 0.1275
[2025-09-12 19:31:24,419][flp2p.graph_runner][INFO] - Train, Round 238 : loss => 2.2973658707737923,  accuracy: 0.12666, gradient_norm : 0.1803911660186607
[2025-09-12 19:31:36,246][flp2p.graph_runner][INFO] - Test, Round 238 : loss => 2.298492674100399,  accuracy: 0.1273
[2025-09-12 19:31:46,794][flp2p.graph_runner][INFO] - Train, Round 239 : loss => 2.297246471941471,  accuracy: 0.12678, gradient_norm : 0.17716200623996695
[2025-09-12 19:31:58,653][flp2p.graph_runner][INFO] - Test, Round 239 : loss => 2.298464842748642,  accuracy: 0.1279
[2025-09-12 19:32:09,152][flp2p.graph_runner][INFO] - Train, Round 240 : loss => 2.2972548323869706,  accuracy: 0.1267, gradient_norm : 0.18213537605081023
[2025-09-12 19:32:20,891][flp2p.graph_runner][INFO] - Test, Round 240 : loss => 2.298436739516258,  accuracy: 0.1281
[2025-09-12 19:32:31,599][flp2p.graph_runner][INFO] - Train, Round 241 : loss => 2.2972333246469496,  accuracy: 0.12696, gradient_norm : 0.18183550671676763
[2025-09-12 19:32:43,710][flp2p.graph_runner][INFO] - Test, Round 241 : loss => 2.2984085263967513,  accuracy: 0.1284
[2025-09-12 19:32:54,390][flp2p.graph_runner][INFO] - Train, Round 242 : loss => 2.2971915233135225,  accuracy: 0.12692, gradient_norm : 0.18475885418891622
[2025-09-12 19:33:06,324][flp2p.graph_runner][INFO] - Test, Round 242 : loss => 2.298380349957943,  accuracy: 0.1284
[2025-09-12 19:33:16,901][flp2p.graph_runner][INFO] - Train, Round 243 : loss => 2.2971484804153444,  accuracy: 0.127, gradient_norm : 0.18172768324282731
[2025-09-12 19:33:28,747][flp2p.graph_runner][INFO] - Test, Round 243 : loss => 2.2983523220062256,  accuracy: 0.1284
[2025-09-12 19:33:39,511][flp2p.graph_runner][INFO] - Train, Round 244 : loss => 2.2971253111958503,  accuracy: 0.1269, gradient_norm : 0.1799607350961166
[2025-09-12 19:33:51,476][flp2p.graph_runner][INFO] - Test, Round 244 : loss => 2.298324007892609,  accuracy: 0.1286
[2025-09-12 19:34:02,271][flp2p.graph_runner][INFO] - Train, Round 245 : loss => 2.297103488445282,  accuracy: 0.12696, gradient_norm : 0.177360378905388
[2025-09-12 19:34:14,173][flp2p.graph_runner][INFO] - Test, Round 245 : loss => 2.298295916199684,  accuracy: 0.1284
[2025-09-12 19:34:24,962][flp2p.graph_runner][INFO] - Train, Round 246 : loss => 2.2970792189240457,  accuracy: 0.12704, gradient_norm : 0.1788762330146964
[2025-09-12 19:34:37,026][flp2p.graph_runner][INFO] - Test, Round 246 : loss => 2.2982674860596655,  accuracy: 0.1285
[2025-09-12 19:34:47,721][flp2p.graph_runner][INFO] - Train, Round 247 : loss => 2.297056803405285,  accuracy: 0.1271, gradient_norm : 0.17964041238126696
[2025-09-12 19:34:59,602][flp2p.graph_runner][INFO] - Test, Round 247 : loss => 2.298239344704151,  accuracy: 0.1284
[2025-09-12 19:35:10,224][flp2p.graph_runner][INFO] - Train, Round 248 : loss => 2.2969974610209465,  accuracy: 0.1271, gradient_norm : 0.17622629090965147
[2025-09-12 19:35:22,137][flp2p.graph_runner][INFO] - Test, Round 248 : loss => 2.298211236810684,  accuracy: 0.1284
[2025-09-12 19:35:32,710][flp2p.graph_runner][INFO] - Train, Round 249 : loss => 2.2969747960567473,  accuracy: 0.12732, gradient_norm : 0.17783036318508835
[2025-09-12 19:35:44,584][flp2p.graph_runner][INFO] - Test, Round 249 : loss => 2.2981829243183136,  accuracy: 0.1284
[2025-09-12 19:35:55,345][flp2p.graph_runner][INFO] - Train, Round 250 : loss => 2.2969343489408494,  accuracy: 0.12726, gradient_norm : 0.17893154847006923
[2025-09-12 19:36:07,118][flp2p.graph_runner][INFO] - Test, Round 250 : loss => 2.298154579949379,  accuracy: 0.1284
[2025-09-12 19:36:17,895][flp2p.graph_runner][INFO] - Train, Round 251 : loss => 2.296912757754326,  accuracy: 0.12748, gradient_norm : 0.17718577223041634
[2025-09-12 19:36:29,695][flp2p.graph_runner][INFO] - Test, Round 251 : loss => 2.2981261746525763,  accuracy: 0.1286
[2025-09-12 19:36:40,204][flp2p.graph_runner][INFO] - Train, Round 252 : loss => 2.2968833211064337,  accuracy: 0.12736, gradient_norm : 0.1795224449045143
[2025-09-12 19:36:52,226][flp2p.graph_runner][INFO] - Test, Round 252 : loss => 2.2980978167891504,  accuracy: 0.1287
[2025-09-12 19:37:02,910][flp2p.graph_runner][INFO] - Train, Round 253 : loss => 2.2967895901203157,  accuracy: 0.12756, gradient_norm : 0.17787985607746276
[2025-09-12 19:37:14,814][flp2p.graph_runner][INFO] - Test, Round 253 : loss => 2.2980698232412338,  accuracy: 0.1286
[2025-09-12 19:37:25,452][flp2p.graph_runner][INFO] - Train, Round 254 : loss => 2.296794183552265,  accuracy: 0.12766, gradient_norm : 0.18454178503286456
[2025-09-12 19:37:37,328][flp2p.graph_runner][INFO] - Test, Round 254 : loss => 2.298041293597221,  accuracy: 0.1286
[2025-09-12 19:37:48,037][flp2p.graph_runner][INFO] - Train, Round 255 : loss => 2.2967260771989824,  accuracy: 0.1277, gradient_norm : 0.17706700979575904
[2025-09-12 19:37:59,901][flp2p.graph_runner][INFO] - Test, Round 255 : loss => 2.298013091957569,  accuracy: 0.1286
[2025-09-12 19:38:10,477][flp2p.graph_runner][INFO] - Train, Round 256 : loss => 2.296713269352913,  accuracy: 0.1279, gradient_norm : 0.18006077039297874
[2025-09-12 19:38:22,275][flp2p.graph_runner][INFO] - Test, Round 256 : loss => 2.297984675323963,  accuracy: 0.1284
[2025-09-12 19:38:32,741][flp2p.graph_runner][INFO] - Train, Round 257 : loss => 2.2966943642497064,  accuracy: 0.12792, gradient_norm : 0.1807743673382166
[2025-09-12 19:38:44,699][flp2p.graph_runner][INFO] - Test, Round 257 : loss => 2.2979562588572504,  accuracy: 0.1284
[2025-09-12 19:38:55,227][flp2p.graph_runner][INFO] - Train, Round 258 : loss => 2.2966694551706315,  accuracy: 0.12816, gradient_norm : 0.18060377427251553
[2025-09-12 19:39:07,105][flp2p.graph_runner][INFO] - Test, Round 258 : loss => 2.297927918624878,  accuracy: 0.1286
[2025-09-12 19:39:17,600][flp2p.graph_runner][INFO] - Train, Round 259 : loss => 2.2966555884480475,  accuracy: 0.12816, gradient_norm : 0.18139183781548765
[2025-09-12 19:39:29,575][flp2p.graph_runner][INFO] - Test, Round 259 : loss => 2.29789938467741,  accuracy: 0.1286
[2025-09-12 19:39:40,411][flp2p.graph_runner][INFO] - Train, Round 260 : loss => 2.2966245090961457,  accuracy: 0.12834, gradient_norm : 0.18012885650170202
[2025-09-12 19:39:52,332][flp2p.graph_runner][INFO] - Test, Round 260 : loss => 2.297870605456829,  accuracy: 0.1285
[2025-09-12 19:40:03,049][flp2p.graph_runner][INFO] - Train, Round 261 : loss => 2.2965965035557745,  accuracy: 0.1284, gradient_norm : 0.18231023951100792
[2025-09-12 19:40:14,896][flp2p.graph_runner][INFO] - Test, Round 261 : loss => 2.2978418411135673,  accuracy: 0.1282
[2025-09-12 19:40:25,458][flp2p.graph_runner][INFO] - Train, Round 262 : loss => 2.296570734381676,  accuracy: 0.1283, gradient_norm : 0.18723657274745914
[2025-09-12 19:40:37,326][flp2p.graph_runner][INFO] - Test, Round 262 : loss => 2.29781302536726,  accuracy: 0.1281
[2025-09-12 19:40:47,933][flp2p.graph_runner][INFO] - Train, Round 263 : loss => 2.2965161699056624,  accuracy: 0.12852, gradient_norm : 0.18075963095128794
[2025-09-12 19:40:59,961][flp2p.graph_runner][INFO] - Test, Round 263 : loss => 2.2977845181584358,  accuracy: 0.1283
[2025-09-12 19:41:10,541][flp2p.graph_runner][INFO] - Train, Round 264 : loss => 2.2965006852149963,  accuracy: 0.12866, gradient_norm : 0.18093451580673228
[2025-09-12 19:41:22,346][flp2p.graph_runner][INFO] - Test, Round 264 : loss => 2.297755933213234,  accuracy: 0.1281
[2025-09-12 19:41:32,861][flp2p.graph_runner][INFO] - Train, Round 265 : loss => 2.296449457705021,  accuracy: 0.12884, gradient_norm : 0.17723817478571302
[2025-09-12 19:41:44,769][flp2p.graph_runner][INFO] - Test, Round 265 : loss => 2.2977275242090225,  accuracy: 0.1283
[2025-09-12 19:41:55,320][flp2p.graph_runner][INFO] - Train, Round 266 : loss => 2.2964271938800813,  accuracy: 0.12898, gradient_norm : 0.1838704886093727
[2025-09-12 19:42:07,205][flp2p.graph_runner][INFO] - Test, Round 266 : loss => 2.297698723745346,  accuracy: 0.1284
[2025-09-12 19:42:17,828][flp2p.graph_runner][INFO] - Train, Round 267 : loss => 2.296349392235279,  accuracy: 0.12904, gradient_norm : 0.18639219588062075
[2025-09-12 19:42:29,679][flp2p.graph_runner][INFO] - Test, Round 267 : loss => 2.29767010884285,  accuracy: 0.1283
[2025-09-12 19:42:40,234][flp2p.graph_runner][INFO] - Train, Round 268 : loss => 2.296319790184498,  accuracy: 0.12904, gradient_norm : 0.1802297799330558
[2025-09-12 19:42:52,284][flp2p.graph_runner][INFO] - Test, Round 268 : loss => 2.2976415879011154,  accuracy: 0.1284
[2025-09-12 19:43:02,804][flp2p.graph_runner][INFO] - Train, Round 269 : loss => 2.2962934651970865,  accuracy: 0.1292, gradient_norm : 0.18065144936704758
[2025-09-12 19:43:14,656][flp2p.graph_runner][INFO] - Test, Round 269 : loss => 2.2976129830002785,  accuracy: 0.1286
[2025-09-12 19:43:25,207][flp2p.graph_runner][INFO] - Train, Round 270 : loss => 2.296319923400879,  accuracy: 0.12914, gradient_norm : 0.17780047809234026
[2025-09-12 19:43:36,988][flp2p.graph_runner][INFO] - Test, Round 270 : loss => 2.2975842435002325,  accuracy: 0.1288
[2025-09-12 19:43:47,587][flp2p.graph_runner][INFO] - Train, Round 271 : loss => 2.296255340874195,  accuracy: 0.12936, gradient_norm : 0.18664322811989004
[2025-09-12 19:43:59,557][flp2p.graph_runner][INFO] - Test, Round 271 : loss => 2.297555313575268,  accuracy: 0.1286
[2025-09-12 19:44:10,207][flp2p.graph_runner][INFO] - Train, Round 272 : loss => 2.2961862102150916,  accuracy: 0.12932, gradient_norm : 0.18064762966165024
[2025-09-12 19:44:22,022][flp2p.graph_runner][INFO] - Test, Round 272 : loss => 2.2975266326189043,  accuracy: 0.1287
[2025-09-12 19:44:32,831][flp2p.graph_runner][INFO] - Train, Round 273 : loss => 2.2961797443032266,  accuracy: 0.12938, gradient_norm : 0.18071461812313117
[2025-09-12 19:44:44,687][flp2p.graph_runner][INFO] - Test, Round 273 : loss => 2.2974979821562767,  accuracy: 0.1289
[2025-09-12 19:44:55,579][flp2p.graph_runner][INFO] - Train, Round 274 : loss => 2.296157985329628,  accuracy: 0.12956, gradient_norm : 0.17855389404789723
[2025-09-12 19:45:07,622][flp2p.graph_runner][INFO] - Test, Round 274 : loss => 2.297469216609001,  accuracy: 0.1288
[2025-09-12 19:45:18,246][flp2p.graph_runner][INFO] - Train, Round 275 : loss => 2.296127892434597,  accuracy: 0.12954, gradient_norm : 0.19001990270698266
[2025-09-12 19:45:30,158][flp2p.graph_runner][INFO] - Test, Round 275 : loss => 2.297440197134018,  accuracy: 0.1287
[2025-09-12 19:45:40,815][flp2p.graph_runner][INFO] - Train, Round 276 : loss => 2.296070356965065,  accuracy: 0.12968, gradient_norm : 0.18719171370169613
[2025-09-12 19:45:52,703][flp2p.graph_runner][INFO] - Test, Round 276 : loss => 2.297411369562149,  accuracy: 0.1289
[2025-09-12 19:46:03,225][flp2p.graph_runner][INFO] - Train, Round 277 : loss => 2.296026230752468,  accuracy: 0.12956, gradient_norm : 0.17950453912447975
[2025-09-12 19:46:15,089][flp2p.graph_runner][INFO] - Test, Round 277 : loss => 2.2973827214956284,  accuracy: 0.1289
[2025-09-12 19:46:25,853][flp2p.graph_runner][INFO] - Train, Round 278 : loss => 2.2960502150654793,  accuracy: 0.12956, gradient_norm : 0.17931186830800036
[2025-09-12 19:46:37,664][flp2p.graph_runner][INFO] - Test, Round 278 : loss => 2.297353886950016,  accuracy: 0.1287
[2025-09-12 19:46:48,355][flp2p.graph_runner][INFO] - Train, Round 279 : loss => 2.295994507074356,  accuracy: 0.12968, gradient_norm : 0.18517338534546215
[2025-09-12 19:47:00,346][flp2p.graph_runner][INFO] - Test, Round 279 : loss => 2.2973248874664307,  accuracy: 0.1285
[2025-09-12 19:47:11,009][flp2p.graph_runner][INFO] - Train, Round 280 : loss => 2.2959738317131997,  accuracy: 0.12974, gradient_norm : 0.18334296922356652
[2025-09-12 19:47:22,832][flp2p.graph_runner][INFO] - Test, Round 280 : loss => 2.297295874679089,  accuracy: 0.1286
[2025-09-12 19:47:33,426][flp2p.graph_runner][INFO] - Train, Round 281 : loss => 2.2959357780218124,  accuracy: 0.12976, gradient_norm : 0.18005238237232532
[2025-09-12 19:47:45,269][flp2p.graph_runner][INFO] - Test, Round 281 : loss => 2.297266952610016,  accuracy: 0.1286
[2025-09-12 19:47:55,937][flp2p.graph_runner][INFO] - Train, Round 282 : loss => 2.2958946177363395,  accuracy: 0.1301, gradient_norm : 0.1850917749552815
[2025-09-12 19:48:07,739][flp2p.graph_runner][INFO] - Test, Round 282 : loss => 2.2972382315158844,  accuracy: 0.1286
[2025-09-12 19:48:18,179][flp2p.graph_runner][INFO] - Train, Round 283 : loss => 2.2958774992823603,  accuracy: 0.13002, gradient_norm : 0.18773428071128598
[2025-09-12 19:48:30,158][flp2p.graph_runner][INFO] - Test, Round 283 : loss => 2.297209069955349,  accuracy: 0.129
[2025-09-12 19:48:40,808][flp2p.graph_runner][INFO] - Train, Round 284 : loss => 2.2958482655882837,  accuracy: 0.13026, gradient_norm : 0.18416548551194445
[2025-09-12 19:48:53,013][flp2p.graph_runner][INFO] - Test, Round 284 : loss => 2.2971801187038423,  accuracy: 0.1291
[2025-09-12 19:49:03,547][flp2p.graph_runner][INFO] - Train, Round 285 : loss => 2.295809445977211,  accuracy: 0.13026, gradient_norm : 0.1779894460635293
[2025-09-12 19:49:15,345][flp2p.graph_runner][INFO] - Test, Round 285 : loss => 2.297151072907448,  accuracy: 0.1292
[2025-09-12 19:49:25,833][flp2p.graph_runner][INFO] - Train, Round 286 : loss => 2.2957705637812613,  accuracy: 0.1302, gradient_norm : 0.18442580286555782
[2025-09-12 19:49:37,717][flp2p.graph_runner][INFO] - Test, Round 286 : loss => 2.2971221075057984,  accuracy: 0.1292
[2025-09-12 19:49:48,416][flp2p.graph_runner][INFO] - Train, Round 287 : loss => 2.2957224372029303,  accuracy: 0.13046, gradient_norm : 0.18191151938117003
[2025-09-12 19:50:00,358][flp2p.graph_runner][INFO] - Test, Round 287 : loss => 2.2970931833386423,  accuracy: 0.1292
[2025-09-12 19:50:10,810][flp2p.graph_runner][INFO] - Train, Round 288 : loss => 2.2956829035282134,  accuracy: 0.13042, gradient_norm : 0.18423594856138142
[2025-09-12 19:50:22,795][flp2p.graph_runner][INFO] - Test, Round 288 : loss => 2.297063896870613,  accuracy: 0.1294
[2025-09-12 19:50:33,409][flp2p.graph_runner][INFO] - Train, Round 289 : loss => 2.2956885340809823,  accuracy: 0.13048, gradient_norm : 0.18280223525447745
[2025-09-12 19:50:45,262][flp2p.graph_runner][INFO] - Test, Round 289 : loss => 2.297034755194187,  accuracy: 0.1296
[2025-09-12 19:50:55,973][flp2p.graph_runner][INFO] - Train, Round 290 : loss => 2.29563040047884,  accuracy: 0.13056, gradient_norm : 0.18140255239517752
[2025-09-12 19:51:08,084][flp2p.graph_runner][INFO] - Test, Round 290 : loss => 2.2970056956887244,  accuracy: 0.1295
[2025-09-12 19:51:18,546][flp2p.graph_runner][INFO] - Train, Round 291 : loss => 2.2955977457761763,  accuracy: 0.13064, gradient_norm : 0.1838736489701776
[2025-09-12 19:51:30,368][flp2p.graph_runner][INFO] - Test, Round 291 : loss => 2.2969765687942503,  accuracy: 0.1296
[2025-09-12 19:51:41,026][flp2p.graph_runner][INFO] - Train, Round 292 : loss => 2.295578135251999,  accuracy: 0.13076, gradient_norm : 0.18296368471216787
[2025-09-12 19:51:52,905][flp2p.graph_runner][INFO] - Test, Round 292 : loss => 2.2969472279787064,  accuracy: 0.1297
[2025-09-12 19:52:03,497][flp2p.graph_runner][INFO] - Train, Round 293 : loss => 2.295548083782196,  accuracy: 0.13078, gradient_norm : 0.19057982098520335
[2025-09-12 19:52:15,327][flp2p.graph_runner][INFO] - Test, Round 293 : loss => 2.296917470896244,  accuracy: 0.1299
[2025-09-12 19:52:26,063][flp2p.graph_runner][INFO] - Train, Round 294 : loss => 2.295488728880882,  accuracy: 0.13086, gradient_norm : 0.1815725354983088
[2025-09-12 19:52:37,907][flp2p.graph_runner][INFO] - Test, Round 294 : loss => 2.2968880658984183,  accuracy: 0.1298
[2025-09-12 19:52:48,518][flp2p.graph_runner][INFO] - Train, Round 295 : loss => 2.2954429852962495,  accuracy: 0.1309, gradient_norm : 0.18481981318985966
[2025-09-12 19:53:00,583][flp2p.graph_runner][INFO] - Test, Round 295 : loss => 2.2968588205695153,  accuracy: 0.1296
[2025-09-12 19:53:11,219][flp2p.graph_runner][INFO] - Train, Round 296 : loss => 2.2954267343878745,  accuracy: 0.13092, gradient_norm : 0.1875708682453141
[2025-09-12 19:53:23,140][flp2p.graph_runner][INFO] - Test, Round 296 : loss => 2.296829423069954,  accuracy: 0.1294
[2025-09-12 19:53:33,608][flp2p.graph_runner][INFO] - Train, Round 297 : loss => 2.295403914153576,  accuracy: 0.13092, gradient_norm : 0.18229035960613388
[2025-09-12 19:53:45,475][flp2p.graph_runner][INFO] - Test, Round 297 : loss => 2.2967997137308123,  accuracy: 0.1294
[2025-09-12 19:53:56,025][flp2p.graph_runner][INFO] - Train, Round 298 : loss => 2.2953553342819215,  accuracy: 0.131, gradient_norm : 0.18013485034887547
[2025-09-12 19:54:07,933][flp2p.graph_runner][INFO] - Test, Round 298 : loss => 2.296770390236378,  accuracy: 0.1293
[2025-09-12 19:54:18,613][flp2p.graph_runner][INFO] - Train, Round 299 : loss => 2.2953136685490607,  accuracy: 0.13118, gradient_norm : 0.18561517458979207
[2025-09-12 19:54:30,471][flp2p.graph_runner][INFO] - Test, Round 299 : loss => 2.296740968799591,  accuracy: 0.1292
[2025-09-12 19:54:40,984][flp2p.graph_runner][INFO] - Train, Round 300 : loss => 2.2952857479453086,  accuracy: 0.13112, gradient_norm : 0.1881659787301321
[2025-09-12 19:54:53,011][flp2p.graph_runner][INFO] - Test, Round 300 : loss => 2.2967114458322526,  accuracy: 0.1294
[2025-09-12 19:55:03,700][flp2p.graph_runner][INFO] - Train, Round 301 : loss => 2.295270974934101,  accuracy: 0.13124, gradient_norm : 0.17996386840766895
[2025-09-12 19:55:15,811][flp2p.graph_runner][INFO] - Test, Round 301 : loss => 2.2966819598555563,  accuracy: 0.1292
[2025-09-12 19:55:26,416][flp2p.graph_runner][INFO] - Train, Round 302 : loss => 2.2951905196905136,  accuracy: 0.1312, gradient_norm : 0.18744479290214267
[2025-09-12 19:55:38,381][flp2p.graph_runner][INFO] - Test, Round 302 : loss => 2.2966524933457375,  accuracy: 0.1294
[2025-09-12 19:55:48,882][flp2p.graph_runner][INFO] - Train, Round 303 : loss => 2.2951797157526017,  accuracy: 0.13122, gradient_norm : 0.17721579353984737
[2025-09-12 19:56:00,742][flp2p.graph_runner][INFO] - Test, Round 303 : loss => 2.2966228838324545,  accuracy: 0.1295
[2025-09-12 19:56:11,365][flp2p.graph_runner][INFO] - Train, Round 304 : loss => 2.2951581865549087,  accuracy: 0.13124, gradient_norm : 0.18802140924412206
[2025-09-12 19:56:23,327][flp2p.graph_runner][INFO] - Test, Round 304 : loss => 2.2965930934548378,  accuracy: 0.1296
[2025-09-12 19:56:34,078][flp2p.graph_runner][INFO] - Train, Round 305 : loss => 2.29510100543499,  accuracy: 0.13132, gradient_norm : 0.1878966317918729
[2025-09-12 19:56:46,007][flp2p.graph_runner][INFO] - Test, Round 305 : loss => 2.2965635241746902,  accuracy: 0.1298
[2025-09-12 19:56:56,507][flp2p.graph_runner][INFO] - Train, Round 306 : loss => 2.2950995987653733,  accuracy: 0.13146, gradient_norm : 0.18216958446234457
[2025-09-12 19:57:08,388][flp2p.graph_runner][INFO] - Test, Round 306 : loss => 2.2965335094690325,  accuracy: 0.1298
[2025-09-12 19:57:18,916][flp2p.graph_runner][INFO] - Train, Round 307 : loss => 2.2950507265329363,  accuracy: 0.13146, gradient_norm : 0.17928973405548426
[2025-09-12 19:57:30,850][flp2p.graph_runner][INFO] - Test, Round 307 : loss => 2.296503990995884,  accuracy: 0.1296
[2025-09-12 19:57:41,330][flp2p.graph_runner][INFO] - Train, Round 308 : loss => 2.2950363019108773,  accuracy: 0.13136, gradient_norm : 0.18898930080213586
[2025-09-12 19:57:53,177][flp2p.graph_runner][INFO] - Test, Round 308 : loss => 2.296473989534378,  accuracy: 0.1293
[2025-09-12 19:58:03,903][flp2p.graph_runner][INFO] - Train, Round 309 : loss => 2.2949837628006935,  accuracy: 0.1315, gradient_norm : 0.18576309929796483
[2025-09-12 19:58:15,813][flp2p.graph_runner][INFO] - Test, Round 309 : loss => 2.296444178175926,  accuracy: 0.1295
[2025-09-12 19:58:26,275][flp2p.graph_runner][INFO] - Train, Round 310 : loss => 2.2949237126111983,  accuracy: 0.13172, gradient_norm : 0.18316427038794564
[2025-09-12 19:58:38,144][flp2p.graph_runner][INFO] - Test, Round 310 : loss => 2.2964143371462824,  accuracy: 0.1294
[2025-09-12 19:58:48,709][flp2p.graph_runner][INFO] - Train, Round 311 : loss => 2.29491724550724,  accuracy: 0.13174, gradient_norm : 0.18283236108977075
[2025-09-12 19:59:00,647][flp2p.graph_runner][INFO] - Test, Round 311 : loss => 2.2963841639518736,  accuracy: 0.1295
[2025-09-12 19:59:11,286][flp2p.graph_runner][INFO] - Train, Round 312 : loss => 2.294869237840176,  accuracy: 0.13176, gradient_norm : 0.18383487260968467
[2025-09-12 19:59:23,105][flp2p.graph_runner][INFO] - Test, Round 312 : loss => 2.2963542304039,  accuracy: 0.1294
[2025-09-12 19:59:33,725][flp2p.graph_runner][INFO] - Train, Round 313 : loss => 2.2948508286476135,  accuracy: 0.13184, gradient_norm : 0.1880809704016551
[2025-09-12 19:59:45,608][flp2p.graph_runner][INFO] - Test, Round 313 : loss => 2.29632446321249,  accuracy: 0.1295
[2025-09-12 19:59:56,298][flp2p.graph_runner][INFO] - Train, Round 314 : loss => 2.2948027074337007,  accuracy: 0.132, gradient_norm : 0.18997355434453536
[2025-09-12 20:00:08,225][flp2p.graph_runner][INFO] - Test, Round 314 : loss => 2.296294345545769,  accuracy: 0.1294
[2025-09-12 20:00:18,934][flp2p.graph_runner][INFO] - Train, Round 315 : loss => 2.2948081895709036,  accuracy: 0.13192, gradient_norm : 0.1861041872097271
[2025-09-12 20:00:30,765][flp2p.graph_runner][INFO] - Test, Round 315 : loss => 2.2962641645908355,  accuracy: 0.1295
[2025-09-12 20:00:41,260][flp2p.graph_runner][INFO] - Train, Round 316 : loss => 2.2947634130716326,  accuracy: 0.1321, gradient_norm : 0.1866073255248302
[2025-09-12 20:00:53,077][flp2p.graph_runner][INFO] - Test, Round 316 : loss => 2.2962337376952173,  accuracy: 0.1296
[2025-09-12 20:01:03,978][flp2p.graph_runner][INFO] - Train, Round 317 : loss => 2.2947122782468794,  accuracy: 0.1321, gradient_norm : 0.1801854694076619
[2025-09-12 20:01:15,946][flp2p.graph_runner][INFO] - Test, Round 317 : loss => 2.2962036214351653,  accuracy: 0.1295
[2025-09-12 20:01:26,778][flp2p.graph_runner][INFO] - Train, Round 318 : loss => 2.2946854767203333,  accuracy: 0.13212, gradient_norm : 0.18785081270455609
[2025-09-12 20:01:38,656][flp2p.graph_runner][INFO] - Test, Round 318 : loss => 2.296173384869099,  accuracy: 0.1296
[2025-09-12 20:01:49,498][flp2p.graph_runner][INFO] - Train, Round 319 : loss => 2.29465412735939,  accuracy: 0.13212, gradient_norm : 0.18887452979516822
[2025-09-12 20:02:01,285][flp2p.graph_runner][INFO] - Test, Round 319 : loss => 2.2961432853341104,  accuracy: 0.1295
[2025-09-12 20:02:11,723][flp2p.graph_runner][INFO] - Train, Round 320 : loss => 2.294648285806179,  accuracy: 0.1321, gradient_norm : 0.18917749556883742
[2025-09-12 20:02:23,603][flp2p.graph_runner][INFO] - Test, Round 320 : loss => 2.2961128551125527,  accuracy: 0.1294
[2025-09-12 20:02:34,067][flp2p.graph_runner][INFO] - Train, Round 321 : loss => 2.294538113772869,  accuracy: 0.1321, gradient_norm : 0.18726674723613837
[2025-09-12 20:02:46,020][flp2p.graph_runner][INFO] - Test, Round 321 : loss => 2.29608266479969,  accuracy: 0.1296
[2025-09-12 20:02:56,444][flp2p.graph_runner][INFO] - Train, Round 322 : loss => 2.294530170559883,  accuracy: 0.13226, gradient_norm : 0.18995584253400385
[2025-09-12 20:03:08,330][flp2p.graph_runner][INFO] - Test, Round 322 : loss => 2.296052499639988,  accuracy: 0.1297
[2025-09-12 20:03:19,055][flp2p.graph_runner][INFO] - Train, Round 323 : loss => 2.294488630592823,  accuracy: 0.13242, gradient_norm : 0.18263687402693676
[2025-09-12 20:03:30,857][flp2p.graph_runner][INFO] - Test, Round 323 : loss => 2.2960223570227623,  accuracy: 0.1297
[2025-09-12 20:03:41,612][flp2p.graph_runner][INFO] - Train, Round 324 : loss => 2.294504274725914,  accuracy: 0.13248, gradient_norm : 0.17766682684251925
[2025-09-12 20:03:53,475][flp2p.graph_runner][INFO] - Test, Round 324 : loss => 2.295991885960102,  accuracy: 0.1297
[2025-09-12 20:04:04,120][flp2p.graph_runner][INFO] - Train, Round 325 : loss => 2.2944544091820718,  accuracy: 0.13248, gradient_norm : 0.1883429131537392
[2025-09-12 20:04:16,002][flp2p.graph_runner][INFO] - Test, Round 325 : loss => 2.2959614033222198,  accuracy: 0.1296
[2025-09-12 20:04:26,805][flp2p.graph_runner][INFO] - Train, Round 326 : loss => 2.2943945440649984,  accuracy: 0.1326, gradient_norm : 0.18249659933220788
[2025-09-12 20:04:38,690][flp2p.graph_runner][INFO] - Test, Round 326 : loss => 2.295931133532524,  accuracy: 0.1297
[2025-09-12 20:04:49,346][flp2p.graph_runner][INFO] - Train, Round 327 : loss => 2.2943825355172156,  accuracy: 0.13274, gradient_norm : 0.18534615402769863
[2025-09-12 20:05:01,183][flp2p.graph_runner][INFO] - Test, Round 327 : loss => 2.2959004490971564,  accuracy: 0.1298
[2025-09-12 20:05:11,785][flp2p.graph_runner][INFO] - Train, Round 328 : loss => 2.2943114542961123,  accuracy: 0.13274, gradient_norm : 0.18573205295677891
[2025-09-12 20:05:23,948][flp2p.graph_runner][INFO] - Test, Round 328 : loss => 2.2958697413086893,  accuracy: 0.1299
[2025-09-12 20:05:34,691][flp2p.graph_runner][INFO] - Train, Round 329 : loss => 2.2943270808458327,  accuracy: 0.13292, gradient_norm : 0.18444000069079677
[2025-09-12 20:05:46,661][flp2p.graph_runner][INFO] - Test, Round 329 : loss => 2.295839021897316,  accuracy: 0.13
[2025-09-12 20:05:57,364][flp2p.graph_runner][INFO] - Train, Round 330 : loss => 2.29428524941206,  accuracy: 0.13288, gradient_norm : 0.18364334254494316
[2025-09-12 20:06:09,347][flp2p.graph_runner][INFO] - Test, Round 330 : loss => 2.2958084607481957,  accuracy: 0.1299
[2025-09-12 20:06:20,136][flp2p.graph_runner][INFO] - Train, Round 331 : loss => 2.2942388886213303,  accuracy: 0.13294, gradient_norm : 0.18533243543525507
[2025-09-12 20:06:32,019][flp2p.graph_runner][INFO] - Test, Round 331 : loss => 2.2957776580452918,  accuracy: 0.1298
[2025-09-12 20:06:42,715][flp2p.graph_runner][INFO] - Train, Round 332 : loss => 2.294206812083721,  accuracy: 0.1329, gradient_norm : 0.18431630755890577
[2025-09-12 20:06:54,615][flp2p.graph_runner][INFO] - Test, Round 332 : loss => 2.2957469997525215,  accuracy: 0.1299
[2025-09-12 20:07:05,172][flp2p.graph_runner][INFO] - Train, Round 333 : loss => 2.294132898449898,  accuracy: 0.13294, gradient_norm : 0.18724248016162875
[2025-09-12 20:07:17,153][flp2p.graph_runner][INFO] - Test, Round 333 : loss => 2.2957166273236274,  accuracy: 0.1301
[2025-09-12 20:07:27,687][flp2p.graph_runner][INFO] - Train, Round 334 : loss => 2.294133151769638,  accuracy: 0.13294, gradient_norm : 0.1855956381833173
[2025-09-12 20:07:39,614][flp2p.graph_runner][INFO] - Test, Round 334 : loss => 2.295685529136658,  accuracy: 0.1304
[2025-09-12 20:07:50,368][flp2p.graph_runner][INFO] - Train, Round 335 : loss => 2.294085413515568,  accuracy: 0.13312, gradient_norm : 0.19302751049613395
[2025-09-12 20:08:02,192][flp2p.graph_runner][INFO] - Test, Round 335 : loss => 2.295654817175865,  accuracy: 0.1305
[2025-09-12 20:08:12,791][flp2p.graph_runner][INFO] - Train, Round 336 : loss => 2.2940682196617126,  accuracy: 0.13304, gradient_norm : 0.18465761899779984
[2025-09-12 20:08:24,636][flp2p.graph_runner][INFO] - Test, Round 336 : loss => 2.295624149429798,  accuracy: 0.1303
[2025-09-12 20:08:35,289][flp2p.graph_runner][INFO] - Train, Round 337 : loss => 2.2939867287874223,  accuracy: 0.13312, gradient_norm : 0.19217302578361828
[2025-09-12 20:08:47,106][flp2p.graph_runner][INFO] - Test, Round 337 : loss => 2.295593075811863,  accuracy: 0.1305
[2025-09-12 20:08:57,704][flp2p.graph_runner][INFO] - Train, Round 338 : loss => 2.2939728453755377,  accuracy: 0.133, gradient_norm : 0.18738657895035604
[2025-09-12 20:09:09,645][flp2p.graph_runner][INFO] - Test, Round 338 : loss => 2.2955621307253837,  accuracy: 0.1307
[2025-09-12 20:09:22,607][flp2p.graph_runner][INFO] - Train, Round 339 : loss => 2.2939418438076973,  accuracy: 0.1331, gradient_norm : 0.18715189134931007
[2025-09-12 20:09:34,749][flp2p.graph_runner][INFO] - Test, Round 339 : loss => 2.2955311284422875,  accuracy: 0.131
[2025-09-12 20:09:45,620][flp2p.graph_runner][INFO] - Train, Round 340 : loss => 2.2939501041173935,  accuracy: 0.13318, gradient_norm : 0.18971753724736712
[2025-09-12 20:09:57,648][flp2p.graph_runner][INFO] - Test, Round 340 : loss => 2.2954999270915986,  accuracy: 0.1309
[2025-09-12 20:10:08,497][flp2p.graph_runner][INFO] - Train, Round 341 : loss => 2.2938615012168886,  accuracy: 0.13318, gradient_norm : 0.18454294005386296
[2025-09-12 20:10:20,655][flp2p.graph_runner][INFO] - Test, Round 341 : loss => 2.295468728148937,  accuracy: 0.131
[2025-09-12 20:10:31,352][flp2p.graph_runner][INFO] - Train, Round 342 : loss => 2.293797660470009,  accuracy: 0.1333, gradient_norm : 0.19002377853243946
[2025-09-12 20:10:43,568][flp2p.graph_runner][INFO] - Test, Round 342 : loss => 2.2954376386284827,  accuracy: 0.1309
[2025-09-12 20:10:54,543][flp2p.graph_runner][INFO] - Train, Round 343 : loss => 2.2937583214044572,  accuracy: 0.13322, gradient_norm : 0.18830530406509227
[2025-09-12 20:11:06,675][flp2p.graph_runner][INFO] - Test, Round 343 : loss => 2.2954063803315163,  accuracy: 0.1307
[2025-09-12 20:11:17,522][flp2p.graph_runner][INFO] - Train, Round 344 : loss => 2.293727063536644,  accuracy: 0.13336, gradient_norm : 0.19252022062249516
[2025-09-12 20:11:29,626][flp2p.graph_runner][INFO] - Test, Round 344 : loss => 2.2953750827074053,  accuracy: 0.1306
[2025-09-12 20:11:40,597][flp2p.graph_runner][INFO] - Train, Round 345 : loss => 2.293731372952461,  accuracy: 0.13338, gradient_norm : 0.18417652649424493
[2025-09-12 20:11:52,658][flp2p.graph_runner][INFO] - Test, Round 345 : loss => 2.2953437618732453,  accuracy: 0.1304
[2025-09-12 20:12:03,618][flp2p.graph_runner][INFO] - Train, Round 346 : loss => 2.293674714267254,  accuracy: 0.13342, gradient_norm : 0.1903898639157002
[2025-09-12 20:12:15,543][flp2p.graph_runner][INFO] - Test, Round 346 : loss => 2.2953123149633408,  accuracy: 0.1304
[2025-09-12 20:12:26,380][flp2p.graph_runner][INFO] - Train, Round 347 : loss => 2.2936723440885545,  accuracy: 0.13352, gradient_norm : 0.1820105361981996
[2025-09-12 20:12:38,401][flp2p.graph_runner][INFO] - Test, Round 347 : loss => 2.2952813476920126,  accuracy: 0.1304
[2025-09-12 20:12:49,206][flp2p.graph_runner][INFO] - Train, Round 348 : loss => 2.293625433444977,  accuracy: 0.13362, gradient_norm : 0.19113058771964295
[2025-09-12 20:13:01,259][flp2p.graph_runner][INFO] - Test, Round 348 : loss => 2.2952497102499008,  accuracy: 0.1305
[2025-09-12 20:13:12,306][flp2p.graph_runner][INFO] - Train, Round 349 : loss => 2.293598857522011,  accuracy: 0.1337, gradient_norm : 0.19256373166143864
[2025-09-12 20:13:24,434][flp2p.graph_runner][INFO] - Test, Round 349 : loss => 2.2952179299712183,  accuracy: 0.1305
[2025-09-12 20:13:35,594][flp2p.graph_runner][INFO] - Train, Round 350 : loss => 2.2935558646917342,  accuracy: 0.13376, gradient_norm : 0.19060156681949914
[2025-09-12 20:13:47,775][flp2p.graph_runner][INFO] - Test, Round 350 : loss => 2.295186197066307,  accuracy: 0.1308
[2025-09-12 20:13:58,625][flp2p.graph_runner][INFO] - Train, Round 351 : loss => 2.293516429960728,  accuracy: 0.13374, gradient_norm : 0.18652440116331284
[2025-09-12 20:14:10,593][flp2p.graph_runner][INFO] - Test, Round 351 : loss => 2.295154386103153,  accuracy: 0.1307
[2025-09-12 20:14:21,470][flp2p.graph_runner][INFO] - Train, Round 352 : loss => 2.293471013903618,  accuracy: 0.13376, gradient_norm : 0.18931811763789372
[2025-09-12 20:14:33,450][flp2p.graph_runner][INFO] - Test, Round 352 : loss => 2.2951227439403534,  accuracy: 0.1307
[2025-09-12 20:14:44,400][flp2p.graph_runner][INFO] - Train, Round 353 : loss => 2.2934460803866386,  accuracy: 0.13378, gradient_norm : 0.18822693488103376
[2025-09-12 20:14:57,828][flp2p.graph_runner][INFO] - Test, Round 353 : loss => 2.2950908672213552,  accuracy: 0.1309
[2025-09-12 20:15:08,704][flp2p.graph_runner][INFO] - Train, Round 354 : loss => 2.2934363865852356,  accuracy: 0.13378, gradient_norm : 0.18577884420615637
[2025-09-12 20:15:23,582][flp2p.graph_runner][INFO] - Test, Round 354 : loss => 2.2950587398409845,  accuracy: 0.131
[2025-09-12 20:15:34,367][flp2p.graph_runner][INFO] - Train, Round 355 : loss => 2.2933598038554193,  accuracy: 0.13382, gradient_norm : 0.19150017336010838
[2025-09-12 20:15:51,201][flp2p.graph_runner][INFO] - Test, Round 355 : loss => 2.2950268000364304,  accuracy: 0.1312
[2025-09-12 20:16:02,084][flp2p.graph_runner][INFO] - Train, Round 356 : loss => 2.293286385834217,  accuracy: 0.1338, gradient_norm : 0.18790496655221442
[2025-09-12 20:16:20,578][flp2p.graph_runner][INFO] - Test, Round 356 : loss => 2.2949949248194694,  accuracy: 0.1312
[2025-09-12 20:16:31,166][flp2p.graph_runner][INFO] - Train, Round 357 : loss => 2.2932475036382676,  accuracy: 0.13388, gradient_norm : 0.19274881664680577
[2025-09-12 20:16:49,940][flp2p.graph_runner][INFO] - Test, Round 357 : loss => 2.2949631440997122,  accuracy: 0.1311
[2025-09-12 20:17:00,522][flp2p.graph_runner][INFO] - Train, Round 358 : loss => 2.2932426065206526,  accuracy: 0.13396, gradient_norm : 0.1974529844246131
[2025-09-12 20:17:19,253][flp2p.graph_runner][INFO] - Test, Round 358 : loss => 2.2949308057308198,  accuracy: 0.1311
[2025-09-12 20:17:30,136][flp2p.graph_runner][INFO] - Train, Round 359 : loss => 2.2932273301482202,  accuracy: 0.13388, gradient_norm : 0.18377520999143354
[2025-09-12 20:17:47,941][flp2p.graph_runner][INFO] - Test, Round 359 : loss => 2.2948985420823096,  accuracy: 0.131
[2025-09-12 20:17:58,624][flp2p.graph_runner][INFO] - Train, Round 360 : loss => 2.293166487812996,  accuracy: 0.13396, gradient_norm : 0.1862320597208627
[2025-09-12 20:18:15,692][flp2p.graph_runner][INFO] - Test, Round 360 : loss => 2.294866438472271,  accuracy: 0.1313
[2025-09-12 20:18:26,348][flp2p.graph_runner][INFO] - Train, Round 361 : loss => 2.2931308570504187,  accuracy: 0.1341, gradient_norm : 0.1871612742082175
[2025-09-12 20:18:42,443][flp2p.graph_runner][INFO] - Test, Round 361 : loss => 2.2948340848207476,  accuracy: 0.1313
[2025-09-12 20:18:52,974][flp2p.graph_runner][INFO] - Train, Round 362 : loss => 2.293114106953144,  accuracy: 0.13422, gradient_norm : 0.19565584745441406
[2025-09-12 20:19:09,343][flp2p.graph_runner][INFO] - Test, Round 362 : loss => 2.2948015629291536,  accuracy: 0.1311
[2025-09-12 20:19:19,967][flp2p.graph_runner][INFO] - Train, Round 363 : loss => 2.293085697889328,  accuracy: 0.13424, gradient_norm : 0.19155312093721882
[2025-09-12 20:19:35,781][flp2p.graph_runner][INFO] - Test, Round 363 : loss => 2.2947689940094946,  accuracy: 0.131
[2025-09-12 20:19:46,442][flp2p.graph_runner][INFO] - Train, Round 364 : loss => 2.2929984840750692,  accuracy: 0.13436, gradient_norm : 0.1906953904786245
[2025-09-12 20:20:02,197][flp2p.graph_runner][INFO] - Test, Round 364 : loss => 2.2947363654613495,  accuracy: 0.1308
[2025-09-12 20:20:12,750][flp2p.graph_runner][INFO] - Train, Round 365 : loss => 2.2930225175619126,  accuracy: 0.13462, gradient_norm : 0.1905128794851187
[2025-09-12 20:20:28,096][flp2p.graph_runner][INFO] - Test, Round 365 : loss => 2.2947038994550706,  accuracy: 0.1309
[2025-09-12 20:20:38,836][flp2p.graph_runner][INFO] - Train, Round 366 : loss => 2.292922262251377,  accuracy: 0.1347, gradient_norm : 0.18982929972779858
[2025-09-12 20:20:53,011][flp2p.graph_runner][INFO] - Test, Round 366 : loss => 2.2946711122393606,  accuracy: 0.1311
[2025-09-12 20:21:03,667][flp2p.graph_runner][INFO] - Train, Round 367 : loss => 2.2929252022504807,  accuracy: 0.13482, gradient_norm : 0.18967109514668917
[2025-09-12 20:21:17,247][flp2p.graph_runner][INFO] - Test, Round 367 : loss => 2.2946383471488954,  accuracy: 0.1309
[2025-09-12 20:21:27,825][flp2p.graph_runner][INFO] - Train, Round 368 : loss => 2.29284328520298,  accuracy: 0.13494, gradient_norm : 0.19186856841669048
[2025-09-12 20:21:41,660][flp2p.graph_runner][INFO] - Test, Round 368 : loss => 2.294605641222,  accuracy: 0.1307
[2025-09-12 20:21:52,353][flp2p.graph_runner][INFO] - Train, Round 369 : loss => 2.292825118303299,  accuracy: 0.13484, gradient_norm : 0.1899427686458629
[2025-09-12 20:22:05,286][flp2p.graph_runner][INFO] - Test, Round 369 : loss => 2.2945727555036544,  accuracy: 0.1305
[2025-09-12 20:22:15,875][flp2p.graph_runner][INFO] - Train, Round 370 : loss => 2.29281185567379,  accuracy: 0.13506, gradient_norm : 0.18817380702738729
[2025-09-12 20:22:28,609][flp2p.graph_runner][INFO] - Test, Round 370 : loss => 2.2945395483613016,  accuracy: 0.1306
[2025-09-12 20:22:39,208][flp2p.graph_runner][INFO] - Train, Round 371 : loss => 2.2927569895982742,  accuracy: 0.13506, gradient_norm : 0.19751630573691223
[2025-09-12 20:22:52,356][flp2p.graph_runner][INFO] - Test, Round 371 : loss => 2.2945066524386406,  accuracy: 0.1303
[2025-09-12 20:23:03,030][flp2p.graph_runner][INFO] - Train, Round 372 : loss => 2.292730438411236,  accuracy: 0.1352, gradient_norm : 0.19033233613248499
[2025-09-12 20:23:14,959][flp2p.graph_runner][INFO] - Test, Round 372 : loss => 2.2944735791921618,  accuracy: 0.1304
[2025-09-12 20:23:25,667][flp2p.graph_runner][INFO] - Train, Round 373 : loss => 2.2926936274766923,  accuracy: 0.13518, gradient_norm : 0.19602366384104994
[2025-09-12 20:23:37,853][flp2p.graph_runner][INFO] - Test, Round 373 : loss => 2.2944400921463965,  accuracy: 0.1303
[2025-09-12 20:23:48,506][flp2p.graph_runner][INFO] - Train, Round 374 : loss => 2.2926512745022776,  accuracy: 0.13522, gradient_norm : 0.1905187873930597
[2025-09-12 20:24:00,860][flp2p.graph_runner][INFO] - Test, Round 374 : loss => 2.294406927013397,  accuracy: 0.1305
[2025-09-12 20:24:11,307][flp2p.graph_runner][INFO] - Train, Round 375 : loss => 2.292588173151016,  accuracy: 0.13536, gradient_norm : 0.19171341176154894
[2025-09-12 20:24:24,137][flp2p.graph_runner][INFO] - Test, Round 375 : loss => 2.2943733944773674,  accuracy: 0.1304
[2025-09-12 20:24:34,959][flp2p.graph_runner][INFO] - Train, Round 376 : loss => 2.2925751128792764,  accuracy: 0.1354, gradient_norm : 0.194084016145426
[2025-09-12 20:24:47,057][flp2p.graph_runner][INFO] - Test, Round 376 : loss => 2.294339797472954,  accuracy: 0.1305
[2025-09-12 20:24:57,602][flp2p.graph_runner][INFO] - Train, Round 377 : loss => 2.2925019496679306,  accuracy: 0.13544, gradient_norm : 0.1989356904510485
[2025-09-12 20:25:09,444][flp2p.graph_runner][INFO] - Test, Round 377 : loss => 2.294306407868862,  accuracy: 0.1305
[2025-09-12 20:25:20,111][flp2p.graph_runner][INFO] - Train, Round 378 : loss => 2.2925008690357207,  accuracy: 0.13548, gradient_norm : 0.1946361403283051
[2025-09-12 20:25:31,945][flp2p.graph_runner][INFO] - Test, Round 378 : loss => 2.294272673165798,  accuracy: 0.1307
[2025-09-12 20:25:42,589][flp2p.graph_runner][INFO] - Train, Round 379 : loss => 2.292467383146286,  accuracy: 0.13552, gradient_norm : 0.1938531324450562
[2025-09-12 20:25:54,507][flp2p.graph_runner][INFO] - Test, Round 379 : loss => 2.2942390047311783,  accuracy: 0.1308
[2025-09-12 20:26:04,952][flp2p.graph_runner][INFO] - Train, Round 380 : loss => 2.29239724367857,  accuracy: 0.13546, gradient_norm : 0.19316717426338484
[2025-09-12 20:26:16,738][flp2p.graph_runner][INFO] - Test, Round 380 : loss => 2.294205060839653,  accuracy: 0.1311
[2025-09-12 20:26:27,444][flp2p.graph_runner][INFO] - Train, Round 381 : loss => 2.292380976378918,  accuracy: 0.13542, gradient_norm : 0.1922225328744955
[2025-09-12 20:26:39,220][flp2p.graph_runner][INFO] - Test, Round 381 : loss => 2.294171022498608,  accuracy: 0.1309
[2025-09-12 20:26:50,016][flp2p.graph_runner][INFO] - Train, Round 382 : loss => 2.2923452946543694,  accuracy: 0.1355, gradient_norm : 0.19791304345447214
[2025-09-12 20:27:02,063][flp2p.graph_runner][INFO] - Test, Round 382 : loss => 2.2941370068192484,  accuracy: 0.1309
[2025-09-12 20:27:12,699][flp2p.graph_runner][INFO] - Train, Round 383 : loss => 2.2922695234417914,  accuracy: 0.13562, gradient_norm : 0.1935103221016746
[2025-09-12 20:27:24,511][flp2p.graph_runner][INFO] - Test, Round 383 : loss => 2.294103095793724,  accuracy: 0.1308
[2025-09-12 20:27:35,010][flp2p.graph_runner][INFO] - Train, Round 384 : loss => 2.292285629808903,  accuracy: 0.13576, gradient_norm : 0.19170542419487915
[2025-09-12 20:27:46,844][flp2p.graph_runner][INFO] - Test, Round 384 : loss => 2.2940689752817156,  accuracy: 0.131
[2025-09-12 20:27:57,490][flp2p.graph_runner][INFO] - Train, Round 385 : loss => 2.292178917825222,  accuracy: 0.13586, gradient_norm : 0.19222715648335742
[2025-09-12 20:28:09,356][flp2p.graph_runner][INFO] - Test, Round 385 : loss => 2.294034790492058,  accuracy: 0.1312
[2025-09-12 20:28:19,909][flp2p.graph_runner][INFO] - Train, Round 386 : loss => 2.2922026497125625,  accuracy: 0.13572, gradient_norm : 0.19064629093855356
[2025-09-12 20:28:31,704][flp2p.graph_runner][INFO] - Test, Round 386 : loss => 2.29400049636364,  accuracy: 0.1315
[2025-09-12 20:28:42,325][flp2p.graph_runner][INFO] - Train, Round 387 : loss => 2.292126368582249,  accuracy: 0.13596, gradient_norm : 0.1937676944945679
[2025-09-12 20:28:54,395][flp2p.graph_runner][INFO] - Test, Round 387 : loss => 2.293966437792778,  accuracy: 0.1316
[2025-09-12 20:29:05,025][flp2p.graph_runner][INFO] - Train, Round 388 : loss => 2.292088701426983,  accuracy: 0.13594, gradient_norm : 0.1946939037623008
[2025-09-12 20:29:16,968][flp2p.graph_runner][INFO] - Test, Round 388 : loss => 2.2939320405483246,  accuracy: 0.1319
[2025-09-12 20:29:27,676][flp2p.graph_runner][INFO] - Train, Round 389 : loss => 2.2920438307523727,  accuracy: 0.13614, gradient_norm : 0.19971165906470048
[2025-09-12 20:29:39,528][flp2p.graph_runner][INFO] - Test, Round 389 : loss => 2.293897634327412,  accuracy: 0.1318
[2025-09-12 20:29:49,986][flp2p.graph_runner][INFO] - Train, Round 390 : loss => 2.292010186910629,  accuracy: 0.13624, gradient_norm : 0.19366103386909672
[2025-09-12 20:30:01,774][flp2p.graph_runner][INFO] - Test, Round 390 : loss => 2.2938631959438323,  accuracy: 0.1316
[2025-09-12 20:30:12,411][flp2p.graph_runner][INFO] - Train, Round 391 : loss => 2.291995757818222,  accuracy: 0.13628, gradient_norm : 0.19610482127306966
[2025-09-12 20:30:24,276][flp2p.graph_runner][INFO] - Test, Round 391 : loss => 2.293828449320793,  accuracy: 0.1318
[2025-09-12 20:30:34,969][flp2p.graph_runner][INFO] - Train, Round 392 : loss => 2.291930448114872,  accuracy: 0.13642, gradient_norm : 0.19534146983690767
[2025-09-12 20:30:46,802][flp2p.graph_runner][INFO] - Test, Round 392 : loss => 2.293793500363827,  accuracy: 0.132
[2025-09-12 20:30:57,182][flp2p.graph_runner][INFO] - Train, Round 393 : loss => 2.291863780319691,  accuracy: 0.13644, gradient_norm : 0.19523370488258254
[2025-09-12 20:31:09,196][flp2p.graph_runner][INFO] - Test, Round 393 : loss => 2.293758460831642,  accuracy: 0.1321
[2025-09-12 20:31:19,890][flp2p.graph_runner][INFO] - Train, Round 394 : loss => 2.291836649775505,  accuracy: 0.13632, gradient_norm : 0.19587532405312655
[2025-09-12 20:31:31,769][flp2p.graph_runner][INFO] - Test, Round 394 : loss => 2.2937236863732338,  accuracy: 0.1322
[2025-09-12 20:31:42,375][flp2p.graph_runner][INFO] - Train, Round 395 : loss => 2.291841430962086,  accuracy: 0.13634, gradient_norm : 0.19194586713233724
[2025-09-12 20:31:54,268][flp2p.graph_runner][INFO] - Test, Round 395 : loss => 2.2936885582327844,  accuracy: 0.1323
[2025-09-12 20:32:04,874][flp2p.graph_runner][INFO] - Train, Round 396 : loss => 2.2917254441976547,  accuracy: 0.13642, gradient_norm : 0.1933106173290743
[2025-09-12 20:32:16,826][flp2p.graph_runner][INFO] - Test, Round 396 : loss => 2.2936537715315817,  accuracy: 0.1324
[2025-09-12 20:32:27,492][flp2p.graph_runner][INFO] - Train, Round 397 : loss => 2.291744183599949,  accuracy: 0.13642, gradient_norm : 0.19580762079272224
[2025-09-12 20:32:39,427][flp2p.graph_runner][INFO] - Test, Round 397 : loss => 2.2936183258771896,  accuracy: 0.1327
[2025-09-12 20:32:49,929][flp2p.graph_runner][INFO] - Train, Round 398 : loss => 2.2916736501455306,  accuracy: 0.1365, gradient_norm : 0.1995642769465635
[2025-09-12 20:33:01,968][flp2p.graph_runner][INFO] - Test, Round 398 : loss => 2.2935829833626746,  accuracy: 0.1327
[2025-09-12 20:33:12,520][flp2p.graph_runner][INFO] - Train, Round 399 : loss => 2.291672666370869,  accuracy: 0.13662, gradient_norm : 0.19294335432933
[2025-09-12 20:33:24,395][flp2p.graph_runner][INFO] - Test, Round 399 : loss => 2.2935474984884263,  accuracy: 0.1331
[2025-09-12 20:33:35,066][flp2p.graph_runner][INFO] - Train, Round 400 : loss => 2.2916058406233786,  accuracy: 0.13662, gradient_norm : 0.19716255662866636
[2025-09-12 20:33:46,921][flp2p.graph_runner][INFO] - Test, Round 400 : loss => 2.2935123057603835,  accuracy: 0.1331
[2025-09-12 20:33:57,609][flp2p.graph_runner][INFO] - Train, Round 401 : loss => 2.2915564557909964,  accuracy: 0.13688, gradient_norm : 0.19555150453178718
[2025-09-12 20:34:09,549][flp2p.graph_runner][INFO] - Test, Round 401 : loss => 2.29347693901062,  accuracy: 0.1333
[2025-09-12 20:34:20,225][flp2p.graph_runner][INFO] - Train, Round 402 : loss => 2.2915425965189935,  accuracy: 0.13696, gradient_norm : 0.1939782072215365
[2025-09-12 20:34:32,131][flp2p.graph_runner][INFO] - Test, Round 402 : loss => 2.293441440820694,  accuracy: 0.1334
[2025-09-12 20:34:42,840][flp2p.graph_runner][INFO] - Train, Round 403 : loss => 2.291468380391598,  accuracy: 0.13698, gradient_norm : 0.19964396512638902
[2025-09-12 20:34:54,886][flp2p.graph_runner][INFO] - Test, Round 403 : loss => 2.2934056255817414,  accuracy: 0.1335
[2025-09-12 20:35:05,459][flp2p.graph_runner][INFO] - Train, Round 404 : loss => 2.291458287537098,  accuracy: 0.13708, gradient_norm : 0.19812261241244303
[2025-09-12 20:35:17,338][flp2p.graph_runner][INFO] - Test, Round 404 : loss => 2.293369738268852,  accuracy: 0.1338
[2025-09-12 20:35:27,942][flp2p.graph_runner][INFO] - Train, Round 405 : loss => 2.291387553215027,  accuracy: 0.13714, gradient_norm : 0.19833138012336365
[2025-09-12 20:35:39,810][flp2p.graph_runner][INFO] - Test, Round 405 : loss => 2.293333876919746,  accuracy: 0.1339
[2025-09-12 20:35:50,444][flp2p.graph_runner][INFO] - Train, Round 406 : loss => 2.2913251760602,  accuracy: 0.13704, gradient_norm : 0.19631877985461985
[2025-09-12 20:36:02,228][flp2p.graph_runner][INFO] - Test, Round 406 : loss => 2.2932981831908226,  accuracy: 0.1341
[2025-09-12 20:36:13,126][flp2p.graph_runner][INFO] - Train, Round 407 : loss => 2.291298749446869,  accuracy: 0.1372, gradient_norm : 0.19902928139192086
[2025-09-12 20:36:24,951][flp2p.graph_runner][INFO] - Test, Round 407 : loss => 2.293262013399601,  accuracy: 0.1341
[2025-09-12 20:36:35,589][flp2p.graph_runner][INFO] - Train, Round 408 : loss => 2.2912666967511175,  accuracy: 0.1374, gradient_norm : 0.19485387412727673
[2025-09-12 20:36:47,585][flp2p.graph_runner][INFO] - Test, Round 408 : loss => 2.293225675010681,  accuracy: 0.1342
[2025-09-12 20:36:58,301][flp2p.graph_runner][INFO] - Train, Round 409 : loss => 2.291237245798111,  accuracy: 0.13744, gradient_norm : 0.19844113419002427
[2025-09-12 20:37:10,357][flp2p.graph_runner][INFO] - Test, Round 409 : loss => 2.2931892460346224,  accuracy: 0.1345
[2025-09-12 20:37:21,009][flp2p.graph_runner][INFO] - Train, Round 410 : loss => 2.2911882108449935,  accuracy: 0.13758, gradient_norm : 0.19963059610933193
[2025-09-12 20:37:32,859][flp2p.graph_runner][INFO] - Test, Round 410 : loss => 2.2931528802394867,  accuracy: 0.1344
[2025-09-12 20:37:43,621][flp2p.graph_runner][INFO] - Train, Round 411 : loss => 2.2911356124281883,  accuracy: 0.13772, gradient_norm : 0.19271910750895951
[2025-09-12 20:37:55,584][flp2p.graph_runner][INFO] - Test, Round 411 : loss => 2.293116364693642,  accuracy: 0.1344
[2025-09-12 20:38:06,221][flp2p.graph_runner][INFO] - Train, Round 412 : loss => 2.2910840681195257,  accuracy: 0.13792, gradient_norm : 0.19958404349148764
[2025-09-12 20:38:18,171][flp2p.graph_runner][INFO] - Test, Round 412 : loss => 2.293079720520973,  accuracy: 0.1342
[2025-09-12 20:38:29,042][flp2p.graph_runner][INFO] - Train, Round 413 : loss => 2.291041423678398,  accuracy: 0.13788, gradient_norm : 0.19456846954780357
[2025-09-12 20:38:40,887][flp2p.graph_runner][INFO] - Test, Round 413 : loss => 2.293043182015419,  accuracy: 0.1343
[2025-09-12 20:38:51,547][flp2p.graph_runner][INFO] - Train, Round 414 : loss => 2.291021827161312,  accuracy: 0.13794, gradient_norm : 0.1961240258920724
[2025-09-12 20:39:03,587][flp2p.graph_runner][INFO] - Test, Round 414 : loss => 2.2930061920881273,  accuracy: 0.1343
[2025-09-12 20:39:14,371][flp2p.graph_runner][INFO] - Train, Round 415 : loss => 2.290951899886131,  accuracy: 0.13812, gradient_norm : 0.20125313189890837
[2025-09-12 20:39:26,384][flp2p.graph_runner][INFO] - Test, Round 415 : loss => 2.292969115626812,  accuracy: 0.1343
[2025-09-12 20:39:36,819][flp2p.graph_runner][INFO] - Train, Round 416 : loss => 2.2909308421611785,  accuracy: 0.13836, gradient_norm : 0.2023541904525378
[2025-09-12 20:39:48,672][flp2p.graph_runner][INFO] - Test, Round 416 : loss => 2.292931995522976,  accuracy: 0.1344
[2025-09-12 20:39:59,246][flp2p.graph_runner][INFO] - Train, Round 417 : loss => 2.290884476900101,  accuracy: 0.13854, gradient_norm : 0.19664015477807378
[2025-09-12 20:40:11,222][flp2p.graph_runner][INFO] - Test, Round 417 : loss => 2.292895029044151,  accuracy: 0.1344
[2025-09-12 20:40:21,885][flp2p.graph_runner][INFO] - Train, Round 418 : loss => 2.2908122810721396,  accuracy: 0.13858, gradient_norm : 0.19984523518516314
[2025-09-12 20:40:33,813][flp2p.graph_runner][INFO] - Test, Round 418 : loss => 2.2928580415487287,  accuracy: 0.1343
[2025-09-12 20:40:44,510][flp2p.graph_runner][INFO] - Train, Round 419 : loss => 2.2908069199323653,  accuracy: 0.1386, gradient_norm : 0.20051045382479288
[2025-09-12 20:40:56,432][flp2p.graph_runner][INFO] - Test, Round 419 : loss => 2.2928206307291985,  accuracy: 0.1343
[2025-09-12 20:41:06,947][flp2p.graph_runner][INFO] - Train, Round 420 : loss => 2.290749015510082,  accuracy: 0.13864, gradient_norm : 0.1965295061420546
[2025-09-12 20:41:19,061][flp2p.graph_runner][INFO] - Test, Round 420 : loss => 2.2927829489350318,  accuracy: 0.1344
[2025-09-12 20:41:29,938][flp2p.graph_runner][INFO] - Train, Round 421 : loss => 2.2907326382398607,  accuracy: 0.13874, gradient_norm : 0.1943151723091886
[2025-09-12 20:41:41,790][flp2p.graph_runner][INFO] - Test, Round 421 : loss => 2.292745323598385,  accuracy: 0.1344
[2025-09-12 20:41:52,581][flp2p.graph_runner][INFO] - Train, Round 422 : loss => 2.2906376791000365,  accuracy: 0.13898, gradient_norm : 0.19773512431482587
[2025-09-12 20:42:04,550][flp2p.graph_runner][INFO] - Test, Round 422 : loss => 2.292707767367363,  accuracy: 0.1344
[2025-09-12 20:42:15,064][flp2p.graph_runner][INFO] - Train, Round 423 : loss => 2.290674799978733,  accuracy: 0.1389, gradient_norm : 0.20612286307568528
[2025-09-12 20:42:26,952][flp2p.graph_runner][INFO] - Test, Round 423 : loss => 2.2926698654532434,  accuracy: 0.1343
[2025-09-12 20:42:37,619][flp2p.graph_runner][INFO] - Train, Round 424 : loss => 2.2905681681632997,  accuracy: 0.1392, gradient_norm : 0.19461387458336296
[2025-09-12 20:42:49,515][flp2p.graph_runner][INFO] - Test, Round 424 : loss => 2.2926319340348242,  accuracy: 0.1346
[2025-09-12 20:43:00,027][flp2p.graph_runner][INFO] - Train, Round 425 : loss => 2.2905769792199133,  accuracy: 0.13926, gradient_norm : 0.19548402580860483
[2025-09-12 20:43:12,150][flp2p.graph_runner][INFO] - Test, Round 425 : loss => 2.2925940391778945,  accuracy: 0.1349
[2025-09-12 20:43:22,632][flp2p.graph_runner][INFO] - Train, Round 426 : loss => 2.2904649236798287,  accuracy: 0.13946, gradient_norm : 0.19723588923724422
[2025-09-12 20:43:34,779][flp2p.graph_runner][INFO] - Test, Round 426 : loss => 2.292556227040291,  accuracy: 0.135
[2025-09-12 20:43:45,443][flp2p.graph_runner][INFO] - Train, Round 427 : loss => 2.290446134209633,  accuracy: 0.13958, gradient_norm : 0.19476281254773917
[2025-09-12 20:43:57,345][flp2p.graph_runner][INFO] - Test, Round 427 : loss => 2.2925178835868834,  accuracy: 0.1355
[2025-09-12 20:44:08,141][flp2p.graph_runner][INFO] - Train, Round 428 : loss => 2.2903839805722237,  accuracy: 0.13972, gradient_norm : 0.1990630424905222
[2025-09-12 20:44:20,083][flp2p.graph_runner][INFO] - Test, Round 428 : loss => 2.2924797544240954,  accuracy: 0.1352
[2025-09-12 20:44:30,516][flp2p.graph_runner][INFO] - Train, Round 429 : loss => 2.290343686044216,  accuracy: 0.13964, gradient_norm : 0.2016134876167587
[2025-09-12 20:44:42,407][flp2p.graph_runner][INFO] - Test, Round 429 : loss => 2.2924411834836005,  accuracy: 0.1354
[2025-09-12 20:44:53,219][flp2p.graph_runner][INFO] - Train, Round 430 : loss => 2.2903088104724882,  accuracy: 0.13972, gradient_norm : 0.20218085169753877
[2025-09-12 20:45:05,080][flp2p.graph_runner][INFO] - Test, Round 430 : loss => 2.2924025847554206,  accuracy: 0.1355
[2025-09-12 20:45:15,840][flp2p.graph_runner][INFO] - Train, Round 431 : loss => 2.290212242305279,  accuracy: 0.13982, gradient_norm : 0.20486983582847948
[2025-09-12 20:45:27,876][flp2p.graph_runner][INFO] - Test, Round 431 : loss => 2.2923640388846396,  accuracy: 0.1355
[2025-09-12 20:45:38,578][flp2p.graph_runner][INFO] - Train, Round 432 : loss => 2.290221420228481,  accuracy: 0.13994, gradient_norm : 0.20347776506221166
[2025-09-12 20:45:50,503][flp2p.graph_runner][INFO] - Test, Round 432 : loss => 2.2923253419876097,  accuracy: 0.1354
[2025-09-12 20:46:01,188][flp2p.graph_runner][INFO] - Train, Round 433 : loss => 2.290142882168293,  accuracy: 0.14, gradient_norm : 0.19835087383315234
[2025-09-12 20:46:13,074][flp2p.graph_runner][INFO] - Test, Round 433 : loss => 2.2922865504860876,  accuracy: 0.1355
[2025-09-12 20:46:23,627][flp2p.graph_runner][INFO] - Train, Round 434 : loss => 2.2901069310307505,  accuracy: 0.14008, gradient_norm : 0.20486840026833533
[2025-09-12 20:46:35,492][flp2p.graph_runner][INFO] - Test, Round 434 : loss => 2.2922477930665015,  accuracy: 0.1356
[2025-09-12 20:46:46,144][flp2p.graph_runner][INFO] - Train, Round 435 : loss => 2.2900957083702087,  accuracy: 0.14012, gradient_norm : 0.20176753691736804
[2025-09-12 20:46:58,070][flp2p.graph_runner][INFO] - Test, Round 435 : loss => 2.292208604550362,  accuracy: 0.1358
[2025-09-12 20:47:08,664][flp2p.graph_runner][INFO] - Train, Round 436 : loss => 2.2900334748625757,  accuracy: 0.14004, gradient_norm : 0.20444520160934768
[2025-09-12 20:47:20,564][flp2p.graph_runner][INFO] - Test, Round 436 : loss => 2.2921696690559386,  accuracy: 0.1358
[2025-09-12 20:47:31,148][flp2p.graph_runner][INFO] - Train, Round 437 : loss => 2.2900012451410294,  accuracy: 0.14032, gradient_norm : 0.19778132723464784
[2025-09-12 20:47:43,076][flp2p.graph_runner][INFO] - Test, Round 437 : loss => 2.2921300458312035,  accuracy: 0.1355
[2025-09-12 20:47:53,857][flp2p.graph_runner][INFO] - Train, Round 438 : loss => 2.289922536313534,  accuracy: 0.14014, gradient_norm : 0.2023113105278074
[2025-09-12 20:48:05,807][flp2p.graph_runner][INFO] - Test, Round 438 : loss => 2.292090559279919,  accuracy: 0.1354
[2025-09-12 20:48:16,486][flp2p.graph_runner][INFO] - Train, Round 439 : loss => 2.2898645129799844,  accuracy: 0.14032, gradient_norm : 0.20152544771940328
[2025-09-12 20:48:28,393][flp2p.graph_runner][INFO] - Test, Round 439 : loss => 2.292050926744938,  accuracy: 0.1356
[2025-09-12 20:48:39,086][flp2p.graph_runner][INFO] - Train, Round 440 : loss => 2.2898675230145455,  accuracy: 0.14044, gradient_norm : 0.20240301310612424
[2025-09-12 20:48:51,107][flp2p.graph_runner][INFO] - Test, Round 440 : loss => 2.2920116423249244,  accuracy: 0.1356
[2025-09-12 20:49:01,634][flp2p.graph_runner][INFO] - Train, Round 441 : loss => 2.2898000818490982,  accuracy: 0.14048, gradient_norm : 0.2043803510262446
[2025-09-12 20:49:13,522][flp2p.graph_runner][INFO] - Test, Round 441 : loss => 2.2919717701792717,  accuracy: 0.1355
[2025-09-12 20:49:24,228][flp2p.graph_runner][INFO] - Train, Round 442 : loss => 2.289729122519493,  accuracy: 0.14034, gradient_norm : 0.2039436219207591
[2025-09-12 20:49:36,324][flp2p.graph_runner][INFO] - Test, Round 442 : loss => 2.2919319922447206,  accuracy: 0.1353
[2025-09-12 20:49:47,050][flp2p.graph_runner][INFO] - Train, Round 443 : loss => 2.2896889546513557,  accuracy: 0.14056, gradient_norm : 0.20676835112078504
[2025-09-12 20:49:58,915][flp2p.graph_runner][INFO] - Test, Round 443 : loss => 2.291891986298561,  accuracy: 0.1354
[2025-09-12 20:50:09,851][flp2p.graph_runner][INFO] - Train, Round 444 : loss => 2.2896367225050924,  accuracy: 0.14036, gradient_norm : 0.20897434014880165
[2025-09-12 20:50:21,793][flp2p.graph_runner][INFO] - Test, Round 444 : loss => 2.2918516258358954,  accuracy: 0.1353
[2025-09-12 20:50:32,431][flp2p.graph_runner][INFO] - Train, Round 445 : loss => 2.289586316347122,  accuracy: 0.14048, gradient_norm : 0.20322345790500418
[2025-09-12 20:50:44,344][flp2p.graph_runner][INFO] - Test, Round 445 : loss => 2.2918110478520393,  accuracy: 0.1351
[2025-09-12 20:50:54,994][flp2p.graph_runner][INFO] - Train, Round 446 : loss => 2.289572652876377,  accuracy: 0.14066, gradient_norm : 0.20457291049664394
[2025-09-12 20:51:06,933][flp2p.graph_runner][INFO] - Test, Round 446 : loss => 2.291770699083805,  accuracy: 0.1351
[2025-09-12 20:51:17,536][flp2p.graph_runner][INFO] - Train, Round 447 : loss => 2.289500294625759,  accuracy: 0.14084, gradient_norm : 0.2014341727926568
[2025-09-12 20:51:29,683][flp2p.graph_runner][INFO] - Test, Round 447 : loss => 2.2917301450252534,  accuracy: 0.1351
[2025-09-12 20:51:40,412][flp2p.graph_runner][INFO] - Train, Round 448 : loss => 2.2894728419184687,  accuracy: 0.14094, gradient_norm : 0.20970945515693615
[2025-09-12 20:51:52,328][flp2p.graph_runner][INFO] - Test, Round 448 : loss => 2.2916894639015197,  accuracy: 0.1351
[2025-09-12 20:52:02,846][flp2p.graph_runner][INFO] - Train, Round 449 : loss => 2.289441547989845,  accuracy: 0.14108, gradient_norm : 0.2062154720043993
[2025-09-12 20:52:14,863][flp2p.graph_runner][INFO] - Test, Round 449 : loss => 2.291648805856705,  accuracy: 0.135
[2025-09-12 20:52:25,644][flp2p.graph_runner][INFO] - Train, Round 450 : loss => 2.2894014352560044,  accuracy: 0.14108, gradient_norm : 0.20745206491085555
[2025-09-12 20:52:37,825][flp2p.graph_runner][INFO] - Test, Round 450 : loss => 2.2916081002354622,  accuracy: 0.1351
[2025-09-12 20:52:48,390][flp2p.graph_runner][INFO] - Train, Round 451 : loss => 2.2892837658524514,  accuracy: 0.1412, gradient_norm : 0.2051306068845927
[2025-09-12 20:53:00,387][flp2p.graph_runner][INFO] - Test, Round 451 : loss => 2.291566895246506,  accuracy: 0.1356
[2025-09-12 20:53:11,075][flp2p.graph_runner][INFO] - Train, Round 452 : loss => 2.289240840673447,  accuracy: 0.14138, gradient_norm : 0.2015679209680626
[2025-09-12 20:53:23,293][flp2p.graph_runner][INFO] - Test, Round 452 : loss => 2.2915261316895483,  accuracy: 0.1356
[2025-09-12 20:53:34,128][flp2p.graph_runner][INFO] - Train, Round 453 : loss => 2.289183255136013,  accuracy: 0.14144, gradient_norm : 0.2054641867710095
[2025-09-12 20:53:46,220][flp2p.graph_runner][INFO] - Test, Round 453 : loss => 2.291485241663456,  accuracy: 0.1358
[2025-09-12 20:53:57,023][flp2p.graph_runner][INFO] - Train, Round 454 : loss => 2.2891295072436333,  accuracy: 0.14162, gradient_norm : 0.20802365982909884
[2025-09-12 20:54:09,196][flp2p.graph_runner][INFO] - Test, Round 454 : loss => 2.2914436152458193,  accuracy: 0.1359
[2025-09-12 20:54:19,721][flp2p.graph_runner][INFO] - Train, Round 455 : loss => 2.2890939840674402,  accuracy: 0.14172, gradient_norm : 0.2104093706394843
[2025-09-12 20:54:31,814][flp2p.graph_runner][INFO] - Test, Round 455 : loss => 2.2914021904468536,  accuracy: 0.1358
[2025-09-12 20:54:42,450][flp2p.graph_runner][INFO] - Train, Round 456 : loss => 2.2890693658590315,  accuracy: 0.14168, gradient_norm : 0.21351802059460254
[2025-09-12 20:54:54,620][flp2p.graph_runner][INFO] - Test, Round 456 : loss => 2.29136012570858,  accuracy: 0.1357
[2025-09-12 20:55:05,317][flp2p.graph_runner][INFO] - Train, Round 457 : loss => 2.2889622223377226,  accuracy: 0.14192, gradient_norm : 0.20446982472848174
[2025-09-12 20:55:17,457][flp2p.graph_runner][INFO] - Test, Round 457 : loss => 2.2913183301091196,  accuracy: 0.1359
[2025-09-12 20:55:28,366][flp2p.graph_runner][INFO] - Train, Round 458 : loss => 2.288990659713745,  accuracy: 0.14188, gradient_norm : 0.21155963277820547
[2025-09-12 20:55:40,580][flp2p.graph_runner][INFO] - Test, Round 458 : loss => 2.2912761931419374,  accuracy: 0.1358
[2025-09-12 20:55:51,759][flp2p.graph_runner][INFO] - Train, Round 459 : loss => 2.288884553909302,  accuracy: 0.14194, gradient_norm : 0.2138144479644912
[2025-09-12 20:56:03,665][flp2p.graph_runner][INFO] - Test, Round 459 : loss => 2.291233947122097,  accuracy: 0.1358
[2025-09-12 20:56:14,512][flp2p.graph_runner][INFO] - Train, Round 460 : loss => 2.288901709616184,  accuracy: 0.14218, gradient_norm : 0.2116401624722933
[2025-09-12 20:56:26,447][flp2p.graph_runner][INFO] - Test, Round 460 : loss => 2.291191732776165,  accuracy: 0.1358
[2025-09-12 20:56:37,445][flp2p.graph_runner][INFO] - Train, Round 461 : loss => 2.2887931326031685,  accuracy: 0.14222, gradient_norm : 0.2124589048200838
[2025-09-12 20:56:49,600][flp2p.graph_runner][INFO] - Test, Round 461 : loss => 2.29114905796051,  accuracy: 0.1362
[2025-09-12 20:57:00,540][flp2p.graph_runner][INFO] - Train, Round 462 : loss => 2.2887340649962424,  accuracy: 0.14238, gradient_norm : 0.20616212246272247
[2025-09-12 20:57:12,423][flp2p.graph_runner][INFO] - Test, Round 462 : loss => 2.291106085884571,  accuracy: 0.1363
[2025-09-12 20:57:23,282][flp2p.graph_runner][INFO] - Train, Round 463 : loss => 2.2886987379193307,  accuracy: 0.14238, gradient_norm : 0.2084860655549328
[2025-09-12 20:57:36,049][flp2p.graph_runner][INFO] - Test, Round 463 : loss => 2.2910631129741668,  accuracy: 0.1366
[2025-09-12 20:57:46,815][flp2p.graph_runner][INFO] - Train, Round 464 : loss => 2.2886798077821733,  accuracy: 0.1427, gradient_norm : 0.19975173144500175
[2025-09-12 20:58:01,472][flp2p.graph_runner][INFO] - Test, Round 464 : loss => 2.291020385468006,  accuracy: 0.1366
[2025-09-12 20:58:12,252][flp2p.graph_runner][INFO] - Train, Round 465 : loss => 2.2886318603157996,  accuracy: 0.14256, gradient_norm : 0.20700620143373327
[2025-09-12 20:58:28,865][flp2p.graph_runner][INFO] - Test, Round 465 : loss => 2.2909772609472276,  accuracy: 0.1366
[2025-09-12 20:58:39,576][flp2p.graph_runner][INFO] - Train, Round 466 : loss => 2.288564332425594,  accuracy: 0.14258, gradient_norm : 0.2146411214837317
[2025-09-12 20:58:56,940][flp2p.graph_runner][INFO] - Test, Round 466 : loss => 2.290934351015091,  accuracy: 0.1367
[2025-09-12 20:59:07,725][flp2p.graph_runner][INFO] - Train, Round 467 : loss => 2.2885653895139693,  accuracy: 0.14272, gradient_norm : 0.20899932777669952
[2025-09-12 20:59:26,144][flp2p.graph_runner][INFO] - Test, Round 467 : loss => 2.290891069030762,  accuracy: 0.1368
[2025-09-12 20:59:36,762][flp2p.graph_runner][INFO] - Train, Round 468 : loss => 2.2884467881917954,  accuracy: 0.14274, gradient_norm : 0.21016601007951963
[2025-09-12 20:59:55,718][flp2p.graph_runner][INFO] - Test, Round 468 : loss => 2.290847189748287,  accuracy: 0.1365
[2025-09-12 21:00:06,425][flp2p.graph_runner][INFO] - Train, Round 469 : loss => 2.2884266683459282,  accuracy: 0.1428, gradient_norm : 0.20799254987068214
[2025-09-12 21:00:24,979][flp2p.graph_runner][INFO] - Test, Round 469 : loss => 2.290804004120827,  accuracy: 0.1363
[2025-09-12 21:00:35,846][flp2p.graph_runner][INFO] - Train, Round 470 : loss => 2.2883413419127465,  accuracy: 0.14292, gradient_norm : 0.2109618293021598
[2025-09-12 21:00:53,749][flp2p.graph_runner][INFO] - Test, Round 470 : loss => 2.290760371863842,  accuracy: 0.1361
[2025-09-12 21:01:04,439][flp2p.graph_runner][INFO] - Train, Round 471 : loss => 2.288258970975876,  accuracy: 0.14304, gradient_norm : 0.20992535206737611
[2025-09-12 21:01:21,735][flp2p.graph_runner][INFO] - Test, Round 471 : loss => 2.290716319155693,  accuracy: 0.1364
[2025-09-12 21:01:32,441][flp2p.graph_runner][INFO] - Train, Round 472 : loss => 2.288260702192783,  accuracy: 0.1429, gradient_norm : 0.21143192245196396
[2025-09-12 21:01:49,332][flp2p.graph_runner][INFO] - Test, Round 472 : loss => 2.290672085916996,  accuracy: 0.1364
[2025-09-12 21:01:59,973][flp2p.graph_runner][INFO] - Train, Round 473 : loss => 2.28823178678751,  accuracy: 0.1433, gradient_norm : 0.20844692663037073
[2025-09-12 21:02:16,250][flp2p.graph_runner][INFO] - Test, Round 473 : loss => 2.290628182888031,  accuracy: 0.1369
[2025-09-12 21:02:26,695][flp2p.graph_runner][INFO] - Train, Round 474 : loss => 2.288186962604523,  accuracy: 0.14322, gradient_norm : 0.2124220285674355
[2025-09-12 21:02:42,525][flp2p.graph_runner][INFO] - Test, Round 474 : loss => 2.2905839125275613,  accuracy: 0.1372
[2025-09-12 21:02:53,136][flp2p.graph_runner][INFO] - Train, Round 475 : loss => 2.288043967485428,  accuracy: 0.14338, gradient_norm : 0.2162143001718858
[2025-09-12 21:03:08,710][flp2p.graph_runner][INFO] - Test, Round 475 : loss => 2.29053900449276,  accuracy: 0.1373
[2025-09-12 21:03:19,505][flp2p.graph_runner][INFO] - Train, Round 476 : loss => 2.288061483800411,  accuracy: 0.14356, gradient_norm : 0.20909944998060934
[2025-09-12 21:03:34,829][flp2p.graph_runner][INFO] - Test, Round 476 : loss => 2.2904941583633422,  accuracy: 0.1374
[2025-09-12 21:03:45,565][flp2p.graph_runner][INFO] - Train, Round 477 : loss => 2.288015167117119,  accuracy: 0.1438, gradient_norm : 0.2148010398228745
[2025-09-12 21:04:00,345][flp2p.graph_runner][INFO] - Test, Round 477 : loss => 2.290449222075939,  accuracy: 0.1376
[2025-09-12 21:04:10,967][flp2p.graph_runner][INFO] - Train, Round 478 : loss => 2.28794995367527,  accuracy: 0.14382, gradient_norm : 0.21242084778629874
[2025-09-12 21:04:25,212][flp2p.graph_runner][INFO] - Test, Round 478 : loss => 2.2904039556264877,  accuracy: 0.1376
[2025-09-12 21:04:35,869][flp2p.graph_runner][INFO] - Train, Round 479 : loss => 2.2878910425305365,  accuracy: 0.14386, gradient_norm : 0.21266582114263627
[2025-09-12 21:04:49,622][flp2p.graph_runner][INFO] - Test, Round 479 : loss => 2.2903588416576386,  accuracy: 0.1375
[2025-09-12 21:05:00,282][flp2p.graph_runner][INFO] - Train, Round 480 : loss => 2.2878100737929343,  accuracy: 0.14412, gradient_norm : 0.21497198540952783
[2025-09-12 21:05:13,513][flp2p.graph_runner][INFO] - Test, Round 480 : loss => 2.290313173520565,  accuracy: 0.1377
[2025-09-12 21:05:24,101][flp2p.graph_runner][INFO] - Train, Round 481 : loss => 2.2877702233195305,  accuracy: 0.1441, gradient_norm : 0.21545047763676553
[2025-09-12 21:05:37,784][flp2p.graph_runner][INFO] - Test, Round 481 : loss => 2.2902676139354705,  accuracy: 0.1374
[2025-09-12 21:05:48,561][flp2p.graph_runner][INFO] - Train, Round 482 : loss => 2.287697510123253,  accuracy: 0.14424, gradient_norm : 0.2172316451588856
[2025-09-12 21:06:01,890][flp2p.graph_runner][INFO] - Test, Round 482 : loss => 2.290222157609463,  accuracy: 0.1374
[2025-09-12 21:06:12,543][flp2p.graph_runner][INFO] - Train, Round 483 : loss => 2.2876819610595702,  accuracy: 0.14444, gradient_norm : 0.21037825336094426
[2025-09-12 21:06:25,402][flp2p.graph_runner][INFO] - Test, Round 483 : loss => 2.290176341509819,  accuracy: 0.1379
[2025-09-12 21:06:36,125][flp2p.graph_runner][INFO] - Train, Round 484 : loss => 2.287624257802963,  accuracy: 0.14438, gradient_norm : 0.21270535789806685
[2025-09-12 21:06:49,411][flp2p.graph_runner][INFO] - Test, Round 484 : loss => 2.2901303074002266,  accuracy: 0.1378
[2025-09-12 21:06:59,919][flp2p.graph_runner][INFO] - Train, Round 485 : loss => 2.287573598623276,  accuracy: 0.14464, gradient_norm : 0.2033475649938536
[2025-09-12 21:07:13,027][flp2p.graph_runner][INFO] - Test, Round 485 : loss => 2.2900841154098512,  accuracy: 0.1377
[2025-09-12 21:07:23,670][flp2p.graph_runner][INFO] - Train, Round 486 : loss => 2.287498973608017,  accuracy: 0.14482, gradient_norm : 0.2160181170320022
[2025-09-12 21:07:36,456][flp2p.graph_runner][INFO] - Test, Round 486 : loss => 2.290037664294243,  accuracy: 0.1377
[2025-09-12 21:07:47,157][flp2p.graph_runner][INFO] - Train, Round 487 : loss => 2.28746208012104,  accuracy: 0.14484, gradient_norm : 0.2145240054037574
[2025-09-12 21:07:59,484][flp2p.graph_runner][INFO] - Test, Round 487 : loss => 2.289990965771675,  accuracy: 0.1377
[2025-09-12 21:08:10,138][flp2p.graph_runner][INFO] - Train, Round 488 : loss => 2.287409909963608,  accuracy: 0.14496, gradient_norm : 0.21427107373029014
[2025-09-12 21:08:22,226][flp2p.graph_runner][INFO] - Test, Round 488 : loss => 2.2899442857027053,  accuracy: 0.1377
[2025-09-12 21:08:32,808][flp2p.graph_runner][INFO] - Train, Round 489 : loss => 2.2873361694812773,  accuracy: 0.14484, gradient_norm : 0.20840555130875763
[2025-09-12 21:08:44,827][flp2p.graph_runner][INFO] - Test, Round 489 : loss => 2.2898975476026533,  accuracy: 0.1378
[2025-09-12 21:08:55,610][flp2p.graph_runner][INFO] - Train, Round 490 : loss => 2.287285148203373,  accuracy: 0.14482, gradient_norm : 0.21069558450782658
[2025-09-12 21:09:07,588][flp2p.graph_runner][INFO] - Test, Round 490 : loss => 2.2898506144523623,  accuracy: 0.1381
[2025-09-12 21:09:18,245][flp2p.graph_runner][INFO] - Train, Round 491 : loss => 2.287201581001282,  accuracy: 0.14494, gradient_norm : 0.21528938548196538
[2025-09-12 21:09:30,172][flp2p.graph_runner][INFO] - Test, Round 491 : loss => 2.289803448998928,  accuracy: 0.138
[2025-09-12 21:09:40,543][flp2p.graph_runner][INFO] - Train, Round 492 : loss => 2.287204843759537,  accuracy: 0.145, gradient_norm : 0.21242709778609914
[2025-09-12 21:09:52,432][flp2p.graph_runner][INFO] - Test, Round 492 : loss => 2.289755875170231,  accuracy: 0.1379
[2025-09-12 21:10:02,919][flp2p.graph_runner][INFO] - Train, Round 493 : loss => 2.2871010410785675,  accuracy: 0.14496, gradient_norm : 0.2123606143248751
[2025-09-12 21:10:14,790][flp2p.graph_runner][INFO] - Test, Round 493 : loss => 2.2897083665132523,  accuracy: 0.1381
[2025-09-12 21:10:25,322][flp2p.graph_runner][INFO] - Train, Round 494 : loss => 2.287087902724743,  accuracy: 0.14516, gradient_norm : 0.2124249563655511
[2025-09-12 21:10:37,184][flp2p.graph_runner][INFO] - Test, Round 494 : loss => 2.2896606529712678,  accuracy: 0.138
[2025-09-12 21:10:47,873][flp2p.graph_runner][INFO] - Train, Round 495 : loss => 2.2870329171419144,  accuracy: 0.14544, gradient_norm : 0.21281591477601616
[2025-09-12 21:10:59,714][flp2p.graph_runner][INFO] - Test, Round 495 : loss => 2.2896128692150115,  accuracy: 0.138
[2025-09-12 21:11:10,385][flp2p.graph_runner][INFO] - Train, Round 496 : loss => 2.2869239062070847,  accuracy: 0.1453, gradient_norm : 0.2199247883173415
[2025-09-12 21:11:22,537][flp2p.graph_runner][INFO] - Test, Round 496 : loss => 2.289564725267887,  accuracy: 0.1383
[2025-09-12 21:11:33,070][flp2p.graph_runner][INFO] - Train, Round 497 : loss => 2.2868619573116304,  accuracy: 0.14544, gradient_norm : 0.21367080049568127
[2025-09-12 21:11:45,003][flp2p.graph_runner][INFO] - Test, Round 497 : loss => 2.2895162510156633,  accuracy: 0.1381
[2025-09-12 21:11:55,904][flp2p.graph_runner][INFO] - Train, Round 498 : loss => 2.286856017708778,  accuracy: 0.1456, gradient_norm : 0.2162524651004583
[2025-09-12 21:12:07,749][flp2p.graph_runner][INFO] - Test, Round 498 : loss => 2.2894679663300512,  accuracy: 0.1381
[2025-09-12 21:12:18,336][flp2p.graph_runner][INFO] - Train, Round 499 : loss => 2.286758017838001,  accuracy: 0.14556, gradient_norm : 0.22179156476550094
[2025-09-12 21:12:30,300][flp2p.graph_runner][INFO] - Test, Round 499 : loss => 2.2894192851543425,  accuracy: 0.1385
[2025-09-12 21:12:30,304][__main__][INFO] - Train, Round 001: loss=2.3053, accuracy=0.1026, gradient_norm=0.1798, 
[2025-09-12 21:12:30,305][__main__][INFO] - Train, Round 002: loss=2.3053, accuracy=0.1025, gradient_norm=0.1745, 
[2025-09-12 21:12:30,305][__main__][INFO] - Train, Round 003: loss=2.3052, accuracy=0.1026, gradient_norm=0.1735, 
[2025-09-12 21:12:30,305][__main__][INFO] - Train, Round 004: loss=2.3052, accuracy=0.1027, gradient_norm=0.1740, 
[2025-09-12 21:12:30,305][__main__][INFO] - Train, Round 005: loss=2.3052, accuracy=0.1029, gradient_norm=0.1815, 
[2025-09-12 21:12:30,305][__main__][INFO] - Train, Round 006: loss=2.3051, accuracy=0.1031, gradient_norm=0.1760, 
[2025-09-12 21:12:30,305][__main__][INFO] - Train, Round 007: loss=2.3051, accuracy=0.1035, gradient_norm=0.1752, 
[2025-09-12 21:12:30,305][__main__][INFO] - Train, Round 008: loss=2.3051, accuracy=0.1035, gradient_norm=0.1808, 
[2025-09-12 21:12:30,305][__main__][INFO] - Train, Round 009: loss=2.3050, accuracy=0.1036, gradient_norm=0.1718, 
[2025-09-12 21:12:30,305][__main__][INFO] - Train, Round 010: loss=2.3050, accuracy=0.1039, gradient_norm=0.1781, 
[2025-09-12 21:12:30,305][__main__][INFO] - Train, Round 011: loss=2.3049, accuracy=0.1042, gradient_norm=0.1752, 
[2025-09-12 21:12:30,305][__main__][INFO] - Train, Round 012: loss=2.3049, accuracy=0.1042, gradient_norm=0.1763, 
[2025-09-12 21:12:30,305][__main__][INFO] - Train, Round 013: loss=2.3049, accuracy=0.1045, gradient_norm=0.1775, 
[2025-09-12 21:12:30,305][__main__][INFO] - Train, Round 014: loss=2.3048, accuracy=0.1045, gradient_norm=0.1723, 
[2025-09-12 21:12:30,305][__main__][INFO] - Train, Round 015: loss=2.3048, accuracy=0.1047, gradient_norm=0.1729, 
[2025-09-12 21:12:30,305][__main__][INFO] - Train, Round 016: loss=2.3048, accuracy=0.1048, gradient_norm=0.1791, 
[2025-09-12 21:12:30,305][__main__][INFO] - Train, Round 017: loss=2.3047, accuracy=0.1052, gradient_norm=0.1749, 
[2025-09-12 21:12:30,305][__main__][INFO] - Train, Round 018: loss=2.3047, accuracy=0.1054, gradient_norm=0.1763, 
[2025-09-12 21:12:30,305][__main__][INFO] - Train, Round 019: loss=2.3046, accuracy=0.1058, gradient_norm=0.1758, 
[2025-09-12 21:12:30,305][__main__][INFO] - Train, Round 020: loss=2.3046, accuracy=0.1059, gradient_norm=0.1760, 
[2025-09-12 21:12:30,305][__main__][INFO] - Train, Round 021: loss=2.3045, accuracy=0.1059, gradient_norm=0.1717, 
[2025-09-12 21:12:30,305][__main__][INFO] - Train, Round 022: loss=2.3045, accuracy=0.1062, gradient_norm=0.1771, 
[2025-09-12 21:12:30,305][__main__][INFO] - Train, Round 023: loss=2.3044, accuracy=0.1067, gradient_norm=0.1665, 
[2025-09-12 21:12:30,305][__main__][INFO] - Train, Round 024: loss=2.3044, accuracy=0.1066, gradient_norm=0.1734, 
[2025-09-12 21:12:30,305][__main__][INFO] - Train, Round 025: loss=2.3044, accuracy=0.1070, gradient_norm=0.1738, 
[2025-09-12 21:12:30,305][__main__][INFO] - Train, Round 026: loss=2.3044, accuracy=0.1071, gradient_norm=0.1801, 
[2025-09-12 21:12:30,305][__main__][INFO] - Train, Round 027: loss=2.3043, accuracy=0.1076, gradient_norm=0.1755, 
[2025-09-12 21:12:30,305][__main__][INFO] - Train, Round 028: loss=2.3043, accuracy=0.1075, gradient_norm=0.1769, 
[2025-09-12 21:12:30,305][__main__][INFO] - Train, Round 029: loss=2.3043, accuracy=0.1077, gradient_norm=0.1780, 
[2025-09-12 21:12:30,305][__main__][INFO] - Train, Round 030: loss=2.3042, accuracy=0.1078, gradient_norm=0.1768, 
[2025-09-12 21:12:30,305][__main__][INFO] - Train, Round 031: loss=2.3042, accuracy=0.1078, gradient_norm=0.1736, 
[2025-09-12 21:12:30,305][__main__][INFO] - Train, Round 032: loss=2.3041, accuracy=0.1081, gradient_norm=0.1725, 
[2025-09-12 21:12:30,305][__main__][INFO] - Train, Round 033: loss=2.3040, accuracy=0.1081, gradient_norm=0.1737, 
[2025-09-12 21:12:30,305][__main__][INFO] - Train, Round 034: loss=2.3040, accuracy=0.1083, gradient_norm=0.1794, 
[2025-09-12 21:12:30,305][__main__][INFO] - Train, Round 035: loss=2.3040, accuracy=0.1083, gradient_norm=0.1745, 
[2025-09-12 21:12:30,305][__main__][INFO] - Train, Round 036: loss=2.3040, accuracy=0.1085, gradient_norm=0.1727, 
[2025-09-12 21:12:30,305][__main__][INFO] - Train, Round 037: loss=2.3039, accuracy=0.1085, gradient_norm=0.1735, 
[2025-09-12 21:12:30,305][__main__][INFO] - Train, Round 038: loss=2.3039, accuracy=0.1087, gradient_norm=0.1797, 
[2025-09-12 21:12:30,305][__main__][INFO] - Train, Round 039: loss=2.3039, accuracy=0.1089, gradient_norm=0.1720, 
[2025-09-12 21:12:30,305][__main__][INFO] - Train, Round 040: loss=2.3039, accuracy=0.1092, gradient_norm=0.1785, 
[2025-09-12 21:12:30,305][__main__][INFO] - Train, Round 041: loss=2.3038, accuracy=0.1091, gradient_norm=0.1802, 
[2025-09-12 21:12:30,305][__main__][INFO] - Train, Round 042: loss=2.3038, accuracy=0.1093, gradient_norm=0.1730, 
[2025-09-12 21:12:30,305][__main__][INFO] - Train, Round 043: loss=2.3037, accuracy=0.1094, gradient_norm=0.1774, 
[2025-09-12 21:12:30,305][__main__][INFO] - Train, Round 044: loss=2.3037, accuracy=0.1094, gradient_norm=0.1801, 
[2025-09-12 21:12:30,305][__main__][INFO] - Train, Round 045: loss=2.3037, accuracy=0.1096, gradient_norm=0.1770, 
[2025-09-12 21:12:30,306][__main__][INFO] - Train, Round 046: loss=2.3037, accuracy=0.1096, gradient_norm=0.1752, 
[2025-09-12 21:12:30,306][__main__][INFO] - Train, Round 047: loss=2.3036, accuracy=0.1099, gradient_norm=0.1792, 
[2025-09-12 21:12:30,306][__main__][INFO] - Train, Round 048: loss=2.3036, accuracy=0.1101, gradient_norm=0.1753, 
[2025-09-12 21:12:30,306][__main__][INFO] - Train, Round 049: loss=2.3035, accuracy=0.1104, gradient_norm=0.1698, 
[2025-09-12 21:12:30,306][__main__][INFO] - Train, Round 050: loss=2.3035, accuracy=0.1102, gradient_norm=0.1730, 
[2025-09-12 21:12:30,306][__main__][INFO] - Train, Round 051: loss=2.3034, accuracy=0.1104, gradient_norm=0.1653, 
[2025-09-12 21:12:30,306][__main__][INFO] - Train, Round 052: loss=2.3034, accuracy=0.1105, gradient_norm=0.1810, 
[2025-09-12 21:12:30,306][__main__][INFO] - Train, Round 053: loss=2.3034, accuracy=0.1110, gradient_norm=0.1715, 
[2025-09-12 21:12:30,306][__main__][INFO] - Train, Round 054: loss=2.3034, accuracy=0.1111, gradient_norm=0.1722, 
[2025-09-12 21:12:30,306][__main__][INFO] - Train, Round 055: loss=2.3033, accuracy=0.1114, gradient_norm=0.1678, 
[2025-09-12 21:12:30,306][__main__][INFO] - Train, Round 056: loss=2.3033, accuracy=0.1117, gradient_norm=0.1729, 
[2025-09-12 21:12:30,306][__main__][INFO] - Train, Round 057: loss=2.3032, accuracy=0.1117, gradient_norm=0.1757, 
[2025-09-12 21:12:30,306][__main__][INFO] - Train, Round 058: loss=2.3032, accuracy=0.1121, gradient_norm=0.1724, 
[2025-09-12 21:12:30,306][__main__][INFO] - Train, Round 059: loss=2.3032, accuracy=0.1123, gradient_norm=0.1800, 
[2025-09-12 21:12:30,306][__main__][INFO] - Train, Round 060: loss=2.3031, accuracy=0.1125, gradient_norm=0.1737, 
[2025-09-12 21:12:30,306][__main__][INFO] - Train, Round 061: loss=2.3031, accuracy=0.1126, gradient_norm=0.1781, 
[2025-09-12 21:12:30,306][__main__][INFO] - Train, Round 062: loss=2.3030, accuracy=0.1127, gradient_norm=0.1693, 
[2025-09-12 21:12:30,306][__main__][INFO] - Train, Round 063: loss=2.3030, accuracy=0.1125, gradient_norm=0.1747, 
[2025-09-12 21:12:30,306][__main__][INFO] - Train, Round 064: loss=2.3031, accuracy=0.1128, gradient_norm=0.1781, 
[2025-09-12 21:12:30,306][__main__][INFO] - Train, Round 065: loss=2.3030, accuracy=0.1133, gradient_norm=0.1760, 
[2025-09-12 21:12:30,306][__main__][INFO] - Train, Round 066: loss=2.3029, accuracy=0.1136, gradient_norm=0.1710, 
[2025-09-12 21:12:30,306][__main__][INFO] - Train, Round 067: loss=2.3029, accuracy=0.1135, gradient_norm=0.1755, 
[2025-09-12 21:12:30,306][__main__][INFO] - Train, Round 068: loss=2.3029, accuracy=0.1138, gradient_norm=0.1764, 
[2025-09-12 21:12:30,306][__main__][INFO] - Train, Round 069: loss=2.3029, accuracy=0.1137, gradient_norm=0.1752, 
[2025-09-12 21:12:30,306][__main__][INFO] - Train, Round 070: loss=2.3028, accuracy=0.1140, gradient_norm=0.1760, 
[2025-09-12 21:12:30,306][__main__][INFO] - Train, Round 071: loss=2.3028, accuracy=0.1142, gradient_norm=0.1698, 
[2025-09-12 21:12:30,306][__main__][INFO] - Train, Round 072: loss=2.3027, accuracy=0.1146, gradient_norm=0.1737, 
[2025-09-12 21:12:30,306][__main__][INFO] - Train, Round 073: loss=2.3027, accuracy=0.1148, gradient_norm=0.1688, 
[2025-09-12 21:12:30,306][__main__][INFO] - Train, Round 074: loss=2.3026, accuracy=0.1146, gradient_norm=0.1718, 
[2025-09-12 21:12:30,306][__main__][INFO] - Train, Round 075: loss=2.3026, accuracy=0.1147, gradient_norm=0.1777, 
[2025-09-12 21:12:30,306][__main__][INFO] - Train, Round 076: loss=2.3026, accuracy=0.1145, gradient_norm=0.1752, 
[2025-09-12 21:12:30,306][__main__][INFO] - Train, Round 077: loss=2.3026, accuracy=0.1148, gradient_norm=0.1800, 
[2025-09-12 21:12:30,306][__main__][INFO] - Train, Round 078: loss=2.3025, accuracy=0.1149, gradient_norm=0.1802, 
[2025-09-12 21:12:30,306][__main__][INFO] - Train, Round 079: loss=2.3025, accuracy=0.1147, gradient_norm=0.1787, 
[2025-09-12 21:12:30,306][__main__][INFO] - Train, Round 080: loss=2.3024, accuracy=0.1147, gradient_norm=0.1797, 
[2025-09-12 21:12:30,306][__main__][INFO] - Train, Round 081: loss=2.3024, accuracy=0.1150, gradient_norm=0.1753, 
[2025-09-12 21:12:30,306][__main__][INFO] - Train, Round 082: loss=2.3024, accuracy=0.1150, gradient_norm=0.1720, 
[2025-09-12 21:12:30,306][__main__][INFO] - Train, Round 083: loss=2.3024, accuracy=0.1153, gradient_norm=0.1783, 
[2025-09-12 21:12:30,306][__main__][INFO] - Train, Round 084: loss=2.3023, accuracy=0.1155, gradient_norm=0.1746, 
[2025-09-12 21:12:30,306][__main__][INFO] - Train, Round 085: loss=2.3023, accuracy=0.1157, gradient_norm=0.1732, 
[2025-09-12 21:12:30,306][__main__][INFO] - Train, Round 086: loss=2.3022, accuracy=0.1158, gradient_norm=0.1709, 
[2025-09-12 21:12:30,306][__main__][INFO] - Train, Round 087: loss=2.3022, accuracy=0.1160, gradient_norm=0.1748, 
[2025-09-12 21:12:30,306][__main__][INFO] - Train, Round 088: loss=2.3022, accuracy=0.1159, gradient_norm=0.1810, 
[2025-09-12 21:12:30,306][__main__][INFO] - Train, Round 089: loss=2.3022, accuracy=0.1159, gradient_norm=0.1758, 
[2025-09-12 21:12:30,306][__main__][INFO] - Train, Round 090: loss=2.3021, accuracy=0.1159, gradient_norm=0.1756, 
[2025-09-12 21:12:30,306][__main__][INFO] - Train, Round 091: loss=2.3021, accuracy=0.1157, gradient_norm=0.1712, 
[2025-09-12 21:12:30,306][__main__][INFO] - Train, Round 092: loss=2.3021, accuracy=0.1158, gradient_norm=0.1746, 
[2025-09-12 21:12:30,306][__main__][INFO] - Train, Round 093: loss=2.3020, accuracy=0.1163, gradient_norm=0.1812, 
[2025-09-12 21:12:30,306][__main__][INFO] - Train, Round 094: loss=2.3020, accuracy=0.1164, gradient_norm=0.1817, 
[2025-09-12 21:12:30,306][__main__][INFO] - Train, Round 095: loss=2.3019, accuracy=0.1165, gradient_norm=0.1735, 
[2025-09-12 21:12:30,306][__main__][INFO] - Train, Round 096: loss=2.3019, accuracy=0.1168, gradient_norm=0.1732, 
[2025-09-12 21:12:30,306][__main__][INFO] - Train, Round 097: loss=2.3019, accuracy=0.1167, gradient_norm=0.1774, 
[2025-09-12 21:12:30,306][__main__][INFO] - Train, Round 098: loss=2.3019, accuracy=0.1169, gradient_norm=0.1789, 
[2025-09-12 21:12:30,306][__main__][INFO] - Train, Round 099: loss=2.3018, accuracy=0.1170, gradient_norm=0.1703, 
[2025-09-12 21:12:30,306][__main__][INFO] - Train, Round 100: loss=2.3018, accuracy=0.1169, gradient_norm=0.1740, 
[2025-09-12 21:12:30,306][__main__][INFO] - Train, Round 101: loss=2.3018, accuracy=0.1171, gradient_norm=0.1727, 
[2025-09-12 21:12:30,306][__main__][INFO] - Train, Round 102: loss=2.3017, accuracy=0.1173, gradient_norm=0.1792, 
[2025-09-12 21:12:30,306][__main__][INFO] - Train, Round 103: loss=2.3017, accuracy=0.1174, gradient_norm=0.1833, 
[2025-09-12 21:12:30,306][__main__][INFO] - Train, Round 104: loss=2.3017, accuracy=0.1175, gradient_norm=0.1790, 
[2025-09-12 21:12:30,306][__main__][INFO] - Train, Round 105: loss=2.3016, accuracy=0.1177, gradient_norm=0.1748, 
[2025-09-12 21:12:30,306][__main__][INFO] - Train, Round 106: loss=2.3016, accuracy=0.1176, gradient_norm=0.1737, 
[2025-09-12 21:12:30,306][__main__][INFO] - Train, Round 107: loss=2.3015, accuracy=0.1178, gradient_norm=0.1799, 
[2025-09-12 21:12:30,306][__main__][INFO] - Train, Round 108: loss=2.3015, accuracy=0.1181, gradient_norm=0.1767, 
[2025-09-12 21:12:30,306][__main__][INFO] - Train, Round 109: loss=2.3015, accuracy=0.1182, gradient_norm=0.1692, 
[2025-09-12 21:12:30,306][__main__][INFO] - Train, Round 110: loss=2.3014, accuracy=0.1181, gradient_norm=0.1733, 
[2025-09-12 21:12:30,306][__main__][INFO] - Train, Round 111: loss=2.3014, accuracy=0.1183, gradient_norm=0.1767, 
[2025-09-12 21:12:30,306][__main__][INFO] - Train, Round 112: loss=2.3013, accuracy=0.1185, gradient_norm=0.1731, 
[2025-09-12 21:12:30,306][__main__][INFO] - Train, Round 113: loss=2.3013, accuracy=0.1185, gradient_norm=0.1712, 
[2025-09-12 21:12:30,306][__main__][INFO] - Train, Round 114: loss=2.3013, accuracy=0.1187, gradient_norm=0.1787, 
[2025-09-12 21:12:30,306][__main__][INFO] - Train, Round 115: loss=2.3013, accuracy=0.1187, gradient_norm=0.1796, 
[2025-09-12 21:12:30,306][__main__][INFO] - Train, Round 116: loss=2.3013, accuracy=0.1189, gradient_norm=0.1769, 
[2025-09-12 21:12:30,306][__main__][INFO] - Train, Round 117: loss=2.3012, accuracy=0.1189, gradient_norm=0.1710, 
[2025-09-12 21:12:30,306][__main__][INFO] - Train, Round 118: loss=2.3012, accuracy=0.1192, gradient_norm=0.1839, 
[2025-09-12 21:12:30,306][__main__][INFO] - Train, Round 119: loss=2.3011, accuracy=0.1195, gradient_norm=0.1787, 
[2025-09-12 21:12:30,306][__main__][INFO] - Train, Round 120: loss=2.3011, accuracy=0.1197, gradient_norm=0.1757, 
[2025-09-12 21:12:30,306][__main__][INFO] - Train, Round 121: loss=2.3011, accuracy=0.1198, gradient_norm=0.1734, 
[2025-09-12 21:12:30,307][__main__][INFO] - Train, Round 122: loss=2.3011, accuracy=0.1201, gradient_norm=0.1847, 
[2025-09-12 21:12:30,307][__main__][INFO] - Train, Round 123: loss=2.3010, accuracy=0.1202, gradient_norm=0.1743, 
[2025-09-12 21:12:30,307][__main__][INFO] - Train, Round 124: loss=2.3010, accuracy=0.1200, gradient_norm=0.1828, 
[2025-09-12 21:12:30,307][__main__][INFO] - Train, Round 125: loss=2.3010, accuracy=0.1202, gradient_norm=0.1790, 
[2025-09-12 21:12:30,307][__main__][INFO] - Train, Round 126: loss=2.3009, accuracy=0.1205, gradient_norm=0.1766, 
[2025-09-12 21:12:30,307][__main__][INFO] - Train, Round 127: loss=2.3009, accuracy=0.1205, gradient_norm=0.1735, 
[2025-09-12 21:12:30,307][__main__][INFO] - Train, Round 128: loss=2.3009, accuracy=0.1205, gradient_norm=0.1751, 
[2025-09-12 21:12:30,307][__main__][INFO] - Train, Round 129: loss=2.3008, accuracy=0.1204, gradient_norm=0.1762, 
[2025-09-12 21:12:30,307][__main__][INFO] - Train, Round 130: loss=2.3008, accuracy=0.1206, gradient_norm=0.1804, 
[2025-09-12 21:12:30,307][__main__][INFO] - Train, Round 131: loss=2.3008, accuracy=0.1206, gradient_norm=0.1769, 
[2025-09-12 21:12:30,307][__main__][INFO] - Train, Round 132: loss=2.3008, accuracy=0.1208, gradient_norm=0.1790, 
[2025-09-12 21:12:30,307][__main__][INFO] - Train, Round 133: loss=2.3007, accuracy=0.1206, gradient_norm=0.1795, 
[2025-09-12 21:12:30,307][__main__][INFO] - Train, Round 134: loss=2.3007, accuracy=0.1208, gradient_norm=0.1721, 
[2025-09-12 21:12:30,307][__main__][INFO] - Train, Round 135: loss=2.3006, accuracy=0.1207, gradient_norm=0.1765, 
[2025-09-12 21:12:30,307][__main__][INFO] - Train, Round 136: loss=2.3006, accuracy=0.1208, gradient_norm=0.1795, 
[2025-09-12 21:12:30,307][__main__][INFO] - Train, Round 137: loss=2.3006, accuracy=0.1209, gradient_norm=0.1759, 
[2025-09-12 21:12:30,307][__main__][INFO] - Train, Round 138: loss=2.3005, accuracy=0.1210, gradient_norm=0.1727, 
[2025-09-12 21:12:30,307][__main__][INFO] - Train, Round 139: loss=2.3005, accuracy=0.1212, gradient_norm=0.1708, 
[2025-09-12 21:12:30,307][__main__][INFO] - Train, Round 140: loss=2.3005, accuracy=0.1212, gradient_norm=0.1750, 
[2025-09-12 21:12:30,307][__main__][INFO] - Train, Round 141: loss=2.3004, accuracy=0.1213, gradient_norm=0.1760, 
[2025-09-12 21:12:30,307][__main__][INFO] - Train, Round 142: loss=2.3004, accuracy=0.1213, gradient_norm=0.1813, 
[2025-09-12 21:12:30,307][__main__][INFO] - Train, Round 143: loss=2.3004, accuracy=0.1215, gradient_norm=0.1738, 
[2025-09-12 21:12:30,307][__main__][INFO] - Train, Round 144: loss=2.3003, accuracy=0.1215, gradient_norm=0.1741, 
[2025-09-12 21:12:30,307][__main__][INFO] - Train, Round 145: loss=2.3003, accuracy=0.1219, gradient_norm=0.1751, 
[2025-09-12 21:12:30,307][__main__][INFO] - Train, Round 146: loss=2.3003, accuracy=0.1219, gradient_norm=0.1786, 
[2025-09-12 21:12:30,307][__main__][INFO] - Train, Round 147: loss=2.3003, accuracy=0.1221, gradient_norm=0.1738, 
[2025-09-12 21:12:30,307][__main__][INFO] - Train, Round 148: loss=2.3002, accuracy=0.1223, gradient_norm=0.1703, 
[2025-09-12 21:12:30,307][__main__][INFO] - Train, Round 149: loss=2.3002, accuracy=0.1224, gradient_norm=0.1760, 
[2025-09-12 21:12:30,307][__main__][INFO] - Train, Round 150: loss=2.3001, accuracy=0.1225, gradient_norm=0.1786, 
[2025-09-12 21:12:30,307][__main__][INFO] - Train, Round 151: loss=2.3001, accuracy=0.1227, gradient_norm=0.1769, 
[2025-09-12 21:12:30,307][__main__][INFO] - Train, Round 152: loss=2.3001, accuracy=0.1228, gradient_norm=0.1767, 
[2025-09-12 21:12:30,307][__main__][INFO] - Train, Round 153: loss=2.3000, accuracy=0.1228, gradient_norm=0.1760, 
[2025-09-12 21:12:30,307][__main__][INFO] - Train, Round 154: loss=2.3000, accuracy=0.1231, gradient_norm=0.1770, 
[2025-09-12 21:12:30,307][__main__][INFO] - Train, Round 155: loss=2.3000, accuracy=0.1230, gradient_norm=0.1708, 
[2025-09-12 21:12:30,307][__main__][INFO] - Train, Round 156: loss=2.2999, accuracy=0.1233, gradient_norm=0.1815, 
[2025-09-12 21:12:30,307][__main__][INFO] - Train, Round 157: loss=2.2999, accuracy=0.1231, gradient_norm=0.1747, 
[2025-09-12 21:12:30,307][__main__][INFO] - Train, Round 158: loss=2.2999, accuracy=0.1230, gradient_norm=0.1761, 
[2025-09-12 21:12:30,307][__main__][INFO] - Train, Round 159: loss=2.2998, accuracy=0.1234, gradient_norm=0.1844, 
[2025-09-12 21:12:30,307][__main__][INFO] - Train, Round 160: loss=2.2998, accuracy=0.1234, gradient_norm=0.1722, 
[2025-09-12 21:12:30,307][__main__][INFO] - Train, Round 161: loss=2.2998, accuracy=0.1236, gradient_norm=0.1789, 
[2025-09-12 21:12:30,307][__main__][INFO] - Train, Round 162: loss=2.2998, accuracy=0.1234, gradient_norm=0.1761, 
[2025-09-12 21:12:30,307][__main__][INFO] - Train, Round 163: loss=2.2997, accuracy=0.1234, gradient_norm=0.1768, 
[2025-09-12 21:12:30,307][__main__][INFO] - Train, Round 164: loss=2.2997, accuracy=0.1233, gradient_norm=0.1751, 
[2025-09-12 21:12:30,307][__main__][INFO] - Train, Round 165: loss=2.2997, accuracy=0.1233, gradient_norm=0.1739, 
[2025-09-12 21:12:30,307][__main__][INFO] - Train, Round 166: loss=2.2996, accuracy=0.1235, gradient_norm=0.1780, 
[2025-09-12 21:12:30,307][__main__][INFO] - Train, Round 167: loss=2.2996, accuracy=0.1236, gradient_norm=0.1765, 
[2025-09-12 21:12:30,307][__main__][INFO] - Train, Round 168: loss=2.2996, accuracy=0.1235, gradient_norm=0.1747, 
[2025-09-12 21:12:30,307][__main__][INFO] - Train, Round 169: loss=2.2995, accuracy=0.1236, gradient_norm=0.1747, 
[2025-09-12 21:12:30,307][__main__][INFO] - Train, Round 170: loss=2.2995, accuracy=0.1237, gradient_norm=0.1732, 
[2025-09-12 21:12:30,307][__main__][INFO] - Train, Round 171: loss=2.2995, accuracy=0.1238, gradient_norm=0.1751, 
[2025-09-12 21:12:30,307][__main__][INFO] - Train, Round 172: loss=2.2994, accuracy=0.1237, gradient_norm=0.1697, 
[2025-09-12 21:12:30,307][__main__][INFO] - Train, Round 173: loss=2.2994, accuracy=0.1237, gradient_norm=0.1782, 
[2025-09-12 21:12:30,307][__main__][INFO] - Train, Round 174: loss=2.2993, accuracy=0.1237, gradient_norm=0.1757, 
[2025-09-12 21:12:30,307][__main__][INFO] - Train, Round 175: loss=2.2993, accuracy=0.1237, gradient_norm=0.1774, 
[2025-09-12 21:12:30,307][__main__][INFO] - Train, Round 176: loss=2.2994, accuracy=0.1238, gradient_norm=0.1728, 
[2025-09-12 21:12:30,307][__main__][INFO] - Train, Round 177: loss=2.2993, accuracy=0.1237, gradient_norm=0.1774, 
[2025-09-12 21:12:30,307][__main__][INFO] - Train, Round 178: loss=2.2992, accuracy=0.1238, gradient_norm=0.1769, 
[2025-09-12 21:12:30,307][__main__][INFO] - Train, Round 179: loss=2.2992, accuracy=0.1237, gradient_norm=0.1789, 
[2025-09-12 21:12:30,307][__main__][INFO] - Train, Round 180: loss=2.2992, accuracy=0.1240, gradient_norm=0.1795, 
[2025-09-12 21:12:30,307][__main__][INFO] - Train, Round 181: loss=2.2992, accuracy=0.1239, gradient_norm=0.1819, 
[2025-09-12 21:12:30,307][__main__][INFO] - Train, Round 182: loss=2.2992, accuracy=0.1238, gradient_norm=0.1780, 
[2025-09-12 21:12:30,307][__main__][INFO] - Train, Round 183: loss=2.2991, accuracy=0.1237, gradient_norm=0.1815, 
[2025-09-12 21:12:30,307][__main__][INFO] - Train, Round 184: loss=2.2991, accuracy=0.1237, gradient_norm=0.1751, 
[2025-09-12 21:12:30,307][__main__][INFO] - Train, Round 185: loss=2.2990, accuracy=0.1239, gradient_norm=0.1817, 
[2025-09-12 21:12:30,307][__main__][INFO] - Train, Round 186: loss=2.2990, accuracy=0.1238, gradient_norm=0.1804, 
[2025-09-12 21:12:30,307][__main__][INFO] - Train, Round 187: loss=2.2990, accuracy=0.1239, gradient_norm=0.1774, 
[2025-09-12 21:12:30,307][__main__][INFO] - Train, Round 188: loss=2.2989, accuracy=0.1240, gradient_norm=0.1699, 
[2025-09-12 21:12:30,307][__main__][INFO] - Train, Round 189: loss=2.2989, accuracy=0.1240, gradient_norm=0.1722, 
[2025-09-12 21:12:30,307][__main__][INFO] - Train, Round 190: loss=2.2989, accuracy=0.1241, gradient_norm=0.1786, 
[2025-09-12 21:12:30,307][__main__][INFO] - Train, Round 191: loss=2.2988, accuracy=0.1242, gradient_norm=0.1778, 
[2025-09-12 21:12:30,307][__main__][INFO] - Train, Round 192: loss=2.2988, accuracy=0.1242, gradient_norm=0.1777, 
[2025-09-12 21:12:30,307][__main__][INFO] - Train, Round 193: loss=2.2987, accuracy=0.1242, gradient_norm=0.1755, 
[2025-09-12 21:12:30,307][__main__][INFO] - Train, Round 194: loss=2.2988, accuracy=0.1244, gradient_norm=0.1781, 
[2025-09-12 21:12:30,307][__main__][INFO] - Train, Round 195: loss=2.2987, accuracy=0.1243, gradient_norm=0.1808, 
[2025-09-12 21:12:30,307][__main__][INFO] - Train, Round 196: loss=2.2987, accuracy=0.1245, gradient_norm=0.1783, 
[2025-09-12 21:12:30,307][__main__][INFO] - Train, Round 197: loss=2.2986, accuracy=0.1245, gradient_norm=0.1708, 
[2025-09-12 21:12:30,307][__main__][INFO] - Train, Round 198: loss=2.2986, accuracy=0.1246, gradient_norm=0.1763, 
[2025-09-12 21:12:30,308][__main__][INFO] - Train, Round 199: loss=2.2986, accuracy=0.1245, gradient_norm=0.1819, 
[2025-09-12 21:12:30,308][__main__][INFO] - Train, Round 200: loss=2.2986, accuracy=0.1247, gradient_norm=0.1776, 
[2025-09-12 21:12:30,308][__main__][INFO] - Train, Round 201: loss=2.2985, accuracy=0.1247, gradient_norm=0.1786, 
[2025-09-12 21:12:30,308][__main__][INFO] - Train, Round 202: loss=2.2985, accuracy=0.1250, gradient_norm=0.1855, 
[2025-09-12 21:12:30,308][__main__][INFO] - Train, Round 203: loss=2.2985, accuracy=0.1250, gradient_norm=0.1805, 
[2025-09-12 21:12:30,308][__main__][INFO] - Train, Round 204: loss=2.2984, accuracy=0.1249, gradient_norm=0.1787, 
[2025-09-12 21:12:30,308][__main__][INFO] - Train, Round 205: loss=2.2984, accuracy=0.1248, gradient_norm=0.1758, 
[2025-09-12 21:12:30,308][__main__][INFO] - Train, Round 206: loss=2.2983, accuracy=0.1249, gradient_norm=0.1791, 
[2025-09-12 21:12:30,308][__main__][INFO] - Train, Round 207: loss=2.2983, accuracy=0.1248, gradient_norm=0.1784, 
[2025-09-12 21:12:30,308][__main__][INFO] - Train, Round 208: loss=2.2983, accuracy=0.1249, gradient_norm=0.1885, 
[2025-09-12 21:12:30,308][__main__][INFO] - Train, Round 209: loss=2.2983, accuracy=0.1248, gradient_norm=0.1837, 
[2025-09-12 21:12:30,308][__main__][INFO] - Train, Round 210: loss=2.2983, accuracy=0.1250, gradient_norm=0.1803, 
[2025-09-12 21:12:30,308][__main__][INFO] - Train, Round 211: loss=2.2982, accuracy=0.1250, gradient_norm=0.1826, 
[2025-09-12 21:12:30,308][__main__][INFO] - Train, Round 212: loss=2.2982, accuracy=0.1252, gradient_norm=0.1764, 
[2025-09-12 21:12:30,308][__main__][INFO] - Train, Round 213: loss=2.2981, accuracy=0.1251, gradient_norm=0.1813, 
[2025-09-12 21:12:30,308][__main__][INFO] - Train, Round 214: loss=2.2981, accuracy=0.1252, gradient_norm=0.1840, 
[2025-09-12 21:12:30,308][__main__][INFO] - Train, Round 215: loss=2.2981, accuracy=0.1252, gradient_norm=0.1771, 
[2025-09-12 21:12:30,308][__main__][INFO] - Train, Round 216: loss=2.2981, accuracy=0.1251, gradient_norm=0.1794, 
[2025-09-12 21:12:30,308][__main__][INFO] - Train, Round 217: loss=2.2980, accuracy=0.1251, gradient_norm=0.1838, 
[2025-09-12 21:12:30,308][__main__][INFO] - Train, Round 218: loss=2.2980, accuracy=0.1252, gradient_norm=0.1802, 
[2025-09-12 21:12:30,308][__main__][INFO] - Train, Round 219: loss=2.2979, accuracy=0.1253, gradient_norm=0.1772, 
[2025-09-12 21:12:30,308][__main__][INFO] - Train, Round 220: loss=2.2979, accuracy=0.1254, gradient_norm=0.1782, 
[2025-09-12 21:12:30,308][__main__][INFO] - Train, Round 221: loss=2.2979, accuracy=0.1254, gradient_norm=0.1772, 
[2025-09-12 21:12:30,308][__main__][INFO] - Train, Round 222: loss=2.2979, accuracy=0.1255, gradient_norm=0.1796, 
[2025-09-12 21:12:30,308][__main__][INFO] - Train, Round 223: loss=2.2979, accuracy=0.1256, gradient_norm=0.1767, 
[2025-09-12 21:12:30,308][__main__][INFO] - Train, Round 224: loss=2.2978, accuracy=0.1256, gradient_norm=0.1836, 
[2025-09-12 21:12:30,308][__main__][INFO] - Train, Round 225: loss=2.2977, accuracy=0.1259, gradient_norm=0.1740, 
[2025-09-12 21:12:30,308][__main__][INFO] - Train, Round 226: loss=2.2977, accuracy=0.1260, gradient_norm=0.1795, 
[2025-09-12 21:12:30,308][__main__][INFO] - Train, Round 227: loss=2.2977, accuracy=0.1261, gradient_norm=0.1759, 
[2025-09-12 21:12:30,308][__main__][INFO] - Train, Round 228: loss=2.2977, accuracy=0.1262, gradient_norm=0.1820, 
[2025-09-12 21:12:30,308][__main__][INFO] - Train, Round 229: loss=2.2977, accuracy=0.1262, gradient_norm=0.1796, 
[2025-09-12 21:12:30,308][__main__][INFO] - Train, Round 230: loss=2.2976, accuracy=0.1264, gradient_norm=0.1749, 
[2025-09-12 21:12:30,308][__main__][INFO] - Train, Round 231: loss=2.2976, accuracy=0.1264, gradient_norm=0.1764, 
[2025-09-12 21:12:30,308][__main__][INFO] - Train, Round 232: loss=2.2975, accuracy=0.1265, gradient_norm=0.1760, 
[2025-09-12 21:12:30,308][__main__][INFO] - Train, Round 233: loss=2.2975, accuracy=0.1266, gradient_norm=0.1789, 
[2025-09-12 21:12:30,308][__main__][INFO] - Train, Round 234: loss=2.2975, accuracy=0.1265, gradient_norm=0.1743, 
[2025-09-12 21:12:30,308][__main__][INFO] - Train, Round 235: loss=2.2974, accuracy=0.1265, gradient_norm=0.1832, 
[2025-09-12 21:12:30,308][__main__][INFO] - Train, Round 236: loss=2.2974, accuracy=0.1268, gradient_norm=0.1747, 
[2025-09-12 21:12:30,308][__main__][INFO] - Train, Round 237: loss=2.2974, accuracy=0.1267, gradient_norm=0.1742, 
[2025-09-12 21:12:30,308][__main__][INFO] - Train, Round 238: loss=2.2974, accuracy=0.1268, gradient_norm=0.1741, 
[2025-09-12 21:12:30,308][__main__][INFO] - Train, Round 239: loss=2.2974, accuracy=0.1267, gradient_norm=0.1804, 
[2025-09-12 21:12:30,308][__main__][INFO] - Train, Round 240: loss=2.2972, accuracy=0.1268, gradient_norm=0.1772, 
[2025-09-12 21:12:30,308][__main__][INFO] - Train, Round 241: loss=2.2973, accuracy=0.1267, gradient_norm=0.1821, 
[2025-09-12 21:12:30,308][__main__][INFO] - Train, Round 242: loss=2.2972, accuracy=0.1270, gradient_norm=0.1818, 
[2025-09-12 21:12:30,308][__main__][INFO] - Train, Round 243: loss=2.2972, accuracy=0.1269, gradient_norm=0.1848, 
[2025-09-12 21:12:30,308][__main__][INFO] - Train, Round 244: loss=2.2971, accuracy=0.1270, gradient_norm=0.1817, 
[2025-09-12 21:12:30,308][__main__][INFO] - Train, Round 245: loss=2.2971, accuracy=0.1269, gradient_norm=0.1800, 
[2025-09-12 21:12:30,308][__main__][INFO] - Train, Round 246: loss=2.2971, accuracy=0.1270, gradient_norm=0.1774, 
[2025-09-12 21:12:30,308][__main__][INFO] - Train, Round 247: loss=2.2971, accuracy=0.1270, gradient_norm=0.1789, 
[2025-09-12 21:12:30,308][__main__][INFO] - Train, Round 248: loss=2.2971, accuracy=0.1271, gradient_norm=0.1796, 
[2025-09-12 21:12:30,308][__main__][INFO] - Train, Round 249: loss=2.2970, accuracy=0.1271, gradient_norm=0.1762, 
[2025-09-12 21:12:30,308][__main__][INFO] - Train, Round 250: loss=2.2970, accuracy=0.1273, gradient_norm=0.1778, 
[2025-09-12 21:12:30,308][__main__][INFO] - Train, Round 251: loss=2.2969, accuracy=0.1273, gradient_norm=0.1789, 
[2025-09-12 21:12:30,308][__main__][INFO] - Train, Round 252: loss=2.2969, accuracy=0.1275, gradient_norm=0.1772, 
[2025-09-12 21:12:30,308][__main__][INFO] - Train, Round 253: loss=2.2969, accuracy=0.1274, gradient_norm=0.1795, 
[2025-09-12 21:12:30,308][__main__][INFO] - Train, Round 254: loss=2.2968, accuracy=0.1276, gradient_norm=0.1779, 
[2025-09-12 21:12:30,308][__main__][INFO] - Train, Round 255: loss=2.2968, accuracy=0.1277, gradient_norm=0.1845, 
[2025-09-12 21:12:30,308][__main__][INFO] - Train, Round 256: loss=2.2967, accuracy=0.1277, gradient_norm=0.1771, 
[2025-09-12 21:12:30,308][__main__][INFO] - Train, Round 257: loss=2.2967, accuracy=0.1279, gradient_norm=0.1801, 
[2025-09-12 21:12:30,308][__main__][INFO] - Train, Round 258: loss=2.2967, accuracy=0.1279, gradient_norm=0.1808, 
[2025-09-12 21:12:30,308][__main__][INFO] - Train, Round 259: loss=2.2967, accuracy=0.1282, gradient_norm=0.1806, 
[2025-09-12 21:12:30,308][__main__][INFO] - Train, Round 260: loss=2.2967, accuracy=0.1282, gradient_norm=0.1814, 
[2025-09-12 21:12:30,308][__main__][INFO] - Train, Round 261: loss=2.2966, accuracy=0.1283, gradient_norm=0.1801, 
[2025-09-12 21:12:30,308][__main__][INFO] - Train, Round 262: loss=2.2966, accuracy=0.1284, gradient_norm=0.1823, 
[2025-09-12 21:12:30,308][__main__][INFO] - Train, Round 263: loss=2.2966, accuracy=0.1283, gradient_norm=0.1872, 
[2025-09-12 21:12:30,308][__main__][INFO] - Train, Round 264: loss=2.2965, accuracy=0.1285, gradient_norm=0.1808, 
[2025-09-12 21:12:30,308][__main__][INFO] - Train, Round 265: loss=2.2965, accuracy=0.1287, gradient_norm=0.1809, 
[2025-09-12 21:12:30,308][__main__][INFO] - Train, Round 266: loss=2.2964, accuracy=0.1288, gradient_norm=0.1772, 
[2025-09-12 21:12:30,308][__main__][INFO] - Train, Round 267: loss=2.2964, accuracy=0.1290, gradient_norm=0.1839, 
[2025-09-12 21:12:30,308][__main__][INFO] - Train, Round 268: loss=2.2963, accuracy=0.1290, gradient_norm=0.1864, 
[2025-09-12 21:12:30,308][__main__][INFO] - Train, Round 269: loss=2.2963, accuracy=0.1290, gradient_norm=0.1802, 
[2025-09-12 21:12:30,308][__main__][INFO] - Train, Round 270: loss=2.2963, accuracy=0.1292, gradient_norm=0.1807, 
[2025-09-12 21:12:30,309][__main__][INFO] - Train, Round 271: loss=2.2963, accuracy=0.1291, gradient_norm=0.1778, 
[2025-09-12 21:12:30,309][__main__][INFO] - Train, Round 272: loss=2.2963, accuracy=0.1294, gradient_norm=0.1866, 
[2025-09-12 21:12:30,309][__main__][INFO] - Train, Round 273: loss=2.2962, accuracy=0.1293, gradient_norm=0.1806, 
[2025-09-12 21:12:30,309][__main__][INFO] - Train, Round 274: loss=2.2962, accuracy=0.1294, gradient_norm=0.1807, 
[2025-09-12 21:12:30,309][__main__][INFO] - Train, Round 275: loss=2.2962, accuracy=0.1296, gradient_norm=0.1786, 
[2025-09-12 21:12:30,309][__main__][INFO] - Train, Round 276: loss=2.2961, accuracy=0.1295, gradient_norm=0.1900, 
[2025-09-12 21:12:30,309][__main__][INFO] - Train, Round 277: loss=2.2961, accuracy=0.1297, gradient_norm=0.1872, 
[2025-09-12 21:12:30,309][__main__][INFO] - Train, Round 278: loss=2.2960, accuracy=0.1296, gradient_norm=0.1795, 
[2025-09-12 21:12:30,309][__main__][INFO] - Train, Round 279: loss=2.2961, accuracy=0.1296, gradient_norm=0.1793, 
[2025-09-12 21:12:30,309][__main__][INFO] - Train, Round 280: loss=2.2960, accuracy=0.1297, gradient_norm=0.1852, 
[2025-09-12 21:12:30,309][__main__][INFO] - Train, Round 281: loss=2.2960, accuracy=0.1297, gradient_norm=0.1833, 
[2025-09-12 21:12:30,309][__main__][INFO] - Train, Round 282: loss=2.2959, accuracy=0.1298, gradient_norm=0.1801, 
[2025-09-12 21:12:30,309][__main__][INFO] - Train, Round 283: loss=2.2959, accuracy=0.1301, gradient_norm=0.1851, 
[2025-09-12 21:12:30,309][__main__][INFO] - Train, Round 284: loss=2.2959, accuracy=0.1300, gradient_norm=0.1877, 
[2025-09-12 21:12:30,309][__main__][INFO] - Train, Round 285: loss=2.2958, accuracy=0.1303, gradient_norm=0.1842, 
[2025-09-12 21:12:30,309][__main__][INFO] - Train, Round 286: loss=2.2958, accuracy=0.1303, gradient_norm=0.1780, 
[2025-09-12 21:12:30,309][__main__][INFO] - Train, Round 287: loss=2.2958, accuracy=0.1302, gradient_norm=0.1844, 
[2025-09-12 21:12:30,309][__main__][INFO] - Train, Round 288: loss=2.2957, accuracy=0.1305, gradient_norm=0.1819, 
[2025-09-12 21:12:30,309][__main__][INFO] - Train, Round 289: loss=2.2957, accuracy=0.1304, gradient_norm=0.1842, 
[2025-09-12 21:12:30,309][__main__][INFO] - Train, Round 290: loss=2.2957, accuracy=0.1305, gradient_norm=0.1828, 
[2025-09-12 21:12:30,309][__main__][INFO] - Train, Round 291: loss=2.2956, accuracy=0.1306, gradient_norm=0.1814, 
[2025-09-12 21:12:30,309][__main__][INFO] - Train, Round 292: loss=2.2956, accuracy=0.1306, gradient_norm=0.1839, 
[2025-09-12 21:12:30,309][__main__][INFO] - Train, Round 293: loss=2.2956, accuracy=0.1308, gradient_norm=0.1830, 
[2025-09-12 21:12:30,309][__main__][INFO] - Train, Round 294: loss=2.2955, accuracy=0.1308, gradient_norm=0.1906, 
[2025-09-12 21:12:30,309][__main__][INFO] - Train, Round 295: loss=2.2955, accuracy=0.1309, gradient_norm=0.1816, 
[2025-09-12 21:12:30,309][__main__][INFO] - Train, Round 296: loss=2.2954, accuracy=0.1309, gradient_norm=0.1848, 
[2025-09-12 21:12:30,309][__main__][INFO] - Train, Round 297: loss=2.2954, accuracy=0.1309, gradient_norm=0.1876, 
[2025-09-12 21:12:30,309][__main__][INFO] - Train, Round 298: loss=2.2954, accuracy=0.1309, gradient_norm=0.1823, 
[2025-09-12 21:12:30,309][__main__][INFO] - Train, Round 299: loss=2.2954, accuracy=0.1310, gradient_norm=0.1801, 
[2025-09-12 21:12:30,309][__main__][INFO] - Train, Round 300: loss=2.2953, accuracy=0.1312, gradient_norm=0.1856, 
[2025-09-12 21:12:30,309][__main__][INFO] - Train, Round 301: loss=2.2953, accuracy=0.1311, gradient_norm=0.1882, 
[2025-09-12 21:12:30,309][__main__][INFO] - Train, Round 302: loss=2.2953, accuracy=0.1312, gradient_norm=0.1800, 
[2025-09-12 21:12:30,309][__main__][INFO] - Train, Round 303: loss=2.2952, accuracy=0.1312, gradient_norm=0.1874, 
[2025-09-12 21:12:30,309][__main__][INFO] - Train, Round 304: loss=2.2952, accuracy=0.1312, gradient_norm=0.1772, 
[2025-09-12 21:12:30,309][__main__][INFO] - Train, Round 305: loss=2.2952, accuracy=0.1312, gradient_norm=0.1880, 
[2025-09-12 21:12:30,309][__main__][INFO] - Train, Round 306: loss=2.2951, accuracy=0.1313, gradient_norm=0.1879, 
[2025-09-12 21:12:30,309][__main__][INFO] - Train, Round 307: loss=2.2951, accuracy=0.1315, gradient_norm=0.1822, 
[2025-09-12 21:12:30,309][__main__][INFO] - Train, Round 308: loss=2.2951, accuracy=0.1315, gradient_norm=0.1793, 
[2025-09-12 21:12:30,309][__main__][INFO] - Train, Round 309: loss=2.2950, accuracy=0.1314, gradient_norm=0.1890, 
[2025-09-12 21:12:30,309][__main__][INFO] - Train, Round 310: loss=2.2950, accuracy=0.1315, gradient_norm=0.1858, 
[2025-09-12 21:12:30,309][__main__][INFO] - Train, Round 311: loss=2.2949, accuracy=0.1317, gradient_norm=0.1832, 
[2025-09-12 21:12:30,309][__main__][INFO] - Train, Round 312: loss=2.2949, accuracy=0.1317, gradient_norm=0.1828, 
[2025-09-12 21:12:30,309][__main__][INFO] - Train, Round 313: loss=2.2949, accuracy=0.1318, gradient_norm=0.1838, 
[2025-09-12 21:12:30,309][__main__][INFO] - Train, Round 314: loss=2.2949, accuracy=0.1318, gradient_norm=0.1881, 
[2025-09-12 21:12:30,309][__main__][INFO] - Train, Round 315: loss=2.2948, accuracy=0.1320, gradient_norm=0.1900, 
[2025-09-12 21:12:30,309][__main__][INFO] - Train, Round 316: loss=2.2948, accuracy=0.1319, gradient_norm=0.1861, 
[2025-09-12 21:12:30,309][__main__][INFO] - Train, Round 317: loss=2.2948, accuracy=0.1321, gradient_norm=0.1866, 
[2025-09-12 21:12:30,309][__main__][INFO] - Train, Round 318: loss=2.2947, accuracy=0.1321, gradient_norm=0.1802, 
[2025-09-12 21:12:30,309][__main__][INFO] - Train, Round 319: loss=2.2947, accuracy=0.1321, gradient_norm=0.1879, 
[2025-09-12 21:12:30,309][__main__][INFO] - Train, Round 320: loss=2.2947, accuracy=0.1321, gradient_norm=0.1889, 
[2025-09-12 21:12:30,309][__main__][INFO] - Train, Round 321: loss=2.2946, accuracy=0.1321, gradient_norm=0.1892, 
[2025-09-12 21:12:30,309][__main__][INFO] - Train, Round 322: loss=2.2945, accuracy=0.1321, gradient_norm=0.1873, 
[2025-09-12 21:12:30,309][__main__][INFO] - Train, Round 323: loss=2.2945, accuracy=0.1323, gradient_norm=0.1900, 
[2025-09-12 21:12:30,309][__main__][INFO] - Train, Round 324: loss=2.2945, accuracy=0.1324, gradient_norm=0.1826, 
[2025-09-12 21:12:30,309][__main__][INFO] - Train, Round 325: loss=2.2945, accuracy=0.1325, gradient_norm=0.1777, 
[2025-09-12 21:12:30,309][__main__][INFO] - Train, Round 326: loss=2.2945, accuracy=0.1325, gradient_norm=0.1883, 
[2025-09-12 21:12:30,309][__main__][INFO] - Train, Round 327: loss=2.2944, accuracy=0.1326, gradient_norm=0.1825, 
[2025-09-12 21:12:30,309][__main__][INFO] - Train, Round 328: loss=2.2944, accuracy=0.1327, gradient_norm=0.1853, 
[2025-09-12 21:12:30,309][__main__][INFO] - Train, Round 329: loss=2.2943, accuracy=0.1327, gradient_norm=0.1857, 
[2025-09-12 21:12:30,309][__main__][INFO] - Train, Round 330: loss=2.2943, accuracy=0.1329, gradient_norm=0.1844, 
[2025-09-12 21:12:30,309][__main__][INFO] - Train, Round 331: loss=2.2943, accuracy=0.1329, gradient_norm=0.1836, 
[2025-09-12 21:12:30,309][__main__][INFO] - Train, Round 332: loss=2.2942, accuracy=0.1329, gradient_norm=0.1853, 
[2025-09-12 21:12:30,309][__main__][INFO] - Train, Round 333: loss=2.2942, accuracy=0.1329, gradient_norm=0.1843, 
[2025-09-12 21:12:30,309][__main__][INFO] - Train, Round 334: loss=2.2941, accuracy=0.1329, gradient_norm=0.1872, 
[2025-09-12 21:12:30,309][__main__][INFO] - Train, Round 335: loss=2.2941, accuracy=0.1329, gradient_norm=0.1856, 
[2025-09-12 21:12:30,309][__main__][INFO] - Train, Round 336: loss=2.2941, accuracy=0.1331, gradient_norm=0.1930, 
[2025-09-12 21:12:30,309][__main__][INFO] - Train, Round 337: loss=2.2941, accuracy=0.1330, gradient_norm=0.1847, 
[2025-09-12 21:12:30,309][__main__][INFO] - Train, Round 338: loss=2.2940, accuracy=0.1331, gradient_norm=0.1922, 
[2025-09-12 21:12:30,309][__main__][INFO] - Train, Round 339: loss=2.2940, accuracy=0.1330, gradient_norm=0.1874, 
[2025-09-12 21:12:30,309][__main__][INFO] - Train, Round 340: loss=2.2939, accuracy=0.1331, gradient_norm=0.1872, 
[2025-09-12 21:12:30,309][__main__][INFO] - Train, Round 341: loss=2.2940, accuracy=0.1332, gradient_norm=0.1897, 
[2025-09-12 21:12:30,310][__main__][INFO] - Train, Round 342: loss=2.2939, accuracy=0.1332, gradient_norm=0.1845, 
[2025-09-12 21:12:30,310][__main__][INFO] - Train, Round 343: loss=2.2938, accuracy=0.1333, gradient_norm=0.1900, 
[2025-09-12 21:12:30,310][__main__][INFO] - Train, Round 344: loss=2.2938, accuracy=0.1332, gradient_norm=0.1883, 
[2025-09-12 21:12:30,310][__main__][INFO] - Train, Round 345: loss=2.2937, accuracy=0.1334, gradient_norm=0.1925, 
[2025-09-12 21:12:30,310][__main__][INFO] - Train, Round 346: loss=2.2937, accuracy=0.1334, gradient_norm=0.1842, 
[2025-09-12 21:12:30,310][__main__][INFO] - Train, Round 347: loss=2.2937, accuracy=0.1334, gradient_norm=0.1904, 
[2025-09-12 21:12:30,310][__main__][INFO] - Train, Round 348: loss=2.2937, accuracy=0.1335, gradient_norm=0.1820, 
[2025-09-12 21:12:30,310][__main__][INFO] - Train, Round 349: loss=2.2936, accuracy=0.1336, gradient_norm=0.1911, 
[2025-09-12 21:12:30,310][__main__][INFO] - Train, Round 350: loss=2.2936, accuracy=0.1337, gradient_norm=0.1926, 
[2025-09-12 21:12:30,310][__main__][INFO] - Train, Round 351: loss=2.2936, accuracy=0.1338, gradient_norm=0.1906, 
[2025-09-12 21:12:30,310][__main__][INFO] - Train, Round 352: loss=2.2935, accuracy=0.1337, gradient_norm=0.1865, 
[2025-09-12 21:12:30,310][__main__][INFO] - Train, Round 353: loss=2.2935, accuracy=0.1338, gradient_norm=0.1893, 
[2025-09-12 21:12:30,310][__main__][INFO] - Train, Round 354: loss=2.2934, accuracy=0.1338, gradient_norm=0.1882, 
[2025-09-12 21:12:30,310][__main__][INFO] - Train, Round 355: loss=2.2934, accuracy=0.1338, gradient_norm=0.1858, 
[2025-09-12 21:12:30,310][__main__][INFO] - Train, Round 356: loss=2.2934, accuracy=0.1338, gradient_norm=0.1915, 
[2025-09-12 21:12:30,310][__main__][INFO] - Train, Round 357: loss=2.2933, accuracy=0.1338, gradient_norm=0.1879, 
[2025-09-12 21:12:30,310][__main__][INFO] - Train, Round 358: loss=2.2932, accuracy=0.1339, gradient_norm=0.1927, 
[2025-09-12 21:12:30,310][__main__][INFO] - Train, Round 359: loss=2.2932, accuracy=0.1340, gradient_norm=0.1975, 
[2025-09-12 21:12:30,310][__main__][INFO] - Train, Round 360: loss=2.2932, accuracy=0.1339, gradient_norm=0.1838, 
[2025-09-12 21:12:30,310][__main__][INFO] - Train, Round 361: loss=2.2932, accuracy=0.1340, gradient_norm=0.1862, 
[2025-09-12 21:12:30,310][__main__][INFO] - Train, Round 362: loss=2.2931, accuracy=0.1341, gradient_norm=0.1872, 
[2025-09-12 21:12:30,310][__main__][INFO] - Train, Round 363: loss=2.2931, accuracy=0.1342, gradient_norm=0.1957, 
[2025-09-12 21:12:30,310][__main__][INFO] - Train, Round 364: loss=2.2931, accuracy=0.1342, gradient_norm=0.1916, 
[2025-09-12 21:12:30,310][__main__][INFO] - Train, Round 365: loss=2.2930, accuracy=0.1344, gradient_norm=0.1907, 
[2025-09-12 21:12:30,310][__main__][INFO] - Train, Round 366: loss=2.2930, accuracy=0.1346, gradient_norm=0.1905, 
[2025-09-12 21:12:30,310][__main__][INFO] - Train, Round 367: loss=2.2929, accuracy=0.1347, gradient_norm=0.1898, 
[2025-09-12 21:12:30,310][__main__][INFO] - Train, Round 368: loss=2.2929, accuracy=0.1348, gradient_norm=0.1897, 
[2025-09-12 21:12:30,310][__main__][INFO] - Train, Round 369: loss=2.2928, accuracy=0.1349, gradient_norm=0.1919, 
[2025-09-12 21:12:30,310][__main__][INFO] - Train, Round 370: loss=2.2928, accuracy=0.1348, gradient_norm=0.1899, 
[2025-09-12 21:12:30,310][__main__][INFO] - Train, Round 371: loss=2.2928, accuracy=0.1351, gradient_norm=0.1882, 
[2025-09-12 21:12:30,310][__main__][INFO] - Train, Round 372: loss=2.2928, accuracy=0.1351, gradient_norm=0.1975, 
[2025-09-12 21:12:30,310][__main__][INFO] - Train, Round 373: loss=2.2927, accuracy=0.1352, gradient_norm=0.1903, 
[2025-09-12 21:12:30,310][__main__][INFO] - Train, Round 374: loss=2.2927, accuracy=0.1352, gradient_norm=0.1960, 
[2025-09-12 21:12:30,310][__main__][INFO] - Train, Round 375: loss=2.2927, accuracy=0.1352, gradient_norm=0.1905, 
[2025-09-12 21:12:30,310][__main__][INFO] - Train, Round 376: loss=2.2926, accuracy=0.1354, gradient_norm=0.1917, 
[2025-09-12 21:12:30,310][__main__][INFO] - Train, Round 377: loss=2.2926, accuracy=0.1354, gradient_norm=0.1941, 
[2025-09-12 21:12:30,310][__main__][INFO] - Train, Round 378: loss=2.2925, accuracy=0.1354, gradient_norm=0.1989, 
[2025-09-12 21:12:30,310][__main__][INFO] - Train, Round 379: loss=2.2925, accuracy=0.1355, gradient_norm=0.1946, 
[2025-09-12 21:12:30,310][__main__][INFO] - Train, Round 380: loss=2.2925, accuracy=0.1355, gradient_norm=0.1939, 
[2025-09-12 21:12:30,310][__main__][INFO] - Train, Round 381: loss=2.2924, accuracy=0.1355, gradient_norm=0.1932, 
[2025-09-12 21:12:30,310][__main__][INFO] - Train, Round 382: loss=2.2924, accuracy=0.1354, gradient_norm=0.1922, 
[2025-09-12 21:12:30,310][__main__][INFO] - Train, Round 383: loss=2.2923, accuracy=0.1355, gradient_norm=0.1979, 
[2025-09-12 21:12:30,310][__main__][INFO] - Train, Round 384: loss=2.2923, accuracy=0.1356, gradient_norm=0.1935, 
[2025-09-12 21:12:30,310][__main__][INFO] - Train, Round 385: loss=2.2923, accuracy=0.1358, gradient_norm=0.1917, 
[2025-09-12 21:12:30,310][__main__][INFO] - Train, Round 386: loss=2.2922, accuracy=0.1359, gradient_norm=0.1922, 
[2025-09-12 21:12:30,310][__main__][INFO] - Train, Round 387: loss=2.2922, accuracy=0.1357, gradient_norm=0.1906, 
[2025-09-12 21:12:30,310][__main__][INFO] - Train, Round 388: loss=2.2921, accuracy=0.1360, gradient_norm=0.1938, 
[2025-09-12 21:12:30,310][__main__][INFO] - Train, Round 389: loss=2.2921, accuracy=0.1359, gradient_norm=0.1947, 
[2025-09-12 21:12:30,310][__main__][INFO] - Train, Round 390: loss=2.2920, accuracy=0.1361, gradient_norm=0.1997, 
[2025-09-12 21:12:30,310][__main__][INFO] - Train, Round 391: loss=2.2920, accuracy=0.1362, gradient_norm=0.1937, 
[2025-09-12 21:12:30,310][__main__][INFO] - Train, Round 392: loss=2.2920, accuracy=0.1363, gradient_norm=0.1961, 
[2025-09-12 21:12:30,310][__main__][INFO] - Train, Round 393: loss=2.2919, accuracy=0.1364, gradient_norm=0.1953, 
[2025-09-12 21:12:30,310][__main__][INFO] - Train, Round 394: loss=2.2919, accuracy=0.1364, gradient_norm=0.1952, 
[2025-09-12 21:12:30,310][__main__][INFO] - Train, Round 395: loss=2.2918, accuracy=0.1363, gradient_norm=0.1959, 
[2025-09-12 21:12:30,310][__main__][INFO] - Train, Round 396: loss=2.2918, accuracy=0.1363, gradient_norm=0.1919, 
[2025-09-12 21:12:30,310][__main__][INFO] - Train, Round 397: loss=2.2917, accuracy=0.1364, gradient_norm=0.1933, 
[2025-09-12 21:12:30,310][__main__][INFO] - Train, Round 398: loss=2.2917, accuracy=0.1364, gradient_norm=0.1958, 
[2025-09-12 21:12:30,310][__main__][INFO] - Train, Round 399: loss=2.2917, accuracy=0.1365, gradient_norm=0.1996, 
[2025-09-12 21:12:30,310][__main__][INFO] - Train, Round 400: loss=2.2917, accuracy=0.1366, gradient_norm=0.1929, 
[2025-09-12 21:12:30,310][__main__][INFO] - Train, Round 401: loss=2.2916, accuracy=0.1366, gradient_norm=0.1972, 
[2025-09-12 21:12:30,310][__main__][INFO] - Train, Round 402: loss=2.2916, accuracy=0.1369, gradient_norm=0.1956, 
[2025-09-12 21:12:30,310][__main__][INFO] - Train, Round 403: loss=2.2915, accuracy=0.1370, gradient_norm=0.1940, 
[2025-09-12 21:12:30,310][__main__][INFO] - Train, Round 404: loss=2.2915, accuracy=0.1370, gradient_norm=0.1996, 
[2025-09-12 21:12:30,310][__main__][INFO] - Train, Round 405: loss=2.2915, accuracy=0.1371, gradient_norm=0.1981, 
[2025-09-12 21:12:30,310][__main__][INFO] - Train, Round 406: loss=2.2914, accuracy=0.1371, gradient_norm=0.1983, 
[2025-09-12 21:12:30,310][__main__][INFO] - Train, Round 407: loss=2.2913, accuracy=0.1370, gradient_norm=0.1963, 
[2025-09-12 21:12:30,310][__main__][INFO] - Train, Round 408: loss=2.2913, accuracy=0.1372, gradient_norm=0.1990, 
[2025-09-12 21:12:30,310][__main__][INFO] - Train, Round 409: loss=2.2913, accuracy=0.1374, gradient_norm=0.1949, 
[2025-09-12 21:12:30,310][__main__][INFO] - Train, Round 410: loss=2.2912, accuracy=0.1374, gradient_norm=0.1984, 
[2025-09-12 21:12:30,310][__main__][INFO] - Train, Round 411: loss=2.2912, accuracy=0.1376, gradient_norm=0.1996, 
[2025-09-12 21:12:30,310][__main__][INFO] - Train, Round 412: loss=2.2911, accuracy=0.1377, gradient_norm=0.1927, 
[2025-09-12 21:12:30,310][__main__][INFO] - Train, Round 413: loss=2.2911, accuracy=0.1379, gradient_norm=0.1996, 
[2025-09-12 21:12:30,310][__main__][INFO] - Train, Round 414: loss=2.2910, accuracy=0.1379, gradient_norm=0.1946, 
[2025-09-12 21:12:30,311][__main__][INFO] - Train, Round 415: loss=2.2910, accuracy=0.1379, gradient_norm=0.1961, 
[2025-09-12 21:12:30,311][__main__][INFO] - Train, Round 416: loss=2.2910, accuracy=0.1381, gradient_norm=0.2013, 
[2025-09-12 21:12:30,311][__main__][INFO] - Train, Round 417: loss=2.2909, accuracy=0.1384, gradient_norm=0.2024, 
[2025-09-12 21:12:30,311][__main__][INFO] - Train, Round 418: loss=2.2909, accuracy=0.1385, gradient_norm=0.1966, 
[2025-09-12 21:12:30,311][__main__][INFO] - Train, Round 419: loss=2.2908, accuracy=0.1386, gradient_norm=0.1998, 
[2025-09-12 21:12:30,311][__main__][INFO] - Train, Round 420: loss=2.2908, accuracy=0.1386, gradient_norm=0.2005, 
[2025-09-12 21:12:30,311][__main__][INFO] - Train, Round 421: loss=2.2907, accuracy=0.1386, gradient_norm=0.1965, 
[2025-09-12 21:12:30,311][__main__][INFO] - Train, Round 422: loss=2.2907, accuracy=0.1387, gradient_norm=0.1943, 
[2025-09-12 21:12:30,311][__main__][INFO] - Train, Round 423: loss=2.2906, accuracy=0.1390, gradient_norm=0.1977, 
[2025-09-12 21:12:30,311][__main__][INFO] - Train, Round 424: loss=2.2907, accuracy=0.1389, gradient_norm=0.2061, 
[2025-09-12 21:12:30,311][__main__][INFO] - Train, Round 425: loss=2.2906, accuracy=0.1392, gradient_norm=0.1946, 
[2025-09-12 21:12:30,311][__main__][INFO] - Train, Round 426: loss=2.2906, accuracy=0.1393, gradient_norm=0.1955, 
[2025-09-12 21:12:30,311][__main__][INFO] - Train, Round 427: loss=2.2905, accuracy=0.1395, gradient_norm=0.1972, 
[2025-09-12 21:12:30,311][__main__][INFO] - Train, Round 428: loss=2.2904, accuracy=0.1396, gradient_norm=0.1948, 
[2025-09-12 21:12:30,311][__main__][INFO] - Train, Round 429: loss=2.2904, accuracy=0.1397, gradient_norm=0.1991, 
[2025-09-12 21:12:30,311][__main__][INFO] - Train, Round 430: loss=2.2903, accuracy=0.1396, gradient_norm=0.2016, 
[2025-09-12 21:12:30,311][__main__][INFO] - Train, Round 431: loss=2.2903, accuracy=0.1397, gradient_norm=0.2022, 
[2025-09-12 21:12:30,311][__main__][INFO] - Train, Round 432: loss=2.2902, accuracy=0.1398, gradient_norm=0.2049, 
[2025-09-12 21:12:30,311][__main__][INFO] - Train, Round 433: loss=2.2902, accuracy=0.1399, gradient_norm=0.2035, 
[2025-09-12 21:12:30,311][__main__][INFO] - Train, Round 434: loss=2.2901, accuracy=0.1400, gradient_norm=0.1984, 
[2025-09-12 21:12:30,311][__main__][INFO] - Train, Round 435: loss=2.2901, accuracy=0.1401, gradient_norm=0.2049, 
[2025-09-12 21:12:30,311][__main__][INFO] - Train, Round 436: loss=2.2901, accuracy=0.1401, gradient_norm=0.2018, 
[2025-09-12 21:12:30,311][__main__][INFO] - Train, Round 437: loss=2.2900, accuracy=0.1400, gradient_norm=0.2044, 
[2025-09-12 21:12:30,311][__main__][INFO] - Train, Round 438: loss=2.2900, accuracy=0.1403, gradient_norm=0.1978, 
[2025-09-12 21:12:30,311][__main__][INFO] - Train, Round 439: loss=2.2899, accuracy=0.1401, gradient_norm=0.2023, 
[2025-09-12 21:12:30,311][__main__][INFO] - Train, Round 440: loss=2.2899, accuracy=0.1403, gradient_norm=0.2015, 
[2025-09-12 21:12:30,311][__main__][INFO] - Train, Round 441: loss=2.2899, accuracy=0.1404, gradient_norm=0.2024, 
[2025-09-12 21:12:30,311][__main__][INFO] - Train, Round 442: loss=2.2898, accuracy=0.1405, gradient_norm=0.2044, 
[2025-09-12 21:12:30,311][__main__][INFO] - Train, Round 443: loss=2.2897, accuracy=0.1403, gradient_norm=0.2039, 
[2025-09-12 21:12:30,311][__main__][INFO] - Train, Round 444: loss=2.2897, accuracy=0.1406, gradient_norm=0.2068, 
[2025-09-12 21:12:30,311][__main__][INFO] - Train, Round 445: loss=2.2896, accuracy=0.1404, gradient_norm=0.2090, 
[2025-09-12 21:12:30,311][__main__][INFO] - Train, Round 446: loss=2.2896, accuracy=0.1405, gradient_norm=0.2032, 
[2025-09-12 21:12:30,311][__main__][INFO] - Train, Round 447: loss=2.2896, accuracy=0.1407, gradient_norm=0.2046, 
[2025-09-12 21:12:30,311][__main__][INFO] - Train, Round 448: loss=2.2895, accuracy=0.1408, gradient_norm=0.2014, 
[2025-09-12 21:12:30,311][__main__][INFO] - Train, Round 449: loss=2.2895, accuracy=0.1409, gradient_norm=0.2097, 
[2025-09-12 21:12:30,311][__main__][INFO] - Train, Round 450: loss=2.2894, accuracy=0.1411, gradient_norm=0.2062, 
[2025-09-12 21:12:30,311][__main__][INFO] - Train, Round 451: loss=2.2894, accuracy=0.1411, gradient_norm=0.2075, 
[2025-09-12 21:12:30,311][__main__][INFO] - Train, Round 452: loss=2.2893, accuracy=0.1412, gradient_norm=0.2051, 
[2025-09-12 21:12:30,311][__main__][INFO] - Train, Round 453: loss=2.2892, accuracy=0.1414, gradient_norm=0.2016, 
[2025-09-12 21:12:30,311][__main__][INFO] - Train, Round 454: loss=2.2892, accuracy=0.1414, gradient_norm=0.2055, 
[2025-09-12 21:12:30,311][__main__][INFO] - Train, Round 455: loss=2.2891, accuracy=0.1416, gradient_norm=0.2080, 
[2025-09-12 21:12:30,311][__main__][INFO] - Train, Round 456: loss=2.2891, accuracy=0.1417, gradient_norm=0.2104, 
[2025-09-12 21:12:30,311][__main__][INFO] - Train, Round 457: loss=2.2891, accuracy=0.1417, gradient_norm=0.2135, 
[2025-09-12 21:12:30,311][__main__][INFO] - Train, Round 458: loss=2.2890, accuracy=0.1419, gradient_norm=0.2045, 
[2025-09-12 21:12:30,311][__main__][INFO] - Train, Round 459: loss=2.2890, accuracy=0.1419, gradient_norm=0.2116, 
[2025-09-12 21:12:30,311][__main__][INFO] - Train, Round 460: loss=2.2889, accuracy=0.1419, gradient_norm=0.2138, 
[2025-09-12 21:12:30,311][__main__][INFO] - Train, Round 461: loss=2.2889, accuracy=0.1422, gradient_norm=0.2116, 
[2025-09-12 21:12:30,311][__main__][INFO] - Train, Round 462: loss=2.2888, accuracy=0.1422, gradient_norm=0.2125, 
[2025-09-12 21:12:30,311][__main__][INFO] - Train, Round 463: loss=2.2887, accuracy=0.1424, gradient_norm=0.2062, 
[2025-09-12 21:12:30,311][__main__][INFO] - Train, Round 464: loss=2.2887, accuracy=0.1424, gradient_norm=0.2085, 
[2025-09-12 21:12:30,311][__main__][INFO] - Train, Round 465: loss=2.2887, accuracy=0.1427, gradient_norm=0.1998, 
[2025-09-12 21:12:30,311][__main__][INFO] - Train, Round 466: loss=2.2886, accuracy=0.1426, gradient_norm=0.2070, 
[2025-09-12 21:12:30,311][__main__][INFO] - Train, Round 467: loss=2.2886, accuracy=0.1426, gradient_norm=0.2146, 
[2025-09-12 21:12:30,311][__main__][INFO] - Train, Round 468: loss=2.2886, accuracy=0.1427, gradient_norm=0.2090, 
[2025-09-12 21:12:30,311][__main__][INFO] - Train, Round 469: loss=2.2884, accuracy=0.1427, gradient_norm=0.2102, 
[2025-09-12 21:12:30,311][__main__][INFO] - Train, Round 470: loss=2.2884, accuracy=0.1428, gradient_norm=0.2080, 
[2025-09-12 21:12:30,311][__main__][INFO] - Train, Round 471: loss=2.2883, accuracy=0.1429, gradient_norm=0.2110, 
[2025-09-12 21:12:30,311][__main__][INFO] - Train, Round 472: loss=2.2883, accuracy=0.1430, gradient_norm=0.2099, 
[2025-09-12 21:12:30,311][__main__][INFO] - Train, Round 473: loss=2.2883, accuracy=0.1429, gradient_norm=0.2114, 
[2025-09-12 21:12:30,311][__main__][INFO] - Train, Round 474: loss=2.2882, accuracy=0.1433, gradient_norm=0.2084, 
[2025-09-12 21:12:30,311][__main__][INFO] - Train, Round 475: loss=2.2882, accuracy=0.1432, gradient_norm=0.2124, 
[2025-09-12 21:12:30,311][__main__][INFO] - Train, Round 476: loss=2.2880, accuracy=0.1434, gradient_norm=0.2162, 
[2025-09-12 21:12:30,311][__main__][INFO] - Train, Round 477: loss=2.2881, accuracy=0.1436, gradient_norm=0.2091, 
[2025-09-12 21:12:30,311][__main__][INFO] - Train, Round 478: loss=2.2880, accuracy=0.1438, gradient_norm=0.2148, 
[2025-09-12 21:12:30,311][__main__][INFO] - Train, Round 479: loss=2.2879, accuracy=0.1438, gradient_norm=0.2124, 
[2025-09-12 21:12:30,311][__main__][INFO] - Train, Round 480: loss=2.2879, accuracy=0.1439, gradient_norm=0.2127, 
[2025-09-12 21:12:30,311][__main__][INFO] - Train, Round 481: loss=2.2878, accuracy=0.1441, gradient_norm=0.2150, 
[2025-09-12 21:12:30,311][__main__][INFO] - Train, Round 482: loss=2.2878, accuracy=0.1441, gradient_norm=0.2155, 
[2025-09-12 21:12:30,311][__main__][INFO] - Train, Round 483: loss=2.2877, accuracy=0.1442, gradient_norm=0.2172, 
[2025-09-12 21:12:30,311][__main__][INFO] - Train, Round 484: loss=2.2877, accuracy=0.1444, gradient_norm=0.2104, 
[2025-09-12 21:12:30,311][__main__][INFO] - Train, Round 485: loss=2.2876, accuracy=0.1444, gradient_norm=0.2127, 
[2025-09-12 21:12:30,312][__main__][INFO] - Train, Round 486: loss=2.2876, accuracy=0.1446, gradient_norm=0.2033, 
[2025-09-12 21:12:30,312][__main__][INFO] - Train, Round 487: loss=2.2875, accuracy=0.1448, gradient_norm=0.2160, 
[2025-09-12 21:12:30,312][__main__][INFO] - Train, Round 488: loss=2.2875, accuracy=0.1448, gradient_norm=0.2145, 
[2025-09-12 21:12:30,312][__main__][INFO] - Train, Round 489: loss=2.2874, accuracy=0.1450, gradient_norm=0.2143, 
[2025-09-12 21:12:30,312][__main__][INFO] - Train, Round 490: loss=2.2873, accuracy=0.1448, gradient_norm=0.2084, 
[2025-09-12 21:12:30,312][__main__][INFO] - Train, Round 491: loss=2.2873, accuracy=0.1448, gradient_norm=0.2107, 
[2025-09-12 21:12:30,312][__main__][INFO] - Train, Round 492: loss=2.2872, accuracy=0.1449, gradient_norm=0.2153, 
[2025-09-12 21:12:30,312][__main__][INFO] - Train, Round 493: loss=2.2872, accuracy=0.1450, gradient_norm=0.2124, 
[2025-09-12 21:12:30,312][__main__][INFO] - Train, Round 494: loss=2.2871, accuracy=0.1450, gradient_norm=0.2124, 
[2025-09-12 21:12:30,312][__main__][INFO] - Train, Round 495: loss=2.2871, accuracy=0.1452, gradient_norm=0.2124, 
[2025-09-12 21:12:30,312][__main__][INFO] - Train, Round 496: loss=2.2870, accuracy=0.1454, gradient_norm=0.2128, 
[2025-09-12 21:12:30,312][__main__][INFO] - Train, Round 497: loss=2.2869, accuracy=0.1453, gradient_norm=0.2199, 
[2025-09-12 21:12:30,312][__main__][INFO] - Train, Round 498: loss=2.2869, accuracy=0.1454, gradient_norm=0.2137, 
[2025-09-12 21:12:30,312][__main__][INFO] - Train, Round 499: loss=2.2869, accuracy=0.1456, gradient_norm=0.2163, 
[2025-09-12 21:12:30,312][__main__][INFO] - Train, Round 500: loss=2.2868, accuracy=0.1456, gradient_norm=0.2218, 
[2025-09-12 21:12:30,312][__main__][INFO] - Test, Round 001: loss=2.3057, accuracy=0.1013, 
[2025-09-12 21:12:30,312][__main__][INFO] - Test, Round 002: loss=2.3057, accuracy=0.1014, 
[2025-09-12 21:12:30,312][__main__][INFO] - Test, Round 003: loss=2.3057, accuracy=0.1016, 
[2025-09-12 21:12:30,312][__main__][INFO] - Test, Round 004: loss=2.3056, accuracy=0.1015, 
[2025-09-12 21:12:30,312][__main__][INFO] - Test, Round 005: loss=2.3056, accuracy=0.1011, 
[2025-09-12 21:12:30,312][__main__][INFO] - Test, Round 006: loss=2.3056, accuracy=0.1013, 
[2025-09-12 21:12:30,312][__main__][INFO] - Test, Round 007: loss=2.3055, accuracy=0.1015, 
[2025-09-12 21:12:30,312][__main__][INFO] - Test, Round 008: loss=2.3055, accuracy=0.1019, 
[2025-09-12 21:12:30,312][__main__][INFO] - Test, Round 009: loss=2.3054, accuracy=0.1015, 
[2025-09-12 21:12:30,312][__main__][INFO] - Test, Round 010: loss=2.3054, accuracy=0.1013, 
[2025-09-12 21:12:30,312][__main__][INFO] - Test, Round 011: loss=2.3054, accuracy=0.1010, 
[2025-09-12 21:12:30,312][__main__][INFO] - Test, Round 012: loss=2.3053, accuracy=0.1012, 
[2025-09-12 21:12:30,312][__main__][INFO] - Test, Round 013: loss=2.3053, accuracy=0.1022, 
[2025-09-12 21:12:30,312][__main__][INFO] - Test, Round 014: loss=2.3053, accuracy=0.1021, 
[2025-09-12 21:12:30,312][__main__][INFO] - Test, Round 015: loss=2.3052, accuracy=0.1025, 
[2025-09-12 21:12:30,312][__main__][INFO] - Test, Round 016: loss=2.3052, accuracy=0.1026, 
[2025-09-12 21:12:30,312][__main__][INFO] - Test, Round 017: loss=2.3052, accuracy=0.1026, 
[2025-09-12 21:12:30,312][__main__][INFO] - Test, Round 018: loss=2.3051, accuracy=0.1023, 
[2025-09-12 21:12:30,312][__main__][INFO] - Test, Round 019: loss=2.3051, accuracy=0.1022, 
[2025-09-12 21:12:30,312][__main__][INFO] - Test, Round 020: loss=2.3051, accuracy=0.1016, 
[2025-09-12 21:12:30,312][__main__][INFO] - Test, Round 021: loss=2.3050, accuracy=0.1022, 
[2025-09-12 21:12:30,312][__main__][INFO] - Test, Round 022: loss=2.3050, accuracy=0.1025, 
[2025-09-12 21:12:30,312][__main__][INFO] - Test, Round 023: loss=2.3050, accuracy=0.1032, 
[2025-09-12 21:12:30,312][__main__][INFO] - Test, Round 024: loss=2.3049, accuracy=0.1036, 
[2025-09-12 21:12:30,312][__main__][INFO] - Test, Round 025: loss=2.3049, accuracy=0.1039, 
[2025-09-12 21:12:30,312][__main__][INFO] - Test, Round 026: loss=2.3049, accuracy=0.1042, 
[2025-09-12 21:12:30,312][__main__][INFO] - Test, Round 027: loss=2.3048, accuracy=0.1048, 
[2025-09-12 21:12:30,312][__main__][INFO] - Test, Round 028: loss=2.3048, accuracy=0.1052, 
[2025-09-12 21:12:30,312][__main__][INFO] - Test, Round 029: loss=2.3048, accuracy=0.1053, 
[2025-09-12 21:12:30,312][__main__][INFO] - Test, Round 030: loss=2.3047, accuracy=0.1057, 
[2025-09-12 21:12:30,312][__main__][INFO] - Test, Round 031: loss=2.3047, accuracy=0.1058, 
[2025-09-12 21:12:30,312][__main__][INFO] - Test, Round 032: loss=2.3047, accuracy=0.1064, 
[2025-09-12 21:12:30,312][__main__][INFO] - Test, Round 033: loss=2.3046, accuracy=0.1068, 
[2025-09-12 21:12:30,312][__main__][INFO] - Test, Round 034: loss=2.3046, accuracy=0.1071, 
[2025-09-12 21:12:30,312][__main__][INFO] - Test, Round 035: loss=2.3046, accuracy=0.1074, 
[2025-09-12 21:12:30,312][__main__][INFO] - Test, Round 036: loss=2.3045, accuracy=0.1074, 
[2025-09-12 21:12:30,312][__main__][INFO] - Test, Round 037: loss=2.3045, accuracy=0.1079, 
[2025-09-12 21:12:30,312][__main__][INFO] - Test, Round 038: loss=2.3045, accuracy=0.1083, 
[2025-09-12 21:12:30,312][__main__][INFO] - Test, Round 039: loss=2.3044, accuracy=0.1091, 
[2025-09-12 21:12:30,312][__main__][INFO] - Test, Round 040: loss=2.3044, accuracy=0.1096, 
[2025-09-12 21:12:30,312][__main__][INFO] - Test, Round 041: loss=2.3044, accuracy=0.1097, 
[2025-09-12 21:12:30,312][__main__][INFO] - Test, Round 042: loss=2.3043, accuracy=0.1104, 
[2025-09-12 21:12:30,312][__main__][INFO] - Test, Round 043: loss=2.3043, accuracy=0.1107, 
[2025-09-12 21:12:30,312][__main__][INFO] - Test, Round 044: loss=2.3043, accuracy=0.1114, 
[2025-09-12 21:12:30,312][__main__][INFO] - Test, Round 045: loss=2.3042, accuracy=0.1109, 
[2025-09-12 21:12:30,312][__main__][INFO] - Test, Round 046: loss=2.3042, accuracy=0.1110, 
[2025-09-12 21:12:30,312][__main__][INFO] - Test, Round 047: loss=2.3042, accuracy=0.1109, 
[2025-09-12 21:12:30,312][__main__][INFO] - Test, Round 048: loss=2.3041, accuracy=0.1113, 
[2025-09-12 21:12:30,312][__main__][INFO] - Test, Round 049: loss=2.3041, accuracy=0.1118, 
[2025-09-12 21:12:30,312][__main__][INFO] - Test, Round 050: loss=2.3041, accuracy=0.1108, 
[2025-09-12 21:12:30,312][__main__][INFO] - Test, Round 051: loss=2.3040, accuracy=0.1116, 
[2025-09-12 21:12:30,312][__main__][INFO] - Test, Round 052: loss=2.3040, accuracy=0.1116, 
[2025-09-12 21:12:30,312][__main__][INFO] - Test, Round 053: loss=2.3040, accuracy=0.1120, 
[2025-09-12 21:12:30,312][__main__][INFO] - Test, Round 054: loss=2.3039, accuracy=0.1117, 
[2025-09-12 21:12:30,312][__main__][INFO] - Test, Round 055: loss=2.3039, accuracy=0.1113, 
[2025-09-12 21:12:30,312][__main__][INFO] - Test, Round 056: loss=2.3039, accuracy=0.1108, 
[2025-09-12 21:12:30,312][__main__][INFO] - Test, Round 057: loss=2.3038, accuracy=0.1109, 
[2025-09-12 21:12:30,312][__main__][INFO] - Test, Round 058: loss=2.3038, accuracy=0.1111, 
[2025-09-12 21:12:30,312][__main__][INFO] - Test, Round 059: loss=2.3038, accuracy=0.1107, 
[2025-09-12 21:12:30,313][__main__][INFO] - Test, Round 060: loss=2.3037, accuracy=0.1104, 
[2025-09-12 21:12:30,313][__main__][INFO] - Test, Round 061: loss=2.3037, accuracy=0.1105, 
[2025-09-12 21:12:30,313][__main__][INFO] - Test, Round 062: loss=2.3037, accuracy=0.1107, 
[2025-09-12 21:12:30,313][__main__][INFO] - Test, Round 063: loss=2.3036, accuracy=0.1105, 
[2025-09-12 21:12:30,313][__main__][INFO] - Test, Round 064: loss=2.3036, accuracy=0.1108, 
[2025-09-12 21:12:30,313][__main__][INFO] - Test, Round 065: loss=2.3036, accuracy=0.1109, 
[2025-09-12 21:12:30,313][__main__][INFO] - Test, Round 066: loss=2.3035, accuracy=0.1109, 
[2025-09-12 21:12:30,313][__main__][INFO] - Test, Round 067: loss=2.3035, accuracy=0.1107, 
[2025-09-12 21:12:30,313][__main__][INFO] - Test, Round 068: loss=2.3035, accuracy=0.1109, 
[2025-09-12 21:12:30,313][__main__][INFO] - Test, Round 069: loss=2.3034, accuracy=0.1108, 
[2025-09-12 21:12:30,313][__main__][INFO] - Test, Round 070: loss=2.3034, accuracy=0.1109, 
[2025-09-12 21:12:30,313][__main__][INFO] - Test, Round 071: loss=2.3034, accuracy=0.1110, 
[2025-09-12 21:12:30,313][__main__][INFO] - Test, Round 072: loss=2.3034, accuracy=0.1116, 
[2025-09-12 21:12:30,313][__main__][INFO] - Test, Round 073: loss=2.3033, accuracy=0.1115, 
[2025-09-12 21:12:30,313][__main__][INFO] - Test, Round 074: loss=2.3033, accuracy=0.1116, 
[2025-09-12 21:12:30,313][__main__][INFO] - Test, Round 075: loss=2.3033, accuracy=0.1121, 
[2025-09-12 21:12:30,313][__main__][INFO] - Test, Round 076: loss=2.3032, accuracy=0.1128, 
[2025-09-12 21:12:30,313][__main__][INFO] - Test, Round 077: loss=2.3032, accuracy=0.1133, 
[2025-09-12 21:12:30,313][__main__][INFO] - Test, Round 078: loss=2.3032, accuracy=0.1133, 
[2025-09-12 21:12:30,313][__main__][INFO] - Test, Round 079: loss=2.3031, accuracy=0.1135, 
[2025-09-12 21:12:30,313][__main__][INFO] - Test, Round 080: loss=2.3031, accuracy=0.1140, 
[2025-09-12 21:12:30,313][__main__][INFO] - Test, Round 081: loss=2.3031, accuracy=0.1140, 
[2025-09-12 21:12:30,313][__main__][INFO] - Test, Round 082: loss=2.3030, accuracy=0.1142, 
[2025-09-12 21:12:30,313][__main__][INFO] - Test, Round 083: loss=2.3030, accuracy=0.1140, 
[2025-09-12 21:12:30,313][__main__][INFO] - Test, Round 084: loss=2.3030, accuracy=0.1143, 
[2025-09-12 21:12:30,313][__main__][INFO] - Test, Round 085: loss=2.3029, accuracy=0.1140, 
[2025-09-12 21:12:30,313][__main__][INFO] - Test, Round 086: loss=2.3029, accuracy=0.1138, 
[2025-09-12 21:12:30,313][__main__][INFO] - Test, Round 087: loss=2.3029, accuracy=0.1139, 
[2025-09-12 21:12:30,313][__main__][INFO] - Test, Round 088: loss=2.3029, accuracy=0.1142, 
[2025-09-12 21:12:30,313][__main__][INFO] - Test, Round 089: loss=2.3028, accuracy=0.1145, 
[2025-09-12 21:12:30,313][__main__][INFO] - Test, Round 090: loss=2.3028, accuracy=0.1145, 
[2025-09-12 21:12:30,313][__main__][INFO] - Test, Round 091: loss=2.3028, accuracy=0.1146, 
[2025-09-12 21:12:30,313][__main__][INFO] - Test, Round 092: loss=2.3027, accuracy=0.1148, 
[2025-09-12 21:12:30,313][__main__][INFO] - Test, Round 093: loss=2.3027, accuracy=0.1153, 
[2025-09-12 21:12:30,313][__main__][INFO] - Test, Round 094: loss=2.3027, accuracy=0.1154, 
[2025-09-12 21:12:30,313][__main__][INFO] - Test, Round 095: loss=2.3026, accuracy=0.1154, 
[2025-09-12 21:12:30,313][__main__][INFO] - Test, Round 096: loss=2.3026, accuracy=0.1156, 
[2025-09-12 21:12:30,313][__main__][INFO] - Test, Round 097: loss=2.3026, accuracy=0.1157, 
[2025-09-12 21:12:30,313][__main__][INFO] - Test, Round 098: loss=2.3025, accuracy=0.1158, 
[2025-09-12 21:12:30,313][__main__][INFO] - Test, Round 099: loss=2.3025, accuracy=0.1161, 
[2025-09-12 21:12:30,313][__main__][INFO] - Test, Round 100: loss=2.3025, accuracy=0.1165, 
[2025-09-12 21:12:30,313][__main__][INFO] - Test, Round 101: loss=2.3025, accuracy=0.1171, 
[2025-09-12 21:12:30,313][__main__][INFO] - Test, Round 102: loss=2.3024, accuracy=0.1171, 
[2025-09-12 21:12:30,313][__main__][INFO] - Test, Round 103: loss=2.3024, accuracy=0.1172, 
[2025-09-12 21:12:30,313][__main__][INFO] - Test, Round 104: loss=2.3024, accuracy=0.1173, 
[2025-09-12 21:12:30,313][__main__][INFO] - Test, Round 105: loss=2.3023, accuracy=0.1173, 
[2025-09-12 21:12:30,313][__main__][INFO] - Test, Round 106: loss=2.3023, accuracy=0.1176, 
[2025-09-12 21:12:30,313][__main__][INFO] - Test, Round 107: loss=2.3023, accuracy=0.1178, 
[2025-09-12 21:12:30,313][__main__][INFO] - Test, Round 108: loss=2.3022, accuracy=0.1183, 
[2025-09-12 21:12:30,313][__main__][INFO] - Test, Round 109: loss=2.3022, accuracy=0.1186, 
[2025-09-12 21:12:30,313][__main__][INFO] - Test, Round 110: loss=2.3022, accuracy=0.1186, 
[2025-09-12 21:12:30,313][__main__][INFO] - Test, Round 111: loss=2.3022, accuracy=0.1188, 
[2025-09-12 21:12:30,313][__main__][INFO] - Test, Round 112: loss=2.3021, accuracy=0.1191, 
[2025-09-12 21:12:30,313][__main__][INFO] - Test, Round 113: loss=2.3021, accuracy=0.1190, 
[2025-09-12 21:12:30,313][__main__][INFO] - Test, Round 114: loss=2.3021, accuracy=0.1194, 
[2025-09-12 21:12:30,313][__main__][INFO] - Test, Round 115: loss=2.3020, accuracy=0.1201, 
[2025-09-12 21:12:30,313][__main__][INFO] - Test, Round 116: loss=2.3020, accuracy=0.1203, 
[2025-09-12 21:12:30,313][__main__][INFO] - Test, Round 117: loss=2.3020, accuracy=0.1205, 
[2025-09-12 21:12:30,313][__main__][INFO] - Test, Round 118: loss=2.3019, accuracy=0.1207, 
[2025-09-12 21:12:30,313][__main__][INFO] - Test, Round 119: loss=2.3019, accuracy=0.1209, 
[2025-09-12 21:12:30,313][__main__][INFO] - Test, Round 120: loss=2.3019, accuracy=0.1214, 
[2025-09-12 21:12:30,313][__main__][INFO] - Test, Round 121: loss=2.3019, accuracy=0.1222, 
[2025-09-12 21:12:30,313][__main__][INFO] - Test, Round 122: loss=2.3018, accuracy=0.1224, 
[2025-09-12 21:12:30,313][__main__][INFO] - Test, Round 123: loss=2.3018, accuracy=0.1222, 
[2025-09-12 21:12:30,313][__main__][INFO] - Test, Round 124: loss=2.3018, accuracy=0.1220, 
[2025-09-12 21:12:30,313][__main__][INFO] - Test, Round 125: loss=2.3017, accuracy=0.1219, 
[2025-09-12 21:12:30,313][__main__][INFO] - Test, Round 126: loss=2.3017, accuracy=0.1219, 
[2025-09-12 21:12:30,313][__main__][INFO] - Test, Round 127: loss=2.3017, accuracy=0.1217, 
[2025-09-12 21:12:30,313][__main__][INFO] - Test, Round 128: loss=2.3016, accuracy=0.1209, 
[2025-09-12 21:12:30,313][__main__][INFO] - Test, Round 129: loss=2.3016, accuracy=0.1207, 
[2025-09-12 21:12:30,313][__main__][INFO] - Test, Round 130: loss=2.3016, accuracy=0.1206, 
[2025-09-12 21:12:30,313][__main__][INFO] - Test, Round 131: loss=2.3016, accuracy=0.1206, 
[2025-09-12 21:12:30,313][__main__][INFO] - Test, Round 132: loss=2.3015, accuracy=0.1205, 
[2025-09-12 21:12:30,313][__main__][INFO] - Test, Round 133: loss=2.3015, accuracy=0.1208, 
[2025-09-12 21:12:30,313][__main__][INFO] - Test, Round 134: loss=2.3015, accuracy=0.1207, 
[2025-09-12 21:12:30,314][__main__][INFO] - Test, Round 135: loss=2.3014, accuracy=0.1209, 
[2025-09-12 21:12:30,314][__main__][INFO] - Test, Round 136: loss=2.3014, accuracy=0.1210, 
[2025-09-12 21:12:30,314][__main__][INFO] - Test, Round 137: loss=2.3014, accuracy=0.1211, 
[2025-09-12 21:12:30,314][__main__][INFO] - Test, Round 138: loss=2.3014, accuracy=0.1212, 
[2025-09-12 21:12:30,314][__main__][INFO] - Test, Round 139: loss=2.3013, accuracy=0.1215, 
[2025-09-12 21:12:30,314][__main__][INFO] - Test, Round 140: loss=2.3013, accuracy=0.1218, 
[2025-09-12 21:12:30,314][__main__][INFO] - Test, Round 141: loss=2.3013, accuracy=0.1218, 
[2025-09-12 21:12:30,314][__main__][INFO] - Test, Round 142: loss=2.3012, accuracy=0.1219, 
[2025-09-12 21:12:30,314][__main__][INFO] - Test, Round 143: loss=2.3012, accuracy=0.1217, 
[2025-09-12 21:12:30,314][__main__][INFO] - Test, Round 144: loss=2.3012, accuracy=0.1226, 
[2025-09-12 21:12:30,314][__main__][INFO] - Test, Round 145: loss=2.3012, accuracy=0.1225, 
[2025-09-12 21:12:30,314][__main__][INFO] - Test, Round 146: loss=2.3011, accuracy=0.1225, 
[2025-09-12 21:12:30,314][__main__][INFO] - Test, Round 147: loss=2.3011, accuracy=0.1222, 
[2025-09-12 21:12:30,314][__main__][INFO] - Test, Round 148: loss=2.3011, accuracy=0.1220, 
[2025-09-12 21:12:30,314][__main__][INFO] - Test, Round 149: loss=2.3010, accuracy=0.1219, 
[2025-09-12 21:12:30,314][__main__][INFO] - Test, Round 150: loss=2.3010, accuracy=0.1222, 
[2025-09-12 21:12:30,314][__main__][INFO] - Test, Round 151: loss=2.3010, accuracy=0.1227, 
[2025-09-12 21:12:30,314][__main__][INFO] - Test, Round 152: loss=2.3010, accuracy=0.1226, 
[2025-09-12 21:12:30,314][__main__][INFO] - Test, Round 153: loss=2.3009, accuracy=0.1229, 
[2025-09-12 21:12:30,314][__main__][INFO] - Test, Round 154: loss=2.3009, accuracy=0.1225, 
[2025-09-12 21:12:30,314][__main__][INFO] - Test, Round 155: loss=2.3009, accuracy=0.1226, 
[2025-09-12 21:12:30,314][__main__][INFO] - Test, Round 156: loss=2.3008, accuracy=0.1230, 
[2025-09-12 21:12:30,314][__main__][INFO] - Test, Round 157: loss=2.3008, accuracy=0.1230, 
[2025-09-12 21:12:30,314][__main__][INFO] - Test, Round 158: loss=2.3008, accuracy=0.1229, 
[2025-09-12 21:12:30,314][__main__][INFO] - Test, Round 159: loss=2.3008, accuracy=0.1231, 
[2025-09-12 21:12:30,314][__main__][INFO] - Test, Round 160: loss=2.3007, accuracy=0.1236, 
[2025-09-12 21:12:30,314][__main__][INFO] - Test, Round 161: loss=2.3007, accuracy=0.1234, 
[2025-09-12 21:12:30,314][__main__][INFO] - Test, Round 162: loss=2.3007, accuracy=0.1235, 
[2025-09-12 21:12:30,314][__main__][INFO] - Test, Round 163: loss=2.3006, accuracy=0.1233, 
[2025-09-12 21:12:30,314][__main__][INFO] - Test, Round 164: loss=2.3006, accuracy=0.1234, 
[2025-09-12 21:12:30,314][__main__][INFO] - Test, Round 165: loss=2.3006, accuracy=0.1233, 
[2025-09-12 21:12:30,314][__main__][INFO] - Test, Round 166: loss=2.3006, accuracy=0.1231, 
[2025-09-12 21:12:30,314][__main__][INFO] - Test, Round 167: loss=2.3005, accuracy=0.1232, 
[2025-09-12 21:12:30,314][__main__][INFO] - Test, Round 168: loss=2.3005, accuracy=0.1233, 
[2025-09-12 21:12:30,314][__main__][INFO] - Test, Round 169: loss=2.3005, accuracy=0.1235, 
[2025-09-12 21:12:30,314][__main__][INFO] - Test, Round 170: loss=2.3004, accuracy=0.1236, 
[2025-09-12 21:12:30,314][__main__][INFO] - Test, Round 171: loss=2.3004, accuracy=0.1234, 
[2025-09-12 21:12:30,314][__main__][INFO] - Test, Round 172: loss=2.3004, accuracy=0.1235, 
[2025-09-12 21:12:30,314][__main__][INFO] - Test, Round 173: loss=2.3004, accuracy=0.1237, 
[2025-09-12 21:12:30,314][__main__][INFO] - Test, Round 174: loss=2.3003, accuracy=0.1239, 
[2025-09-12 21:12:30,314][__main__][INFO] - Test, Round 175: loss=2.3003, accuracy=0.1237, 
[2025-09-12 21:12:30,314][__main__][INFO] - Test, Round 176: loss=2.3003, accuracy=0.1236, 
[2025-09-12 21:12:30,314][__main__][INFO] - Test, Round 177: loss=2.3002, accuracy=0.1234, 
[2025-09-12 21:12:30,314][__main__][INFO] - Test, Round 178: loss=2.3002, accuracy=0.1235, 
[2025-09-12 21:12:30,314][__main__][INFO] - Test, Round 179: loss=2.3002, accuracy=0.1233, 
[2025-09-12 21:12:30,314][__main__][INFO] - Test, Round 180: loss=2.3002, accuracy=0.1232, 
[2025-09-12 21:12:30,314][__main__][INFO] - Test, Round 181: loss=2.3001, accuracy=0.1233, 
[2025-09-12 21:12:30,314][__main__][INFO] - Test, Round 182: loss=2.3001, accuracy=0.1235, 
[2025-09-12 21:12:30,314][__main__][INFO] - Test, Round 183: loss=2.3001, accuracy=0.1236, 
[2025-09-12 21:12:30,314][__main__][INFO] - Test, Round 184: loss=2.3000, accuracy=0.1236, 
[2025-09-12 21:12:30,314][__main__][INFO] - Test, Round 185: loss=2.3000, accuracy=0.1237, 
[2025-09-12 21:12:30,314][__main__][INFO] - Test, Round 186: loss=2.3000, accuracy=0.1238, 
[2025-09-12 21:12:30,314][__main__][INFO] - Test, Round 187: loss=2.3000, accuracy=0.1239, 
[2025-09-12 21:12:30,314][__main__][INFO] - Test, Round 188: loss=2.2999, accuracy=0.1241, 
[2025-09-12 21:12:30,314][__main__][INFO] - Test, Round 189: loss=2.2999, accuracy=0.1243, 
[2025-09-12 21:12:30,314][__main__][INFO] - Test, Round 190: loss=2.2999, accuracy=0.1245, 
[2025-09-12 21:12:30,314][__main__][INFO] - Test, Round 191: loss=2.2998, accuracy=0.1243, 
[2025-09-12 21:12:30,314][__main__][INFO] - Test, Round 192: loss=2.2998, accuracy=0.1246, 
[2025-09-12 21:12:30,314][__main__][INFO] - Test, Round 193: loss=2.2998, accuracy=0.1244, 
[2025-09-12 21:12:30,314][__main__][INFO] - Test, Round 194: loss=2.2998, accuracy=0.1240, 
[2025-09-12 21:12:30,314][__main__][INFO] - Test, Round 195: loss=2.2997, accuracy=0.1244, 
[2025-09-12 21:12:30,314][__main__][INFO] - Test, Round 196: loss=2.2997, accuracy=0.1249, 
[2025-09-12 21:12:30,314][__main__][INFO] - Test, Round 197: loss=2.2997, accuracy=0.1248, 
[2025-09-12 21:12:30,314][__main__][INFO] - Test, Round 198: loss=2.2996, accuracy=0.1250, 
[2025-09-12 21:12:30,314][__main__][INFO] - Test, Round 199: loss=2.2996, accuracy=0.1248, 
[2025-09-12 21:12:30,314][__main__][INFO] - Test, Round 200: loss=2.2996, accuracy=0.1245, 
[2025-09-12 21:12:30,314][__main__][INFO] - Test, Round 201: loss=2.2996, accuracy=0.1242, 
[2025-09-12 21:12:30,314][__main__][INFO] - Test, Round 202: loss=2.2995, accuracy=0.1244, 
[2025-09-12 21:12:30,314][__main__][INFO] - Test, Round 203: loss=2.2995, accuracy=0.1243, 
[2025-09-12 21:12:30,314][__main__][INFO] - Test, Round 204: loss=2.2995, accuracy=0.1243, 
[2025-09-12 21:12:30,314][__main__][INFO] - Test, Round 205: loss=2.2994, accuracy=0.1244, 
[2025-09-12 21:12:30,314][__main__][INFO] - Test, Round 206: loss=2.2994, accuracy=0.1241, 
[2025-09-12 21:12:30,314][__main__][INFO] - Test, Round 207: loss=2.2994, accuracy=0.1243, 
[2025-09-12 21:12:30,314][__main__][INFO] - Test, Round 208: loss=2.2994, accuracy=0.1245, 
[2025-09-12 21:12:30,314][__main__][INFO] - Test, Round 209: loss=2.2993, accuracy=0.1249, 
[2025-09-12 21:12:30,314][__main__][INFO] - Test, Round 210: loss=2.2993, accuracy=0.1252, 
[2025-09-12 21:12:30,315][__main__][INFO] - Test, Round 211: loss=2.2993, accuracy=0.1253, 
[2025-09-12 21:12:30,315][__main__][INFO] - Test, Round 212: loss=2.2993, accuracy=0.1257, 
[2025-09-12 21:12:30,315][__main__][INFO] - Test, Round 213: loss=2.2992, accuracy=0.1259, 
[2025-09-12 21:12:30,315][__main__][INFO] - Test, Round 214: loss=2.2992, accuracy=0.1259, 
[2025-09-12 21:12:30,315][__main__][INFO] - Test, Round 215: loss=2.2992, accuracy=0.1263, 
[2025-09-12 21:12:30,315][__main__][INFO] - Test, Round 216: loss=2.2991, accuracy=0.1263, 
[2025-09-12 21:12:30,315][__main__][INFO] - Test, Round 217: loss=2.2991, accuracy=0.1264, 
[2025-09-12 21:12:30,315][__main__][INFO] - Test, Round 218: loss=2.2991, accuracy=0.1263, 
[2025-09-12 21:12:30,315][__main__][INFO] - Test, Round 219: loss=2.2991, accuracy=0.1257, 
[2025-09-12 21:12:30,315][__main__][INFO] - Test, Round 220: loss=2.2990, accuracy=0.1258, 
[2025-09-12 21:12:30,315][__main__][INFO] - Test, Round 221: loss=2.2990, accuracy=0.1261, 
[2025-09-12 21:12:30,315][__main__][INFO] - Test, Round 222: loss=2.2990, accuracy=0.1259, 
[2025-09-12 21:12:30,315][__main__][INFO] - Test, Round 223: loss=2.2989, accuracy=0.1260, 
[2025-09-12 21:12:30,315][__main__][INFO] - Test, Round 224: loss=2.2989, accuracy=0.1261, 
[2025-09-12 21:12:30,315][__main__][INFO] - Test, Round 225: loss=2.2989, accuracy=0.1261, 
[2025-09-12 21:12:30,315][__main__][INFO] - Test, Round 226: loss=2.2989, accuracy=0.1263, 
[2025-09-12 21:12:30,315][__main__][INFO] - Test, Round 227: loss=2.2988, accuracy=0.1264, 
[2025-09-12 21:12:30,315][__main__][INFO] - Test, Round 228: loss=2.2988, accuracy=0.1263, 
[2025-09-12 21:12:30,315][__main__][INFO] - Test, Round 229: loss=2.2988, accuracy=0.1264, 
[2025-09-12 21:12:30,315][__main__][INFO] - Test, Round 230: loss=2.2987, accuracy=0.1264, 
[2025-09-12 21:12:30,315][__main__][INFO] - Test, Round 231: loss=2.2987, accuracy=0.1263, 
[2025-09-12 21:12:30,315][__main__][INFO] - Test, Round 232: loss=2.2987, accuracy=0.1264, 
[2025-09-12 21:12:30,315][__main__][INFO] - Test, Round 233: loss=2.2987, accuracy=0.1265, 
[2025-09-12 21:12:30,315][__main__][INFO] - Test, Round 234: loss=2.2986, accuracy=0.1266, 
[2025-09-12 21:12:30,315][__main__][INFO] - Test, Round 235: loss=2.2986, accuracy=0.1267, 
[2025-09-12 21:12:30,315][__main__][INFO] - Test, Round 236: loss=2.2986, accuracy=0.1272, 
[2025-09-12 21:12:30,315][__main__][INFO] - Test, Round 237: loss=2.2985, accuracy=0.1274, 
[2025-09-12 21:12:30,315][__main__][INFO] - Test, Round 238: loss=2.2985, accuracy=0.1275, 
[2025-09-12 21:12:30,315][__main__][INFO] - Test, Round 239: loss=2.2985, accuracy=0.1273, 
[2025-09-12 21:12:30,315][__main__][INFO] - Test, Round 240: loss=2.2985, accuracy=0.1279, 
[2025-09-12 21:12:30,315][__main__][INFO] - Test, Round 241: loss=2.2984, accuracy=0.1281, 
[2025-09-12 21:12:30,315][__main__][INFO] - Test, Round 242: loss=2.2984, accuracy=0.1284, 
[2025-09-12 21:12:30,315][__main__][INFO] - Test, Round 243: loss=2.2984, accuracy=0.1284, 
[2025-09-12 21:12:30,315][__main__][INFO] - Test, Round 244: loss=2.2984, accuracy=0.1284, 
[2025-09-12 21:12:30,315][__main__][INFO] - Test, Round 245: loss=2.2983, accuracy=0.1286, 
[2025-09-12 21:12:30,315][__main__][INFO] - Test, Round 246: loss=2.2983, accuracy=0.1284, 
[2025-09-12 21:12:30,315][__main__][INFO] - Test, Round 247: loss=2.2983, accuracy=0.1285, 
[2025-09-12 21:12:30,315][__main__][INFO] - Test, Round 248: loss=2.2982, accuracy=0.1284, 
[2025-09-12 21:12:30,315][__main__][INFO] - Test, Round 249: loss=2.2982, accuracy=0.1284, 
[2025-09-12 21:12:30,315][__main__][INFO] - Test, Round 250: loss=2.2982, accuracy=0.1284, 
[2025-09-12 21:12:30,315][__main__][INFO] - Test, Round 251: loss=2.2982, accuracy=0.1284, 
[2025-09-12 21:12:30,315][__main__][INFO] - Test, Round 252: loss=2.2981, accuracy=0.1286, 
[2025-09-12 21:12:30,315][__main__][INFO] - Test, Round 253: loss=2.2981, accuracy=0.1287, 
[2025-09-12 21:12:30,315][__main__][INFO] - Test, Round 254: loss=2.2981, accuracy=0.1286, 
[2025-09-12 21:12:30,315][__main__][INFO] - Test, Round 255: loss=2.2980, accuracy=0.1286, 
[2025-09-12 21:12:30,315][__main__][INFO] - Test, Round 256: loss=2.2980, accuracy=0.1286, 
[2025-09-12 21:12:30,315][__main__][INFO] - Test, Round 257: loss=2.2980, accuracy=0.1284, 
[2025-09-12 21:12:30,315][__main__][INFO] - Test, Round 258: loss=2.2980, accuracy=0.1284, 
[2025-09-12 21:12:30,315][__main__][INFO] - Test, Round 259: loss=2.2979, accuracy=0.1286, 
[2025-09-12 21:12:30,315][__main__][INFO] - Test, Round 260: loss=2.2979, accuracy=0.1286, 
[2025-09-12 21:12:30,315][__main__][INFO] - Test, Round 261: loss=2.2979, accuracy=0.1285, 
[2025-09-12 21:12:30,315][__main__][INFO] - Test, Round 262: loss=2.2978, accuracy=0.1282, 
[2025-09-12 21:12:30,315][__main__][INFO] - Test, Round 263: loss=2.2978, accuracy=0.1281, 
[2025-09-12 21:12:30,315][__main__][INFO] - Test, Round 264: loss=2.2978, accuracy=0.1283, 
[2025-09-12 21:12:30,315][__main__][INFO] - Test, Round 265: loss=2.2978, accuracy=0.1281, 
[2025-09-12 21:12:30,315][__main__][INFO] - Test, Round 266: loss=2.2977, accuracy=0.1283, 
[2025-09-12 21:12:30,315][__main__][INFO] - Test, Round 267: loss=2.2977, accuracy=0.1284, 
[2025-09-12 21:12:30,315][__main__][INFO] - Test, Round 268: loss=2.2977, accuracy=0.1283, 
[2025-09-12 21:12:30,315][__main__][INFO] - Test, Round 269: loss=2.2976, accuracy=0.1284, 
[2025-09-12 21:12:30,315][__main__][INFO] - Test, Round 270: loss=2.2976, accuracy=0.1286, 
[2025-09-12 21:12:30,315][__main__][INFO] - Test, Round 271: loss=2.2976, accuracy=0.1288, 
[2025-09-12 21:12:30,315][__main__][INFO] - Test, Round 272: loss=2.2976, accuracy=0.1286, 
[2025-09-12 21:12:30,315][__main__][INFO] - Test, Round 273: loss=2.2975, accuracy=0.1287, 
[2025-09-12 21:12:30,315][__main__][INFO] - Test, Round 274: loss=2.2975, accuracy=0.1289, 
[2025-09-12 21:12:30,315][__main__][INFO] - Test, Round 275: loss=2.2975, accuracy=0.1288, 
[2025-09-12 21:12:30,315][__main__][INFO] - Test, Round 276: loss=2.2974, accuracy=0.1287, 
[2025-09-12 21:12:30,315][__main__][INFO] - Test, Round 277: loss=2.2974, accuracy=0.1289, 
[2025-09-12 21:12:30,315][__main__][INFO] - Test, Round 278: loss=2.2974, accuracy=0.1289, 
[2025-09-12 21:12:30,315][__main__][INFO] - Test, Round 279: loss=2.2974, accuracy=0.1287, 
[2025-09-12 21:12:30,315][__main__][INFO] - Test, Round 280: loss=2.2973, accuracy=0.1285, 
[2025-09-12 21:12:30,315][__main__][INFO] - Test, Round 281: loss=2.2973, accuracy=0.1286, 
[2025-09-12 21:12:30,315][__main__][INFO] - Test, Round 282: loss=2.2973, accuracy=0.1286, 
[2025-09-12 21:12:30,315][__main__][INFO] - Test, Round 283: loss=2.2972, accuracy=0.1286, 
[2025-09-12 21:12:30,315][__main__][INFO] - Test, Round 284: loss=2.2972, accuracy=0.1290, 
[2025-09-12 21:12:30,315][__main__][INFO] - Test, Round 285: loss=2.2972, accuracy=0.1291, 
[2025-09-12 21:12:30,315][__main__][INFO] - Test, Round 286: loss=2.2972, accuracy=0.1292, 
[2025-09-12 21:12:30,316][__main__][INFO] - Test, Round 287: loss=2.2971, accuracy=0.1292, 
[2025-09-12 21:12:30,316][__main__][INFO] - Test, Round 288: loss=2.2971, accuracy=0.1292, 
[2025-09-12 21:12:30,316][__main__][INFO] - Test, Round 289: loss=2.2971, accuracy=0.1294, 
[2025-09-12 21:12:30,316][__main__][INFO] - Test, Round 290: loss=2.2970, accuracy=0.1296, 
[2025-09-12 21:12:30,316][__main__][INFO] - Test, Round 291: loss=2.2970, accuracy=0.1295, 
[2025-09-12 21:12:30,316][__main__][INFO] - Test, Round 292: loss=2.2970, accuracy=0.1296, 
[2025-09-12 21:12:30,316][__main__][INFO] - Test, Round 293: loss=2.2969, accuracy=0.1297, 
[2025-09-12 21:12:30,316][__main__][INFO] - Test, Round 294: loss=2.2969, accuracy=0.1299, 
[2025-09-12 21:12:30,316][__main__][INFO] - Test, Round 295: loss=2.2969, accuracy=0.1298, 
[2025-09-12 21:12:30,316][__main__][INFO] - Test, Round 296: loss=2.2969, accuracy=0.1296, 
[2025-09-12 21:12:30,316][__main__][INFO] - Test, Round 297: loss=2.2968, accuracy=0.1294, 
[2025-09-12 21:12:30,316][__main__][INFO] - Test, Round 298: loss=2.2968, accuracy=0.1294, 
[2025-09-12 21:12:30,316][__main__][INFO] - Test, Round 299: loss=2.2968, accuracy=0.1293, 
[2025-09-12 21:12:30,316][__main__][INFO] - Test, Round 300: loss=2.2967, accuracy=0.1292, 
[2025-09-12 21:12:30,316][__main__][INFO] - Test, Round 301: loss=2.2967, accuracy=0.1294, 
[2025-09-12 21:12:30,316][__main__][INFO] - Test, Round 302: loss=2.2967, accuracy=0.1292, 
[2025-09-12 21:12:30,316][__main__][INFO] - Test, Round 303: loss=2.2967, accuracy=0.1294, 
[2025-09-12 21:12:30,316][__main__][INFO] - Test, Round 304: loss=2.2966, accuracy=0.1295, 
[2025-09-12 21:12:30,316][__main__][INFO] - Test, Round 305: loss=2.2966, accuracy=0.1296, 
[2025-09-12 21:12:30,316][__main__][INFO] - Test, Round 306: loss=2.2966, accuracy=0.1298, 
[2025-09-12 21:12:30,316][__main__][INFO] - Test, Round 307: loss=2.2965, accuracy=0.1298, 
[2025-09-12 21:12:30,316][__main__][INFO] - Test, Round 308: loss=2.2965, accuracy=0.1296, 
[2025-09-12 21:12:30,316][__main__][INFO] - Test, Round 309: loss=2.2965, accuracy=0.1293, 
[2025-09-12 21:12:30,316][__main__][INFO] - Test, Round 310: loss=2.2964, accuracy=0.1295, 
[2025-09-12 21:12:30,316][__main__][INFO] - Test, Round 311: loss=2.2964, accuracy=0.1294, 
[2025-09-12 21:12:30,316][__main__][INFO] - Test, Round 312: loss=2.2964, accuracy=0.1295, 
[2025-09-12 21:12:30,316][__main__][INFO] - Test, Round 313: loss=2.2964, accuracy=0.1294, 
[2025-09-12 21:12:30,316][__main__][INFO] - Test, Round 314: loss=2.2963, accuracy=0.1295, 
[2025-09-12 21:12:30,316][__main__][INFO] - Test, Round 315: loss=2.2963, accuracy=0.1294, 
[2025-09-12 21:12:30,316][__main__][INFO] - Test, Round 316: loss=2.2963, accuracy=0.1295, 
[2025-09-12 21:12:30,316][__main__][INFO] - Test, Round 317: loss=2.2962, accuracy=0.1296, 
[2025-09-12 21:12:30,316][__main__][INFO] - Test, Round 318: loss=2.2962, accuracy=0.1295, 
[2025-09-12 21:12:30,316][__main__][INFO] - Test, Round 319: loss=2.2962, accuracy=0.1296, 
[2025-09-12 21:12:30,316][__main__][INFO] - Test, Round 320: loss=2.2961, accuracy=0.1295, 
[2025-09-12 21:12:30,316][__main__][INFO] - Test, Round 321: loss=2.2961, accuracy=0.1294, 
[2025-09-12 21:12:30,316][__main__][INFO] - Test, Round 322: loss=2.2961, accuracy=0.1296, 
[2025-09-12 21:12:30,316][__main__][INFO] - Test, Round 323: loss=2.2961, accuracy=0.1297, 
[2025-09-12 21:12:30,316][__main__][INFO] - Test, Round 324: loss=2.2960, accuracy=0.1297, 
[2025-09-12 21:12:30,316][__main__][INFO] - Test, Round 325: loss=2.2960, accuracy=0.1297, 
[2025-09-12 21:12:30,316][__main__][INFO] - Test, Round 326: loss=2.2960, accuracy=0.1296, 
[2025-09-12 21:12:30,316][__main__][INFO] - Test, Round 327: loss=2.2959, accuracy=0.1297, 
[2025-09-12 21:12:30,316][__main__][INFO] - Test, Round 328: loss=2.2959, accuracy=0.1298, 
[2025-09-12 21:12:30,316][__main__][INFO] - Test, Round 329: loss=2.2959, accuracy=0.1299, 
[2025-09-12 21:12:30,316][__main__][INFO] - Test, Round 330: loss=2.2958, accuracy=0.1300, 
[2025-09-12 21:12:30,316][__main__][INFO] - Test, Round 331: loss=2.2958, accuracy=0.1299, 
[2025-09-12 21:12:30,316][__main__][INFO] - Test, Round 332: loss=2.2958, accuracy=0.1298, 
[2025-09-12 21:12:30,316][__main__][INFO] - Test, Round 333: loss=2.2957, accuracy=0.1299, 
[2025-09-12 21:12:30,316][__main__][INFO] - Test, Round 334: loss=2.2957, accuracy=0.1301, 
[2025-09-12 21:12:30,316][__main__][INFO] - Test, Round 335: loss=2.2957, accuracy=0.1304, 
[2025-09-12 21:12:30,316][__main__][INFO] - Test, Round 336: loss=2.2957, accuracy=0.1305, 
[2025-09-12 21:12:30,316][__main__][INFO] - Test, Round 337: loss=2.2956, accuracy=0.1303, 
[2025-09-12 21:12:30,316][__main__][INFO] - Test, Round 338: loss=2.2956, accuracy=0.1305, 
[2025-09-12 21:12:30,316][__main__][INFO] - Test, Round 339: loss=2.2956, accuracy=0.1307, 
[2025-09-12 21:12:30,316][__main__][INFO] - Test, Round 340: loss=2.2955, accuracy=0.1310, 
[2025-09-12 21:12:30,316][__main__][INFO] - Test, Round 341: loss=2.2955, accuracy=0.1309, 
[2025-09-12 21:12:30,316][__main__][INFO] - Test, Round 342: loss=2.2955, accuracy=0.1310, 
[2025-09-12 21:12:30,316][__main__][INFO] - Test, Round 343: loss=2.2954, accuracy=0.1309, 
[2025-09-12 21:12:30,316][__main__][INFO] - Test, Round 344: loss=2.2954, accuracy=0.1307, 
[2025-09-12 21:12:30,316][__main__][INFO] - Test, Round 345: loss=2.2954, accuracy=0.1306, 
[2025-09-12 21:12:30,316][__main__][INFO] - Test, Round 346: loss=2.2953, accuracy=0.1304, 
[2025-09-12 21:12:30,316][__main__][INFO] - Test, Round 347: loss=2.2953, accuracy=0.1304, 
[2025-09-12 21:12:30,316][__main__][INFO] - Test, Round 348: loss=2.2953, accuracy=0.1304, 
[2025-09-12 21:12:30,316][__main__][INFO] - Test, Round 349: loss=2.2952, accuracy=0.1305, 
[2025-09-12 21:12:30,316][__main__][INFO] - Test, Round 350: loss=2.2952, accuracy=0.1305, 
[2025-09-12 21:12:30,316][__main__][INFO] - Test, Round 351: loss=2.2952, accuracy=0.1308, 
[2025-09-12 21:12:30,316][__main__][INFO] - Test, Round 352: loss=2.2952, accuracy=0.1307, 
[2025-09-12 21:12:30,316][__main__][INFO] - Test, Round 353: loss=2.2951, accuracy=0.1307, 
[2025-09-12 21:12:30,316][__main__][INFO] - Test, Round 354: loss=2.2951, accuracy=0.1309, 
[2025-09-12 21:12:30,316][__main__][INFO] - Test, Round 355: loss=2.2951, accuracy=0.1310, 
[2025-09-12 21:12:30,316][__main__][INFO] - Test, Round 356: loss=2.2950, accuracy=0.1312, 
[2025-09-12 21:12:30,316][__main__][INFO] - Test, Round 357: loss=2.2950, accuracy=0.1312, 
[2025-09-12 21:12:30,316][__main__][INFO] - Test, Round 358: loss=2.2950, accuracy=0.1311, 
[2025-09-12 21:12:30,316][__main__][INFO] - Test, Round 359: loss=2.2949, accuracy=0.1311, 
[2025-09-12 21:12:30,316][__main__][INFO] - Test, Round 360: loss=2.2949, accuracy=0.1310, 
[2025-09-12 21:12:30,316][__main__][INFO] - Test, Round 361: loss=2.2949, accuracy=0.1313, 
[2025-09-12 21:12:30,317][__main__][INFO] - Test, Round 362: loss=2.2948, accuracy=0.1313, 
[2025-09-12 21:12:30,317][__main__][INFO] - Test, Round 363: loss=2.2948, accuracy=0.1311, 
[2025-09-12 21:12:30,317][__main__][INFO] - Test, Round 364: loss=2.2948, accuracy=0.1310, 
[2025-09-12 21:12:30,317][__main__][INFO] - Test, Round 365: loss=2.2947, accuracy=0.1308, 
[2025-09-12 21:12:30,317][__main__][INFO] - Test, Round 366: loss=2.2947, accuracy=0.1309, 
[2025-09-12 21:12:30,317][__main__][INFO] - Test, Round 367: loss=2.2947, accuracy=0.1311, 
[2025-09-12 21:12:30,317][__main__][INFO] - Test, Round 368: loss=2.2946, accuracy=0.1309, 
[2025-09-12 21:12:30,317][__main__][INFO] - Test, Round 369: loss=2.2946, accuracy=0.1307, 
[2025-09-12 21:12:30,317][__main__][INFO] - Test, Round 370: loss=2.2946, accuracy=0.1305, 
[2025-09-12 21:12:30,317][__main__][INFO] - Test, Round 371: loss=2.2945, accuracy=0.1306, 
[2025-09-12 21:12:30,317][__main__][INFO] - Test, Round 372: loss=2.2945, accuracy=0.1303, 
[2025-09-12 21:12:30,317][__main__][INFO] - Test, Round 373: loss=2.2945, accuracy=0.1304, 
[2025-09-12 21:12:30,317][__main__][INFO] - Test, Round 374: loss=2.2944, accuracy=0.1303, 
[2025-09-12 21:12:30,317][__main__][INFO] - Test, Round 375: loss=2.2944, accuracy=0.1305, 
[2025-09-12 21:12:30,317][__main__][INFO] - Test, Round 376: loss=2.2944, accuracy=0.1304, 
[2025-09-12 21:12:30,317][__main__][INFO] - Test, Round 377: loss=2.2943, accuracy=0.1305, 
[2025-09-12 21:12:30,317][__main__][INFO] - Test, Round 378: loss=2.2943, accuracy=0.1305, 
[2025-09-12 21:12:30,317][__main__][INFO] - Test, Round 379: loss=2.2943, accuracy=0.1307, 
[2025-09-12 21:12:30,317][__main__][INFO] - Test, Round 380: loss=2.2942, accuracy=0.1308, 
[2025-09-12 21:12:30,317][__main__][INFO] - Test, Round 381: loss=2.2942, accuracy=0.1311, 
[2025-09-12 21:12:30,317][__main__][INFO] - Test, Round 382: loss=2.2942, accuracy=0.1309, 
[2025-09-12 21:12:30,317][__main__][INFO] - Test, Round 383: loss=2.2941, accuracy=0.1309, 
[2025-09-12 21:12:30,317][__main__][INFO] - Test, Round 384: loss=2.2941, accuracy=0.1308, 
[2025-09-12 21:12:30,317][__main__][INFO] - Test, Round 385: loss=2.2941, accuracy=0.1310, 
[2025-09-12 21:12:30,317][__main__][INFO] - Test, Round 386: loss=2.2940, accuracy=0.1312, 
[2025-09-12 21:12:30,317][__main__][INFO] - Test, Round 387: loss=2.2940, accuracy=0.1315, 
[2025-09-12 21:12:30,317][__main__][INFO] - Test, Round 388: loss=2.2940, accuracy=0.1316, 
[2025-09-12 21:12:30,317][__main__][INFO] - Test, Round 389: loss=2.2939, accuracy=0.1319, 
[2025-09-12 21:12:30,317][__main__][INFO] - Test, Round 390: loss=2.2939, accuracy=0.1318, 
[2025-09-12 21:12:30,317][__main__][INFO] - Test, Round 391: loss=2.2939, accuracy=0.1316, 
[2025-09-12 21:12:30,317][__main__][INFO] - Test, Round 392: loss=2.2938, accuracy=0.1318, 
[2025-09-12 21:12:30,317][__main__][INFO] - Test, Round 393: loss=2.2938, accuracy=0.1320, 
[2025-09-12 21:12:30,317][__main__][INFO] - Test, Round 394: loss=2.2938, accuracy=0.1321, 
[2025-09-12 21:12:30,317][__main__][INFO] - Test, Round 395: loss=2.2937, accuracy=0.1322, 
[2025-09-12 21:12:30,317][__main__][INFO] - Test, Round 396: loss=2.2937, accuracy=0.1323, 
[2025-09-12 21:12:30,317][__main__][INFO] - Test, Round 397: loss=2.2937, accuracy=0.1324, 
[2025-09-12 21:12:30,317][__main__][INFO] - Test, Round 398: loss=2.2936, accuracy=0.1327, 
[2025-09-12 21:12:30,317][__main__][INFO] - Test, Round 399: loss=2.2936, accuracy=0.1327, 
[2025-09-12 21:12:30,317][__main__][INFO] - Test, Round 400: loss=2.2935, accuracy=0.1331, 
[2025-09-12 21:12:30,317][__main__][INFO] - Test, Round 401: loss=2.2935, accuracy=0.1331, 
[2025-09-12 21:12:30,317][__main__][INFO] - Test, Round 402: loss=2.2935, accuracy=0.1333, 
[2025-09-12 21:12:30,317][__main__][INFO] - Test, Round 403: loss=2.2934, accuracy=0.1334, 
[2025-09-12 21:12:30,317][__main__][INFO] - Test, Round 404: loss=2.2934, accuracy=0.1335, 
[2025-09-12 21:12:30,317][__main__][INFO] - Test, Round 405: loss=2.2934, accuracy=0.1338, 
[2025-09-12 21:12:30,317][__main__][INFO] - Test, Round 406: loss=2.2933, accuracy=0.1339, 
[2025-09-12 21:12:30,317][__main__][INFO] - Test, Round 407: loss=2.2933, accuracy=0.1341, 
[2025-09-12 21:12:30,317][__main__][INFO] - Test, Round 408: loss=2.2933, accuracy=0.1341, 
[2025-09-12 21:12:30,317][__main__][INFO] - Test, Round 409: loss=2.2932, accuracy=0.1342, 
[2025-09-12 21:12:30,317][__main__][INFO] - Test, Round 410: loss=2.2932, accuracy=0.1345, 
[2025-09-12 21:12:30,317][__main__][INFO] - Test, Round 411: loss=2.2932, accuracy=0.1344, 
[2025-09-12 21:12:30,317][__main__][INFO] - Test, Round 412: loss=2.2931, accuracy=0.1344, 
[2025-09-12 21:12:30,317][__main__][INFO] - Test, Round 413: loss=2.2931, accuracy=0.1342, 
[2025-09-12 21:12:30,317][__main__][INFO] - Test, Round 414: loss=2.2930, accuracy=0.1343, 
[2025-09-12 21:12:30,317][__main__][INFO] - Test, Round 415: loss=2.2930, accuracy=0.1343, 
[2025-09-12 21:12:30,317][__main__][INFO] - Test, Round 416: loss=2.2930, accuracy=0.1343, 
[2025-09-12 21:12:30,317][__main__][INFO] - Test, Round 417: loss=2.2929, accuracy=0.1344, 
[2025-09-12 21:12:30,317][__main__][INFO] - Test, Round 418: loss=2.2929, accuracy=0.1344, 
[2025-09-12 21:12:30,317][__main__][INFO] - Test, Round 419: loss=2.2929, accuracy=0.1343, 
[2025-09-12 21:12:30,317][__main__][INFO] - Test, Round 420: loss=2.2928, accuracy=0.1343, 
[2025-09-12 21:12:30,317][__main__][INFO] - Test, Round 421: loss=2.2928, accuracy=0.1344, 
[2025-09-12 21:12:30,317][__main__][INFO] - Test, Round 422: loss=2.2927, accuracy=0.1344, 
[2025-09-12 21:12:30,317][__main__][INFO] - Test, Round 423: loss=2.2927, accuracy=0.1344, 
[2025-09-12 21:12:30,317][__main__][INFO] - Test, Round 424: loss=2.2927, accuracy=0.1343, 
[2025-09-12 21:12:30,317][__main__][INFO] - Test, Round 425: loss=2.2926, accuracy=0.1346, 
[2025-09-12 21:12:30,317][__main__][INFO] - Test, Round 426: loss=2.2926, accuracy=0.1349, 
[2025-09-12 21:12:30,317][__main__][INFO] - Test, Round 427: loss=2.2926, accuracy=0.1350, 
[2025-09-12 21:12:30,317][__main__][INFO] - Test, Round 428: loss=2.2925, accuracy=0.1355, 
[2025-09-12 21:12:30,317][__main__][INFO] - Test, Round 429: loss=2.2925, accuracy=0.1352, 
[2025-09-12 21:12:30,317][__main__][INFO] - Test, Round 430: loss=2.2924, accuracy=0.1354, 
[2025-09-12 21:12:30,317][__main__][INFO] - Test, Round 431: loss=2.2924, accuracy=0.1355, 
[2025-09-12 21:12:30,317][__main__][INFO] - Test, Round 432: loss=2.2924, accuracy=0.1355, 
[2025-09-12 21:12:30,317][__main__][INFO] - Test, Round 433: loss=2.2923, accuracy=0.1354, 
[2025-09-12 21:12:30,317][__main__][INFO] - Test, Round 434: loss=2.2923, accuracy=0.1355, 
[2025-09-12 21:12:30,317][__main__][INFO] - Test, Round 435: loss=2.2922, accuracy=0.1356, 
[2025-09-12 21:12:30,317][__main__][INFO] - Test, Round 436: loss=2.2922, accuracy=0.1358, 
[2025-09-12 21:12:30,318][__main__][INFO] - Test, Round 437: loss=2.2922, accuracy=0.1358, 
[2025-09-12 21:12:30,318][__main__][INFO] - Test, Round 438: loss=2.2921, accuracy=0.1355, 
[2025-09-12 21:12:30,318][__main__][INFO] - Test, Round 439: loss=2.2921, accuracy=0.1354, 
[2025-09-12 21:12:30,318][__main__][INFO] - Test, Round 440: loss=2.2921, accuracy=0.1356, 
[2025-09-12 21:12:30,318][__main__][INFO] - Test, Round 441: loss=2.2920, accuracy=0.1356, 
[2025-09-12 21:12:30,318][__main__][INFO] - Test, Round 442: loss=2.2920, accuracy=0.1355, 
[2025-09-12 21:12:30,318][__main__][INFO] - Test, Round 443: loss=2.2919, accuracy=0.1353, 
[2025-09-12 21:12:30,318][__main__][INFO] - Test, Round 444: loss=2.2919, accuracy=0.1354, 
[2025-09-12 21:12:30,318][__main__][INFO] - Test, Round 445: loss=2.2919, accuracy=0.1353, 
[2025-09-12 21:12:30,318][__main__][INFO] - Test, Round 446: loss=2.2918, accuracy=0.1351, 
[2025-09-12 21:12:30,318][__main__][INFO] - Test, Round 447: loss=2.2918, accuracy=0.1351, 
[2025-09-12 21:12:30,318][__main__][INFO] - Test, Round 448: loss=2.2917, accuracy=0.1351, 
[2025-09-12 21:12:30,318][__main__][INFO] - Test, Round 449: loss=2.2917, accuracy=0.1351, 
[2025-09-12 21:12:30,318][__main__][INFO] - Test, Round 450: loss=2.2916, accuracy=0.1350, 
[2025-09-12 21:12:30,318][__main__][INFO] - Test, Round 451: loss=2.2916, accuracy=0.1351, 
[2025-09-12 21:12:30,318][__main__][INFO] - Test, Round 452: loss=2.2916, accuracy=0.1356, 
[2025-09-12 21:12:30,318][__main__][INFO] - Test, Round 453: loss=2.2915, accuracy=0.1356, 
[2025-09-12 21:12:30,318][__main__][INFO] - Test, Round 454: loss=2.2915, accuracy=0.1358, 
[2025-09-12 21:12:30,318][__main__][INFO] - Test, Round 455: loss=2.2914, accuracy=0.1359, 
[2025-09-12 21:12:30,318][__main__][INFO] - Test, Round 456: loss=2.2914, accuracy=0.1358, 
[2025-09-12 21:12:30,318][__main__][INFO] - Test, Round 457: loss=2.2914, accuracy=0.1357, 
[2025-09-12 21:12:30,318][__main__][INFO] - Test, Round 458: loss=2.2913, accuracy=0.1359, 
[2025-09-12 21:12:30,318][__main__][INFO] - Test, Round 459: loss=2.2913, accuracy=0.1358, 
[2025-09-12 21:12:30,318][__main__][INFO] - Test, Round 460: loss=2.2912, accuracy=0.1358, 
[2025-09-12 21:12:30,318][__main__][INFO] - Test, Round 461: loss=2.2912, accuracy=0.1358, 
[2025-09-12 21:12:30,318][__main__][INFO] - Test, Round 462: loss=2.2911, accuracy=0.1362, 
[2025-09-12 21:12:30,318][__main__][INFO] - Test, Round 463: loss=2.2911, accuracy=0.1363, 
[2025-09-12 21:12:30,318][__main__][INFO] - Test, Round 464: loss=2.2911, accuracy=0.1366, 
[2025-09-12 21:12:30,318][__main__][INFO] - Test, Round 465: loss=2.2910, accuracy=0.1366, 
[2025-09-12 21:12:30,318][__main__][INFO] - Test, Round 466: loss=2.2910, accuracy=0.1366, 
[2025-09-12 21:12:30,318][__main__][INFO] - Test, Round 467: loss=2.2909, accuracy=0.1367, 
[2025-09-12 21:12:30,318][__main__][INFO] - Test, Round 468: loss=2.2909, accuracy=0.1368, 
[2025-09-12 21:12:30,318][__main__][INFO] - Test, Round 469: loss=2.2908, accuracy=0.1365, 
[2025-09-12 21:12:30,318][__main__][INFO] - Test, Round 470: loss=2.2908, accuracy=0.1363, 
[2025-09-12 21:12:30,318][__main__][INFO] - Test, Round 471: loss=2.2908, accuracy=0.1361, 
[2025-09-12 21:12:30,318][__main__][INFO] - Test, Round 472: loss=2.2907, accuracy=0.1364, 
[2025-09-12 21:12:30,318][__main__][INFO] - Test, Round 473: loss=2.2907, accuracy=0.1364, 
[2025-09-12 21:12:30,318][__main__][INFO] - Test, Round 474: loss=2.2906, accuracy=0.1369, 
[2025-09-12 21:12:30,318][__main__][INFO] - Test, Round 475: loss=2.2906, accuracy=0.1372, 
[2025-09-12 21:12:30,318][__main__][INFO] - Test, Round 476: loss=2.2905, accuracy=0.1373, 
[2025-09-12 21:12:30,318][__main__][INFO] - Test, Round 477: loss=2.2905, accuracy=0.1374, 
[2025-09-12 21:12:30,318][__main__][INFO] - Test, Round 478: loss=2.2904, accuracy=0.1376, 
[2025-09-12 21:12:30,318][__main__][INFO] - Test, Round 479: loss=2.2904, accuracy=0.1376, 
[2025-09-12 21:12:30,318][__main__][INFO] - Test, Round 480: loss=2.2904, accuracy=0.1375, 
[2025-09-12 21:12:30,318][__main__][INFO] - Test, Round 481: loss=2.2903, accuracy=0.1377, 
[2025-09-12 21:12:30,318][__main__][INFO] - Test, Round 482: loss=2.2903, accuracy=0.1374, 
[2025-09-12 21:12:30,318][__main__][INFO] - Test, Round 483: loss=2.2902, accuracy=0.1374, 
[2025-09-12 21:12:30,318][__main__][INFO] - Test, Round 484: loss=2.2902, accuracy=0.1379, 
[2025-09-12 21:12:30,318][__main__][INFO] - Test, Round 485: loss=2.2901, accuracy=0.1378, 
[2025-09-12 21:12:30,318][__main__][INFO] - Test, Round 486: loss=2.2901, accuracy=0.1377, 
[2025-09-12 21:12:30,318][__main__][INFO] - Test, Round 487: loss=2.2900, accuracy=0.1377, 
[2025-09-12 21:12:30,318][__main__][INFO] - Test, Round 488: loss=2.2900, accuracy=0.1377, 
[2025-09-12 21:12:30,318][__main__][INFO] - Test, Round 489: loss=2.2899, accuracy=0.1377, 
[2025-09-12 21:12:30,318][__main__][INFO] - Test, Round 490: loss=2.2899, accuracy=0.1378, 
[2025-09-12 21:12:30,318][__main__][INFO] - Test, Round 491: loss=2.2899, accuracy=0.1381, 
[2025-09-12 21:12:30,318][__main__][INFO] - Test, Round 492: loss=2.2898, accuracy=0.1380, 
[2025-09-12 21:12:30,318][__main__][INFO] - Test, Round 493: loss=2.2898, accuracy=0.1379, 
[2025-09-12 21:12:30,318][__main__][INFO] - Test, Round 494: loss=2.2897, accuracy=0.1381, 
[2025-09-12 21:12:30,318][__main__][INFO] - Test, Round 495: loss=2.2897, accuracy=0.1380, 
[2025-09-12 21:12:30,318][__main__][INFO] - Test, Round 496: loss=2.2896, accuracy=0.1380, 
[2025-09-12 21:12:30,318][__main__][INFO] - Test, Round 497: loss=2.2896, accuracy=0.1383, 
[2025-09-12 21:12:30,318][__main__][INFO] - Test, Round 498: loss=2.2895, accuracy=0.1381, 
[2025-09-12 21:12:30,318][__main__][INFO] - Test, Round 499: loss=2.2895, accuracy=0.1381, 
[2025-09-12 21:12:30,318][__main__][INFO] - Test, Round 500: loss=2.2894, accuracy=0.1385, 
