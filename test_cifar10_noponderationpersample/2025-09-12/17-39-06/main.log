[2025-09-12 17:39:20,666][flp2p.graph_runner][INFO] - Train, Round 0 : loss => 2.305329055786133,  accuracy: 0.10256, gradient_norm : 0.1797738444952477
[2025-09-12 17:39:37,800][flp2p.graph_runner][INFO] - Test, Round 0 : loss => 2.3055541890621187,  accuracy: 0.1018
[2025-09-12 17:39:48,389][flp2p.graph_runner][INFO] - Train, Round 1 : loss => 2.3051415675878526,  accuracy: 0.1031, gradient_norm : 0.1743264544058358
[2025-09-12 17:40:19,885][flp2p.graph_runner][INFO] - Test, Round 1 : loss => 2.3053266015172005,  accuracy: 0.1032
[2025-09-12 17:40:30,632][flp2p.graph_runner][INFO] - Train, Round 2 : loss => 2.3048796486854553,  accuracy: 0.10354, gradient_norm : 0.17352256831670107
[2025-09-12 17:41:01,123][flp2p.graph_runner][INFO] - Test, Round 2 : loss => 2.3051186161875723,  accuracy: 0.1031
[2025-09-12 17:41:11,952][flp2p.graph_runner][INFO] - Train, Round 3 : loss => 2.30468171954155,  accuracy: 0.10456, gradient_norm : 0.17374075955278154
[2025-09-12 17:41:34,024][flp2p.graph_runner][INFO] - Test, Round 3 : loss => 2.3049277366399763,  accuracy: 0.1025
[2025-09-12 17:41:45,215][flp2p.graph_runner][INFO] - Train, Round 4 : loss => 2.3045342057943343,  accuracy: 0.10492, gradient_norm : 0.1810821241587886
[2025-09-12 17:42:04,544][flp2p.graph_runner][INFO] - Test, Round 4 : loss => 2.304708141207695,  accuracy: 0.1032
[2025-09-12 17:42:15,335][flp2p.graph_runner][INFO] - Train, Round 5 : loss => 2.304280351102352,  accuracy: 0.10558, gradient_norm : 0.17561336333549035
[2025-09-12 17:42:37,892][flp2p.graph_runner][INFO] - Test, Round 5 : loss => 2.3045239500522614,  accuracy: 0.1036
[2025-09-12 17:42:48,743][flp2p.graph_runner][INFO] - Train, Round 6 : loss => 2.3040982633829117,  accuracy: 0.10512, gradient_norm : 0.17489494195544736
[2025-09-12 17:43:18,940][flp2p.graph_runner][INFO] - Test, Round 6 : loss => 2.3043283726334574,  accuracy: 0.1036
[2025-09-12 17:43:29,837][flp2p.graph_runner][INFO] - Train, Round 7 : loss => 2.3039054331183433,  accuracy: 0.10526, gradient_norm : 0.18045625549972016
[2025-09-12 17:44:01,367][flp2p.graph_runner][INFO] - Test, Round 7 : loss => 2.304136411321163,  accuracy: 0.104
[2025-09-12 17:44:12,320][flp2p.graph_runner][INFO] - Train, Round 8 : loss => 2.3036906218528745,  accuracy: 0.10564, gradient_norm : 0.17154392553385645
[2025-09-12 17:44:37,302][flp2p.graph_runner][INFO] - Test, Round 8 : loss => 2.3039536938548086,  accuracy: 0.105
[2025-09-12 17:44:48,178][flp2p.graph_runner][INFO] - Train, Round 9 : loss => 2.303514096736908,  accuracy: 0.1067, gradient_norm : 0.17864138151243011
[2025-09-12 17:44:59,981][flp2p.graph_runner][INFO] - Test, Round 9 : loss => 2.3037771512627603,  accuracy: 0.1064
[2025-09-12 17:45:11,254][flp2p.graph_runner][INFO] - Train, Round 10 : loss => 2.303344108462334,  accuracy: 0.10654, gradient_norm : 0.1750855166952334
[2025-09-12 17:45:22,612][flp2p.graph_runner][INFO] - Test, Round 10 : loss => 2.3035910361647605,  accuracy: 0.1054
[2025-09-12 17:45:33,773][flp2p.graph_runner][INFO] - Train, Round 11 : loss => 2.3031953167915344,  accuracy: 0.10644, gradient_norm : 0.17653222757830528
[2025-09-12 17:45:51,921][flp2p.graph_runner][INFO] - Test, Round 11 : loss => 2.3033888212680815,  accuracy: 0.1055
[2025-09-12 17:46:03,133][flp2p.graph_runner][INFO] - Train, Round 12 : loss => 2.30298298895359,  accuracy: 0.10706, gradient_norm : 0.17819867638309567
[2025-09-12 17:46:32,756][flp2p.graph_runner][INFO] - Test, Round 12 : loss => 2.3031841364860535,  accuracy: 0.1066
[2025-09-12 17:46:43,766][flp2p.graph_runner][INFO] - Train, Round 13 : loss => 2.302772624492645,  accuracy: 0.10896, gradient_norm : 0.17313218669315192
[2025-09-12 17:47:13,500][flp2p.graph_runner][INFO] - Test, Round 13 : loss => 2.302993408584595,  accuracy: 0.1095
[2025-09-12 17:47:24,361][flp2p.graph_runner][INFO] - Train, Round 14 : loss => 2.3025860244035723,  accuracy: 0.11004, gradient_norm : 0.17420660865963455
[2025-09-12 17:47:52,229][flp2p.graph_runner][INFO] - Test, Round 14 : loss => 2.3027877896785736,  accuracy: 0.112
[2025-09-12 17:48:03,351][flp2p.graph_runner][INFO] - Train, Round 15 : loss => 2.3023907297849657,  accuracy: 0.11214, gradient_norm : 0.18035675694426184
[2025-09-12 17:48:22,588][flp2p.graph_runner][INFO] - Test, Round 15 : loss => 2.302578421652317,  accuracy: 0.1144
[2025-09-12 17:48:33,603][flp2p.graph_runner][INFO] - Train, Round 16 : loss => 2.302203351557255,  accuracy: 0.11562, gradient_norm : 0.1760419806637579
[2025-09-12 17:48:51,997][flp2p.graph_runner][INFO] - Test, Round 16 : loss => 2.3023598385691644,  accuracy: 0.1197
[2025-09-12 17:49:02,757][flp2p.graph_runner][INFO] - Train, Round 17 : loss => 2.3019587993621826,  accuracy: 0.11934, gradient_norm : 0.1790095601132367
[2025-09-12 17:49:29,484][flp2p.graph_runner][INFO] - Test, Round 17 : loss => 2.3021580597877502,  accuracy: 0.1221
[2025-09-12 17:49:40,263][flp2p.graph_runner][INFO] - Train, Round 18 : loss => 2.3017704364657403,  accuracy: 0.12124, gradient_norm : 0.17871018205636713
[2025-09-12 17:50:09,175][flp2p.graph_runner][INFO] - Test, Round 18 : loss => 2.301954346513748,  accuracy: 0.123
[2025-09-12 17:50:20,391][flp2p.graph_runner][INFO] - Train, Round 19 : loss => 2.3015438216924666,  accuracy: 0.12414, gradient_norm : 0.17926379257898906
[2025-09-12 17:50:40,753][flp2p.graph_runner][INFO] - Test, Round 19 : loss => 2.3017603848814963,  accuracy: 0.1263
[2025-09-12 17:50:51,783][flp2p.graph_runner][INFO] - Train, Round 20 : loss => 2.3013529869914056,  accuracy: 0.127, gradient_norm : 0.17519446373925593
[2025-09-12 17:51:17,039][flp2p.graph_runner][INFO] - Test, Round 20 : loss => 2.301555132484436,  accuracy: 0.1315
[2025-09-12 17:51:28,150][flp2p.graph_runner][INFO] - Train, Round 21 : loss => 2.301150335073471,  accuracy: 0.13222, gradient_norm : 0.1815000972192469
[2025-09-12 17:51:53,045][flp2p.graph_runner][INFO] - Test, Round 21 : loss => 2.3013571476697923,  accuracy: 0.134
[2025-09-12 17:52:04,079][flp2p.graph_runner][INFO] - Train, Round 22 : loss => 2.30091906696558,  accuracy: 0.13364, gradient_norm : 0.17270078592467172
[2025-09-12 17:52:22,122][flp2p.graph_runner][INFO] - Test, Round 22 : loss => 2.301165471637249,  accuracy: 0.137
[2025-09-12 17:52:33,147][flp2p.graph_runner][INFO] - Train, Round 23 : loss => 2.300770493745804,  accuracy: 0.13684, gradient_norm : 0.1793077719483681
[2025-09-12 17:52:52,077][flp2p.graph_runner][INFO] - Test, Round 23 : loss => 2.3009396171212195,  accuracy: 0.1391
[2025-09-12 17:53:02,947][flp2p.graph_runner][INFO] - Train, Round 24 : loss => 2.300532157123089,  accuracy: 0.1373, gradient_norm : 0.18116567973701048
[2025-09-12 17:53:31,727][flp2p.graph_runner][INFO] - Test, Round 24 : loss => 2.3007209013342855,  accuracy: 0.1389
[2025-09-12 17:53:42,505][flp2p.graph_runner][INFO] - Train, Round 25 : loss => 2.300329214632511,  accuracy: 0.13852, gradient_norm : 0.18706660231725458
[2025-09-12 17:54:16,426][flp2p.graph_runner][INFO] - Test, Round 25 : loss => 2.30049502645731,  accuracy: 0.1433
[2025-09-12 17:54:27,361][flp2p.graph_runner][INFO] - Train, Round 26 : loss => 2.3001067087054254,  accuracy: 0.14048, gradient_norm : 0.18314826290283384
[2025-09-12 17:55:05,132][flp2p.graph_runner][INFO] - Test, Round 26 : loss => 2.300251714324951,  accuracy: 0.1467
[2025-09-12 17:55:16,164][flp2p.graph_runner][INFO] - Train, Round 27 : loss => 2.2998531123995782,  accuracy: 0.14444, gradient_norm : 0.18628701129183145
[2025-09-12 17:55:41,804][flp2p.graph_runner][INFO] - Test, Round 27 : loss => 2.300004923212528,  accuracy: 0.1464
[2025-09-12 17:55:52,744][flp2p.graph_runner][INFO] - Train, Round 28 : loss => 2.2996182948350907,  accuracy: 0.14616, gradient_norm : 0.18778880375484872
[2025-09-12 17:56:11,267][flp2p.graph_runner][INFO] - Test, Round 28 : loss => 2.2997621316194534,  accuracy: 0.1466
[2025-09-12 17:56:22,216][flp2p.graph_runner][INFO] - Train, Round 29 : loss => 2.2993767923116684,  accuracy: 0.14738, gradient_norm : 0.1879552203309154
[2025-09-12 17:56:41,454][flp2p.graph_runner][INFO] - Test, Round 29 : loss => 2.299504829001427,  accuracy: 0.149
[2025-09-12 17:56:52,593][flp2p.graph_runner][INFO] - Train, Round 30 : loss => 2.29913227468729,  accuracy: 0.14776, gradient_norm : 0.18400534996344267
[2025-09-12 17:57:19,279][flp2p.graph_runner][INFO] - Test, Round 30 : loss => 2.299244969713688,  accuracy: 0.1512
[2025-09-12 17:57:30,145][flp2p.graph_runner][INFO] - Train, Round 31 : loss => 2.2988276264071463,  accuracy: 0.14866, gradient_norm : 0.18552747505015765
[2025-09-12 17:57:52,613][flp2p.graph_runner][INFO] - Test, Round 31 : loss => 2.2989969912409784,  accuracy: 0.1505
[2025-09-12 17:58:03,581][flp2p.graph_runner][INFO] - Train, Round 32 : loss => 2.2985526737570763,  accuracy: 0.1491, gradient_norm : 0.18887550911761372
[2025-09-12 17:58:24,444][flp2p.graph_runner][INFO] - Test, Round 32 : loss => 2.2987472285151482,  accuracy: 0.1531
[2025-09-12 17:58:35,349][flp2p.graph_runner][INFO] - Train, Round 33 : loss => 2.298330123722553,  accuracy: 0.15014, gradient_norm : 0.19487495266466526
[2025-09-12 17:59:05,179][flp2p.graph_runner][INFO] - Test, Round 33 : loss => 2.2984813391566274,  accuracy: 0.1523
[2025-09-12 17:59:16,104][flp2p.graph_runner][INFO] - Train, Round 34 : loss => 2.298056648373604,  accuracy: 0.15142, gradient_norm : 0.19197715859476752
[2025-09-12 17:59:44,760][flp2p.graph_runner][INFO] - Test, Round 34 : loss => 2.29819476262331,  accuracy: 0.1535
[2025-09-12 17:59:55,802][flp2p.graph_runner][INFO] - Train, Round 35 : loss => 2.2977867090702055,  accuracy: 0.15234, gradient_norm : 0.19074271891657815
[2025-09-12 18:00:21,254][flp2p.graph_runner][INFO] - Test, Round 35 : loss => 2.29788666292429,  accuracy: 0.1551
[2025-09-12 18:00:32,271][flp2p.graph_runner][INFO] - Train, Round 36 : loss => 2.2974514174461365,  accuracy: 0.15188, gradient_norm : 0.19482881918031686
[2025-09-12 18:00:51,043][flp2p.graph_runner][INFO] - Test, Round 36 : loss => 2.297574490606785,  accuracy: 0.1558
[2025-09-12 18:01:02,247][flp2p.graph_runner][INFO] - Train, Round 37 : loss => 2.2971700471639633,  accuracy: 0.15236, gradient_norm : 0.20065798642224347
[2025-09-12 18:01:20,892][flp2p.graph_runner][INFO] - Test, Round 37 : loss => 2.2972476784706117,  accuracy: 0.157
[2025-09-12 18:01:31,919][flp2p.graph_runner][INFO] - Train, Round 38 : loss => 2.2968317279219628,  accuracy: 0.15346, gradient_norm : 0.19414568730470264
[2025-09-12 18:01:57,691][flp2p.graph_runner][INFO] - Test, Round 38 : loss => 2.2969038034915923,  accuracy: 0.1589
[2025-09-12 18:02:08,362][flp2p.graph_runner][INFO] - Train, Round 39 : loss => 2.296493963301182,  accuracy: 0.15412, gradient_norm : 0.2031380168078886
[2025-09-12 18:02:32,672][flp2p.graph_runner][INFO] - Test, Round 39 : loss => 2.296549736058712,  accuracy: 0.1595
[2025-09-12 18:02:43,447][flp2p.graph_runner][INFO] - Train, Round 40 : loss => 2.2961409133672714,  accuracy: 0.15506, gradient_norm : 0.206551035396127
[2025-09-12 18:03:03,150][flp2p.graph_runner][INFO] - Test, Round 40 : loss => 2.296175669884682,  accuracy: 0.1606
[2025-09-12 18:03:14,126][flp2p.graph_runner][INFO] - Train, Round 41 : loss => 2.295761269032955,  accuracy: 0.15642, gradient_norm : 0.20154174247762702
[2025-09-12 18:03:42,096][flp2p.graph_runner][INFO] - Test, Round 41 : loss => 2.2957909113168715,  accuracy: 0.1597
[2025-09-12 18:03:52,987][flp2p.graph_runner][INFO] - Train, Round 42 : loss => 2.295361913740635,  accuracy: 0.15694, gradient_norm : 0.2080590832696457
[2025-09-12 18:04:25,824][flp2p.graph_runner][INFO] - Test, Round 42 : loss => 2.295379025232792,  accuracy: 0.1603
[2025-09-12 18:04:36,771][flp2p.graph_runner][INFO] - Train, Round 43 : loss => 2.2949630242586134,  accuracy: 0.15782, gradient_norm : 0.21232384748253763
[2025-09-12 18:05:07,225][flp2p.graph_runner][INFO] - Test, Round 43 : loss => 2.294930010354519,  accuracy: 0.162
[2025-09-12 18:05:18,154][flp2p.graph_runner][INFO] - Train, Round 44 : loss => 2.29450338691473,  accuracy: 0.15902, gradient_norm : 0.21058023519078087
[2025-09-12 18:05:40,844][flp2p.graph_runner][INFO] - Test, Round 44 : loss => 2.294500173842907,  accuracy: 0.1633
[2025-09-12 18:05:51,957][flp2p.graph_runner][INFO] - Train, Round 45 : loss => 2.2940721252560614,  accuracy: 0.15956, gradient_norm : 0.21184428074565242
[2025-09-12 18:06:10,854][flp2p.graph_runner][INFO] - Test, Round 45 : loss => 2.2940246312856676,  accuracy: 0.163
[2025-09-12 18:06:21,871][flp2p.graph_runner][INFO] - Train, Round 46 : loss => 2.2935862079262734,  accuracy: 0.16076, gradient_norm : 0.21837236692900214
[2025-09-12 18:06:41,514][flp2p.graph_runner][INFO] - Test, Round 46 : loss => 2.2935089210271835,  accuracy: 0.1631
[2025-09-12 18:06:52,361][flp2p.graph_runner][INFO] - Train, Round 47 : loss => 2.2930459266901018,  accuracy: 0.1606, gradient_norm : 0.21793856450862348
[2025-09-12 18:07:15,001][flp2p.graph_runner][INFO] - Test, Round 47 : loss => 2.292972668945789,  accuracy: 0.1617
[2025-09-12 18:07:25,780][flp2p.graph_runner][INFO] - Train, Round 48 : loss => 2.292492121756077,  accuracy: 0.16166, gradient_norm : 0.21520039230303226
[2025-09-12 18:07:44,320][flp2p.graph_runner][INFO] - Test, Round 48 : loss => 2.292413184607029,  accuracy: 0.1622
[2025-09-12 18:07:55,399][flp2p.graph_runner][INFO] - Train, Round 49 : loss => 2.2919522339105605,  accuracy: 0.16326, gradient_norm : 0.21937683610314174
[2025-09-12 18:08:14,747][flp2p.graph_runner][INFO] - Test, Round 49 : loss => 2.291853800368309,  accuracy: 0.1629
[2025-09-12 18:08:25,673][flp2p.graph_runner][INFO] - Train, Round 50 : loss => 2.291351044178009,  accuracy: 0.16464, gradient_norm : 0.21573560834866778
[2025-09-12 18:08:37,539][flp2p.graph_runner][INFO] - Test, Round 50 : loss => 2.2912670410990716,  accuracy: 0.1642
[2025-09-12 18:08:48,567][flp2p.graph_runner][INFO] - Train, Round 51 : loss => 2.290744189620018,  accuracy: 0.16522, gradient_norm : 0.23609356174214224
[2025-09-12 18:09:00,568][flp2p.graph_runner][INFO] - Test, Round 51 : loss => 2.2906079434990883,  accuracy: 0.1659
[2025-09-12 18:09:11,888][flp2p.graph_runner][INFO] - Train, Round 52 : loss => 2.2901045340299606,  accuracy: 0.16648, gradient_norm : 0.22753038867728612
[2025-09-12 18:09:23,121][flp2p.graph_runner][INFO] - Test, Round 52 : loss => 2.289901921641827,  accuracy: 0.1675
[2025-09-12 18:09:34,049][flp2p.graph_runner][INFO] - Train, Round 53 : loss => 2.2894013199210166,  accuracy: 0.16896, gradient_norm : 0.23289234237168277
[2025-09-12 18:09:59,339][flp2p.graph_runner][INFO] - Test, Round 53 : loss => 2.289194853305817,  accuracy: 0.1709
[2025-09-12 18:10:10,524][flp2p.graph_runner][INFO] - Train, Round 54 : loss => 2.2886577650904654,  accuracy: 0.17078, gradient_norm : 0.2304979596203913
[2025-09-12 18:10:36,531][flp2p.graph_runner][INFO] - Test, Round 54 : loss => 2.2884278850317,  accuracy: 0.1709
[2025-09-12 18:10:47,697][flp2p.graph_runner][INFO] - Train, Round 55 : loss => 2.2878789576888083,  accuracy: 0.1713, gradient_norm : 0.2427893783057903
[2025-09-12 18:11:06,426][flp2p.graph_runner][INFO] - Test, Round 55 : loss => 2.2875940205693244,  accuracy: 0.1707
[2025-09-12 18:11:17,598][flp2p.graph_runner][INFO] - Train, Round 56 : loss => 2.2870114651322364,  accuracy: 0.17168, gradient_norm : 0.2516289673509849
[2025-09-12 18:11:36,160][flp2p.graph_runner][INFO] - Test, Round 56 : loss => 2.2867261284947396,  accuracy: 0.172
[2025-09-12 18:11:47,217][flp2p.graph_runner][INFO] - Train, Round 57 : loss => 2.286104553639889,  accuracy: 0.17286, gradient_norm : 0.25489621414006797
[2025-09-12 18:12:10,675][flp2p.graph_runner][INFO] - Test, Round 57 : loss => 2.2857602323055266,  accuracy: 0.1735
[2025-09-12 18:12:21,590][flp2p.graph_runner][INFO] - Train, Round 58 : loss => 2.2851320558786394,  accuracy: 0.1739, gradient_norm : 0.2680597251458712
[2025-09-12 18:12:40,402][flp2p.graph_runner][INFO] - Test, Round 58 : loss => 2.284764714539051,  accuracy: 0.1766
[2025-09-12 18:12:50,942][flp2p.graph_runner][INFO] - Train, Round 59 : loss => 2.28411832511425,  accuracy: 0.17584, gradient_norm : 0.2630521833586244
[2025-09-12 18:13:08,062][flp2p.graph_runner][INFO] - Test, Round 59 : loss => 2.2837050452947616,  accuracy: 0.1777
[2025-09-12 18:13:18,984][flp2p.graph_runner][INFO] - Train, Round 60 : loss => 2.283034203052521,  accuracy: 0.17682, gradient_norm : 0.2747596219169536
[2025-09-12 18:13:35,522][flp2p.graph_runner][INFO] - Test, Round 60 : loss => 2.2825830153346063,  accuracy: 0.178
[2025-09-12 18:13:46,459][flp2p.graph_runner][INFO] - Train, Round 61 : loss => 2.28183430314064,  accuracy: 0.17738, gradient_norm : 0.2723194382974523
[2025-09-12 18:13:58,324][flp2p.graph_runner][INFO] - Test, Round 61 : loss => 2.281312641429901,  accuracy: 0.178
[2025-09-12 18:14:09,490][flp2p.graph_runner][INFO] - Train, Round 62 : loss => 2.280545516014099,  accuracy: 0.17852, gradient_norm : 0.2870031201522404
[2025-09-12 18:14:21,003][flp2p.graph_runner][INFO] - Test, Round 62 : loss => 2.2799916844367982,  accuracy: 0.1754
[2025-09-12 18:14:32,420][flp2p.graph_runner][INFO] - Train, Round 63 : loss => 2.2791964712738992,  accuracy: 0.17846, gradient_norm : 0.2989652322938953
[2025-09-12 18:14:47,947][flp2p.graph_runner][INFO] - Test, Round 63 : loss => 2.2785269679665565,  accuracy: 0.1753
[2025-09-12 18:14:58,928][flp2p.graph_runner][INFO] - Train, Round 64 : loss => 2.2776504537463187,  accuracy: 0.17842, gradient_norm : 0.30956004690344746
[2025-09-12 18:15:21,051][flp2p.graph_runner][INFO] - Test, Round 64 : loss => 2.2769087712287903,  accuracy: 0.1753
[2025-09-12 18:15:32,102][flp2p.graph_runner][INFO] - Train, Round 65 : loss => 2.275956630110741,  accuracy: 0.17766, gradient_norm : 0.3140144162774886
[2025-09-12 18:15:49,906][flp2p.graph_runner][INFO] - Test, Round 65 : loss => 2.2751675983309747,  accuracy: 0.1724
[2025-09-12 18:16:00,962][flp2p.graph_runner][INFO] - Train, Round 66 : loss => 2.2742184141278265,  accuracy: 0.17696, gradient_norm : 0.3234592369504942
[2025-09-12 18:16:19,632][flp2p.graph_runner][INFO] - Test, Round 66 : loss => 2.2733660548329353,  accuracy: 0.1744
[2025-09-12 18:16:30,669][flp2p.graph_runner][INFO] - Train, Round 67 : loss => 2.272305759191513,  accuracy: 0.17992, gradient_norm : 0.34219836669534276
[2025-09-12 18:16:58,709][flp2p.graph_runner][INFO] - Test, Round 67 : loss => 2.271328717982769,  accuracy: 0.175
[2025-09-12 18:17:09,590][flp2p.graph_runner][INFO] - Train, Round 68 : loss => 2.2702553656697275,  accuracy: 0.17942, gradient_norm : 0.34603238639327477
[2025-09-12 18:17:35,543][flp2p.graph_runner][INFO] - Test, Round 68 : loss => 2.269237328219414,  accuracy: 0.1757
[2025-09-12 18:17:46,440][flp2p.graph_runner][INFO] - Train, Round 69 : loss => 2.268031820654869,  accuracy: 0.17872, gradient_norm : 0.36320216894074747
[2025-09-12 18:18:07,262][flp2p.graph_runner][INFO] - Test, Round 69 : loss => 2.2668604018211367,  accuracy: 0.1751
[2025-09-12 18:18:18,225][flp2p.graph_runner][INFO] - Train, Round 70 : loss => 2.265600347518921,  accuracy: 0.1781, gradient_norm : 0.37013989848025247
[2025-09-12 18:18:44,203][flp2p.graph_runner][INFO] - Test, Round 70 : loss => 2.2643736847400664,  accuracy: 0.1768
[2025-09-12 18:18:55,208][flp2p.graph_runner][INFO] - Train, Round 71 : loss => 2.263035995066166,  accuracy: 0.17938, gradient_norm : 0.39372369384974826
[2025-09-12 18:19:25,364][flp2p.graph_runner][INFO] - Test, Round 71 : loss => 2.2617021591424944,  accuracy: 0.1764
[2025-09-12 18:19:36,181][flp2p.graph_runner][INFO] - Train, Round 72 : loss => 2.260244942307472,  accuracy: 0.17992, gradient_norm : 0.38684602148780456
[2025-09-12 18:20:02,105][flp2p.graph_runner][INFO] - Test, Round 72 : loss => 2.2587580628156663,  accuracy: 0.1813
[2025-09-12 18:20:13,052][flp2p.graph_runner][INFO] - Train, Round 73 : loss => 2.257159456908703,  accuracy: 0.18214, gradient_norm : 0.4241784961006264
[2025-09-12 18:20:32,227][flp2p.graph_runner][INFO] - Test, Round 73 : loss => 2.255446564280987,  accuracy: 0.1803
[2025-09-12 18:20:43,269][flp2p.graph_runner][INFO] - Train, Round 74 : loss => 2.253705143034458,  accuracy: 0.1807, gradient_norm : 0.44538592325997867
[2025-09-12 18:21:01,871][flp2p.graph_runner][INFO] - Test, Round 74 : loss => 2.2517827224731444,  accuracy: 0.1798
[2025-09-12 18:21:12,811][flp2p.graph_runner][INFO] - Train, Round 75 : loss => 2.250057207942009,  accuracy: 0.18116, gradient_norm : 0.4581028087896965
[2025-09-12 18:21:36,862][flp2p.graph_runner][INFO] - Test, Round 75 : loss => 2.2482131749153136,  accuracy: 0.1818
[2025-09-12 18:21:47,767][flp2p.graph_runner][INFO] - Train, Round 76 : loss => 2.2463520473241805,  accuracy: 0.18136, gradient_norm : 0.4921157352264122
[2025-09-12 18:22:14,102][flp2p.graph_runner][INFO] - Test, Round 76 : loss => 2.244264405310154,  accuracy: 0.1834
[2025-09-12 18:22:25,118][flp2p.graph_runner][INFO] - Train, Round 77 : loss => 2.2421829181909563,  accuracy: 0.18392, gradient_norm : 0.5264582265662506
[2025-09-12 18:22:45,740][flp2p.graph_runner][INFO] - Test, Round 77 : loss => 2.239963580417633,  accuracy: 0.1848
[2025-09-12 18:22:56,706][flp2p.graph_runner][INFO] - Train, Round 78 : loss => 2.237739461660385,  accuracy: 0.18476, gradient_norm : 0.5442948886649287
[2025-09-12 18:23:22,445][flp2p.graph_runner][INFO] - Test, Round 78 : loss => 2.235281366968155,  accuracy: 0.187
[2025-09-12 18:23:33,227][flp2p.graph_runner][INFO] - Train, Round 79 : loss => 2.232851856648922,  accuracy: 0.18744, gradient_norm : 0.5643873212546914
[2025-09-12 18:24:04,379][flp2p.graph_runner][INFO] - Test, Round 79 : loss => 2.230242251431942,  accuracy: 0.1891
[2025-09-12 18:24:15,290][flp2p.graph_runner][INFO] - Train, Round 80 : loss => 2.22770705640316,  accuracy: 0.18794, gradient_norm : 0.5886582619447691
[2025-09-12 18:24:38,032][flp2p.graph_runner][INFO] - Test, Round 80 : loss => 2.2246860797524453,  accuracy: 0.1893
[2025-09-12 18:24:49,262][flp2p.graph_runner][INFO] - Train, Round 81 : loss => 2.222003245949745,  accuracy: 0.18668, gradient_norm : 0.5884546848511466
[2025-09-12 18:25:08,006][flp2p.graph_runner][INFO] - Test, Round 81 : loss => 2.219339637553692,  accuracy: 0.1889
[2025-09-12 18:25:18,782][flp2p.graph_runner][INFO] - Train, Round 82 : loss => 2.21641509860754,  accuracy: 0.18706, gradient_norm : 0.6352375718410969
[2025-09-12 18:25:40,990][flp2p.graph_runner][INFO] - Test, Round 82 : loss => 2.213580561363697,  accuracy: 0.1889
[2025-09-12 18:25:51,709][flp2p.graph_runner][INFO] - Train, Round 83 : loss => 2.210347292125225,  accuracy: 0.18796, gradient_norm : 0.6762062259492797
[2025-09-12 18:26:19,245][flp2p.graph_runner][INFO] - Test, Round 83 : loss => 2.207119208776951,  accuracy: 0.1891
[2025-09-12 18:26:29,972][flp2p.graph_runner][INFO] - Train, Round 84 : loss => 2.203735911846161,  accuracy: 0.18944, gradient_norm : 0.7020556007367621
[2025-09-12 18:26:51,531][flp2p.graph_runner][INFO] - Test, Round 84 : loss => 2.200900997507572,  accuracy: 0.1868
[2025-09-12 18:27:02,417][flp2p.graph_runner][INFO] - Train, Round 85 : loss => 2.197313277721405,  accuracy: 0.18942, gradient_norm : 0.7248066911680895
[2025-09-12 18:27:23,829][flp2p.graph_runner][INFO] - Test, Round 85 : loss => 2.194397353053093,  accuracy: 0.1944
[2025-09-12 18:27:34,904][flp2p.graph_runner][INFO] - Train, Round 86 : loss => 2.190356889665127,  accuracy: 0.1924, gradient_norm : 0.7724735134136675
[2025-09-12 18:28:07,336][flp2p.graph_runner][INFO] - Test, Round 86 : loss => 2.187306627190113,  accuracy: 0.1953
[2025-09-12 18:28:18,344][flp2p.graph_runner][INFO] - Train, Round 87 : loss => 2.1831770974397657,  accuracy: 0.19344, gradient_norm : 0.847089825277558
[2025-09-12 18:28:48,590][flp2p.graph_runner][INFO] - Test, Round 87 : loss => 2.180550571668148,  accuracy: 0.1975
[2025-09-12 18:28:59,653][flp2p.graph_runner][INFO] - Train, Round 88 : loss => 2.1760473987460136,  accuracy: 0.19788, gradient_norm : 0.8735394567144084
[2025-09-12 18:29:22,178][flp2p.graph_runner][INFO] - Test, Round 88 : loss => 2.1740387888789177,  accuracy: 0.1929
[2025-09-12 18:29:33,340][flp2p.graph_runner][INFO] - Train, Round 89 : loss => 2.169074604213238,  accuracy: 0.19682, gradient_norm : 0.9007514762986988
[2025-09-12 18:29:51,568][flp2p.graph_runner][INFO] - Test, Round 89 : loss => 2.166500657963753,  accuracy: 0.1993
[2025-09-12 18:30:02,529][flp2p.graph_runner][INFO] - Train, Round 90 : loss => 2.1613339415192603,  accuracy: 0.1995, gradient_norm : 0.9253204948406463
[2025-09-12 18:30:22,948][flp2p.graph_runner][INFO] - Test, Round 90 : loss => 2.1593732221007347,  accuracy: 0.1981
[2025-09-12 18:30:33,743][flp2p.graph_runner][INFO] - Train, Round 91 : loss => 2.1539197647571564,  accuracy: 0.20174, gradient_norm : 0.9737591296395384
[2025-09-12 18:31:01,049][flp2p.graph_runner][INFO] - Test, Round 91 : loss => 2.152639176619053,  accuracy: 0.2006
[2025-09-12 18:31:11,914][flp2p.graph_runner][INFO] - Train, Round 92 : loss => 2.146705064177513,  accuracy: 0.2032, gradient_norm : 1.0307557287703952
[2025-09-12 18:31:41,256][flp2p.graph_runner][INFO] - Test, Round 92 : loss => 2.146274304807186,  accuracy: 0.2044
[2025-09-12 18:31:51,976][flp2p.graph_runner][INFO] - Train, Round 93 : loss => 2.13971445441246,  accuracy: 0.20934, gradient_norm : 1.0701574530235247
[2025-09-12 18:32:13,032][flp2p.graph_runner][INFO] - Test, Round 93 : loss => 2.1392950565695763,  accuracy: 0.2093
[2025-09-12 18:32:24,056][flp2p.graph_runner][INFO] - Train, Round 94 : loss => 2.132634361386299,  accuracy: 0.21174, gradient_norm : 1.112542671130201
[2025-09-12 18:32:53,122][flp2p.graph_runner][INFO] - Test, Round 94 : loss => 2.1343612067341806,  accuracy: 0.208
[2025-09-12 18:33:04,003][flp2p.graph_runner][INFO] - Train, Round 95 : loss => 2.1268580439686775,  accuracy: 0.21134, gradient_norm : 1.1470530557224756
[2025-09-12 18:33:34,022][flp2p.graph_runner][INFO] - Test, Round 95 : loss => 2.1276444173574447,  accuracy: 0.2134
[2025-09-12 18:33:45,257][flp2p.graph_runner][INFO] - Train, Round 96 : loss => 2.1199668687582016,  accuracy: 0.21906, gradient_norm : 1.1988550978921824
[2025-09-12 18:34:03,541][flp2p.graph_runner][INFO] - Test, Round 96 : loss => 2.1212472681462766,  accuracy: 0.2193
[2025-09-12 18:34:14,775][flp2p.graph_runner][INFO] - Train, Round 97 : loss => 2.113061331361532,  accuracy: 0.22356, gradient_norm : 1.2432431835021225
[2025-09-12 18:34:34,403][flp2p.graph_runner][INFO] - Test, Round 97 : loss => 2.114295389676094,  accuracy: 0.2227
[2025-09-12 18:34:45,337][flp2p.graph_runner][INFO] - Train, Round 98 : loss => 2.1065315483510494,  accuracy: 0.22438, gradient_norm : 1.229633444186346
[2025-09-12 18:35:07,452][flp2p.graph_runner][INFO] - Test, Round 98 : loss => 2.1079149398088455,  accuracy: 0.2271
[2025-09-12 18:35:18,369][flp2p.graph_runner][INFO] - Train, Round 99 : loss => 2.099794330894947,  accuracy: 0.2316, gradient_norm : 1.319492999766565
[2025-09-12 18:35:45,686][flp2p.graph_runner][INFO] - Test, Round 99 : loss => 2.103282012695074,  accuracy: 0.2318
[2025-09-12 18:35:56,419][flp2p.graph_runner][INFO] - Train, Round 100 : loss => 2.094513229727745,  accuracy: 0.23582, gradient_norm : 1.3015980194301529
[2025-09-12 18:36:16,366][flp2p.graph_runner][INFO] - Test, Round 100 : loss => 2.0977762667536735,  accuracy: 0.2353
[2025-09-12 18:36:27,382][flp2p.graph_runner][INFO] - Train, Round 101 : loss => 2.088116294890642,  accuracy: 0.23994, gradient_norm : 1.3697371273009076
[2025-09-12 18:36:49,871][flp2p.graph_runner][INFO] - Test, Round 101 : loss => 2.091497984004021,  accuracy: 0.2404
[2025-09-12 18:37:00,915][flp2p.graph_runner][INFO] - Train, Round 102 : loss => 2.082098340690136,  accuracy: 0.24724, gradient_norm : 1.4608266635790634
[2025-09-12 18:37:33,077][flp2p.graph_runner][INFO] - Test, Round 102 : loss => 2.088125060492754,  accuracy: 0.2414
[2025-09-12 18:37:44,186][flp2p.graph_runner][INFO] - Train, Round 103 : loss => 2.0774163138866424,  accuracy: 0.25042, gradient_norm : 1.5097026301443912
[2025-09-12 18:38:12,175][flp2p.graph_runner][INFO] - Test, Round 103 : loss => 2.0936879899859426,  accuracy: 0.245
[2025-09-12 18:38:23,339][flp2p.graph_runner][INFO] - Train, Round 104 : loss => 2.080541621595621,  accuracy: 0.2534, gradient_norm : 1.6573912082066982
[2025-09-12 18:38:48,100][flp2p.graph_runner][INFO] - Test, Round 104 : loss => 2.1580537609815598,  accuracy: 0.2221
[2025-09-12 18:38:59,213][flp2p.graph_runner][INFO] - Train, Round 105 : loss => 2.1264516919851304,  accuracy: 0.22942, gradient_norm : 2.4216819830003162
[2025-09-12 18:39:17,459][flp2p.graph_runner][INFO] - Test, Round 105 : loss => 2.4050835857868194,  accuracy: 0.1638
[2025-09-12 18:39:30,764][flp2p.graph_runner][INFO] - Train, Round 106 : loss => 2.360765125155449,  accuracy: 0.1737, gradient_norm : 3.5944978556533296
[2025-09-12 18:39:44,766][flp2p.graph_runner][INFO] - Test, Round 106 : loss => 2.4964438341021538,  accuracy: 0.1654
[2025-09-12 18:39:55,776][flp2p.graph_runner][INFO] - Train, Round 107 : loss => 2.4344978576898573,  accuracy: 0.17202, gradient_norm : 3.934992900933442
[2025-09-12 18:40:09,642][flp2p.graph_runner][INFO] - Test, Round 107 : loss => 2.2797398821473123,  accuracy: 0.1929
[2025-09-12 18:40:20,637][flp2p.graph_runner][INFO] - Train, Round 108 : loss => 2.27081871509552,  accuracy: 0.19514, gradient_norm : 1.5895439752858962
[2025-09-12 18:40:33,869][flp2p.graph_runner][INFO] - Test, Round 108 : loss => 2.151900200986862,  accuracy: 0.2599
[2025-09-12 18:40:44,736][flp2p.graph_runner][INFO] - Train, Round 109 : loss => 2.145298700034618,  accuracy: 0.26518, gradient_norm : 0.9886533425916972
[2025-09-12 18:41:00,341][flp2p.graph_runner][INFO] - Test, Round 109 : loss => 2.1187879439353945,  accuracy: 0.2782
[2025-09-12 18:41:11,334][flp2p.graph_runner][INFO] - Train, Round 110 : loss => 2.1113773441314696,  accuracy: 0.28342, gradient_norm : 0.9474885207843404
[2025-09-12 18:41:24,611][flp2p.graph_runner][INFO] - Test, Round 110 : loss => 2.0996393901467325,  accuracy: 0.281
[2025-09-12 18:41:35,639][flp2p.graph_runner][INFO] - Train, Round 111 : loss => 2.091550903916359,  accuracy: 0.28184, gradient_norm : 1.0204058540691892
[2025-09-12 18:41:48,965][flp2p.graph_runner][INFO] - Test, Round 111 : loss => 2.0845524210333823,  accuracy: 0.2752
[2025-09-12 18:41:59,753][flp2p.graph_runner][INFO] - Train, Round 112 : loss => 2.0760681337118148,  accuracy: 0.2749, gradient_norm : 1.0383325499804619
[2025-09-12 18:42:12,021][flp2p.graph_runner][INFO] - Test, Round 112 : loss => 2.07270140940547,  accuracy: 0.2731
[2025-09-12 18:42:22,902][flp2p.graph_runner][INFO] - Train, Round 113 : loss => 2.0634085108339786,  accuracy: 0.27802, gradient_norm : 1.1555838195407417
[2025-09-12 18:42:35,031][flp2p.graph_runner][INFO] - Test, Round 113 : loss => 2.06503427952528,  accuracy: 0.2714
[2025-09-12 18:42:46,011][flp2p.graph_runner][INFO] - Train, Round 114 : loss => 2.0550973270833492,  accuracy: 0.27582, gradient_norm : 1.2873327498083893
[2025-09-12 18:42:58,054][flp2p.graph_runner][INFO] - Test, Round 114 : loss => 2.0595189055383205,  accuracy: 0.2699
[2025-09-12 18:43:08,716][flp2p.graph_runner][INFO] - Train, Round 115 : loss => 2.048891838490963,  accuracy: 0.27726, gradient_norm : 1.352473064544135
[2025-09-12 18:43:20,639][flp2p.graph_runner][INFO] - Test, Round 115 : loss => 2.056816724795103,  accuracy: 0.2769
[2025-09-12 18:43:31,421][flp2p.graph_runner][INFO] - Train, Round 116 : loss => 2.0444768904149533,  accuracy: 0.27938, gradient_norm : 1.4651696327135892
[2025-09-12 18:43:43,308][flp2p.graph_runner][INFO] - Test, Round 116 : loss => 2.0678232878625393,  accuracy: 0.2643
[2025-09-12 18:43:54,235][flp2p.graph_runner][INFO] - Train, Round 117 : loss => 2.05355094820261,  accuracy: 0.27058, gradient_norm : 1.733038066108677
[2025-09-12 18:44:06,120][flp2p.graph_runner][INFO] - Test, Round 117 : loss => 2.0883884919941424,  accuracy: 0.2608
[2025-09-12 18:44:16,837][flp2p.graph_runner][INFO] - Train, Round 118 : loss => 2.0677303092181685,  accuracy: 0.26898, gradient_norm : 2.0392084693234738
[2025-09-12 18:44:28,788][flp2p.graph_runner][INFO] - Test, Round 118 : loss => 2.165089309936762,  accuracy: 0.2307
[2025-09-12 18:44:39,482][flp2p.graph_runner][INFO] - Train, Round 119 : loss => 2.14036207690835,  accuracy: 0.24064, gradient_norm : 2.3947362331522273
[2025-09-12 18:44:51,489][flp2p.graph_runner][INFO] - Test, Round 119 : loss => 2.1792261550843715,  accuracy: 0.2349
[2025-09-12 18:45:02,365][flp2p.graph_runner][INFO] - Train, Round 120 : loss => 2.14566752538085,  accuracy: 0.24484, gradient_norm : 2.599836452111237
[2025-09-12 18:45:14,273][flp2p.graph_runner][INFO] - Test, Round 120 : loss => 2.195688798415661,  accuracy: 0.2214
[2025-09-12 18:45:25,235][flp2p.graph_runner][INFO] - Train, Round 121 : loss => 2.175408341139555,  accuracy: 0.23048, gradient_norm : 2.2262374167679178
[2025-09-12 18:45:37,053][flp2p.graph_runner][INFO] - Test, Round 121 : loss => 2.071335305118561,  accuracy: 0.2743
[2025-09-12 18:45:48,103][flp2p.graph_runner][INFO] - Train, Round 122 : loss => 2.0572719587385655,  accuracy: 0.27912, gradient_norm : 1.4701098138457116
[2025-09-12 18:46:00,149][flp2p.graph_runner][INFO] - Test, Round 122 : loss => 2.0494786282300947,  accuracy: 0.2804
[2025-09-12 18:46:10,929][flp2p.graph_runner][INFO] - Train, Round 123 : loss => 2.036897909194231,  accuracy: 0.29024, gradient_norm : 1.4498366783107857
[2025-09-12 18:46:22,564][flp2p.graph_runner][INFO] - Test, Round 123 : loss => 2.034842304599285,  accuracy: 0.2884
[2025-09-12 18:46:33,669][flp2p.graph_runner][INFO] - Train, Round 124 : loss => 2.0219167287647726,  accuracy: 0.29408, gradient_norm : 1.4960522768998854
[2025-09-12 18:46:45,263][flp2p.graph_runner][INFO] - Test, Round 124 : loss => 2.0281238203465937,  accuracy: 0.292
[2025-09-12 18:46:56,308][flp2p.graph_runner][INFO] - Train, Round 125 : loss => 2.0146764692664147,  accuracy: 0.29788, gradient_norm : 1.5021257944343358
[2025-09-12 18:47:07,615][flp2p.graph_runner][INFO] - Test, Round 125 : loss => 2.022685547184944,  accuracy: 0.2896
[2025-09-12 18:47:18,656][flp2p.graph_runner][INFO] - Train, Round 126 : loss => 2.008166164159775,  accuracy: 0.29652, gradient_norm : 1.6265393959755452
[2025-09-12 18:47:37,714][flp2p.graph_runner][INFO] - Test, Round 126 : loss => 2.0356100675463678,  accuracy: 0.2783
[2025-09-12 18:47:48,791][flp2p.graph_runner][INFO] - Train, Round 127 : loss => 2.0182826654613018,  accuracy: 0.2849, gradient_norm : 1.769018120315517
[2025-09-12 18:48:04,731][flp2p.graph_runner][INFO] - Test, Round 127 : loss => 2.0728892316520215,  accuracy: 0.2709
[2025-09-12 18:48:15,813][flp2p.graph_runner][INFO] - Train, Round 128 : loss => 2.0461338594555856,  accuracy: 0.27836, gradient_norm : 2.235070413137328
[2025-09-12 18:48:32,287][flp2p.graph_runner][INFO] - Test, Round 128 : loss => 2.148026046192646,  accuracy: 0.2377
[2025-09-12 18:48:43,165][flp2p.graph_runner][INFO] - Train, Round 129 : loss => 2.118679455369711,  accuracy: 0.2447, gradient_norm : 2.5636685629205918
[2025-09-12 18:49:00,311][flp2p.graph_runner][INFO] - Test, Round 129 : loss => 2.1417550075650214,  accuracy: 0.2422
[2025-09-12 18:49:11,002][flp2p.graph_runner][INFO] - Train, Round 130 : loss => 2.107757480591536,  accuracy: 0.25478, gradient_norm : 2.7837499277178805
[2025-09-12 18:49:33,570][flp2p.graph_runner][INFO] - Test, Round 130 : loss => 2.17515099170804,  accuracy: 0.2337
[2025-09-12 18:49:44,205][flp2p.graph_runner][INFO] - Train, Round 131 : loss => 2.1528550796210766,  accuracy: 0.24504, gradient_norm : 2.3670057449023005
[2025-09-12 18:50:00,744][flp2p.graph_runner][INFO] - Test, Round 131 : loss => 2.031655918085575,  accuracy: 0.2872
[2025-09-12 18:50:11,758][flp2p.graph_runner][INFO] - Train, Round 132 : loss => 2.0172730553150178,  accuracy: 0.29028, gradient_norm : 1.478935107025174
[2025-09-12 18:50:28,529][flp2p.graph_runner][INFO] - Test, Round 132 : loss => 2.0057233241796495,  accuracy: 0.3002
[2025-09-12 18:50:39,526][flp2p.graph_runner][INFO] - Train, Round 133 : loss => 1.9923812238872052,  accuracy: 0.30828, gradient_norm : 1.3894039042736497
[2025-09-12 18:50:56,057][flp2p.graph_runner][INFO] - Test, Round 133 : loss => 1.98982686971426,  accuracy: 0.3045
[2025-09-12 18:51:06,922][flp2p.graph_runner][INFO] - Train, Round 134 : loss => 1.9761532427370547,  accuracy: 0.30854, gradient_norm : 1.4672882708049655
[2025-09-12 18:51:18,894][flp2p.graph_runner][INFO] - Test, Round 134 : loss => 1.9849282710433007,  accuracy: 0.3086
[2025-09-12 18:51:29,877][flp2p.graph_runner][INFO] - Train, Round 135 : loss => 1.969822846353054,  accuracy: 0.31048, gradient_norm : 1.5972006886462493
[2025-09-12 18:51:41,417][flp2p.graph_runner][INFO] - Test, Round 135 : loss => 1.9867678921282292,  accuracy: 0.3013
[2025-09-12 18:51:52,571][flp2p.graph_runner][INFO] - Train, Round 136 : loss => 1.9702752706408502,  accuracy: 0.30822, gradient_norm : 1.683763714250637
[2025-09-12 18:52:07,755][flp2p.graph_runner][INFO] - Test, Round 136 : loss => 1.991486722868681,  accuracy: 0.3006
[2025-09-12 18:52:18,665][flp2p.graph_runner][INFO] - Train, Round 137 : loss => 1.9724245209991933,  accuracy: 0.30714, gradient_norm : 1.833209605413137
[2025-09-12 18:52:41,047][flp2p.graph_runner][INFO] - Test, Round 137 : loss => 2.0132921579241754,  accuracy: 0.2923
[2025-09-12 18:52:52,238][flp2p.graph_runner][INFO] - Train, Round 138 : loss => 1.989491548538208,  accuracy: 0.29674, gradient_norm : 2.1874371109523274
[2025-09-12 18:53:09,103][flp2p.graph_runner][INFO] - Test, Round 138 : loss => 2.0823264450371264,  accuracy: 0.264
[2025-09-12 18:53:20,297][flp2p.graph_runner][INFO] - Train, Round 139 : loss => 2.0530344691872595,  accuracy: 0.27338, gradient_norm : 2.5093694690538184
[2025-09-12 18:53:37,062][flp2p.graph_runner][INFO] - Test, Round 139 : loss => 2.0971125156223773,  accuracy: 0.2571
[2025-09-12 18:53:48,035][flp2p.graph_runner][INFO] - Train, Round 140 : loss => 2.0615565232932567,  accuracy: 0.26528, gradient_norm : 2.6971892863616604
[2025-09-12 18:54:04,935][flp2p.graph_runner][INFO] - Test, Round 140 : loss => 2.104465239059925,  accuracy: 0.2583
[2025-09-12 18:54:15,678][flp2p.graph_runner][INFO] - Train, Round 141 : loss => 2.0794845613837243,  accuracy: 0.26648, gradient_norm : 2.402446477228276
[2025-09-12 18:54:42,187][flp2p.graph_runner][INFO] - Test, Round 141 : loss => 2.0065572162926197,  accuracy: 0.2866
[2025-09-12 18:54:53,063][flp2p.graph_runner][INFO] - Train, Round 142 : loss => 1.9878734350204468,  accuracy: 0.29122, gradient_norm : 1.8198711636782583
[2025-09-12 18:55:11,927][flp2p.graph_runner][INFO] - Test, Round 142 : loss => 1.9822993735015393,  accuracy: 0.3008
[2025-09-12 18:55:22,883][flp2p.graph_runner][INFO] - Train, Round 143 : loss => 1.9660403813421725,  accuracy: 0.30626, gradient_norm : 1.7415229776394086
[2025-09-12 18:55:41,388][flp2p.graph_runner][INFO] - Test, Round 143 : loss => 1.9657352567076682,  accuracy: 0.3071
[2025-09-12 18:55:52,340][flp2p.graph_runner][INFO] - Train, Round 144 : loss => 1.9481243620812894,  accuracy: 0.3098, gradient_norm : 1.72665257076626
[2025-09-12 18:56:15,140][flp2p.graph_runner][INFO] - Test, Round 144 : loss => 1.9632018068373203,  accuracy: 0.3054
[2025-09-12 18:56:26,176][flp2p.graph_runner][INFO] - Train, Round 145 : loss => 1.9453744897246361,  accuracy: 0.30932, gradient_norm : 1.864700101429072
[2025-09-12 18:56:37,862][flp2p.graph_runner][INFO] - Test, Round 145 : loss => 1.9745035923182965,  accuracy: 0.295
[2025-09-12 18:56:49,057][flp2p.graph_runner][INFO] - Train, Round 146 : loss => 1.9524156272411346,  accuracy: 0.30252, gradient_norm : 2.0272338011096984
[2025-09-12 18:57:00,509][flp2p.graph_runner][INFO] - Test, Round 146 : loss => 1.9994495110332966,  accuracy: 0.2914
[2025-09-12 18:57:11,695][flp2p.graph_runner][INFO] - Train, Round 147 : loss => 1.9758031100034714,  accuracy: 0.2993, gradient_norm : 2.1152976015661946
[2025-09-12 18:57:29,698][flp2p.graph_runner][INFO] - Test, Round 147 : loss => 1.996081176584959,  accuracy: 0.287
[2025-09-12 18:57:40,617][flp2p.graph_runner][INFO] - Train, Round 148 : loss => 1.9687530800700188,  accuracy: 0.29684, gradient_norm : 2.2470121723131866
[2025-09-12 18:58:06,260][flp2p.graph_runner][INFO] - Test, Round 148 : loss => 2.008739974528551,  accuracy: 0.2861
[2025-09-12 18:58:17,378][flp2p.graph_runner][INFO] - Train, Round 149 : loss => 1.983537011295557,  accuracy: 0.29364, gradient_norm : 2.3143406642625868
[2025-09-12 18:58:35,832][flp2p.graph_runner][INFO] - Test, Round 149 : loss => 2.0033505144596098,  accuracy: 0.2845
[2025-09-12 18:58:47,095][flp2p.graph_runner][INFO] - Train, Round 150 : loss => 1.9755294981598854,  accuracy: 0.29256, gradient_norm : 2.440630995720926
[2025-09-12 18:59:05,238][flp2p.graph_runner][INFO] - Test, Round 150 : loss => 2.0366054585874083,  accuracy: 0.2828
[2025-09-12 18:59:16,351][flp2p.graph_runner][INFO] - Train, Round 151 : loss => 2.0088240857422353,  accuracy: 0.28944, gradient_norm : 2.3064962404783715
[2025-09-12 18:59:36,820][flp2p.graph_runner][INFO] - Test, Round 151 : loss => 1.966691795808077,  accuracy: 0.2977
[2025-09-12 18:59:47,770][flp2p.graph_runner][INFO] - Train, Round 152 : loss => 1.9451579856872558,  accuracy: 0.30446, gradient_norm : 2.0107704996250106
[2025-09-12 19:00:09,496][flp2p.graph_runner][INFO] - Test, Round 152 : loss => 1.9642500573635102,  accuracy: 0.3068
[2025-09-12 19:00:20,398][flp2p.graph_runner][INFO] - Train, Round 153 : loss => 1.941388520747423,  accuracy: 0.30874, gradient_norm : 1.9762888018290239
[2025-09-12 19:00:39,545][flp2p.graph_runner][INFO] - Test, Round 153 : loss => 1.9433520817279815,  accuracy: 0.3092
[2025-09-12 19:00:50,470][flp2p.graph_runner][INFO] - Train, Round 154 : loss => 1.9218477036058903,  accuracy: 0.31378, gradient_norm : 1.8562081940221404
[2025-09-12 19:01:13,693][flp2p.graph_runner][INFO] - Test, Round 154 : loss => 1.9407644256234169,  accuracy: 0.3164
[2025-09-12 19:01:24,656][flp2p.graph_runner][INFO] - Train, Round 155 : loss => 1.916487782150507,  accuracy: 0.31796, gradient_norm : 1.9137964855804155
[2025-09-12 19:01:39,949][flp2p.graph_runner][INFO] - Test, Round 155 : loss => 1.9305824088335037,  accuracy: 0.3103
[2025-09-12 19:01:51,083][flp2p.graph_runner][INFO] - Train, Round 156 : loss => 1.9083778831362723,  accuracy: 0.31422, gradient_norm : 2.043194976193362
[2025-09-12 19:02:02,911][flp2p.graph_runner][INFO] - Test, Round 156 : loss => 1.9552588165223599,  accuracy: 0.3098
[2025-09-12 19:02:14,215][flp2p.graph_runner][INFO] - Train, Round 157 : loss => 1.92683418571949,  accuracy: 0.31314, gradient_norm : 2.1725470313208213
[2025-09-12 19:02:25,455][flp2p.graph_runner][INFO] - Test, Round 157 : loss => 1.9487636108219624,  accuracy: 0.2988
[2025-09-12 19:02:36,664][flp2p.graph_runner][INFO] - Train, Round 158 : loss => 1.9237929075956344,  accuracy: 0.30526, gradient_norm : 2.2714349377290137
[2025-09-12 19:03:00,677][flp2p.graph_runner][INFO] - Test, Round 158 : loss => 1.9703722952008247,  accuracy: 0.2988
[2025-09-12 19:03:11,745][flp2p.graph_runner][INFO] - Train, Round 159 : loss => 1.9389540357887745,  accuracy: 0.30354, gradient_norm : 2.198532780134255
[2025-09-12 19:03:42,785][flp2p.graph_runner][INFO] - Test, Round 159 : loss => 1.9456492644548415,  accuracy: 0.3051
[2025-09-12 19:03:53,843][flp2p.graph_runner][INFO] - Train, Round 160 : loss => 1.9217009384930135,  accuracy: 0.30932, gradient_norm : 2.1781863343038266
[2025-09-12 19:04:07,726][flp2p.graph_runner][INFO] - Test, Round 160 : loss => 1.9531041574954986,  accuracy: 0.3091
[2025-09-12 19:04:18,677][flp2p.graph_runner][INFO] - Train, Round 161 : loss => 1.9227990943193436,  accuracy: 0.31178, gradient_norm : 2.067382862404873
[2025-09-12 19:04:34,112][flp2p.graph_runner][INFO] - Test, Round 161 : loss => 1.925005002963543,  accuracy: 0.3088
[2025-09-12 19:04:45,052][flp2p.graph_runner][INFO] - Train, Round 162 : loss => 1.9016329136490822,  accuracy: 0.31718, gradient_norm : 2.0807023095859463
[2025-09-12 19:05:01,677][flp2p.graph_runner][INFO] - Test, Round 162 : loss => 1.9476352404356003,  accuracy: 0.3025
[2025-09-12 19:05:12,647][flp2p.graph_runner][INFO] - Train, Round 163 : loss => 1.91469142049551,  accuracy: 0.31186, gradient_norm : 2.1691546571665317
[2025-09-12 19:05:31,001][flp2p.graph_runner][INFO] - Test, Round 163 : loss => 1.9361676041603089,  accuracy: 0.3043
[2025-09-12 19:05:41,731][flp2p.graph_runner][INFO] - Train, Round 164 : loss => 1.9107237268984318,  accuracy: 0.3136, gradient_norm : 2.250701221995271
[2025-09-12 19:06:00,291][flp2p.graph_runner][INFO] - Test, Round 164 : loss => 1.9658864363789559,  accuracy: 0.2886
[2025-09-12 19:06:11,172][flp2p.graph_runner][INFO] - Train, Round 165 : loss => 1.9299336226284505,  accuracy: 0.30204, gradient_norm : 2.3872469009812534
[2025-09-12 19:06:29,524][flp2p.graph_runner][INFO] - Test, Round 165 : loss => 1.96310693885684,  accuracy: 0.2949
[2025-09-12 19:06:40,484][flp2p.graph_runner][INFO] - Train, Round 166 : loss => 1.936329265087843,  accuracy: 0.30354, gradient_norm : 2.3284271874911293
[2025-09-12 19:06:58,536][flp2p.graph_runner][INFO] - Test, Round 166 : loss => 1.9454733480930329,  accuracy: 0.2975
[2025-09-12 19:07:09,168][flp2p.graph_runner][INFO] - Train, Round 167 : loss => 1.9125201734900474,  accuracy: 0.30912, gradient_norm : 2.0092235343534437
[2025-09-12 19:07:26,011][flp2p.graph_runner][INFO] - Test, Round 167 : loss => 1.8963482009768486,  accuracy: 0.3172
[2025-09-12 19:07:36,783][flp2p.graph_runner][INFO] - Train, Round 168 : loss => 1.8733061270415783,  accuracy: 0.32648, gradient_norm : 1.8355399834320876
[2025-09-12 19:07:53,062][flp2p.graph_runner][INFO] - Test, Round 168 : loss => 1.8904754100322723,  accuracy: 0.331
[2025-09-12 19:08:03,957][flp2p.graph_runner][INFO] - Train, Round 169 : loss => 1.862012647688389,  accuracy: 0.33502, gradient_norm : 1.7890636692473627
[2025-09-12 19:08:19,030][flp2p.graph_runner][INFO] - Test, Round 169 : loss => 1.8786434776186942,  accuracy: 0.3295
[2025-09-12 19:08:29,794][flp2p.graph_runner][INFO] - Train, Round 170 : loss => 1.85307727470994,  accuracy: 0.33766, gradient_norm : 1.856859194624806
[2025-09-12 19:08:44,916][flp2p.graph_runner][INFO] - Test, Round 170 : loss => 1.8809196461439133,  accuracy: 0.3286
[2025-09-12 19:08:55,902][flp2p.graph_runner][INFO] - Train, Round 171 : loss => 1.8515182121098042,  accuracy: 0.3377, gradient_norm : 1.8920546513852543
[2025-09-12 19:09:10,844][flp2p.graph_runner][INFO] - Test, Round 171 : loss => 1.8853377109766007,  accuracy: 0.3304
[2025-09-12 19:09:21,633][flp2p.graph_runner][INFO] - Train, Round 172 : loss => 1.8556153896450995,  accuracy: 0.33582, gradient_norm : 2.123460621171728
[2025-09-12 19:09:35,891][flp2p.graph_runner][INFO] - Test, Round 172 : loss => 1.9255149271249772,  accuracy: 0.3191
[2025-09-12 19:09:46,612][flp2p.graph_runner][INFO] - Train, Round 173 : loss => 1.8883699490129948,  accuracy: 0.32656, gradient_norm : 2.279462512363051
[2025-09-12 19:10:01,099][flp2p.graph_runner][INFO] - Test, Round 173 : loss => 1.9466107378780841,  accuracy: 0.3109
[2025-09-12 19:10:11,963][flp2p.graph_runner][INFO] - Train, Round 174 : loss => 1.9047724311053753,  accuracy: 0.32068, gradient_norm : 2.575826576869681
[2025-09-12 19:10:25,741][flp2p.graph_runner][INFO] - Test, Round 174 : loss => 2.008475362175703,  accuracy: 0.2923
[2025-09-12 19:10:36,628][flp2p.graph_runner][INFO] - Train, Round 175 : loss => 1.963801762163639,  accuracy: 0.30116, gradient_norm : 2.8127290050618923
[2025-09-12 19:10:49,118][flp2p.graph_runner][INFO] - Test, Round 175 : loss => 2.0109493227064608,  accuracy: 0.2807
[2025-09-12 19:10:59,956][flp2p.graph_runner][INFO] - Train, Round 176 : loss => 1.9606088054180146,  accuracy: 0.29766, gradient_norm : 2.8141219450258417
[2025-09-12 19:11:12,811][flp2p.graph_runner][INFO] - Test, Round 176 : loss => 2.004923039954901,  accuracy: 0.2876
[2025-09-12 19:11:23,716][flp2p.graph_runner][INFO] - Train, Round 177 : loss => 1.9667076790332794,  accuracy: 0.29564, gradient_norm : 2.538381988633239
[2025-09-12 19:11:36,150][flp2p.graph_runner][INFO] - Test, Round 177 : loss => 1.9227497768580915,  accuracy: 0.3055
[2025-09-12 19:11:46,980][flp2p.graph_runner][INFO] - Train, Round 178 : loss => 1.8879669930040837,  accuracy: 0.32008, gradient_norm : 2.0567270642964655
[2025-09-12 19:11:59,004][flp2p.graph_runner][INFO] - Test, Round 178 : loss => 1.8876072901904584,  accuracy: 0.3301
[2025-09-12 19:12:09,765][flp2p.graph_runner][INFO] - Train, Round 179 : loss => 1.857103574424982,  accuracy: 0.33884, gradient_norm : 1.8820311311101432
[2025-09-12 19:12:21,320][flp2p.graph_runner][INFO] - Test, Round 179 : loss => 1.869752339977026,  accuracy: 0.3347
[2025-09-12 19:12:32,080][flp2p.graph_runner][INFO] - Train, Round 180 : loss => 1.837564378976822,  accuracy: 0.34142, gradient_norm : 1.827247138262276
[2025-09-12 19:12:43,782][flp2p.graph_runner][INFO] - Test, Round 180 : loss => 1.857944368338585,  accuracy: 0.3415
[2025-09-12 19:12:54,552][flp2p.graph_runner][INFO] - Train, Round 181 : loss => 1.8256542484462261,  accuracy: 0.34732, gradient_norm : 1.89262379169885
[2025-09-12 19:13:06,268][flp2p.graph_runner][INFO] - Test, Round 181 : loss => 1.8584611673533917,  accuracy: 0.3414
[2025-09-12 19:13:17,228][flp2p.graph_runner][INFO] - Train, Round 182 : loss => 1.823859760761261,  accuracy: 0.34506, gradient_norm : 1.976879240739647
[2025-09-12 19:13:28,918][flp2p.graph_runner][INFO] - Test, Round 182 : loss => 1.8629713088929654,  accuracy: 0.3356
[2025-09-12 19:13:39,646][flp2p.graph_runner][INFO] - Train, Round 183 : loss => 1.8273472598195075,  accuracy: 0.34396, gradient_norm : 2.072448160301342
[2025-09-12 19:13:51,325][flp2p.graph_runner][INFO] - Test, Round 183 : loss => 1.891747240960598,  accuracy: 0.313
[2025-09-12 19:14:02,351][flp2p.graph_runner][INFO] - Train, Round 184 : loss => 1.8487891700863839,  accuracy: 0.32802, gradient_norm : 2.3626789351046855
[2025-09-12 19:14:13,882][flp2p.graph_runner][INFO] - Test, Round 184 : loss => 1.9226654275774955,  accuracy: 0.3076
[2025-09-12 19:14:24,585][flp2p.graph_runner][INFO] - Train, Round 185 : loss => 1.8800174514949322,  accuracy: 0.3196, gradient_norm : 2.5186298480898133
[2025-09-12 19:14:36,258][flp2p.graph_runner][INFO] - Test, Round 185 : loss => 1.9322675470173358,  accuracy: 0.2948
[2025-09-12 19:14:47,045][flp2p.graph_runner][INFO] - Train, Round 186 : loss => 1.8826415127515792,  accuracy: 0.30986, gradient_norm : 2.433610279859478
[2025-09-12 19:14:58,710][flp2p.graph_runner][INFO] - Test, Round 186 : loss => 1.9112514846980573,  accuracy: 0.313
[2025-09-12 19:15:09,380][flp2p.graph_runner][INFO] - Train, Round 187 : loss => 1.8707384400069713,  accuracy: 0.32732, gradient_norm : 2.327418641750445
[2025-09-12 19:15:21,114][flp2p.graph_runner][INFO] - Test, Round 187 : loss => 1.8998392045795918,  accuracy: 0.3066
[2025-09-12 19:15:31,870][flp2p.graph_runner][INFO] - Train, Round 188 : loss => 1.8554618057608605,  accuracy: 0.32366, gradient_norm : 2.261004318589197
[2025-09-12 19:15:43,494][flp2p.graph_runner][INFO] - Test, Round 188 : loss => 1.8745652833521367,  accuracy: 0.3224
[2025-09-12 19:15:54,289][flp2p.graph_runner][INFO] - Train, Round 189 : loss => 1.8370904739201068,  accuracy: 0.33808, gradient_norm : 2.1737257919782844
[2025-09-12 19:16:05,992][flp2p.graph_runner][INFO] - Test, Round 189 : loss => 1.8767188331067561,  accuracy: 0.3217
[2025-09-12 19:16:17,038][flp2p.graph_runner][INFO] - Train, Round 190 : loss => 1.8332693971693517,  accuracy: 0.33694, gradient_norm : 2.142295742344101
[2025-09-12 19:16:29,022][flp2p.graph_runner][INFO] - Test, Round 190 : loss => 1.8639476802885533,  accuracy: 0.3292
[2025-09-12 19:16:39,862][flp2p.graph_runner][INFO] - Train, Round 191 : loss => 1.8250208528339862,  accuracy: 0.3435, gradient_norm : 2.1280573473536335
[2025-09-12 19:16:51,509][flp2p.graph_runner][INFO] - Test, Round 191 : loss => 1.872379321461916,  accuracy: 0.3209
[2025-09-12 19:17:02,419][flp2p.graph_runner][INFO] - Train, Round 192 : loss => 1.8257032655179501,  accuracy: 0.33758, gradient_norm : 2.148703608376976
[2025-09-12 19:17:14,133][flp2p.graph_runner][INFO] - Test, Round 192 : loss => 1.8515277409076691,  accuracy: 0.3368
[2025-09-12 19:17:24,902][flp2p.graph_runner][INFO] - Train, Round 193 : loss => 1.8106964088976383,  accuracy: 0.35156, gradient_norm : 2.176082791984564
[2025-09-12 19:17:36,425][flp2p.graph_runner][INFO] - Test, Round 193 : loss => 1.868908624202013,  accuracy: 0.3267
[2025-09-12 19:17:47,272][flp2p.graph_runner][INFO] - Train, Round 194 : loss => 1.8211241687834263,  accuracy: 0.34196, gradient_norm : 2.2450446179675128
[2025-09-12 19:17:58,749][flp2p.graph_runner][INFO] - Test, Round 194 : loss => 1.862849032229185,  accuracy: 0.3351
[2025-09-12 19:18:09,588][flp2p.graph_runner][INFO] - Train, Round 195 : loss => 1.8181763911247253,  accuracy: 0.34888, gradient_norm : 2.2986586687942943
[2025-09-12 19:18:21,249][flp2p.graph_runner][INFO] - Test, Round 195 : loss => 1.874993808978796,  accuracy: 0.3325
[2025-09-12 19:18:32,198][flp2p.graph_runner][INFO] - Train, Round 196 : loss => 1.8255449330806732,  accuracy: 0.3471, gradient_norm : 2.4486764831687884
[2025-09-12 19:18:43,781][flp2p.graph_runner][INFO] - Test, Round 196 : loss => 1.9098325082600116,  accuracy: 0.3287
[2025-09-12 19:18:54,749][flp2p.graph_runner][INFO] - Train, Round 197 : loss => 1.8570340825617313,  accuracy: 0.33902, gradient_norm : 2.5771656433467287
[2025-09-12 19:19:06,431][flp2p.graph_runner][INFO] - Test, Round 197 : loss => 1.9150254101395607,  accuracy: 0.325
[2025-09-12 19:19:17,116][flp2p.graph_runner][INFO] - Train, Round 198 : loss => 1.859607426226139,  accuracy: 0.3409, gradient_norm : 2.757806694210297
[2025-09-12 19:19:28,895][flp2p.graph_runner][INFO] - Test, Round 198 : loss => 1.970497433423996,  accuracy: 0.3029
[2025-09-12 19:19:39,724][flp2p.graph_runner][INFO] - Train, Round 199 : loss => 1.910891612917185,  accuracy: 0.31964, gradient_norm : 2.901887811933434
[2025-09-12 19:19:51,289][flp2p.graph_runner][INFO] - Test, Round 199 : loss => 1.9505910968482494,  accuracy: 0.3075
[2025-09-12 19:20:02,154][flp2p.graph_runner][INFO] - Train, Round 200 : loss => 1.8971841552853583,  accuracy: 0.3254, gradient_norm : 2.86156157410129
[2025-09-12 19:20:13,778][flp2p.graph_runner][INFO] - Test, Round 200 : loss => 1.9548770162582398,  accuracy: 0.2932
[2025-09-12 19:20:24,554][flp2p.graph_runner][INFO] - Train, Round 201 : loss => 1.9010364671051503,  accuracy: 0.31362, gradient_norm : 2.5540499907128793
[2025-09-12 19:20:36,257][flp2p.graph_runner][INFO] - Test, Round 201 : loss => 1.8615673659443854,  accuracy: 0.3321
[2025-09-12 19:20:47,128][flp2p.graph_runner][INFO] - Train, Round 202 : loss => 1.8207759024202823,  accuracy: 0.34702, gradient_norm : 2.1006088263475196
[2025-09-12 19:20:58,672][flp2p.graph_runner][INFO] - Test, Round 202 : loss => 1.8359250640571116,  accuracy: 0.3465
[2025-09-12 19:21:09,396][flp2p.graph_runner][INFO] - Train, Round 203 : loss => 1.789547058492899,  accuracy: 0.36158, gradient_norm : 1.9232296208083925
[2025-09-12 19:21:20,874][flp2p.graph_runner][INFO] - Test, Round 203 : loss => 1.820912499755621,  accuracy: 0.3503
[2025-09-12 19:21:31,742][flp2p.graph_runner][INFO] - Train, Round 204 : loss => 1.7775825276970862,  accuracy: 0.36646, gradient_norm : 1.9437796039627622
[2025-09-12 19:21:43,329][flp2p.graph_runner][INFO] - Test, Round 204 : loss => 1.828064718604088,  accuracy: 0.3399
[2025-09-12 19:21:54,188][flp2p.graph_runner][INFO] - Train, Round 205 : loss => 1.7767391830682755,  accuracy: 0.36148, gradient_norm : 2.1079963453259616
[2025-09-12 19:22:05,789][flp2p.graph_runner][INFO] - Test, Round 205 : loss => 1.8319193449556828,  accuracy: 0.3428
[2025-09-12 19:22:16,652][flp2p.graph_runner][INFO] - Train, Round 206 : loss => 1.784876090735197,  accuracy: 0.35996, gradient_norm : 2.2828622709475193
[2025-09-12 19:22:28,296][flp2p.graph_runner][INFO] - Test, Round 206 : loss => 1.8477012603342533,  accuracy: 0.3324
[2025-09-12 19:22:39,123][flp2p.graph_runner][INFO] - Train, Round 207 : loss => 1.7908550137281418,  accuracy: 0.3522, gradient_norm : 2.3753837945830703
[2025-09-12 19:22:50,821][flp2p.graph_runner][INFO] - Test, Round 207 : loss => 1.8505179148495197,  accuracy: 0.3404
[2025-09-12 19:23:01,737][flp2p.graph_runner][INFO] - Train, Round 208 : loss => 1.7988911148905755,  accuracy: 0.35688, gradient_norm : 2.4917973278836416
[2025-09-12 19:23:13,497][flp2p.graph_runner][INFO] - Test, Round 208 : loss => 1.8624890207111835,  accuracy: 0.3283
[2025-09-12 19:23:24,411][flp2p.graph_runner][INFO] - Train, Round 209 : loss => 1.803123258650303,  accuracy: 0.3466, gradient_norm : 2.3734974579115127
[2025-09-12 19:23:36,060][flp2p.graph_runner][INFO] - Test, Round 209 : loss => 1.844122565126419,  accuracy: 0.3386
[2025-09-12 19:23:46,801][flp2p.graph_runner][INFO] - Train, Round 210 : loss => 1.792349435389042,  accuracy: 0.35646, gradient_norm : 2.4332114639589433
[2025-09-12 19:23:58,210][flp2p.graph_runner][INFO] - Test, Round 210 : loss => 1.8527130362451076,  accuracy: 0.3355
[2025-09-12 19:24:09,125][flp2p.graph_runner][INFO] - Train, Round 211 : loss => 1.7937693658471108,  accuracy: 0.35496, gradient_norm : 2.3994082670598793
[2025-09-12 19:24:20,811][flp2p.graph_runner][INFO] - Test, Round 211 : loss => 1.8435287823617459,  accuracy: 0.3413
[2025-09-12 19:24:31,653][flp2p.graph_runner][INFO] - Train, Round 212 : loss => 1.790052888840437,  accuracy: 0.36088, gradient_norm : 2.387579564358244
[2025-09-12 19:24:43,301][flp2p.graph_runner][INFO] - Test, Round 212 : loss => 1.8404945688068868,  accuracy: 0.3399
[2025-09-12 19:24:54,069][flp2p.graph_runner][INFO] - Train, Round 213 : loss => 1.7804503808915615,  accuracy: 0.36006, gradient_norm : 2.3458980107566805
[2025-09-12 19:25:05,725][flp2p.graph_runner][INFO] - Test, Round 213 : loss => 1.833902677088976,  accuracy: 0.3482
[2025-09-12 19:25:16,485][flp2p.graph_runner][INFO] - Train, Round 214 : loss => 1.7784555403888225,  accuracy: 0.36644, gradient_norm : 2.390440155324683
[2025-09-12 19:25:28,795][flp2p.graph_runner][INFO] - Test, Round 214 : loss => 1.8303523854374886,  accuracy: 0.3468
[2025-09-12 19:25:39,648][flp2p.graph_runner][INFO] - Train, Round 215 : loss => 1.7705423571169376,  accuracy: 0.36442, gradient_norm : 2.2781781180572427
[2025-09-12 19:25:51,612][flp2p.graph_runner][INFO] - Test, Round 215 : loss => 1.8228556143701076,  accuracy: 0.3499
[2025-09-12 19:26:02,523][flp2p.graph_runner][INFO] - Train, Round 216 : loss => 1.7646304623782634,  accuracy: 0.36948, gradient_norm : 2.3668159052264928
[2025-09-12 19:26:15,095][flp2p.graph_runner][INFO] - Test, Round 216 : loss => 1.8227066544950008,  accuracy: 0.3533
[2025-09-12 19:26:25,929][flp2p.graph_runner][INFO] - Train, Round 217 : loss => 1.7602825909852982,  accuracy: 0.37198, gradient_norm : 2.4091826585378273
[2025-09-12 19:26:37,847][flp2p.graph_runner][INFO] - Test, Round 217 : loss => 1.8531199032604695,  accuracy: 0.3432
[2025-09-12 19:26:48,875][flp2p.graph_runner][INFO] - Train, Round 218 : loss => 1.7853086836636067,  accuracy: 0.36408, gradient_norm : 2.764325898390506
[2025-09-12 19:27:01,217][flp2p.graph_runner][INFO] - Test, Round 218 : loss => 1.9246827088177205,  accuracy: 0.3246
[2025-09-12 19:27:12,005][flp2p.graph_runner][INFO] - Train, Round 219 : loss => 1.8498240377008914,  accuracy: 0.34226, gradient_norm : 3.1039891422750525
[2025-09-12 19:27:24,010][flp2p.graph_runner][INFO] - Test, Round 219 : loss => 1.9844344099402427,  accuracy: 0.3014
[2025-09-12 19:27:34,958][flp2p.graph_runner][INFO] - Train, Round 220 : loss => 1.8956152406334876,  accuracy: 0.32658, gradient_norm : 3.329501485898912
[2025-09-12 19:27:47,046][flp2p.graph_runner][INFO] - Test, Round 220 : loss => 1.9759734430730342,  accuracy: 0.2987
[2025-09-12 19:27:57,739][flp2p.graph_runner][INFO] - Train, Round 221 : loss => 1.905662604123354,  accuracy: 0.31398, gradient_norm : 2.901210636265468
[2025-09-12 19:28:09,833][flp2p.graph_runner][INFO] - Test, Round 221 : loss => 1.834169322103262,  accuracy: 0.3481
[2025-09-12 19:28:20,625][flp2p.graph_runner][INFO] - Train, Round 222 : loss => 1.7746330380439759,  accuracy: 0.36882, gradient_norm : 2.137399358740589
[2025-09-12 19:28:32,232][flp2p.graph_runner][INFO] - Test, Round 222 : loss => 1.7907992033839226,  accuracy: 0.3674
[2025-09-12 19:28:43,068][flp2p.graph_runner][INFO] - Train, Round 223 : loss => 1.734255052804947,  accuracy: 0.38876, gradient_norm : 2.083182076777534
[2025-09-12 19:28:54,610][flp2p.graph_runner][INFO] - Test, Round 223 : loss => 1.791168928092718,  accuracy: 0.3623
[2025-09-12 19:29:05,579][flp2p.graph_runner][INFO] - Train, Round 224 : loss => 1.7304482366144658,  accuracy: 0.38432, gradient_norm : 2.094988002139837
[2025-09-12 19:29:17,210][flp2p.graph_runner][INFO] - Test, Round 224 : loss => 1.795051759004593,  accuracy: 0.357
[2025-09-12 19:29:28,075][flp2p.graph_runner][INFO] - Train, Round 225 : loss => 1.7322559536993503,  accuracy: 0.3819, gradient_norm : 2.319466426363502
[2025-09-12 19:29:39,800][flp2p.graph_runner][INFO] - Test, Round 225 : loss => 1.822769310349226,  accuracy: 0.3479
[2025-09-12 19:29:50,717][flp2p.graph_runner][INFO] - Train, Round 226 : loss => 1.7540112207829952,  accuracy: 0.37278, gradient_norm : 2.38040102070408
[2025-09-12 19:30:02,503][flp2p.graph_runner][INFO] - Test, Round 226 : loss => 1.8262784348487855,  accuracy: 0.345
[2025-09-12 19:30:13,211][flp2p.graph_runner][INFO] - Train, Round 227 : loss => 1.7549552677571774,  accuracy: 0.37392, gradient_norm : 2.660746704771826
[2025-09-12 19:30:24,995][flp2p.graph_runner][INFO] - Test, Round 227 : loss => 1.8775864870369434,  accuracy: 0.3315
[2025-09-12 19:30:35,718][flp2p.graph_runner][INFO] - Train, Round 228 : loss => 1.801307393014431,  accuracy: 0.35314, gradient_norm : 2.6955661873453796
[2025-09-12 19:30:47,114][flp2p.graph_runner][INFO] - Test, Round 228 : loss => 1.8382361505627631,  accuracy: 0.3406
[2025-09-12 19:30:57,925][flp2p.graph_runner][INFO] - Train, Round 229 : loss => 1.7692744778096676,  accuracy: 0.36658, gradient_norm : 2.634156889768593
[2025-09-12 19:31:09,519][flp2p.graph_runner][INFO] - Test, Round 229 : loss => 1.8455925171554088,  accuracy: 0.3329
[2025-09-12 19:31:20,391][flp2p.graph_runner][INFO] - Train, Round 230 : loss => 1.7725369344651698,  accuracy: 0.35812, gradient_norm : 2.596982579812256
[2025-09-12 19:31:32,186][flp2p.graph_runner][INFO] - Test, Round 230 : loss => 1.8354548584640027,  accuracy: 0.3348
[2025-09-12 19:31:42,984][flp2p.graph_runner][INFO] - Train, Round 231 : loss => 1.7698593252897263,  accuracy: 0.3596, gradient_norm : 2.6197722669599517
[2025-09-12 19:31:54,776][flp2p.graph_runner][INFO] - Test, Round 231 : loss => 1.8330699064910412,  accuracy: 0.3318
[2025-09-12 19:32:05,654][flp2p.graph_runner][INFO] - Train, Round 232 : loss => 1.7603058305382728,  accuracy: 0.35772, gradient_norm : 2.4210259988339935
[2025-09-12 19:32:17,419][flp2p.graph_runner][INFO] - Test, Round 232 : loss => 1.8048156219363212,  accuracy: 0.3456
[2025-09-12 19:32:28,358][flp2p.graph_runner][INFO] - Train, Round 233 : loss => 1.7415415920317172,  accuracy: 0.37152, gradient_norm : 2.37056799294098
[2025-09-12 19:32:40,006][flp2p.graph_runner][INFO] - Test, Round 233 : loss => 1.8033955716669559,  accuracy: 0.3485
[2025-09-12 19:32:50,784][flp2p.graph_runner][INFO] - Train, Round 234 : loss => 1.7325124284625053,  accuracy: 0.37646, gradient_norm : 2.419553119227695
[2025-09-12 19:33:02,253][flp2p.graph_runner][INFO] - Test, Round 234 : loss => 1.808706131464243,  accuracy: 0.3452
[2025-09-12 19:33:13,073][flp2p.graph_runner][INFO] - Train, Round 235 : loss => 1.7399565072357654,  accuracy: 0.37128, gradient_norm : 2.4641429096617804
[2025-09-12 19:33:24,649][flp2p.graph_runner][INFO] - Test, Round 235 : loss => 1.8087044587731362,  accuracy: 0.3471
[2025-09-12 19:33:35,499][flp2p.graph_runner][INFO] - Train, Round 236 : loss => 1.7343812687695026,  accuracy: 0.37642, gradient_norm : 2.385376973385409
[2025-09-12 19:33:47,179][flp2p.graph_runner][INFO] - Test, Round 236 : loss => 1.7877006713449954,  accuracy: 0.3561
[2025-09-12 19:33:58,272][flp2p.graph_runner][INFO] - Train, Round 237 : loss => 1.7179313303530217,  accuracy: 0.385, gradient_norm : 2.4212272288927648
[2025-09-12 19:34:09,903][flp2p.graph_runner][INFO] - Test, Round 237 : loss => 1.8117506191313266,  accuracy: 0.3519
[2025-09-12 19:34:20,523][flp2p.graph_runner][INFO] - Train, Round 238 : loss => 1.7340301170945167,  accuracy: 0.37658, gradient_norm : 2.496801305566009
[2025-09-12 19:34:32,012][flp2p.graph_runner][INFO] - Test, Round 238 : loss => 1.795024807226658,  accuracy: 0.3596
[2025-09-12 19:34:42,813][flp2p.graph_runner][INFO] - Train, Round 239 : loss => 1.7213122528791427,  accuracy: 0.3848, gradient_norm : 2.5840237659982814
[2025-09-12 19:34:54,548][flp2p.graph_runner][INFO] - Test, Round 239 : loss => 1.8311181672096253,  accuracy: 0.3515
[2025-09-12 19:35:05,449][flp2p.graph_runner][INFO] - Train, Round 240 : loss => 1.7499908047914505,  accuracy: 0.37802, gradient_norm : 2.6916345714593364
[2025-09-12 19:35:17,026][flp2p.graph_runner][INFO] - Test, Round 240 : loss => 1.8313665016293526,  accuracy: 0.3493
[2025-09-12 19:35:27,869][flp2p.graph_runner][INFO] - Train, Round 241 : loss => 1.7509167855978012,  accuracy: 0.37548, gradient_norm : 2.769212762325962
[2025-09-12 19:35:39,651][flp2p.graph_runner][INFO] - Test, Round 241 : loss => 1.8455687887191772,  accuracy: 0.3501
[2025-09-12 19:35:50,644][flp2p.graph_runner][INFO] - Train, Round 242 : loss => 1.7617657808959484,  accuracy: 0.37472, gradient_norm : 2.7440350659406243
[2025-09-12 19:36:02,287][flp2p.graph_runner][INFO] - Test, Round 242 : loss => 1.8295209830760957,  accuracy: 0.3544
[2025-09-12 19:36:13,116][flp2p.graph_runner][INFO] - Train, Round 243 : loss => 1.7454942163825036,  accuracy: 0.3804, gradient_norm : 2.8373565655227693
[2025-09-12 19:36:24,912][flp2p.graph_runner][INFO] - Test, Round 243 : loss => 1.871572101122141,  accuracy: 0.3434
[2025-09-12 19:36:35,714][flp2p.graph_runner][INFO] - Train, Round 244 : loss => 1.7830179366469383,  accuracy: 0.36842, gradient_norm : 2.9645876953257524
[2025-09-12 19:36:47,279][flp2p.graph_runner][INFO] - Test, Round 244 : loss => 1.857435129928589,  accuracy: 0.3495
[2025-09-12 19:36:58,276][flp2p.graph_runner][INFO] - Train, Round 245 : loss => 1.7694366382062434,  accuracy: 0.37712, gradient_norm : 2.9528050352346713
[2025-09-12 19:37:09,954][flp2p.graph_runner][INFO] - Test, Round 245 : loss => 1.8618344157755375,  accuracy: 0.3453
[2025-09-12 19:37:20,863][flp2p.graph_runner][INFO] - Train, Round 246 : loss => 1.7773699268698693,  accuracy: 0.36904, gradient_norm : 2.8178381090107347
[2025-09-12 19:37:32,618][flp2p.graph_runner][INFO] - Test, Round 246 : loss => 1.8196107556223868,  accuracy: 0.3592
[2025-09-12 19:37:43,241][flp2p.graph_runner][INFO] - Train, Round 247 : loss => 1.7382663963735103,  accuracy: 0.38444, gradient_norm : 2.566277277611456
[2025-09-12 19:37:54,980][flp2p.graph_runner][INFO] - Test, Round 247 : loss => 1.781565126144886,  accuracy: 0.3663
[2025-09-12 19:38:05,816][flp2p.graph_runner][INFO] - Train, Round 248 : loss => 1.702118551582098,  accuracy: 0.39562, gradient_norm : 2.356590507255068
[2025-09-12 19:38:17,539][flp2p.graph_runner][INFO] - Test, Round 248 : loss => 1.7602303837954998,  accuracy: 0.3751
[2025-09-12 19:38:28,361][flp2p.graph_runner][INFO] - Train, Round 249 : loss => 1.6799182078242303,  accuracy: 0.40426, gradient_norm : 2.25826131909345
[2025-09-12 19:38:39,960][flp2p.graph_runner][INFO] - Test, Round 249 : loss => 1.7425055835068226,  accuracy: 0.3749
[2025-09-12 19:38:50,791][flp2p.graph_runner][INFO] - Train, Round 250 : loss => 1.6619873447716236,  accuracy: 0.40638, gradient_norm : 2.328192117667511
[2025-09-12 19:39:02,390][flp2p.graph_runner][INFO] - Test, Round 250 : loss => 1.7440296865403653,  accuracy: 0.3779
[2025-09-12 19:39:13,388][flp2p.graph_runner][INFO] - Train, Round 251 : loss => 1.6617088161408902,  accuracy: 0.41016, gradient_norm : 2.2514581166430174
[2025-09-12 19:39:24,904][flp2p.graph_runner][INFO] - Test, Round 251 : loss => 1.7536570121705533,  accuracy: 0.3749
[2025-09-12 19:39:35,611][flp2p.graph_runner][INFO] - Train, Round 252 : loss => 1.66682582706213,  accuracy: 0.4044, gradient_norm : 2.533521570696145
[2025-09-12 19:39:47,368][flp2p.graph_runner][INFO] - Test, Round 252 : loss => 1.788471144670248,  accuracy: 0.361
[2025-09-12 19:39:58,296][flp2p.graph_runner][INFO] - Train, Round 253 : loss => 1.6958826959133149,  accuracy: 0.39296, gradient_norm : 2.6324486066278716
[2025-09-12 19:40:09,800][flp2p.graph_runner][INFO] - Test, Round 253 : loss => 1.8005117348253727,  accuracy: 0.3567
[2025-09-12 19:40:20,577][flp2p.graph_runner][INFO] - Train, Round 254 : loss => 1.705832516402006,  accuracy: 0.3849, gradient_norm : 2.80830521649869
[2025-09-12 19:40:32,334][flp2p.graph_runner][INFO] - Test, Round 254 : loss => 1.8194622075915337,  accuracy: 0.3435
[2025-09-12 19:40:43,271][flp2p.graph_runner][INFO] - Train, Round 255 : loss => 1.722189749032259,  accuracy: 0.37772, gradient_norm : 2.8896005214079343
[2025-09-12 19:40:54,885][flp2p.graph_runner][INFO] - Test, Round 255 : loss => 1.8547815579116345,  accuracy: 0.3391
[2025-09-12 19:41:05,763][flp2p.graph_runner][INFO] - Train, Round 256 : loss => 1.7563006599247455,  accuracy: 0.36562, gradient_norm : 3.0171626490379144
[2025-09-12 19:41:17,507][flp2p.graph_runner][INFO] - Test, Round 256 : loss => 1.8166568314373492,  accuracy: 0.3432
[2025-09-12 19:41:28,469][flp2p.graph_runner][INFO] - Train, Round 257 : loss => 1.7283297607302666,  accuracy: 0.37598, gradient_norm : 2.7308949920327166
[2025-09-12 19:41:40,145][flp2p.graph_runner][INFO] - Test, Round 257 : loss => 1.7894188926815986,  accuracy: 0.3537
[2025-09-12 19:41:51,058][flp2p.graph_runner][INFO] - Train, Round 258 : loss => 1.7013323092460633,  accuracy: 0.38624, gradient_norm : 2.6562203945595178
[2025-09-12 19:42:02,596][flp2p.graph_runner][INFO] - Test, Round 258 : loss => 1.7888523777246474,  accuracy: 0.3554
[2025-09-12 19:42:13,346][flp2p.graph_runner][INFO] - Train, Round 259 : loss => 1.7010992768406867,  accuracy: 0.38878, gradient_norm : 2.5962499042174603
[2025-09-12 19:42:25,003][flp2p.graph_runner][INFO] - Test, Round 259 : loss => 1.7911143617808818,  accuracy: 0.3472
[2025-09-12 19:42:35,881][flp2p.graph_runner][INFO] - Train, Round 260 : loss => 1.7002570420503615,  accuracy: 0.38152, gradient_norm : 2.748403224962905
[2025-09-12 19:42:47,599][flp2p.graph_runner][INFO] - Test, Round 260 : loss => 1.8015651869893075,  accuracy: 0.3505
[2025-09-12 19:42:58,378][flp2p.graph_runner][INFO] - Train, Round 261 : loss => 1.710771025568247,  accuracy: 0.3802, gradient_norm : 2.731071025672062
[2025-09-12 19:43:10,159][flp2p.graph_runner][INFO] - Test, Round 261 : loss => 1.7938289162695409,  accuracy: 0.3499
[2025-09-12 19:43:21,080][flp2p.graph_runner][INFO] - Train, Round 262 : loss => 1.7022542780637742,  accuracy: 0.38648, gradient_norm : 2.668269235283768
[2025-09-12 19:43:32,736][flp2p.graph_runner][INFO] - Test, Round 262 : loss => 1.7587858767330646,  accuracy: 0.3709
[2025-09-12 19:43:43,549][flp2p.graph_runner][INFO] - Train, Round 263 : loss => 1.6668654736876487,  accuracy: 0.40598, gradient_norm : 2.4718065437559757
[2025-09-12 19:43:55,252][flp2p.graph_runner][INFO] - Test, Round 263 : loss => 1.7543871475040913,  accuracy: 0.3681
[2025-09-12 19:44:06,062][flp2p.graph_runner][INFO] - Train, Round 264 : loss => 1.6592886154353619,  accuracy: 0.40208, gradient_norm : 2.653211907868985
[2025-09-12 19:44:17,478][flp2p.graph_runner][INFO] - Test, Round 264 : loss => 1.7944110446751118,  accuracy: 0.3647
[2025-09-12 19:44:28,186][flp2p.graph_runner][INFO] - Train, Round 265 : loss => 1.6923745593428612,  accuracy: 0.39914, gradient_norm : 2.6494747330135873
[2025-09-12 19:44:39,786][flp2p.graph_runner][INFO] - Test, Round 265 : loss => 1.7744700207948685,  accuracy: 0.3625
[2025-09-12 19:44:50,590][flp2p.graph_runner][INFO] - Train, Round 266 : loss => 1.6733526995778083,  accuracy: 0.39612, gradient_norm : 2.8256180741808925
[2025-09-12 19:45:02,231][flp2p.graph_runner][INFO] - Test, Round 266 : loss => 1.8244463945269584,  accuracy: 0.3593
[2025-09-12 19:45:13,094][flp2p.graph_runner][INFO] - Train, Round 267 : loss => 1.7157665579020978,  accuracy: 0.39176, gradient_norm : 2.980882559974069
[2025-09-12 19:45:24,959][flp2p.graph_runner][INFO] - Test, Round 267 : loss => 1.8177324236571788,  accuracy: 0.357
[2025-09-12 19:45:35,673][flp2p.graph_runner][INFO] - Train, Round 268 : loss => 1.7123763225972652,  accuracy: 0.38816, gradient_norm : 2.970031563377033
[2025-09-12 19:45:47,570][flp2p.graph_runner][INFO] - Test, Round 268 : loss => 1.8176207831680775,  accuracy: 0.3605
[2025-09-12 19:45:58,488][flp2p.graph_runner][INFO] - Train, Round 269 : loss => 1.7116642224788665,  accuracy: 0.39658, gradient_norm : 2.904857943150068
[2025-09-12 19:46:10,235][flp2p.graph_runner][INFO] - Test, Round 269 : loss => 1.7879755626022815,  accuracy: 0.3645
[2025-09-12 19:46:20,998][flp2p.graph_runner][INFO] - Train, Round 270 : loss => 1.6878549365699291,  accuracy: 0.39638, gradient_norm : 2.753461414254416
[2025-09-12 19:46:32,527][flp2p.graph_runner][INFO] - Test, Round 270 : loss => 1.767681672781706,  accuracy: 0.374
[2025-09-12 19:46:43,344][flp2p.graph_runner][INFO] - Train, Round 271 : loss => 1.6646860440075397,  accuracy: 0.41274, gradient_norm : 2.6818922749302594
[2025-09-12 19:46:55,084][flp2p.graph_runner][INFO] - Test, Round 271 : loss => 1.7581543031454085,  accuracy: 0.3762
[2025-09-12 19:47:05,932][flp2p.graph_runner][INFO] - Train, Round 272 : loss => 1.6560414764285087,  accuracy: 0.40802, gradient_norm : 2.656111033855458
[2025-09-12 19:47:17,580][flp2p.graph_runner][INFO] - Test, Round 272 : loss => 1.765733275705576,  accuracy: 0.3736
[2025-09-12 19:47:28,308][flp2p.graph_runner][INFO] - Train, Round 273 : loss => 1.658036738038063,  accuracy: 0.4148, gradient_norm : 2.6470558598921543
[2025-09-12 19:47:39,929][flp2p.graph_runner][INFO] - Test, Round 273 : loss => 1.7529786582231521,  accuracy: 0.3781
[2025-09-12 19:47:50,900][flp2p.graph_runner][INFO] - Train, Round 274 : loss => 1.6473274321854114,  accuracy: 0.41378, gradient_norm : 2.6763597216893653
[2025-09-12 19:48:02,285][flp2p.graph_runner][INFO] - Test, Round 274 : loss => 1.7685202335894108,  accuracy: 0.3716
[2025-09-12 19:48:13,257][flp2p.graph_runner][INFO] - Train, Round 275 : loss => 1.6567039699852466,  accuracy: 0.41162, gradient_norm : 2.8494843527801534
[2025-09-12 19:48:24,938][flp2p.graph_runner][INFO] - Test, Round 275 : loss => 1.7733234511971474,  accuracy: 0.3737
[2025-09-12 19:48:35,862][flp2p.graph_runner][INFO] - Train, Round 276 : loss => 1.6648478138446807,  accuracy: 0.4086, gradient_norm : 2.8371674331810666
[2025-09-12 19:48:47,395][flp2p.graph_runner][INFO] - Test, Round 276 : loss => 1.7788745354950428,  accuracy: 0.3635
[2025-09-12 19:48:58,266][flp2p.graph_runner][INFO] - Train, Round 277 : loss => 1.6631700178980828,  accuracy: 0.40344, gradient_norm : 2.7976886454618786
[2025-09-12 19:49:09,907][flp2p.graph_runner][INFO] - Test, Round 277 : loss => 1.7514702872514725,  accuracy: 0.3772
[2025-09-12 19:49:20,870][flp2p.graph_runner][INFO] - Train, Round 278 : loss => 1.6420630677044392,  accuracy: 0.41516, gradient_norm : 2.828511231101583
[2025-09-12 19:49:32,595][flp2p.graph_runner][INFO] - Test, Round 278 : loss => 1.770026469987631,  accuracy: 0.3701
[2025-09-12 19:49:43,618][flp2p.graph_runner][INFO] - Train, Round 279 : loss => 1.6509237642586232,  accuracy: 0.41178, gradient_norm : 2.940735201781538
[2025-09-12 19:49:55,342][flp2p.graph_runner][INFO] - Test, Round 279 : loss => 1.779035743010044,  accuracy: 0.3716
[2025-09-12 19:50:06,185][flp2p.graph_runner][INFO] - Train, Round 280 : loss => 1.6655022285878658,  accuracy: 0.40974, gradient_norm : 2.8836271203143027
[2025-09-12 19:50:17,694][flp2p.graph_runner][INFO] - Test, Round 280 : loss => 1.7581061531305313,  accuracy: 0.375
[2025-09-12 19:50:28,520][flp2p.graph_runner][INFO] - Train, Round 281 : loss => 1.6403115199506282,  accuracy: 0.41514, gradient_norm : 2.6899508527415077
[2025-09-12 19:50:40,130][flp2p.graph_runner][INFO] - Test, Round 281 : loss => 1.7295741456985474,  accuracy: 0.385
[2025-09-12 19:50:50,784][flp2p.graph_runner][INFO] - Train, Round 282 : loss => 1.617694679647684,  accuracy: 0.42594, gradient_norm : 2.6947903695977424
[2025-09-12 19:51:02,407][flp2p.graph_runner][INFO] - Test, Round 282 : loss => 1.7311079779803753,  accuracy: 0.3889
[2025-09-12 19:51:13,417][flp2p.graph_runner][INFO] - Train, Round 283 : loss => 1.612633314281702,  accuracy: 0.42746, gradient_norm : 2.673246898065359
[2025-09-12 19:51:25,082][flp2p.graph_runner][INFO] - Test, Round 283 : loss => 1.728088429361582,  accuracy: 0.3878
[2025-09-12 19:51:35,873][flp2p.graph_runner][INFO] - Train, Round 284 : loss => 1.6094337528944016,  accuracy: 0.428, gradient_norm : 2.763465079657664
[2025-09-12 19:51:47,750][flp2p.graph_runner][INFO] - Test, Round 284 : loss => 1.7579449163794518,  accuracy: 0.3849
[2025-09-12 19:51:58,819][flp2p.graph_runner][INFO] - Train, Round 285 : loss => 1.6319825349748134,  accuracy: 0.4248, gradient_norm : 2.9404616485012607
[2025-09-12 19:52:10,472][flp2p.graph_runner][INFO] - Test, Round 285 : loss => 1.7877305630624294,  accuracy: 0.3651
[2025-09-12 19:52:21,424][flp2p.graph_runner][INFO] - Train, Round 286 : loss => 1.6596942035853863,  accuracy: 0.40638, gradient_norm : 3.103235982977401
[2025-09-12 19:52:33,130][flp2p.graph_runner][INFO] - Test, Round 286 : loss => 1.8179837106645107,  accuracy: 0.3645
[2025-09-12 19:52:44,141][flp2p.graph_runner][INFO] - Train, Round 287 : loss => 1.689987436980009,  accuracy: 0.4066, gradient_norm : 3.2324891544647767
[2025-09-12 19:52:55,725][flp2p.graph_runner][INFO] - Test, Round 287 : loss => 1.8067462479352951,  accuracy: 0.3556
[2025-09-12 19:53:06,488][flp2p.graph_runner][INFO] - Train, Round 288 : loss => 1.6812841022014617,  accuracy: 0.39392, gradient_norm : 3.084239255978781
[2025-09-12 19:53:18,127][flp2p.graph_runner][INFO] - Test, Round 288 : loss => 1.7811143643140792,  accuracy: 0.3686
[2025-09-12 19:53:28,970][flp2p.graph_runner][INFO] - Train, Round 289 : loss => 1.6615215155482292,  accuracy: 0.41074, gradient_norm : 2.9136939927298897
[2025-09-12 19:53:40,695][flp2p.graph_runner][INFO] - Test, Round 289 : loss => 1.7494592796742916,  accuracy: 0.3654
[2025-09-12 19:53:51,606][flp2p.graph_runner][INFO] - Train, Round 290 : loss => 1.6295786234736442,  accuracy: 0.41368, gradient_norm : 2.65818688063257
[2025-09-12 19:54:03,458][flp2p.graph_runner][INFO] - Test, Round 290 : loss => 1.7103277054607868,  accuracy: 0.3957
[2025-09-12 19:54:14,303][flp2p.graph_runner][INFO] - Train, Round 291 : loss => 1.5916549414396286,  accuracy: 0.4323, gradient_norm : 2.53734076191182
[2025-09-12 19:54:25,779][flp2p.graph_runner][INFO] - Test, Round 291 : loss => 1.7164928876578809,  accuracy: 0.3846
[2025-09-12 19:54:36,499][flp2p.graph_runner][INFO] - Train, Round 292 : loss => 1.5873921713232995,  accuracy: 0.43356, gradient_norm : 2.5745874567010616
[2025-09-12 19:54:48,127][flp2p.graph_runner][INFO] - Test, Round 292 : loss => 1.7286409767448903,  accuracy: 0.3792
[2025-09-12 19:54:58,923][flp2p.graph_runner][INFO] - Train, Round 293 : loss => 1.5959502378106116,  accuracy: 0.42882, gradient_norm : 2.87192033748388
[2025-09-12 19:55:10,498][flp2p.graph_runner][INFO] - Test, Round 293 : loss => 1.8289898500502109,  accuracy: 0.347
[2025-09-12 19:55:21,418][flp2p.graph_runner][INFO] - Train, Round 294 : loss => 1.6700555880367756,  accuracy: 0.40014, gradient_norm : 3.424048933044794
[2025-09-12 19:55:33,151][flp2p.graph_runner][INFO] - Test, Round 294 : loss => 1.9777875486254692,  accuracy: 0.3305
[2025-09-12 19:55:43,965][flp2p.graph_runner][INFO] - Train, Round 295 : loss => 1.8216159927845001,  accuracy: 0.36744, gradient_norm : 3.937709464662939
[2025-09-12 19:55:55,802][flp2p.graph_runner][INFO] - Test, Round 295 : loss => 1.952747717565298,  accuracy: 0.2909
[2025-09-12 19:56:06,661][flp2p.graph_runner][INFO] - Train, Round 296 : loss => 1.8051579482853413,  accuracy: 0.33342, gradient_norm : 3.6450006321453245
[2025-09-12 19:56:18,399][flp2p.graph_runner][INFO] - Test, Round 296 : loss => 1.8457872431755067,  accuracy: 0.3415
[2025-09-12 19:56:29,089][flp2p.graph_runner][INFO] - Train, Round 297 : loss => 1.726679768562317,  accuracy: 0.37764, gradient_norm : 2.9806296952007383
[2025-09-12 19:56:40,714][flp2p.graph_runner][INFO] - Test, Round 297 : loss => 1.7417953928351402,  accuracy: 0.3731
[2025-09-12 19:56:51,456][flp2p.graph_runner][INFO] - Train, Round 298 : loss => 1.6315612936019896,  accuracy: 0.41348, gradient_norm : 2.4469802944061074
[2025-09-12 19:57:03,081][flp2p.graph_runner][INFO] - Test, Round 298 : loss => 1.7086273550868034,  accuracy: 0.3809
[2025-09-12 19:57:13,823][flp2p.graph_runner][INFO] - Train, Round 299 : loss => 1.5916968275606633,  accuracy: 0.42842, gradient_norm : 2.4232929746742844
[2025-09-12 19:57:25,528][flp2p.graph_runner][INFO] - Test, Round 299 : loss => 1.6987620868504048,  accuracy: 0.3907
[2025-09-12 19:57:36,455][flp2p.graph_runner][INFO] - Train, Round 300 : loss => 1.579629522562027,  accuracy: 0.43186, gradient_norm : 2.4613134361249482
[2025-09-12 19:57:48,131][flp2p.graph_runner][INFO] - Test, Round 300 : loss => 1.7118863767921924,  accuracy: 0.3812
[2025-09-12 19:57:59,188][flp2p.graph_runner][INFO] - Train, Round 301 : loss => 1.5822208972275258,  accuracy: 0.43016, gradient_norm : 2.5721121103577973
[2025-09-12 19:58:11,017][flp2p.graph_runner][INFO] - Test, Round 301 : loss => 1.7124680093050002,  accuracy: 0.3845
[2025-09-12 19:58:22,023][flp2p.graph_runner][INFO] - Train, Round 302 : loss => 1.5826720808446408,  accuracy: 0.42922, gradient_norm : 2.6743281936346466
[2025-09-12 19:58:33,776][flp2p.graph_runner][INFO] - Test, Round 302 : loss => 1.7231633527636527,  accuracy: 0.3776
[2025-09-12 19:58:44,648][flp2p.graph_runner][INFO] - Train, Round 303 : loss => 1.5869734832644462,  accuracy: 0.42648, gradient_norm : 2.774008661153536
[2025-09-12 19:58:56,201][flp2p.graph_runner][INFO] - Test, Round 303 : loss => 1.7352773864030837,  accuracy: 0.3769
[2025-09-12 19:59:07,019][flp2p.graph_runner][INFO] - Train, Round 304 : loss => 1.5998596136271954,  accuracy: 0.4217, gradient_norm : 2.898722092288531
[2025-09-12 19:59:18,586][flp2p.graph_runner][INFO] - Test, Round 304 : loss => 1.751146162557602,  accuracy: 0.365
[2025-09-12 19:59:29,391][flp2p.graph_runner][INFO] - Train, Round 305 : loss => 1.6085722175240518,  accuracy: 0.41374, gradient_norm : 2.96460977795901
[2025-09-12 19:59:40,940][flp2p.graph_runner][INFO] - Test, Round 305 : loss => 1.7422020793914794,  accuracy: 0.3727
[2025-09-12 19:59:51,836][flp2p.graph_runner][INFO] - Train, Round 306 : loss => 1.6027824671566486,  accuracy: 0.4187, gradient_norm : 2.797128453503456
[2025-09-12 20:00:03,695][flp2p.graph_runner][INFO] - Test, Round 306 : loss => 1.7376087705492973,  accuracy: 0.3729
[2025-09-12 20:00:14,421][flp2p.graph_runner][INFO] - Train, Round 307 : loss => 1.59357716396451,  accuracy: 0.4194, gradient_norm : 3.0073975743328254
[2025-09-12 20:00:26,168][flp2p.graph_runner][INFO] - Test, Round 307 : loss => 1.7764833980619907,  accuracy: 0.3646
[2025-09-12 20:00:37,037][flp2p.graph_runner][INFO] - Train, Round 308 : loss => 1.6301899494230747,  accuracy: 0.4086, gradient_norm : 3.10977372297873
[2025-09-12 20:00:48,836][flp2p.graph_runner][INFO] - Test, Round 308 : loss => 1.7799785162329673,  accuracy: 0.35
[2025-09-12 20:00:59,680][flp2p.graph_runner][INFO] - Train, Round 309 : loss => 1.6342327773571015,  accuracy: 0.3979, gradient_norm : 3.1610441048276487
[2025-09-12 20:01:11,388][flp2p.graph_runner][INFO] - Test, Round 309 : loss => 1.7765982468962669,  accuracy: 0.3641
[2025-09-12 20:01:22,303][flp2p.graph_runner][INFO] - Train, Round 310 : loss => 1.6339934712648392,  accuracy: 0.41112, gradient_norm : 3.0217162792548176
[2025-09-12 20:01:33,857][flp2p.graph_runner][INFO] - Test, Round 310 : loss => 1.7396042219698429,  accuracy: 0.3641
[2025-09-12 20:01:44,595][flp2p.graph_runner][INFO] - Train, Round 311 : loss => 1.600898472070694,  accuracy: 0.40862, gradient_norm : 2.8239607214741427
[2025-09-12 20:01:56,310][flp2p.graph_runner][INFO] - Test, Round 311 : loss => 1.731566183167696,  accuracy: 0.3813
[2025-09-12 20:02:07,202][flp2p.graph_runner][INFO] - Train, Round 312 : loss => 1.591491333693266,  accuracy: 0.43046, gradient_norm : 2.795180472310696
[2025-09-12 20:02:18,955][flp2p.graph_runner][INFO] - Test, Round 312 : loss => 1.709096803498268,  accuracy: 0.3793
[2025-09-12 20:02:29,789][flp2p.graph_runner][INFO] - Train, Round 313 : loss => 1.569387967288494,  accuracy: 0.42816, gradient_norm : 2.8773367471895614
[2025-09-12 20:02:41,571][flp2p.graph_runner][INFO] - Test, Round 313 : loss => 1.7448392968058586,  accuracy: 0.3749
[2025-09-12 20:02:52,189][flp2p.graph_runner][INFO] - Train, Round 314 : loss => 1.5959646528959275,  accuracy: 0.42884, gradient_norm : 2.775549025573733
[2025-09-12 20:03:03,924][flp2p.graph_runner][INFO] - Test, Round 314 : loss => 1.6975232792794706,  accuracy: 0.3939
[2025-09-12 20:03:14,623][flp2p.graph_runner][INFO] - Train, Round 315 : loss => 1.5583503973484039,  accuracy: 0.43922, gradient_norm : 2.7485286554544706
[2025-09-12 20:03:26,259][flp2p.graph_runner][INFO] - Test, Round 315 : loss => 1.7316770945072175,  accuracy: 0.3805
[2025-09-12 20:03:37,041][flp2p.graph_runner][INFO] - Train, Round 316 : loss => 1.574222073405981,  accuracy: 0.43728, gradient_norm : 2.927509202056706
[2025-09-12 20:03:48,211][flp2p.graph_runner][INFO] - Test, Round 316 : loss => 1.7564753419041634,  accuracy: 0.3828
[2025-09-12 20:03:59,028][flp2p.graph_runner][INFO] - Train, Round 317 : loss => 1.6061657616496086,  accuracy: 0.42894, gradient_norm : 3.0472860021148933
[2025-09-12 20:04:10,781][flp2p.graph_runner][INFO] - Test, Round 317 : loss => 1.7748222171664239,  accuracy: 0.3743
[2025-09-12 20:04:21,701][flp2p.graph_runner][INFO] - Train, Round 318 : loss => 1.6136624623835087,  accuracy: 0.42876, gradient_norm : 3.1735447557919496
[2025-09-12 20:04:33,399][flp2p.graph_runner][INFO] - Test, Round 318 : loss => 1.7835017541646958,  accuracy: 0.3776
[2025-09-12 20:04:44,283][flp2p.graph_runner][INFO] - Train, Round 319 : loss => 1.628531860858202,  accuracy: 0.42646, gradient_norm : 3.340466797509913
[2025-09-12 20:04:56,038][flp2p.graph_runner][INFO] - Test, Round 319 : loss => 1.8351509331464768,  accuracy: 0.369
[2025-09-12 20:05:07,060][flp2p.graph_runner][INFO] - Train, Round 320 : loss => 1.6718516983091831,  accuracy: 0.4159, gradient_norm : 3.4790184272998634
[2025-09-12 20:05:18,390][flp2p.graph_runner][INFO] - Test, Round 320 : loss => 1.8170387448787688,  accuracy: 0.3655
[2025-09-12 20:05:29,044][flp2p.graph_runner][INFO] - Train, Round 321 : loss => 1.6595870377123356,  accuracy: 0.41556, gradient_norm : 3.410774283138224
[2025-09-12 20:05:40,745][flp2p.graph_runner][INFO] - Test, Round 321 : loss => 1.826617662036419,  accuracy: 0.3572
[2025-09-12 20:05:51,673][flp2p.graph_runner][INFO] - Train, Round 322 : loss => 1.6738141983747483,  accuracy: 0.40174, gradient_norm : 3.289848184334661
[2025-09-12 20:06:03,486][flp2p.graph_runner][INFO] - Test, Round 322 : loss => 1.739034943664074,  accuracy: 0.3799
[2025-09-12 20:06:14,256][flp2p.graph_runner][INFO] - Train, Round 323 : loss => 1.5947291673719883,  accuracy: 0.433, gradient_norm : 2.723812886858547
[2025-09-12 20:06:25,897][flp2p.graph_runner][INFO] - Test, Round 323 : loss => 1.6829080351352692,  accuracy: 0.398
[2025-09-12 20:06:36,712][flp2p.graph_runner][INFO] - Train, Round 324 : loss => 1.5375188590586186,  accuracy: 0.45302, gradient_norm : 2.4603945697165504
[2025-09-12 20:06:48,277][flp2p.graph_runner][INFO] - Test, Round 324 : loss => 1.6572445494771004,  accuracy: 0.4081
[2025-09-12 20:06:59,192][flp2p.graph_runner][INFO] - Train, Round 325 : loss => 1.5090179531276227,  accuracy: 0.46706, gradient_norm : 2.508080190764101
[2025-09-12 20:07:10,682][flp2p.graph_runner][INFO] - Test, Round 325 : loss => 1.6609967309474944,  accuracy: 0.4061
[2025-09-12 20:07:21,847][flp2p.graph_runner][INFO] - Train, Round 326 : loss => 1.5030026802420615,  accuracy: 0.46276, gradient_norm : 2.532945501450146
[2025-09-12 20:07:33,463][flp2p.graph_runner][INFO] - Test, Round 326 : loss => 1.6509671322464943,  accuracy: 0.4106
[2025-09-12 20:07:44,360][flp2p.graph_runner][INFO] - Train, Round 327 : loss => 1.4906187702715397,  accuracy: 0.4698, gradient_norm : 2.609346513779808
[2025-09-12 20:07:55,818][flp2p.graph_runner][INFO] - Test, Round 327 : loss => 1.6652148259341717,  accuracy: 0.407
[2025-09-12 20:08:06,616][flp2p.graph_runner][INFO] - Train, Round 328 : loss => 1.495790491849184,  accuracy: 0.46546, gradient_norm : 2.6691330432421654
[2025-09-12 20:08:18,148][flp2p.graph_runner][INFO] - Test, Round 328 : loss => 1.6706050475299359,  accuracy: 0.4103
[2025-09-12 20:08:28,955][flp2p.graph_runner][INFO] - Train, Round 329 : loss => 1.498749369084835,  accuracy: 0.46376, gradient_norm : 2.8562311090750785
[2025-09-12 20:08:40,550][flp2p.graph_runner][INFO] - Test, Round 329 : loss => 1.7087298214435578,  accuracy: 0.3969
[2025-09-12 20:08:51,185][flp2p.graph_runner][INFO] - Train, Round 330 : loss => 1.5301041495800018,  accuracy: 0.45568, gradient_norm : 3.018458477963392
[2025-09-12 20:09:02,903][flp2p.graph_runner][INFO] - Test, Round 330 : loss => 1.7573686068773269,  accuracy: 0.3771
[2025-09-12 20:09:13,928][flp2p.graph_runner][INFO] - Train, Round 331 : loss => 1.5749193198978901,  accuracy: 0.4366, gradient_norm : 3.4195939152425905
[2025-09-12 20:09:25,376][flp2p.graph_runner][INFO] - Test, Round 331 : loss => 1.8306421178758145,  accuracy: 0.3594
[2025-09-12 20:09:36,211][flp2p.graph_runner][INFO] - Train, Round 332 : loss => 1.6444142827391623,  accuracy: 0.41874, gradient_norm : 3.562514381988614
[2025-09-12 20:09:47,354][flp2p.graph_runner][INFO] - Test, Round 332 : loss => 1.800379781240225,  accuracy: 0.3512
[2025-09-12 20:09:58,195][flp2p.graph_runner][INFO] - Train, Round 333 : loss => 1.6222121238708496,  accuracy: 0.4069, gradient_norm : 3.426367951594057
[2025-09-12 20:10:09,647][flp2p.graph_runner][INFO] - Test, Round 333 : loss => 1.7810894903182983,  accuracy: 0.3654
[2025-09-12 20:10:20,677][flp2p.graph_runner][INFO] - Train, Round 334 : loss => 1.6109990768134594,  accuracy: 0.41964, gradient_norm : 3.0738938581881787
[2025-09-12 20:10:31,874][flp2p.graph_runner][INFO] - Test, Round 334 : loss => 1.6997110412299632,  accuracy: 0.3862
[2025-09-12 20:10:42,960][flp2p.graph_runner][INFO] - Train, Round 335 : loss => 1.5338484148681164,  accuracy: 0.44306, gradient_norm : 2.871354310149025
[2025-09-12 20:10:54,117][flp2p.graph_runner][INFO] - Test, Round 335 : loss => 1.7005415709376335,  accuracy: 0.3908
[2025-09-12 20:11:05,186][flp2p.graph_runner][INFO] - Train, Round 336 : loss => 1.5244792203605175,  accuracy: 0.45414, gradient_norm : 2.7936540162438566
[2025-09-12 20:11:16,280][flp2p.graph_runner][INFO] - Test, Round 336 : loss => 1.7117725449502468,  accuracy: 0.39
[2025-09-12 20:11:27,221][flp2p.graph_runner][INFO] - Train, Round 337 : loss => 1.533259510397911,  accuracy: 0.44538, gradient_norm : 3.0835669673294563
[2025-09-12 20:11:38,341][flp2p.graph_runner][INFO] - Test, Round 337 : loss => 1.7562467317402364,  accuracy: 0.3729
[2025-09-12 20:11:49,512][flp2p.graph_runner][INFO] - Train, Round 338 : loss => 1.5680756053328515,  accuracy: 0.43632, gradient_norm : 3.201274991920565
[2025-09-12 20:12:00,428][flp2p.graph_runner][INFO] - Test, Round 338 : loss => 1.7735608417749404,  accuracy: 0.3706
[2025-09-12 20:12:13,483][flp2p.graph_runner][INFO] - Train, Round 339 : loss => 1.5839777818322183,  accuracy: 0.42738, gradient_norm : 3.278695493030259
[2025-09-12 20:12:24,727][flp2p.graph_runner][INFO] - Test, Round 339 : loss => 1.7519025846838951,  accuracy: 0.3763
[2025-09-12 20:12:36,081][flp2p.graph_runner][INFO] - Train, Round 340 : loss => 1.5721544912457466,  accuracy: 0.4379, gradient_norm : 3.1507479375567238
[2025-09-12 20:12:47,157][flp2p.graph_runner][INFO] - Test, Round 340 : loss => 1.734540649986267,  accuracy: 0.3813
[2025-09-12 20:12:58,304][flp2p.graph_runner][INFO] - Train, Round 341 : loss => 1.5512066496908665,  accuracy: 0.44154, gradient_norm : 3.0783498469365607
[2025-09-12 20:13:09,296][flp2p.graph_runner][INFO] - Test, Round 341 : loss => 1.7190533309936524,  accuracy: 0.3928
[2025-09-12 20:13:20,273][flp2p.graph_runner][INFO] - Train, Round 342 : loss => 1.5428203791379929,  accuracy: 0.45126, gradient_norm : 2.9901971621281693
[2025-09-12 20:13:31,188][flp2p.graph_runner][INFO] - Test, Round 342 : loss => 1.712894604408741,  accuracy: 0.3816
[2025-09-12 20:13:42,070][flp2p.graph_runner][INFO] - Train, Round 343 : loss => 1.5289354871213436,  accuracy: 0.4487, gradient_norm : 3.0421520856168915
[2025-09-12 20:13:52,926][flp2p.graph_runner][INFO] - Test, Round 343 : loss => 1.723891394609213,  accuracy: 0.3875
[2025-09-12 20:14:03,844][flp2p.graph_runner][INFO] - Train, Round 344 : loss => 1.541553069204092,  accuracy: 0.44586, gradient_norm : 3.147195287194736
[2025-09-12 20:14:14,977][flp2p.graph_runner][INFO] - Test, Round 344 : loss => 1.753221381163597,  accuracy: 0.384
[2025-09-12 20:14:25,910][flp2p.graph_runner][INFO] - Train, Round 345 : loss => 1.5658539636433124,  accuracy: 0.44464, gradient_norm : 3.253252601686878
[2025-09-12 20:14:36,748][flp2p.graph_runner][INFO] - Test, Round 345 : loss => 1.7588773624420166,  accuracy: 0.3808
[2025-09-12 20:14:47,761][flp2p.graph_runner][INFO] - Train, Round 346 : loss => 1.5733635757863522,  accuracy: 0.4392, gradient_norm : 3.2660782321471156
[2025-09-12 20:15:00,251][flp2p.graph_runner][INFO] - Test, Round 346 : loss => 1.7667795289218426,  accuracy: 0.3832
[2025-09-12 20:15:11,251][flp2p.graph_runner][INFO] - Train, Round 347 : loss => 1.582432054579258,  accuracy: 0.44378, gradient_norm : 3.296668725418598
[2025-09-12 20:15:25,286][flp2p.graph_runner][INFO] - Test, Round 347 : loss => 1.7298579280197621,  accuracy: 0.384
[2025-09-12 20:15:36,151][flp2p.graph_runner][INFO] - Train, Round 348 : loss => 1.5482943785190582,  accuracy: 0.4476, gradient_norm : 3.0977642928556075
[2025-09-12 20:15:52,181][flp2p.graph_runner][INFO] - Test, Round 348 : loss => 1.7038969081699848,  accuracy: 0.4004
[2025-09-12 20:16:02,987][flp2p.graph_runner][INFO] - Train, Round 349 : loss => 1.5222086556255818,  accuracy: 0.46408, gradient_norm : 2.893490478014003
[2025-09-12 20:16:20,896][flp2p.graph_runner][INFO] - Test, Round 349 : loss => 1.665731812953949,  accuracy: 0.404
[2025-09-12 20:16:31,649][flp2p.graph_runner][INFO] - Train, Round 350 : loss => 1.4814723156392575,  accuracy: 0.46626, gradient_norm : 2.802182747358236
[2025-09-12 20:16:49,941][flp2p.graph_runner][INFO] - Test, Round 350 : loss => 1.6857120816707611,  accuracy: 0.4014
[2025-09-12 20:17:00,517][flp2p.graph_runner][INFO] - Train, Round 351 : loss => 1.4915416172146798,  accuracy: 0.4706, gradient_norm : 2.8627016030239876
[2025-09-12 20:17:18,767][flp2p.graph_runner][INFO] - Test, Round 351 : loss => 1.6621762145400047,  accuracy: 0.4055
[2025-09-12 20:17:29,663][flp2p.graph_runner][INFO] - Train, Round 352 : loss => 1.4688752184808254,  accuracy: 0.4697, gradient_norm : 2.888532668467379
[2025-09-12 20:17:47,069][flp2p.graph_runner][INFO] - Test, Round 352 : loss => 1.686006904476881,  accuracy: 0.4008
[2025-09-12 20:17:57,852][flp2p.graph_runner][INFO] - Train, Round 353 : loss => 1.4827535925805568,  accuracy: 0.47146, gradient_norm : 2.9689394131956517
[2025-09-12 20:18:14,318][flp2p.graph_runner][INFO] - Test, Round 353 : loss => 1.6902706227719784,  accuracy: 0.3968
[2025-09-12 20:18:25,066][flp2p.graph_runner][INFO] - Train, Round 354 : loss => 1.4877892018854617,  accuracy: 0.45788, gradient_norm : 3.2076042484417964
[2025-09-12 20:18:40,878][flp2p.graph_runner][INFO] - Test, Round 354 : loss => 1.775363299816847,  accuracy: 0.3717
[2025-09-12 20:18:51,703][flp2p.graph_runner][INFO] - Train, Round 355 : loss => 1.5577705313265324,  accuracy: 0.44066, gradient_norm : 3.546180678988315
[2025-09-12 20:19:07,916][flp2p.graph_runner][INFO] - Test, Round 355 : loss => 1.775838840520382,  accuracy: 0.3691
[2025-09-12 20:19:18,630][flp2p.graph_runner][INFO] - Train, Round 356 : loss => 1.5718323799967766,  accuracy: 0.4228, gradient_norm : 3.4070130683231885
[2025-09-12 20:19:34,257][flp2p.graph_runner][INFO] - Test, Round 356 : loss => 1.7625123851060867,  accuracy: 0.3627
[2025-09-12 20:19:45,097][flp2p.graph_runner][INFO] - Train, Round 357 : loss => 1.5556550498306752,  accuracy: 0.43578, gradient_norm : 3.1558017455920306
[2025-09-12 20:20:00,412][flp2p.graph_runner][INFO] - Test, Round 357 : loss => 1.6856774835050106,  accuracy: 0.3867
[2025-09-12 20:20:11,197][flp2p.graph_runner][INFO] - Train, Round 358 : loss => 1.495494179278612,  accuracy: 0.44956, gradient_norm : 3.0349679913367447
[2025-09-12 20:20:26,162][flp2p.graph_runner][INFO] - Test, Round 358 : loss => 1.7123135191857815,  accuracy: 0.3792
[2025-09-12 20:20:36,973][flp2p.graph_runner][INFO] - Train, Round 359 : loss => 1.51049581438303,  accuracy: 0.45678, gradient_norm : 2.9908842324414424
[2025-09-12 20:20:50,452][flp2p.graph_runner][INFO] - Test, Round 359 : loss => 1.6632839829862118,  accuracy: 0.4011
[2025-09-12 20:21:01,304][flp2p.graph_runner][INFO] - Train, Round 360 : loss => 1.4675054788589477,  accuracy: 0.46418, gradient_norm : 2.86398236449631
[2025-09-12 20:21:14,791][flp2p.graph_runner][INFO] - Test, Round 360 : loss => 1.702113507014513,  accuracy: 0.3885
[2025-09-12 20:21:25,535][flp2p.graph_runner][INFO] - Train, Round 361 : loss => 1.492420229613781,  accuracy: 0.46636, gradient_norm : 2.999314824381015
[2025-09-12 20:21:39,037][flp2p.graph_runner][INFO] - Test, Round 361 : loss => 1.673068359953165,  accuracy: 0.3981
[2025-09-12 20:21:49,744][flp2p.graph_runner][INFO] - Train, Round 362 : loss => 1.4695111706852912,  accuracy: 0.4654, gradient_norm : 3.0168424855020612
[2025-09-12 20:22:02,437][flp2p.graph_runner][INFO] - Test, Round 362 : loss => 1.6917745182335377,  accuracy: 0.3925
[2025-09-12 20:22:13,192][flp2p.graph_runner][INFO] - Train, Round 363 : loss => 1.4766256952285766,  accuracy: 0.46962, gradient_norm : 3.0722466782720277
[2025-09-12 20:22:25,771][flp2p.graph_runner][INFO] - Test, Round 363 : loss => 1.692330997467041,  accuracy: 0.3969
[2025-09-12 20:22:36,579][flp2p.graph_runner][INFO] - Train, Round 364 : loss => 1.482255826741457,  accuracy: 0.4628, gradient_norm : 3.1404118364034
[2025-09-12 20:22:49,373][flp2p.graph_runner][INFO] - Test, Round 364 : loss => 1.7139471061825753,  accuracy: 0.3846
[2025-09-12 20:23:00,015][flp2p.graph_runner][INFO] - Train, Round 365 : loss => 1.4907423177361487,  accuracy: 0.46194, gradient_norm : 3.23384438757078
[2025-09-12 20:23:11,630][flp2p.graph_runner][INFO] - Test, Round 365 : loss => 1.7191906165122985,  accuracy: 0.3917
[2025-09-12 20:23:22,605][flp2p.graph_runner][INFO] - Train, Round 366 : loss => 1.5034197457134724,  accuracy: 0.45606, gradient_norm : 3.3541659086454136
[2025-09-12 20:23:34,621][flp2p.graph_runner][INFO] - Test, Round 366 : loss => 1.7442263838529586,  accuracy: 0.3774
[2025-09-12 20:23:45,586][flp2p.graph_runner][INFO] - Train, Round 367 : loss => 1.5196400079131127,  accuracy: 0.44866, gradient_norm : 3.265145621750061
[2025-09-12 20:23:57,680][flp2p.graph_runner][INFO] - Test, Round 367 : loss => 1.7101644236266613,  accuracy: 0.3883
[2025-09-12 20:24:08,513][flp2p.graph_runner][INFO] - Train, Round 368 : loss => 1.4961133064329624,  accuracy: 0.45688, gradient_norm : 3.318595833400445
[2025-09-12 20:24:20,963][flp2p.graph_runner][INFO] - Test, Round 368 : loss => 1.7358726730644702,  accuracy: 0.384
[2025-09-12 20:24:31,744][flp2p.graph_runner][INFO] - Train, Round 369 : loss => 1.5179917094111444,  accuracy: 0.45562, gradient_norm : 3.180257060188304
[2025-09-12 20:24:43,198][flp2p.graph_runner][INFO] - Test, Round 369 : loss => 1.7028800474643708,  accuracy: 0.3847
[2025-09-12 20:24:54,156][flp2p.graph_runner][INFO] - Train, Round 370 : loss => 1.4851895733177662,  accuracy: 0.4632, gradient_norm : 3.205833671900306
[2025-09-12 20:25:05,580][flp2p.graph_runner][INFO] - Test, Round 370 : loss => 1.7437151630818843,  accuracy: 0.3779
[2025-09-12 20:25:16,495][flp2p.graph_runner][INFO] - Train, Round 371 : loss => 1.5195038245618344,  accuracy: 0.45518, gradient_norm : 3.3110203977196955
[2025-09-12 20:25:28,227][flp2p.graph_runner][INFO] - Test, Round 371 : loss => 1.7431916027545928,  accuracy: 0.3807
[2025-09-12 20:25:39,120][flp2p.graph_runner][INFO] - Train, Round 372 : loss => 1.5165356765687465,  accuracy: 0.44724, gradient_norm : 3.478755347545857
[2025-09-12 20:25:50,615][flp2p.graph_runner][INFO] - Test, Round 372 : loss => 1.7729799891769886,  accuracy: 0.3598
[2025-09-12 20:26:01,327][flp2p.graph_runner][INFO] - Train, Round 373 : loss => 1.5541393582522869,  accuracy: 0.4354, gradient_norm : 3.4926155066815934
[2025-09-12 20:26:12,997][flp2p.graph_runner][INFO] - Test, Round 373 : loss => 1.7389294900119305,  accuracy: 0.3787
[2025-09-12 20:26:23,755][flp2p.graph_runner][INFO] - Train, Round 374 : loss => 1.5177909533679486,  accuracy: 0.44502, gradient_norm : 3.2329307239858944
[2025-09-12 20:26:35,463][flp2p.graph_runner][INFO] - Test, Round 374 : loss => 1.6754820527791976,  accuracy: 0.3939
[2025-09-12 20:26:46,031][flp2p.graph_runner][INFO] - Train, Round 375 : loss => 1.4686455476284026,  accuracy: 0.47128, gradient_norm : 2.918544153188877
[2025-09-12 20:26:57,655][flp2p.graph_runner][INFO] - Test, Round 375 : loss => 1.6756728291392327,  accuracy: 0.405
[2025-09-12 20:27:08,464][flp2p.graph_runner][INFO] - Train, Round 376 : loss => 1.4500789180397988,  accuracy: 0.48202, gradient_norm : 3.0646039752928687
[2025-09-12 20:27:20,153][flp2p.graph_runner][INFO] - Test, Round 376 : loss => 1.7268083395481109,  accuracy: 0.398
[2025-09-12 20:27:30,875][flp2p.graph_runner][INFO] - Train, Round 377 : loss => 1.4978695604205132,  accuracy: 0.47002, gradient_norm : 3.4153064165117604
[2025-09-12 20:27:42,476][flp2p.graph_runner][INFO] - Test, Round 377 : loss => 1.810636343705654,  accuracy: 0.3775
[2025-09-12 20:27:53,302][flp2p.graph_runner][INFO] - Train, Round 378 : loss => 1.5605456380546092,  accuracy: 0.45116, gradient_norm : 3.860191992547724
[2025-09-12 20:28:04,926][flp2p.graph_runner][INFO] - Test, Round 378 : loss => 1.8951745323240756,  accuracy: 0.3597
[2025-09-12 20:28:15,715][flp2p.graph_runner][INFO] - Train, Round 379 : loss => 1.6449041974544525,  accuracy: 0.42366, gradient_norm : 4.365941114251223
[2025-09-12 20:28:27,138][flp2p.graph_runner][INFO] - Test, Round 379 : loss => 1.9694365396618843,  accuracy: 0.3447
[2025-09-12 20:28:37,972][flp2p.graph_runner][INFO] - Train, Round 380 : loss => 1.7446801735460757,  accuracy: 0.40682, gradient_norm : 4.082058177577027
[2025-09-12 20:28:49,369][flp2p.graph_runner][INFO] - Test, Round 380 : loss => 1.7248286689698695,  accuracy: 0.3869
[2025-09-12 20:29:00,447][flp2p.graph_runner][INFO] - Train, Round 381 : loss => 1.5242054949700832,  accuracy: 0.46322, gradient_norm : 2.8912289949606524
[2025-09-12 20:29:12,087][flp2p.graph_runner][INFO] - Test, Round 381 : loss => 1.6441549512863158,  accuracy: 0.4175
[2025-09-12 20:29:22,880][flp2p.graph_runner][INFO] - Train, Round 382 : loss => 1.4361936154961585,  accuracy: 0.49526, gradient_norm : 2.5996610575523147
[2025-09-12 20:29:34,697][flp2p.graph_runner][INFO] - Test, Round 382 : loss => 1.607244929856062,  accuracy: 0.4242
[2025-09-12 20:29:45,534][flp2p.graph_runner][INFO] - Train, Round 383 : loss => 1.3869724230468272,  accuracy: 0.51302, gradient_norm : 2.5087373466508667
[2025-09-12 20:29:57,205][flp2p.graph_runner][INFO] - Test, Round 383 : loss => 1.6087998535096646,  accuracy: 0.4252
[2025-09-12 20:30:07,891][flp2p.graph_runner][INFO] - Train, Round 384 : loss => 1.3767035436630248,  accuracy: 0.50834, gradient_norm : 2.730172825336752
[2025-09-12 20:30:19,368][flp2p.graph_runner][INFO] - Test, Round 384 : loss => 1.6258093693792819,  accuracy: 0.4214
[2025-09-12 20:30:30,202][flp2p.graph_runner][INFO] - Train, Round 385 : loss => 1.3824015709757804,  accuracy: 0.50974, gradient_norm : 2.843054613417832
[2025-09-12 20:30:41,797][flp2p.graph_runner][INFO] - Test, Round 385 : loss => 1.6805281916558743,  accuracy: 0.409
[2025-09-12 20:30:52,552][flp2p.graph_runner][INFO] - Train, Round 386 : loss => 1.4248569969832898,  accuracy: 0.4828, gradient_norm : 3.3808034002706715
[2025-09-12 20:31:04,228][flp2p.graph_runner][INFO] - Test, Round 386 : loss => 1.7853651039838792,  accuracy: 0.3695
[2025-09-12 20:31:15,041][flp2p.graph_runner][INFO] - Train, Round 387 : loss => 1.522337487488985,  accuracy: 0.45058, gradient_norm : 3.7443049408968823
[2025-09-12 20:31:26,700][flp2p.graph_runner][INFO] - Test, Round 387 : loss => 1.7839440185904503,  accuracy: 0.3745
[2025-09-12 20:31:37,825][flp2p.graph_runner][INFO] - Train, Round 388 : loss => 1.5314893387258053,  accuracy: 0.4361, gradient_norm : 3.783231805943455
[2025-09-12 20:31:49,310][flp2p.graph_runner][INFO] - Test, Round 388 : loss => 1.7714458674788476,  accuracy: 0.3595
[2025-09-12 20:32:00,232][flp2p.graph_runner][INFO] - Train, Round 389 : loss => 1.5300713188946247,  accuracy: 0.43806, gradient_norm : 3.403160804006868
[2025-09-12 20:32:11,934][flp2p.graph_runner][INFO] - Test, Round 389 : loss => 1.6659045778572559,  accuracy: 0.3938
[2025-09-12 20:32:22,788][flp2p.graph_runner][INFO] - Train, Round 390 : loss => 1.4361076645553112,  accuracy: 0.46952, gradient_norm : 2.9494617200104534
[2025-09-12 20:32:34,568][flp2p.graph_runner][INFO] - Test, Round 390 : loss => 1.643126445698738,  accuracy: 0.4095
[2025-09-12 20:32:45,320][flp2p.graph_runner][INFO] - Train, Round 391 : loss => 1.4084371830523015,  accuracy: 0.49954, gradient_norm : 2.7490876226880827
[2025-09-12 20:32:56,915][flp2p.graph_runner][INFO] - Test, Round 391 : loss => 1.6227331722855567,  accuracy: 0.4135
[2025-09-12 20:33:07,712][flp2p.graph_runner][INFO] - Train, Round 392 : loss => 1.3782735718041659,  accuracy: 0.50206, gradient_norm : 2.8116119750510027
[2025-09-12 20:33:19,341][flp2p.graph_runner][INFO] - Test, Round 392 : loss => 1.6364220883190632,  accuracy: 0.4141
[2025-09-12 20:33:30,389][flp2p.graph_runner][INFO] - Train, Round 393 : loss => 1.3834910628199577,  accuracy: 0.50426, gradient_norm : 2.9850724218007247
[2025-09-12 20:33:42,155][flp2p.graph_runner][INFO] - Test, Round 393 : loss => 1.6937954218745233,  accuracy: 0.3928
[2025-09-12 20:33:52,983][flp2p.graph_runner][INFO] - Train, Round 394 : loss => 1.4192091296613216,  accuracy: 0.4795, gradient_norm : 3.4686736267962424
[2025-09-12 20:34:04,582][flp2p.graph_runner][INFO] - Test, Round 394 : loss => 1.7982495847284794,  accuracy: 0.3735
[2025-09-12 20:34:15,468][flp2p.graph_runner][INFO] - Train, Round 395 : loss => 1.5289412248134613,  accuracy: 0.44928, gradient_norm : 3.740441324439959
[2025-09-12 20:34:26,721][flp2p.graph_runner][INFO] - Test, Round 395 : loss => 1.763027452558279,  accuracy: 0.3731
[2025-09-12 20:34:37,630][flp2p.graph_runner][INFO] - Train, Round 396 : loss => 1.5014537572860718,  accuracy: 0.4478, gradient_norm : 3.4662926441749367
[2025-09-12 20:34:49,339][flp2p.graph_runner][INFO] - Test, Round 396 : loss => 1.6848880922973155,  accuracy: 0.4003
[2025-09-12 20:35:00,047][flp2p.graph_runner][INFO] - Train, Round 397 : loss => 1.4420849198102952,  accuracy: 0.4714, gradient_norm : 3.050504225778256
[2025-09-12 20:35:11,657][flp2p.graph_runner][INFO] - Test, Round 397 : loss => 1.6538202951192855,  accuracy: 0.4027
[2025-09-12 20:35:22,511][flp2p.graph_runner][INFO] - Train, Round 398 : loss => 1.3955799351632594,  accuracy: 0.4973, gradient_norm : 3.0002826175044937
[2025-09-12 20:35:34,307][flp2p.graph_runner][INFO] - Test, Round 398 : loss => 1.6466723645448684,  accuracy: 0.4111
[2025-09-12 20:35:45,131][flp2p.graph_runner][INFO] - Train, Round 399 : loss => 1.3874720638990403,  accuracy: 0.49302, gradient_norm : 3.059553250596233
[2025-09-12 20:35:56,728][flp2p.graph_runner][INFO] - Test, Round 399 : loss => 1.6886438903093337,  accuracy: 0.4052
[2025-09-12 20:36:07,579][flp2p.graph_runner][INFO] - Train, Round 400 : loss => 1.4126929642260075,  accuracy: 0.49184, gradient_norm : 3.317820581750391
[2025-09-12 20:36:19,142][flp2p.graph_runner][INFO] - Test, Round 400 : loss => 1.714424769026041,  accuracy: 0.3984
[2025-09-12 20:36:30,127][flp2p.graph_runner][INFO] - Train, Round 401 : loss => 1.4437924580276011,  accuracy: 0.47248, gradient_norm : 3.4878878779559983
[2025-09-12 20:36:41,839][flp2p.graph_runner][INFO] - Test, Round 401 : loss => 1.7670794696509837,  accuracy: 0.3865
[2025-09-12 20:36:52,870][flp2p.graph_runner][INFO] - Train, Round 402 : loss => 1.487676474303007,  accuracy: 0.47332, gradient_norm : 3.561064407039964
[2025-09-12 20:37:04,478][flp2p.graph_runner][INFO] - Test, Round 402 : loss => 1.739145437115431,  accuracy: 0.3936
[2025-09-12 20:37:15,580][flp2p.graph_runner][INFO] - Train, Round 403 : loss => 1.4713261915743352,  accuracy: 0.47324, gradient_norm : 3.653992777315572
[2025-09-12 20:37:27,170][flp2p.graph_runner][INFO] - Test, Round 403 : loss => 1.7827745599925517,  accuracy: 0.3845
[2025-09-12 20:37:38,059][flp2p.graph_runner][INFO] - Train, Round 404 : loss => 1.509605960100889,  accuracy: 0.47396, gradient_norm : 3.7661998000582093
[2025-09-12 20:37:49,683][flp2p.graph_runner][INFO] - Test, Round 404 : loss => 1.7769967291414737,  accuracy: 0.3816
[2025-09-12 20:38:00,354][flp2p.graph_runner][INFO] - Train, Round 405 : loss => 1.5074720759689808,  accuracy: 0.46442, gradient_norm : 3.8262299081422384
[2025-09-12 20:38:12,025][flp2p.graph_runner][INFO] - Test, Round 405 : loss => 1.8008184950828552,  accuracy: 0.3811
[2025-09-12 20:38:22,942][flp2p.graph_runner][INFO] - Train, Round 406 : loss => 1.5336241185665132,  accuracy: 0.4656, gradient_norm : 3.7403130640812026
[2025-09-12 20:38:34,626][flp2p.graph_runner][INFO] - Test, Round 406 : loss => 1.714692817234993,  accuracy: 0.396
[2025-09-12 20:38:45,577][flp2p.graph_runner][INFO] - Train, Round 407 : loss => 1.455148233473301,  accuracy: 0.4879, gradient_norm : 3.310236349620928
[2025-09-12 20:38:57,231][flp2p.graph_runner][INFO] - Test, Round 407 : loss => 1.6783209579050542,  accuracy: 0.4117
[2025-09-12 20:39:08,270][flp2p.graph_runner][INFO] - Train, Round 408 : loss => 1.4105264927446841,  accuracy: 0.50584, gradient_norm : 3.0827776919647927
[2025-09-12 20:39:19,889][flp2p.graph_runner][INFO] - Test, Round 408 : loss => 1.6356980220913886,  accuracy: 0.4218
[2025-09-12 20:39:30,626][flp2p.graph_runner][INFO] - Train, Round 409 : loss => 1.3636840876936913,  accuracy: 0.51862, gradient_norm : 2.8853323671701014
[2025-09-12 20:39:42,499][flp2p.graph_runner][INFO] - Test, Round 409 : loss => 1.6273244171977044,  accuracy: 0.424
[2025-09-12 20:39:53,451][flp2p.graph_runner][INFO] - Train, Round 410 : loss => 1.3422724808007478,  accuracy: 0.5237, gradient_norm : 2.997020110623325
[2025-09-12 20:40:05,136][flp2p.graph_runner][INFO] - Test, Round 410 : loss => 1.6593683831930162,  accuracy: 0.4121
[2025-09-12 20:40:15,895][flp2p.graph_runner][INFO] - Train, Round 411 : loss => 1.364656243622303,  accuracy: 0.51422, gradient_norm : 3.157914612323221
[2025-09-12 20:40:27,514][flp2p.graph_runner][INFO] - Test, Round 411 : loss => 1.719131864118576,  accuracy: 0.3989
[2025-09-12 20:40:38,403][flp2p.graph_runner][INFO] - Train, Round 412 : loss => 1.4039987224340438,  accuracy: 0.48994, gradient_norm : 3.7651626633261475
[2025-09-12 20:40:50,116][flp2p.graph_runner][INFO] - Test, Round 412 : loss => 1.8319609462201596,  accuracy: 0.3645
[2025-09-12 20:41:00,885][flp2p.graph_runner][INFO] - Train, Round 413 : loss => 1.5270158290863036,  accuracy: 0.4509, gradient_norm : 4.092034365505727
[2025-09-12 20:41:12,555][flp2p.graph_runner][INFO] - Test, Round 413 : loss => 1.813452218568325,  accuracy: 0.3666
[2025-09-12 20:41:23,538][flp2p.graph_runner][INFO] - Train, Round 414 : loss => 1.517094716578722,  accuracy: 0.44682, gradient_norm : 3.828007137739943
[2025-09-12 20:41:35,367][flp2p.graph_runner][INFO] - Test, Round 414 : loss => 1.7058388811171055,  accuracy: 0.3959
[2025-09-12 20:41:46,184][flp2p.graph_runner][INFO] - Train, Round 415 : loss => 1.4419858148694038,  accuracy: 0.4844, gradient_norm : 3.1291261796691394
[2025-09-12 20:41:57,865][flp2p.graph_runner][INFO] - Test, Round 415 : loss => 1.619074904102087,  accuracy: 0.4159
[2025-09-12 20:42:08,953][flp2p.graph_runner][INFO] - Train, Round 416 : loss => 1.3468670198321342,  accuracy: 0.52268, gradient_norm : 2.778524876550522
[2025-09-12 20:42:20,568][flp2p.graph_runner][INFO] - Test, Round 416 : loss => 1.601975094383955,  accuracy: 0.4278
[2025-09-12 20:42:31,211][flp2p.graph_runner][INFO] - Train, Round 417 : loss => 1.318292242884636,  accuracy: 0.5334, gradient_norm : 2.8681584400596956
[2025-09-12 20:42:42,928][flp2p.graph_runner][INFO] - Test, Round 417 : loss => 1.6389615352690219,  accuracy: 0.4175
[2025-09-12 20:42:53,804][flp2p.graph_runner][INFO] - Train, Round 418 : loss => 1.3345182345062494,  accuracy: 0.5232, gradient_norm : 3.1534488816019643
[2025-09-12 20:43:05,433][flp2p.graph_runner][INFO] - Test, Round 418 : loss => 1.6910591203868388,  accuracy: 0.4019
[2025-09-12 20:43:16,124][flp2p.graph_runner][INFO] - Train, Round 419 : loss => 1.3801268339157104,  accuracy: 0.50378, gradient_norm : 3.4694975866489877
[2025-09-12 20:43:27,442][flp2p.graph_runner][INFO] - Test, Round 419 : loss => 1.755245837622881,  accuracy: 0.3866
[2025-09-12 20:43:38,337][flp2p.graph_runner][INFO] - Train, Round 420 : loss => 1.4353600043058394,  accuracy: 0.48136, gradient_norm : 3.763701130999115
[2025-09-12 20:43:50,113][flp2p.graph_runner][INFO] - Test, Round 420 : loss => 1.7994891826212407,  accuracy: 0.3691
[2025-09-12 20:44:01,037][flp2p.graph_runner][INFO] - Train, Round 421 : loss => 1.4850836458802223,  accuracy: 0.45948, gradient_norm : 3.8871205428228177
[2025-09-12 20:44:12,762][flp2p.graph_runner][INFO] - Test, Round 421 : loss => 1.8025712814807893,  accuracy: 0.3608
[2025-09-12 20:44:23,685][flp2p.graph_runner][INFO] - Train, Round 422 : loss => 1.4827586789429188,  accuracy: 0.4518, gradient_norm : 3.961812932102182
[2025-09-12 20:44:35,321][flp2p.graph_runner][INFO] - Test, Round 422 : loss => 1.775633756285906,  accuracy: 0.3844
[2025-09-12 20:44:46,477][flp2p.graph_runner][INFO] - Train, Round 423 : loss => 1.483195822238922,  accuracy: 0.4687, gradient_norm : 3.5166629619713325
[2025-09-12 20:44:58,336][flp2p.graph_runner][INFO] - Test, Round 423 : loss => 1.6281646335124969,  accuracy: 0.4185
[2025-09-12 20:45:09,019][flp2p.graph_runner][INFO] - Train, Round 424 : loss => 1.3389669324457645,  accuracy: 0.51852, gradient_norm : 2.8432949572480446
[2025-09-12 20:45:20,642][flp2p.graph_runner][INFO] - Test, Round 424 : loss => 1.613293678277731,  accuracy: 0.4256
[2025-09-12 20:45:31,552][flp2p.graph_runner][INFO] - Train, Round 425 : loss => 1.3108380776643753,  accuracy: 0.5341, gradient_norm : 2.8851009261453453
[2025-09-12 20:45:43,365][flp2p.graph_runner][INFO] - Test, Round 425 : loss => 1.6255232581675052,  accuracy: 0.4199
[2025-09-12 20:45:53,924][flp2p.graph_runner][INFO] - Train, Round 426 : loss => 1.3068265427649022,  accuracy: 0.52674, gradient_norm : 3.151684331940633
[2025-09-12 20:46:05,573][flp2p.graph_runner][INFO] - Test, Round 426 : loss => 1.6863542421817779,  accuracy: 0.4072
[2025-09-12 20:46:16,391][flp2p.graph_runner][INFO] - Train, Round 427 : loss => 1.3648218876123428,  accuracy: 0.50472, gradient_norm : 3.3558928120455738
[2025-09-12 20:46:28,065][flp2p.graph_runner][INFO] - Test, Round 427 : loss => 1.7209656380176543,  accuracy: 0.4021
[2025-09-12 20:46:38,867][flp2p.graph_runner][INFO] - Train, Round 428 : loss => 1.3834134742617608,  accuracy: 0.49486, gradient_norm : 3.715384379684114
[2025-09-12 20:46:50,569][flp2p.graph_runner][INFO] - Test, Round 428 : loss => 1.7871339764356613,  accuracy: 0.3816
[2025-09-12 20:47:01,583][flp2p.graph_runner][INFO] - Train, Round 429 : loss => 1.4656929309666156,  accuracy: 0.46194, gradient_norm : 3.9603181909045704
[2025-09-12 20:47:13,022][flp2p.graph_runner][INFO] - Test, Round 429 : loss => 1.7967796768844129,  accuracy: 0.3812
[2025-09-12 20:47:23,750][flp2p.graph_runner][INFO] - Train, Round 430 : loss => 1.4781870682537557,  accuracy: 0.4722, gradient_norm : 3.7946575711636386
[2025-09-12 20:47:35,367][flp2p.graph_runner][INFO] - Test, Round 430 : loss => 1.7108597345769405,  accuracy: 0.3986
[2025-09-12 20:47:46,296][flp2p.graph_runner][INFO] - Train, Round 431 : loss => 1.4195629027485848,  accuracy: 0.49418, gradient_norm : 3.4120579474014674
[2025-09-12 20:47:58,086][flp2p.graph_runner][INFO] - Test, Round 431 : loss => 1.6766372798740863,  accuracy: 0.41
[2025-09-12 20:48:09,056][flp2p.graph_runner][INFO] - Train, Round 432 : loss => 1.36876428976655,  accuracy: 0.51652, gradient_norm : 3.333838806742922
[2025-09-12 20:48:20,789][flp2p.graph_runner][INFO] - Test, Round 432 : loss => 1.6585076175630094,  accuracy: 0.4131
[2025-09-12 20:48:31,715][flp2p.graph_runner][INFO] - Train, Round 433 : loss => 1.3512304973602296,  accuracy: 0.52266, gradient_norm : 3.2595181969713862
[2025-09-12 20:48:43,261][flp2p.graph_runner][INFO] - Test, Round 433 : loss => 1.6901680291712284,  accuracy: 0.413
[2025-09-12 20:48:54,111][flp2p.graph_runner][INFO] - Train, Round 434 : loss => 1.3618028390407562,  accuracy: 0.52028, gradient_norm : 3.4919519341665013
[2025-09-12 20:49:05,606][flp2p.graph_runner][INFO] - Test, Round 434 : loss => 1.7196105559825898,  accuracy: 0.4055
[2025-09-12 20:49:16,446][flp2p.graph_runner][INFO] - Train, Round 435 : loss => 1.3850215534120798,  accuracy: 0.51188, gradient_norm : 3.8811908561101287
[2025-09-12 20:49:27,967][flp2p.graph_runner][INFO] - Test, Round 435 : loss => 1.8738218849599362,  accuracy: 0.3715
[2025-09-12 20:49:38,826][flp2p.graph_runner][INFO] - Train, Round 436 : loss => 1.5237453532218934,  accuracy: 0.4714, gradient_norm : 4.496172598970226
[2025-09-12 20:49:50,584][flp2p.graph_runner][INFO] - Test, Round 436 : loss => 1.9432202280521393,  accuracy: 0.3534
[2025-09-12 20:50:01,368][flp2p.graph_runner][INFO] - Train, Round 437 : loss => 1.5978479543328286,  accuracy: 0.442, gradient_norm : 4.572960626077732
[2025-09-12 20:50:13,004][flp2p.graph_runner][INFO] - Test, Round 437 : loss => 1.8795765939891338,  accuracy: 0.3567
[2025-09-12 20:50:23,805][flp2p.graph_runner][INFO] - Train, Round 438 : loss => 1.5666859166324139,  accuracy: 0.45508, gradient_norm : 3.796057589219386
[2025-09-12 20:50:35,461][flp2p.graph_runner][INFO] - Test, Round 438 : loss => 1.659349193930626,  accuracy: 0.4051
[2025-09-12 20:50:46,444][flp2p.graph_runner][INFO] - Train, Round 439 : loss => 1.3678803066909313,  accuracy: 0.50966, gradient_norm : 3.0270642291302954
[2025-09-12 20:50:57,989][flp2p.graph_runner][INFO] - Test, Round 439 : loss => 1.6380538532495499,  accuracy: 0.4146
[2025-09-12 20:51:08,810][flp2p.graph_runner][INFO] - Train, Round 440 : loss => 1.324734385460615,  accuracy: 0.5349, gradient_norm : 2.930404887500978
[2025-09-12 20:51:20,281][flp2p.graph_runner][INFO] - Test, Round 440 : loss => 1.6201265081346035,  accuracy: 0.4206
[2025-09-12 20:51:31,202][flp2p.graph_runner][INFO] - Train, Round 441 : loss => 1.2920301194489001,  accuracy: 0.5342, gradient_norm : 3.0479231525328623
[2025-09-12 20:51:42,797][flp2p.graph_runner][INFO] - Test, Round 441 : loss => 1.6497613499462604,  accuracy: 0.4158
[2025-09-12 20:51:53,825][flp2p.graph_runner][INFO] - Train, Round 442 : loss => 1.2992635374516248,  accuracy: 0.5345, gradient_norm : 3.209583469211103
[2025-09-12 20:52:05,340][flp2p.graph_runner][INFO] - Test, Round 442 : loss => 1.6916261451005936,  accuracy: 0.4081
[2025-09-12 20:52:16,480][flp2p.graph_runner][INFO] - Train, Round 443 : loss => 1.3346143186837436,  accuracy: 0.51188, gradient_norm : 3.616839863411406
[2025-09-12 20:52:27,866][flp2p.graph_runner][INFO] - Test, Round 443 : loss => 1.7895410107851029,  accuracy: 0.3771
[2025-09-12 20:52:38,771][flp2p.graph_runner][INFO] - Train, Round 444 : loss => 1.4151738926023245,  accuracy: 0.48378, gradient_norm : 3.9411899305400904
[2025-09-12 20:52:50,228][flp2p.graph_runner][INFO] - Test, Round 444 : loss => 1.7729404321610926,  accuracy: 0.3893
[2025-09-12 20:53:01,373][flp2p.graph_runner][INFO] - Train, Round 445 : loss => 1.42151082649827,  accuracy: 0.48218, gradient_norm : 3.939438305154327
[2025-09-12 20:53:12,739][flp2p.graph_runner][INFO] - Test, Round 445 : loss => 1.7555757141530515,  accuracy: 0.3779
[2025-09-12 20:53:23,751][flp2p.graph_runner][INFO] - Train, Round 446 : loss => 1.408989592641592,  accuracy: 0.48542, gradient_norm : 3.528717833843676
[2025-09-12 20:53:35,027][flp2p.graph_runner][INFO] - Test, Round 446 : loss => 1.6698421103239058,  accuracy: 0.4092
[2025-09-12 20:53:46,212][flp2p.graph_runner][INFO] - Train, Round 447 : loss => 1.3236192571371794,  accuracy: 0.5229, gradient_norm : 3.3952474973409488
[2025-09-12 20:53:57,550][flp2p.graph_runner][INFO] - Test, Round 447 : loss => 1.7166531133532523,  accuracy: 0.3887
[2025-09-12 20:54:08,450][flp2p.graph_runner][INFO] - Train, Round 448 : loss => 1.3584471547603607,  accuracy: 0.51418, gradient_norm : 3.5539302878362133
[2025-09-12 20:54:19,653][flp2p.graph_runner][INFO] - Test, Round 448 : loss => 1.7410763074159623,  accuracy: 0.3906
[2025-09-12 20:54:30,608][flp2p.graph_runner][INFO] - Train, Round 449 : loss => 1.3695660377293826,  accuracy: 0.50492, gradient_norm : 3.8519530969893343
[2025-09-12 20:54:41,670][flp2p.graph_runner][INFO] - Test, Round 449 : loss => 1.816083512187004,  accuracy: 0.365
[2025-09-12 20:54:52,828][flp2p.graph_runner][INFO] - Train, Round 450 : loss => 1.4454425613582134,  accuracy: 0.48098, gradient_norm : 4.147738456243408
[2025-09-12 20:55:03,999][flp2p.graph_runner][INFO] - Test, Round 450 : loss => 1.8466825592815876,  accuracy: 0.3711
[2025-09-12 20:55:15,134][flp2p.graph_runner][INFO] - Train, Round 451 : loss => 1.4765298399329185,  accuracy: 0.46608, gradient_norm : 4.157395025099692
[2025-09-12 20:55:26,240][flp2p.graph_runner][INFO] - Test, Round 451 : loss => 1.7243965607583522,  accuracy: 0.3872
[2025-09-12 20:55:37,378][flp2p.graph_runner][INFO] - Train, Round 452 : loss => 1.3943795569241046,  accuracy: 0.50004, gradient_norm : 3.361882303189363
[2025-09-12 20:55:48,321][flp2p.graph_runner][INFO] - Test, Round 452 : loss => 1.6395464465737344,  accuracy: 0.4133
[2025-09-12 20:55:59,343][flp2p.graph_runner][INFO] - Train, Round 453 : loss => 1.2999461568146944,  accuracy: 0.53312, gradient_norm : 3.1397660106727585
[2025-09-12 20:56:10,247][flp2p.graph_runner][INFO] - Test, Round 453 : loss => 1.6377461620688438,  accuracy: 0.4214
[2025-09-12 20:56:21,290][flp2p.graph_runner][INFO] - Train, Round 454 : loss => 1.2833924495428801,  accuracy: 0.5445, gradient_norm : 3.1510837578247446
[2025-09-12 20:56:32,365][flp2p.graph_runner][INFO] - Test, Round 454 : loss => 1.6502398807942866,  accuracy: 0.4207
[2025-09-12 20:56:43,251][flp2p.graph_runner][INFO] - Train, Round 455 : loss => 1.284451538398862,  accuracy: 0.53912, gradient_norm : 3.371978143543342
[2025-09-12 20:56:54,215][flp2p.graph_runner][INFO] - Test, Round 455 : loss => 1.712432366102934,  accuracy: 0.4097
[2025-09-12 20:57:05,316][flp2p.graph_runner][INFO] - Train, Round 456 : loss => 1.3253169307112693,  accuracy: 0.53274, gradient_norm : 3.7556965876185706
[2025-09-12 20:57:16,340][flp2p.graph_runner][INFO] - Test, Round 456 : loss => 1.774343927025795,  accuracy: 0.3928
[2025-09-12 20:57:27,349][flp2p.graph_runner][INFO] - Train, Round 457 : loss => 1.3953219947218896,  accuracy: 0.49434, gradient_norm : 4.078684866025961
[2025-09-12 20:57:38,696][flp2p.graph_runner][INFO] - Test, Round 457 : loss => 1.830399011683464,  accuracy: 0.3853
[2025-09-12 20:57:49,478][flp2p.graph_runner][INFO] - Train, Round 458 : loss => 1.4426894254982472,  accuracy: 0.5002, gradient_norm : 4.066712158226527
[2025-09-12 20:58:03,231][flp2p.graph_runner][INFO] - Test, Round 458 : loss => 1.73531925303936,  accuracy: 0.4005
[2025-09-12 20:58:14,111][flp2p.graph_runner][INFO] - Train, Round 459 : loss => 1.3742220165580512,  accuracy: 0.511, gradient_norm : 3.6709766822632237
[2025-09-12 20:58:29,992][flp2p.graph_runner][INFO] - Test, Round 459 : loss => 1.7042027258992196,  accuracy: 0.4096
[2025-09-12 20:58:40,837][flp2p.graph_runner][INFO] - Train, Round 460 : loss => 1.3277879194170237,  accuracy: 0.5383, gradient_norm : 3.6531441976169634
[2025-09-12 20:58:57,668][flp2p.graph_runner][INFO] - Test, Round 460 : loss => 1.7034234582901,  accuracy: 0.4143
[2025-09-12 20:59:08,440][flp2p.graph_runner][INFO] - Train, Round 461 : loss => 1.330043220371008,  accuracy: 0.535, gradient_norm : 3.6335844029556834
[2025-09-12 20:59:26,494][flp2p.graph_runner][INFO] - Test, Round 461 : loss => 1.72046368560791,  accuracy: 0.408
[2025-09-12 20:59:37,217][flp2p.graph_runner][INFO] - Train, Round 462 : loss => 1.3266857016086577,  accuracy: 0.53654, gradient_norm : 3.7296907465989704
[2025-09-12 20:59:55,709][flp2p.graph_runner][INFO] - Test, Round 462 : loss => 1.7369982665479182,  accuracy: 0.4024
[2025-09-12 21:00:06,547][flp2p.graph_runner][INFO] - Train, Round 463 : loss => 1.3466339643299579,  accuracy: 0.52698, gradient_norm : 3.8082883052908496
[2025-09-12 21:00:24,624][flp2p.graph_runner][INFO] - Test, Round 463 : loss => 1.76692653580904,  accuracy: 0.3947
[2025-09-12 21:00:35,559][flp2p.graph_runner][INFO] - Train, Round 464 : loss => 1.3620974625647069,  accuracy: 0.51694, gradient_norm : 3.8680550802961653
[2025-09-12 21:00:53,020][flp2p.graph_runner][INFO] - Test, Round 464 : loss => 1.7421419955551625,  accuracy: 0.3948
[2025-09-12 21:01:03,817][flp2p.graph_runner][INFO] - Train, Round 465 : loss => 1.351520210802555,  accuracy: 0.5181, gradient_norm : 3.8242756398717317
[2025-09-12 21:01:20,707][flp2p.graph_runner][INFO] - Test, Round 465 : loss => 1.7452405642688273,  accuracy: 0.3955
[2025-09-12 21:01:31,605][flp2p.graph_runner][INFO] - Train, Round 466 : loss => 1.3425701711326838,  accuracy: 0.51362, gradient_norm : 3.8384090104669433
[2025-09-12 21:01:48,105][flp2p.graph_runner][INFO] - Test, Round 466 : loss => 1.7291259985148906,  accuracy: 0.3945
[2025-09-12 21:01:58,906][flp2p.graph_runner][INFO] - Train, Round 467 : loss => 1.338656758442521,  accuracy: 0.51946, gradient_norm : 3.6825229278318066
[2025-09-12 21:02:14,774][flp2p.graph_runner][INFO] - Test, Round 467 : loss => 1.7067444627165795,  accuracy: 0.3997
[2025-09-12 21:02:25,371][flp2p.graph_runner][INFO] - Train, Round 468 : loss => 1.3090138693153859,  accuracy: 0.5215, gradient_norm : 3.629707134926523
[2025-09-12 21:02:40,768][flp2p.graph_runner][INFO] - Test, Round 468 : loss => 1.700267685008049,  accuracy: 0.4058
[2025-09-12 21:02:51,556][flp2p.graph_runner][INFO] - Train, Round 469 : loss => 1.2971644704043865,  accuracy: 0.5329, gradient_norm : 3.616917851694895
[2025-09-12 21:03:07,075][flp2p.graph_runner][INFO] - Test, Round 469 : loss => 1.7041522365272046,  accuracy: 0.4033
[2025-09-12 21:03:17,965][flp2p.graph_runner][INFO] - Train, Round 470 : loss => 1.3009859767556191,  accuracy: 0.52184, gradient_norm : 3.6320518294231845
[2025-09-12 21:03:32,969][flp2p.graph_runner][INFO] - Test, Round 470 : loss => 1.6816262048900128,  accuracy: 0.4118
[2025-09-12 21:03:43,863][flp2p.graph_runner][INFO] - Train, Round 471 : loss => 1.274392807111144,  accuracy: 0.54018, gradient_norm : 3.523870000604392
[2025-09-12 21:03:58,334][flp2p.graph_runner][INFO] - Test, Round 471 : loss => 1.6886710994780063,  accuracy: 0.4067
[2025-09-12 21:04:08,914][flp2p.graph_runner][INFO] - Train, Round 472 : loss => 1.2750138721615076,  accuracy: 0.52416, gradient_norm : 3.6484038007660025
[2025-09-12 21:04:22,947][flp2p.graph_runner][INFO] - Test, Round 472 : loss => 1.7294392128229141,  accuracy: 0.3981
[2025-09-12 21:04:33,592][flp2p.graph_runner][INFO] - Train, Round 473 : loss => 1.3060705404728652,  accuracy: 0.5246, gradient_norm : 3.715136132751405
[2025-09-12 21:04:46,950][flp2p.graph_runner][INFO] - Test, Round 473 : loss => 1.7064052615761758,  accuracy: 0.4089
[2025-09-12 21:04:57,641][flp2p.graph_runner][INFO] - Train, Round 474 : loss => 1.2947262607514858,  accuracy: 0.52406, gradient_norm : 3.741854872743586
[2025-09-12 21:05:10,951][flp2p.graph_runner][INFO] - Test, Round 474 : loss => 1.733889481151104,  accuracy: 0.3913
[2025-09-12 21:05:21,883][flp2p.graph_runner][INFO] - Train, Round 475 : loss => 1.307683910727501,  accuracy: 0.52796, gradient_norm : 3.7838631316821405
[2025-09-12 21:05:35,309][flp2p.graph_runner][INFO] - Test, Round 475 : loss => 1.7630375765681268,  accuracy: 0.3932
[2025-09-12 21:05:46,188][flp2p.graph_runner][INFO] - Train, Round 476 : loss => 1.328655998185277,  accuracy: 0.50964, gradient_norm : 4.118165228312525
[2025-09-12 21:05:58,983][flp2p.graph_runner][INFO] - Test, Round 476 : loss => 1.8589789134740828,  accuracy: 0.3647
[2025-09-12 21:06:09,839][flp2p.graph_runner][INFO] - Train, Round 477 : loss => 1.421174546778202,  accuracy: 0.48852, gradient_norm : 4.200599166109237
[2025-09-12 21:06:22,589][flp2p.graph_runner][INFO] - Test, Round 477 : loss => 1.755992834097147,  accuracy: 0.3868
[2025-09-12 21:06:33,574][flp2p.graph_runner][INFO] - Train, Round 478 : loss => 1.327712555155158,  accuracy: 0.50698, gradient_norm : 4.059779741169261
[2025-09-12 21:06:46,576][flp2p.graph_runner][INFO] - Test, Round 478 : loss => 1.7802070639073848,  accuracy: 0.3904
[2025-09-12 21:06:57,358][flp2p.graph_runner][INFO] - Train, Round 479 : loss => 1.3731928598880767,  accuracy: 0.5237, gradient_norm : 3.6422363681152716
[2025-09-12 21:07:10,203][flp2p.graph_runner][INFO] - Test, Round 479 : loss => 1.6041003061711787,  accuracy: 0.4322
[2025-09-12 21:07:21,065][flp2p.graph_runner][INFO] - Train, Round 480 : loss => 1.201240773946047,  accuracy: 0.56906, gradient_norm : 3.0682060281384405
[2025-09-12 21:07:33,539][flp2p.graph_runner][INFO] - Test, Round 480 : loss => 1.6266474502027035,  accuracy: 0.4222
[2025-09-12 21:07:44,332][flp2p.graph_runner][INFO] - Train, Round 481 : loss => 1.1967881385982038,  accuracy: 0.58142, gradient_norm : 3.182749873919732
[2025-09-12 21:07:56,243][flp2p.graph_runner][INFO] - Test, Round 481 : loss => 1.6459494228601457,  accuracy: 0.4213
[2025-09-12 21:08:07,198][flp2p.graph_runner][INFO] - Train, Round 482 : loss => 1.195470373108983,  accuracy: 0.56764, gradient_norm : 3.500488742599457
[2025-09-12 21:08:18,967][flp2p.graph_runner][INFO] - Test, Round 482 : loss => 1.719538712888956,  accuracy: 0.4008
[2025-09-12 21:08:29,896][flp2p.graph_runner][INFO] - Train, Round 483 : loss => 1.2524821857362987,  accuracy: 0.54306, gradient_norm : 3.8776564521239494
[2025-09-12 21:08:41,840][flp2p.graph_runner][INFO] - Test, Round 483 : loss => 1.8320232684373856,  accuracy: 0.3866
[2025-09-12 21:08:52,469][flp2p.graph_runner][INFO] - Train, Round 484 : loss => 1.3519234420359134,  accuracy: 0.50432, gradient_norm : 4.46930622133322
[2025-09-12 21:09:03,954][flp2p.graph_runner][INFO] - Test, Round 484 : loss => 1.8826318154513837,  accuracy: 0.3585
[2025-09-12 21:09:14,749][flp2p.graph_runner][INFO] - Train, Round 485 : loss => 1.423375523239374,  accuracy: 0.46826, gradient_norm : 4.392817352384539
[2025-09-12 21:09:26,148][flp2p.graph_runner][INFO] - Test, Round 485 : loss => 1.7335475937604905,  accuracy: 0.3989
[2025-09-12 21:09:37,001][flp2p.graph_runner][INFO] - Train, Round 486 : loss => 1.3110853025317193,  accuracy: 0.52518, gradient_norm : 3.7054408545752455
[2025-09-12 21:09:48,769][flp2p.graph_runner][INFO] - Test, Round 486 : loss => 1.6620758771419526,  accuracy: 0.4109
[2025-09-12 21:09:59,664][flp2p.graph_runner][INFO] - Train, Round 487 : loss => 1.2363960205763578,  accuracy: 0.55602, gradient_norm : 3.4212305040197357
[2025-09-12 21:10:11,300][flp2p.graph_runner][INFO] - Test, Round 487 : loss => 1.6756631211817266,  accuracy: 0.4165
[2025-09-12 21:10:22,118][flp2p.graph_runner][INFO] - Train, Round 488 : loss => 1.22921017318964,  accuracy: 0.5572, gradient_norm : 3.628740982398294
[2025-09-12 21:10:33,727][flp2p.graph_runner][INFO] - Test, Round 488 : loss => 1.74676657192111,  accuracy: 0.3929
[2025-09-12 21:10:44,705][flp2p.graph_runner][INFO] - Train, Round 489 : loss => 1.2756258019804954,  accuracy: 0.53032, gradient_norm : 3.9892408727061035
[2025-09-12 21:10:56,359][flp2p.graph_runner][INFO] - Test, Round 489 : loss => 1.8630460987865924,  accuracy: 0.3829
[2025-09-12 21:11:07,203][flp2p.graph_runner][INFO] - Train, Round 490 : loss => 1.3905515870451928,  accuracy: 0.49858, gradient_norm : 4.357783958550587
[2025-09-12 21:11:19,138][flp2p.graph_runner][INFO] - Test, Round 490 : loss => 1.856086427205801,  accuracy: 0.3625
[2025-09-12 21:11:29,841][flp2p.graph_runner][INFO] - Train, Round 491 : loss => 1.388929123878479,  accuracy: 0.4801, gradient_norm : 4.515187019989295
[2025-09-12 21:11:41,459][flp2p.graph_runner][INFO] - Test, Round 491 : loss => 1.8124306431829929,  accuracy: 0.3852
[2025-09-12 21:11:52,297][flp2p.graph_runner][INFO] - Train, Round 492 : loss => 1.3967597073316573,  accuracy: 0.4912, gradient_norm : 3.9893895016093786
[2025-09-12 21:12:03,830][flp2p.graph_runner][INFO] - Test, Round 492 : loss => 1.6763258144021034,  accuracy: 0.4066
[2025-09-12 21:12:14,576][flp2p.graph_runner][INFO] - Train, Round 493 : loss => 1.2647686687856912,  accuracy: 0.5426, gradient_norm : 3.3247327772541757
[2025-09-12 21:12:26,061][flp2p.graph_runner][INFO] - Test, Round 493 : loss => 1.6202963310301304,  accuracy: 0.4219
[2025-09-12 21:12:36,282][flp2p.graph_runner][INFO] - Train, Round 494 : loss => 1.2017763543128968,  accuracy: 0.57696, gradient_norm : 3.199241089576592
[2025-09-12 21:12:45,788][flp2p.graph_runner][INFO] - Test, Round 494 : loss => 1.653837157779932,  accuracy: 0.4247
[2025-09-12 21:12:55,805][flp2p.graph_runner][INFO] - Train, Round 495 : loss => 1.200727415010333,  accuracy: 0.56716, gradient_norm : 3.396458467619935
[2025-09-12 21:13:05,312][flp2p.graph_runner][INFO] - Test, Round 495 : loss => 1.6919584090471267,  accuracy: 0.4071
[2025-09-12 21:13:15,284][flp2p.graph_runner][INFO] - Train, Round 496 : loss => 1.2217657333612442,  accuracy: 0.56122, gradient_norm : 3.785919196685892
[2025-09-12 21:13:24,828][flp2p.graph_runner][INFO] - Test, Round 496 : loss => 1.8340995314896107,  accuracy: 0.3948
[2025-09-12 21:13:34,922][flp2p.graph_runner][INFO] - Train, Round 497 : loss => 1.3390209525823593,  accuracy: 0.52328, gradient_norm : 4.458092976692944
[2025-09-12 21:13:44,562][flp2p.graph_runner][INFO] - Test, Round 497 : loss => 1.8846556223809718,  accuracy: 0.3624
[2025-09-12 21:13:54,487][flp2p.graph_runner][INFO] - Train, Round 498 : loss => 1.4236665444076062,  accuracy: 0.49276, gradient_norm : 4.351870725009642
[2025-09-12 21:14:04,043][flp2p.graph_runner][INFO] - Test, Round 498 : loss => 1.7208029455542564,  accuracy: 0.4034
[2025-09-12 21:14:13,960][flp2p.graph_runner][INFO] - Train, Round 499 : loss => 1.2744756887853146,  accuracy: 0.53932, gradient_norm : 3.7491097044432666
[2025-09-12 21:14:23,469][flp2p.graph_runner][INFO] - Test, Round 499 : loss => 1.6726939035654067,  accuracy: 0.4133
[2025-09-12 21:14:23,474][__main__][INFO] - Train, Round 001: loss=2.3053, accuracy=0.1026, gradient_norm=0.1798, 
[2025-09-12 21:14:23,474][__main__][INFO] - Train, Round 002: loss=2.3051, accuracy=0.1031, gradient_norm=0.1743, 
[2025-09-12 21:14:23,474][__main__][INFO] - Train, Round 003: loss=2.3049, accuracy=0.1035, gradient_norm=0.1735, 
[2025-09-12 21:14:23,474][__main__][INFO] - Train, Round 004: loss=2.3047, accuracy=0.1046, gradient_norm=0.1737, 
[2025-09-12 21:14:23,474][__main__][INFO] - Train, Round 005: loss=2.3045, accuracy=0.1049, gradient_norm=0.1811, 
[2025-09-12 21:14:23,474][__main__][INFO] - Train, Round 006: loss=2.3043, accuracy=0.1056, gradient_norm=0.1756, 
[2025-09-12 21:14:23,474][__main__][INFO] - Train, Round 007: loss=2.3041, accuracy=0.1051, gradient_norm=0.1749, 
[2025-09-12 21:14:23,474][__main__][INFO] - Train, Round 008: loss=2.3039, accuracy=0.1053, gradient_norm=0.1805, 
[2025-09-12 21:14:23,474][__main__][INFO] - Train, Round 009: loss=2.3037, accuracy=0.1056, gradient_norm=0.1715, 
[2025-09-12 21:14:23,474][__main__][INFO] - Train, Round 010: loss=2.3035, accuracy=0.1067, gradient_norm=0.1786, 
[2025-09-12 21:14:23,474][__main__][INFO] - Train, Round 011: loss=2.3033, accuracy=0.1065, gradient_norm=0.1751, 
[2025-09-12 21:14:23,474][__main__][INFO] - Train, Round 012: loss=2.3032, accuracy=0.1064, gradient_norm=0.1765, 
[2025-09-12 21:14:23,474][__main__][INFO] - Train, Round 013: loss=2.3030, accuracy=0.1071, gradient_norm=0.1782, 
[2025-09-12 21:14:23,474][__main__][INFO] - Train, Round 014: loss=2.3028, accuracy=0.1090, gradient_norm=0.1731, 
[2025-09-12 21:14:23,474][__main__][INFO] - Train, Round 015: loss=2.3026, accuracy=0.1100, gradient_norm=0.1742, 
[2025-09-12 21:14:23,475][__main__][INFO] - Train, Round 016: loss=2.3024, accuracy=0.1121, gradient_norm=0.1804, 
[2025-09-12 21:14:23,475][__main__][INFO] - Train, Round 017: loss=2.3022, accuracy=0.1156, gradient_norm=0.1760, 
[2025-09-12 21:14:23,475][__main__][INFO] - Train, Round 018: loss=2.3020, accuracy=0.1193, gradient_norm=0.1790, 
[2025-09-12 21:14:23,475][__main__][INFO] - Train, Round 019: loss=2.3018, accuracy=0.1212, gradient_norm=0.1787, 
[2025-09-12 21:14:23,475][__main__][INFO] - Train, Round 020: loss=2.3015, accuracy=0.1241, gradient_norm=0.1793, 
[2025-09-12 21:14:23,475][__main__][INFO] - Train, Round 021: loss=2.3014, accuracy=0.1270, gradient_norm=0.1752, 
[2025-09-12 21:14:23,475][__main__][INFO] - Train, Round 022: loss=2.3012, accuracy=0.1322, gradient_norm=0.1815, 
[2025-09-12 21:14:23,475][__main__][INFO] - Train, Round 023: loss=2.3009, accuracy=0.1336, gradient_norm=0.1727, 
[2025-09-12 21:14:23,475][__main__][INFO] - Train, Round 024: loss=2.3008, accuracy=0.1368, gradient_norm=0.1793, 
[2025-09-12 21:14:23,475][__main__][INFO] - Train, Round 025: loss=2.3005, accuracy=0.1373, gradient_norm=0.1812, 
[2025-09-12 21:14:23,475][__main__][INFO] - Train, Round 026: loss=2.3003, accuracy=0.1385, gradient_norm=0.1871, 
[2025-09-12 21:14:23,475][__main__][INFO] - Train, Round 027: loss=2.3001, accuracy=0.1405, gradient_norm=0.1831, 
[2025-09-12 21:14:23,475][__main__][INFO] - Train, Round 028: loss=2.2999, accuracy=0.1444, gradient_norm=0.1863, 
[2025-09-12 21:14:23,475][__main__][INFO] - Train, Round 029: loss=2.2996, accuracy=0.1462, gradient_norm=0.1878, 
[2025-09-12 21:14:23,475][__main__][INFO] - Train, Round 030: loss=2.2994, accuracy=0.1474, gradient_norm=0.1880, 
[2025-09-12 21:14:23,475][__main__][INFO] - Train, Round 031: loss=2.2991, accuracy=0.1478, gradient_norm=0.1840, 
[2025-09-12 21:14:23,475][__main__][INFO] - Train, Round 032: loss=2.2988, accuracy=0.1487, gradient_norm=0.1855, 
[2025-09-12 21:14:23,475][__main__][INFO] - Train, Round 033: loss=2.2986, accuracy=0.1491, gradient_norm=0.1889, 
[2025-09-12 21:14:23,475][__main__][INFO] - Train, Round 034: loss=2.2983, accuracy=0.1501, gradient_norm=0.1949, 
[2025-09-12 21:14:23,475][__main__][INFO] - Train, Round 035: loss=2.2981, accuracy=0.1514, gradient_norm=0.1920, 
[2025-09-12 21:14:23,475][__main__][INFO] - Train, Round 036: loss=2.2978, accuracy=0.1523, gradient_norm=0.1907, 
[2025-09-12 21:14:23,475][__main__][INFO] - Train, Round 037: loss=2.2975, accuracy=0.1519, gradient_norm=0.1948, 
[2025-09-12 21:14:23,475][__main__][INFO] - Train, Round 038: loss=2.2972, accuracy=0.1524, gradient_norm=0.2007, 
[2025-09-12 21:14:23,475][__main__][INFO] - Train, Round 039: loss=2.2968, accuracy=0.1535, gradient_norm=0.1941, 
[2025-09-12 21:14:23,475][__main__][INFO] - Train, Round 040: loss=2.2965, accuracy=0.1541, gradient_norm=0.2031, 
[2025-09-12 21:14:23,475][__main__][INFO] - Train, Round 041: loss=2.2961, accuracy=0.1551, gradient_norm=0.2066, 
[2025-09-12 21:14:23,475][__main__][INFO] - Train, Round 042: loss=2.2958, accuracy=0.1564, gradient_norm=0.2015, 
[2025-09-12 21:14:23,475][__main__][INFO] - Train, Round 043: loss=2.2954, accuracy=0.1569, gradient_norm=0.2081, 
[2025-09-12 21:14:23,475][__main__][INFO] - Train, Round 044: loss=2.2950, accuracy=0.1578, gradient_norm=0.2123, 
[2025-09-12 21:14:23,475][__main__][INFO] - Train, Round 045: loss=2.2945, accuracy=0.1590, gradient_norm=0.2106, 
[2025-09-12 21:14:23,475][__main__][INFO] - Train, Round 046: loss=2.2941, accuracy=0.1596, gradient_norm=0.2118, 
[2025-09-12 21:14:23,475][__main__][INFO] - Train, Round 047: loss=2.2936, accuracy=0.1608, gradient_norm=0.2184, 
[2025-09-12 21:14:23,475][__main__][INFO] - Train, Round 048: loss=2.2930, accuracy=0.1606, gradient_norm=0.2179, 
[2025-09-12 21:14:23,475][__main__][INFO] - Train, Round 049: loss=2.2925, accuracy=0.1617, gradient_norm=0.2152, 
[2025-09-12 21:14:23,475][__main__][INFO] - Train, Round 050: loss=2.2920, accuracy=0.1633, gradient_norm=0.2194, 
[2025-09-12 21:14:23,475][__main__][INFO] - Train, Round 051: loss=2.2914, accuracy=0.1646, gradient_norm=0.2157, 
[2025-09-12 21:14:23,475][__main__][INFO] - Train, Round 052: loss=2.2907, accuracy=0.1652, gradient_norm=0.2361, 
[2025-09-12 21:14:23,475][__main__][INFO] - Train, Round 053: loss=2.2901, accuracy=0.1665, gradient_norm=0.2275, 
[2025-09-12 21:14:23,475][__main__][INFO] - Train, Round 054: loss=2.2894, accuracy=0.1690, gradient_norm=0.2329, 
[2025-09-12 21:14:23,475][__main__][INFO] - Train, Round 055: loss=2.2887, accuracy=0.1708, gradient_norm=0.2305, 
[2025-09-12 21:14:23,475][__main__][INFO] - Train, Round 056: loss=2.2879, accuracy=0.1713, gradient_norm=0.2428, 
[2025-09-12 21:14:23,475][__main__][INFO] - Train, Round 057: loss=2.2870, accuracy=0.1717, gradient_norm=0.2516, 
[2025-09-12 21:14:23,475][__main__][INFO] - Train, Round 058: loss=2.2861, accuracy=0.1729, gradient_norm=0.2549, 
[2025-09-12 21:14:23,475][__main__][INFO] - Train, Round 059: loss=2.2851, accuracy=0.1739, gradient_norm=0.2681, 
[2025-09-12 21:14:23,475][__main__][INFO] - Train, Round 060: loss=2.2841, accuracy=0.1758, gradient_norm=0.2631, 
[2025-09-12 21:14:23,475][__main__][INFO] - Train, Round 061: loss=2.2830, accuracy=0.1768, gradient_norm=0.2748, 
[2025-09-12 21:14:23,475][__main__][INFO] - Train, Round 062: loss=2.2818, accuracy=0.1774, gradient_norm=0.2723, 
[2025-09-12 21:14:23,475][__main__][INFO] - Train, Round 063: loss=2.2805, accuracy=0.1785, gradient_norm=0.2870, 
[2025-09-12 21:14:23,475][__main__][INFO] - Train, Round 064: loss=2.2792, accuracy=0.1785, gradient_norm=0.2990, 
[2025-09-12 21:14:23,475][__main__][INFO] - Train, Round 065: loss=2.2777, accuracy=0.1784, gradient_norm=0.3096, 
[2025-09-12 21:14:23,475][__main__][INFO] - Train, Round 066: loss=2.2760, accuracy=0.1777, gradient_norm=0.3140, 
[2025-09-12 21:14:23,475][__main__][INFO] - Train, Round 067: loss=2.2742, accuracy=0.1770, gradient_norm=0.3235, 
[2025-09-12 21:14:23,475][__main__][INFO] - Train, Round 068: loss=2.2723, accuracy=0.1799, gradient_norm=0.3422, 
[2025-09-12 21:14:23,475][__main__][INFO] - Train, Round 069: loss=2.2703, accuracy=0.1794, gradient_norm=0.3460, 
[2025-09-12 21:14:23,475][__main__][INFO] - Train, Round 070: loss=2.2680, accuracy=0.1787, gradient_norm=0.3632, 
[2025-09-12 21:14:23,475][__main__][INFO] - Train, Round 071: loss=2.2656, accuracy=0.1781, gradient_norm=0.3701, 
[2025-09-12 21:14:23,475][__main__][INFO] - Train, Round 072: loss=2.2630, accuracy=0.1794, gradient_norm=0.3937, 
[2025-09-12 21:14:23,475][__main__][INFO] - Train, Round 073: loss=2.2602, accuracy=0.1799, gradient_norm=0.3868, 
[2025-09-12 21:14:23,475][__main__][INFO] - Train, Round 074: loss=2.2572, accuracy=0.1821, gradient_norm=0.4242, 
[2025-09-12 21:14:23,475][__main__][INFO] - Train, Round 075: loss=2.2537, accuracy=0.1807, gradient_norm=0.4454, 
[2025-09-12 21:14:23,475][__main__][INFO] - Train, Round 076: loss=2.2501, accuracy=0.1812, gradient_norm=0.4581, 
[2025-09-12 21:14:23,475][__main__][INFO] - Train, Round 077: loss=2.2464, accuracy=0.1814, gradient_norm=0.4921, 
[2025-09-12 21:14:23,475][__main__][INFO] - Train, Round 078: loss=2.2422, accuracy=0.1839, gradient_norm=0.5265, 
[2025-09-12 21:14:23,475][__main__][INFO] - Train, Round 079: loss=2.2377, accuracy=0.1848, gradient_norm=0.5443, 
[2025-09-12 21:14:23,475][__main__][INFO] - Train, Round 080: loss=2.2329, accuracy=0.1874, gradient_norm=0.5644, 
[2025-09-12 21:14:23,475][__main__][INFO] - Train, Round 081: loss=2.2277, accuracy=0.1879, gradient_norm=0.5887, 
[2025-09-12 21:14:23,475][__main__][INFO] - Train, Round 082: loss=2.2220, accuracy=0.1867, gradient_norm=0.5885, 
[2025-09-12 21:14:23,475][__main__][INFO] - Train, Round 083: loss=2.2164, accuracy=0.1871, gradient_norm=0.6352, 
[2025-09-12 21:14:23,475][__main__][INFO] - Train, Round 084: loss=2.2103, accuracy=0.1880, gradient_norm=0.6762, 
[2025-09-12 21:14:23,475][__main__][INFO] - Train, Round 085: loss=2.2037, accuracy=0.1894, gradient_norm=0.7021, 
[2025-09-12 21:14:23,475][__main__][INFO] - Train, Round 086: loss=2.1973, accuracy=0.1894, gradient_norm=0.7248, 
[2025-09-12 21:14:23,475][__main__][INFO] - Train, Round 087: loss=2.1904, accuracy=0.1924, gradient_norm=0.7725, 
[2025-09-12 21:14:23,475][__main__][INFO] - Train, Round 088: loss=2.1832, accuracy=0.1934, gradient_norm=0.8471, 
[2025-09-12 21:14:23,476][__main__][INFO] - Train, Round 089: loss=2.1760, accuracy=0.1979, gradient_norm=0.8735, 
[2025-09-12 21:14:23,476][__main__][INFO] - Train, Round 090: loss=2.1691, accuracy=0.1968, gradient_norm=0.9008, 
[2025-09-12 21:14:23,476][__main__][INFO] - Train, Round 091: loss=2.1613, accuracy=0.1995, gradient_norm=0.9253, 
[2025-09-12 21:14:23,476][__main__][INFO] - Train, Round 092: loss=2.1539, accuracy=0.2017, gradient_norm=0.9738, 
[2025-09-12 21:14:23,476][__main__][INFO] - Train, Round 093: loss=2.1467, accuracy=0.2032, gradient_norm=1.0308, 
[2025-09-12 21:14:23,476][__main__][INFO] - Train, Round 094: loss=2.1397, accuracy=0.2093, gradient_norm=1.0702, 
[2025-09-12 21:14:23,476][__main__][INFO] - Train, Round 095: loss=2.1326, accuracy=0.2117, gradient_norm=1.1125, 
[2025-09-12 21:14:23,476][__main__][INFO] - Train, Round 096: loss=2.1269, accuracy=0.2113, gradient_norm=1.1471, 
[2025-09-12 21:14:23,476][__main__][INFO] - Train, Round 097: loss=2.1200, accuracy=0.2191, gradient_norm=1.1989, 
[2025-09-12 21:14:23,476][__main__][INFO] - Train, Round 098: loss=2.1131, accuracy=0.2236, gradient_norm=1.2432, 
[2025-09-12 21:14:23,476][__main__][INFO] - Train, Round 099: loss=2.1065, accuracy=0.2244, gradient_norm=1.2296, 
[2025-09-12 21:14:23,476][__main__][INFO] - Train, Round 100: loss=2.0998, accuracy=0.2316, gradient_norm=1.3195, 
[2025-09-12 21:14:23,476][__main__][INFO] - Train, Round 101: loss=2.0945, accuracy=0.2358, gradient_norm=1.3016, 
[2025-09-12 21:14:23,476][__main__][INFO] - Train, Round 102: loss=2.0881, accuracy=0.2399, gradient_norm=1.3697, 
[2025-09-12 21:14:23,476][__main__][INFO] - Train, Round 103: loss=2.0821, accuracy=0.2472, gradient_norm=1.4608, 
[2025-09-12 21:14:23,476][__main__][INFO] - Train, Round 104: loss=2.0774, accuracy=0.2504, gradient_norm=1.5097, 
[2025-09-12 21:14:23,476][__main__][INFO] - Train, Round 105: loss=2.0805, accuracy=0.2534, gradient_norm=1.6574, 
[2025-09-12 21:14:23,476][__main__][INFO] - Train, Round 106: loss=2.1265, accuracy=0.2294, gradient_norm=2.4217, 
[2025-09-12 21:14:23,476][__main__][INFO] - Train, Round 107: loss=2.3608, accuracy=0.1737, gradient_norm=3.5945, 
[2025-09-12 21:14:23,476][__main__][INFO] - Train, Round 108: loss=2.4345, accuracy=0.1720, gradient_norm=3.9350, 
[2025-09-12 21:14:23,476][__main__][INFO] - Train, Round 109: loss=2.2708, accuracy=0.1951, gradient_norm=1.5895, 
[2025-09-12 21:14:23,476][__main__][INFO] - Train, Round 110: loss=2.1453, accuracy=0.2652, gradient_norm=0.9887, 
[2025-09-12 21:14:23,476][__main__][INFO] - Train, Round 111: loss=2.1114, accuracy=0.2834, gradient_norm=0.9475, 
[2025-09-12 21:14:23,476][__main__][INFO] - Train, Round 112: loss=2.0916, accuracy=0.2818, gradient_norm=1.0204, 
[2025-09-12 21:14:23,476][__main__][INFO] - Train, Round 113: loss=2.0761, accuracy=0.2749, gradient_norm=1.0383, 
[2025-09-12 21:14:23,476][__main__][INFO] - Train, Round 114: loss=2.0634, accuracy=0.2780, gradient_norm=1.1556, 
[2025-09-12 21:14:23,476][__main__][INFO] - Train, Round 115: loss=2.0551, accuracy=0.2758, gradient_norm=1.2873, 
[2025-09-12 21:14:23,476][__main__][INFO] - Train, Round 116: loss=2.0489, accuracy=0.2773, gradient_norm=1.3525, 
[2025-09-12 21:14:23,476][__main__][INFO] - Train, Round 117: loss=2.0445, accuracy=0.2794, gradient_norm=1.4652, 
[2025-09-12 21:14:23,476][__main__][INFO] - Train, Round 118: loss=2.0536, accuracy=0.2706, gradient_norm=1.7330, 
[2025-09-12 21:14:23,476][__main__][INFO] - Train, Round 119: loss=2.0677, accuracy=0.2690, gradient_norm=2.0392, 
[2025-09-12 21:14:23,476][__main__][INFO] - Train, Round 120: loss=2.1404, accuracy=0.2406, gradient_norm=2.3947, 
[2025-09-12 21:14:23,476][__main__][INFO] - Train, Round 121: loss=2.1457, accuracy=0.2448, gradient_norm=2.5998, 
[2025-09-12 21:14:23,476][__main__][INFO] - Train, Round 122: loss=2.1754, accuracy=0.2305, gradient_norm=2.2262, 
[2025-09-12 21:14:23,476][__main__][INFO] - Train, Round 123: loss=2.0573, accuracy=0.2791, gradient_norm=1.4701, 
[2025-09-12 21:14:23,476][__main__][INFO] - Train, Round 124: loss=2.0369, accuracy=0.2902, gradient_norm=1.4498, 
[2025-09-12 21:14:23,476][__main__][INFO] - Train, Round 125: loss=2.0219, accuracy=0.2941, gradient_norm=1.4961, 
[2025-09-12 21:14:23,476][__main__][INFO] - Train, Round 126: loss=2.0147, accuracy=0.2979, gradient_norm=1.5021, 
[2025-09-12 21:14:23,476][__main__][INFO] - Train, Round 127: loss=2.0082, accuracy=0.2965, gradient_norm=1.6265, 
[2025-09-12 21:14:23,476][__main__][INFO] - Train, Round 128: loss=2.0183, accuracy=0.2849, gradient_norm=1.7690, 
[2025-09-12 21:14:23,476][__main__][INFO] - Train, Round 129: loss=2.0461, accuracy=0.2784, gradient_norm=2.2351, 
[2025-09-12 21:14:23,476][__main__][INFO] - Train, Round 130: loss=2.1187, accuracy=0.2447, gradient_norm=2.5637, 
[2025-09-12 21:14:23,476][__main__][INFO] - Train, Round 131: loss=2.1078, accuracy=0.2548, gradient_norm=2.7837, 
[2025-09-12 21:14:23,476][__main__][INFO] - Train, Round 132: loss=2.1529, accuracy=0.2450, gradient_norm=2.3670, 
[2025-09-12 21:14:23,476][__main__][INFO] - Train, Round 133: loss=2.0173, accuracy=0.2903, gradient_norm=1.4789, 
[2025-09-12 21:14:23,476][__main__][INFO] - Train, Round 134: loss=1.9924, accuracy=0.3083, gradient_norm=1.3894, 
[2025-09-12 21:14:23,476][__main__][INFO] - Train, Round 135: loss=1.9762, accuracy=0.3085, gradient_norm=1.4673, 
[2025-09-12 21:14:23,476][__main__][INFO] - Train, Round 136: loss=1.9698, accuracy=0.3105, gradient_norm=1.5972, 
[2025-09-12 21:14:23,476][__main__][INFO] - Train, Round 137: loss=1.9703, accuracy=0.3082, gradient_norm=1.6838, 
[2025-09-12 21:14:23,476][__main__][INFO] - Train, Round 138: loss=1.9724, accuracy=0.3071, gradient_norm=1.8332, 
[2025-09-12 21:14:23,476][__main__][INFO] - Train, Round 139: loss=1.9895, accuracy=0.2967, gradient_norm=2.1874, 
[2025-09-12 21:14:23,476][__main__][INFO] - Train, Round 140: loss=2.0530, accuracy=0.2734, gradient_norm=2.5094, 
[2025-09-12 21:14:23,476][__main__][INFO] - Train, Round 141: loss=2.0616, accuracy=0.2653, gradient_norm=2.6972, 
[2025-09-12 21:14:23,476][__main__][INFO] - Train, Round 142: loss=2.0795, accuracy=0.2665, gradient_norm=2.4024, 
[2025-09-12 21:14:23,476][__main__][INFO] - Train, Round 143: loss=1.9879, accuracy=0.2912, gradient_norm=1.8199, 
[2025-09-12 21:14:23,476][__main__][INFO] - Train, Round 144: loss=1.9660, accuracy=0.3063, gradient_norm=1.7415, 
[2025-09-12 21:14:23,476][__main__][INFO] - Train, Round 145: loss=1.9481, accuracy=0.3098, gradient_norm=1.7267, 
[2025-09-12 21:14:23,476][__main__][INFO] - Train, Round 146: loss=1.9454, accuracy=0.3093, gradient_norm=1.8647, 
[2025-09-12 21:14:23,476][__main__][INFO] - Train, Round 147: loss=1.9524, accuracy=0.3025, gradient_norm=2.0272, 
[2025-09-12 21:14:23,476][__main__][INFO] - Train, Round 148: loss=1.9758, accuracy=0.2993, gradient_norm=2.1153, 
[2025-09-12 21:14:23,476][__main__][INFO] - Train, Round 149: loss=1.9688, accuracy=0.2968, gradient_norm=2.2470, 
[2025-09-12 21:14:23,476][__main__][INFO] - Train, Round 150: loss=1.9835, accuracy=0.2936, gradient_norm=2.3143, 
[2025-09-12 21:14:23,476][__main__][INFO] - Train, Round 151: loss=1.9755, accuracy=0.2926, gradient_norm=2.4406, 
[2025-09-12 21:14:23,476][__main__][INFO] - Train, Round 152: loss=2.0088, accuracy=0.2894, gradient_norm=2.3065, 
[2025-09-12 21:14:23,476][__main__][INFO] - Train, Round 153: loss=1.9452, accuracy=0.3045, gradient_norm=2.0108, 
[2025-09-12 21:14:23,476][__main__][INFO] - Train, Round 154: loss=1.9414, accuracy=0.3087, gradient_norm=1.9763, 
[2025-09-12 21:14:23,476][__main__][INFO] - Train, Round 155: loss=1.9218, accuracy=0.3138, gradient_norm=1.8562, 
[2025-09-12 21:14:23,476][__main__][INFO] - Train, Round 156: loss=1.9165, accuracy=0.3180, gradient_norm=1.9138, 
[2025-09-12 21:14:23,476][__main__][INFO] - Train, Round 157: loss=1.9084, accuracy=0.3142, gradient_norm=2.0432, 
[2025-09-12 21:14:23,476][__main__][INFO] - Train, Round 158: loss=1.9268, accuracy=0.3131, gradient_norm=2.1725, 
[2025-09-12 21:14:23,476][__main__][INFO] - Train, Round 159: loss=1.9238, accuracy=0.3053, gradient_norm=2.2714, 
[2025-09-12 21:14:23,476][__main__][INFO] - Train, Round 160: loss=1.9390, accuracy=0.3035, gradient_norm=2.1985, 
[2025-09-12 21:14:23,476][__main__][INFO] - Train, Round 161: loss=1.9217, accuracy=0.3093, gradient_norm=2.1782, 
[2025-09-12 21:14:23,476][__main__][INFO] - Train, Round 162: loss=1.9228, accuracy=0.3118, gradient_norm=2.0674, 
[2025-09-12 21:14:23,476][__main__][INFO] - Train, Round 163: loss=1.9016, accuracy=0.3172, gradient_norm=2.0807, 
[2025-09-12 21:14:23,477][__main__][INFO] - Train, Round 164: loss=1.9147, accuracy=0.3119, gradient_norm=2.1692, 
[2025-09-12 21:14:23,477][__main__][INFO] - Train, Round 165: loss=1.9107, accuracy=0.3136, gradient_norm=2.2507, 
[2025-09-12 21:14:23,477][__main__][INFO] - Train, Round 166: loss=1.9299, accuracy=0.3020, gradient_norm=2.3872, 
[2025-09-12 21:14:23,477][__main__][INFO] - Train, Round 167: loss=1.9363, accuracy=0.3035, gradient_norm=2.3284, 
[2025-09-12 21:14:23,477][__main__][INFO] - Train, Round 168: loss=1.9125, accuracy=0.3091, gradient_norm=2.0092, 
[2025-09-12 21:14:23,477][__main__][INFO] - Train, Round 169: loss=1.8733, accuracy=0.3265, gradient_norm=1.8355, 
[2025-09-12 21:14:23,477][__main__][INFO] - Train, Round 170: loss=1.8620, accuracy=0.3350, gradient_norm=1.7891, 
[2025-09-12 21:14:23,477][__main__][INFO] - Train, Round 171: loss=1.8531, accuracy=0.3377, gradient_norm=1.8569, 
[2025-09-12 21:14:23,477][__main__][INFO] - Train, Round 172: loss=1.8515, accuracy=0.3377, gradient_norm=1.8921, 
[2025-09-12 21:14:23,477][__main__][INFO] - Train, Round 173: loss=1.8556, accuracy=0.3358, gradient_norm=2.1235, 
[2025-09-12 21:14:23,477][__main__][INFO] - Train, Round 174: loss=1.8884, accuracy=0.3266, gradient_norm=2.2795, 
[2025-09-12 21:14:23,477][__main__][INFO] - Train, Round 175: loss=1.9048, accuracy=0.3207, gradient_norm=2.5758, 
[2025-09-12 21:14:23,477][__main__][INFO] - Train, Round 176: loss=1.9638, accuracy=0.3012, gradient_norm=2.8127, 
[2025-09-12 21:14:23,477][__main__][INFO] - Train, Round 177: loss=1.9606, accuracy=0.2977, gradient_norm=2.8141, 
[2025-09-12 21:14:23,477][__main__][INFO] - Train, Round 178: loss=1.9667, accuracy=0.2956, gradient_norm=2.5384, 
[2025-09-12 21:14:23,477][__main__][INFO] - Train, Round 179: loss=1.8880, accuracy=0.3201, gradient_norm=2.0567, 
[2025-09-12 21:14:23,477][__main__][INFO] - Train, Round 180: loss=1.8571, accuracy=0.3388, gradient_norm=1.8820, 
[2025-09-12 21:14:23,477][__main__][INFO] - Train, Round 181: loss=1.8376, accuracy=0.3414, gradient_norm=1.8272, 
[2025-09-12 21:14:23,477][__main__][INFO] - Train, Round 182: loss=1.8257, accuracy=0.3473, gradient_norm=1.8926, 
[2025-09-12 21:14:23,477][__main__][INFO] - Train, Round 183: loss=1.8239, accuracy=0.3451, gradient_norm=1.9769, 
[2025-09-12 21:14:23,477][__main__][INFO] - Train, Round 184: loss=1.8273, accuracy=0.3440, gradient_norm=2.0724, 
[2025-09-12 21:14:23,477][__main__][INFO] - Train, Round 185: loss=1.8488, accuracy=0.3280, gradient_norm=2.3627, 
[2025-09-12 21:14:23,477][__main__][INFO] - Train, Round 186: loss=1.8800, accuracy=0.3196, gradient_norm=2.5186, 
[2025-09-12 21:14:23,477][__main__][INFO] - Train, Round 187: loss=1.8826, accuracy=0.3099, gradient_norm=2.4336, 
[2025-09-12 21:14:23,477][__main__][INFO] - Train, Round 188: loss=1.8707, accuracy=0.3273, gradient_norm=2.3274, 
[2025-09-12 21:14:23,477][__main__][INFO] - Train, Round 189: loss=1.8555, accuracy=0.3237, gradient_norm=2.2610, 
[2025-09-12 21:14:23,477][__main__][INFO] - Train, Round 190: loss=1.8371, accuracy=0.3381, gradient_norm=2.1737, 
[2025-09-12 21:14:23,477][__main__][INFO] - Train, Round 191: loss=1.8333, accuracy=0.3369, gradient_norm=2.1423, 
[2025-09-12 21:14:23,477][__main__][INFO] - Train, Round 192: loss=1.8250, accuracy=0.3435, gradient_norm=2.1281, 
[2025-09-12 21:14:23,477][__main__][INFO] - Train, Round 193: loss=1.8257, accuracy=0.3376, gradient_norm=2.1487, 
[2025-09-12 21:14:23,477][__main__][INFO] - Train, Round 194: loss=1.8107, accuracy=0.3516, gradient_norm=2.1761, 
[2025-09-12 21:14:23,477][__main__][INFO] - Train, Round 195: loss=1.8211, accuracy=0.3420, gradient_norm=2.2450, 
[2025-09-12 21:14:23,477][__main__][INFO] - Train, Round 196: loss=1.8182, accuracy=0.3489, gradient_norm=2.2987, 
[2025-09-12 21:14:23,477][__main__][INFO] - Train, Round 197: loss=1.8255, accuracy=0.3471, gradient_norm=2.4487, 
[2025-09-12 21:14:23,477][__main__][INFO] - Train, Round 198: loss=1.8570, accuracy=0.3390, gradient_norm=2.5772, 
[2025-09-12 21:14:23,477][__main__][INFO] - Train, Round 199: loss=1.8596, accuracy=0.3409, gradient_norm=2.7578, 
[2025-09-12 21:14:23,477][__main__][INFO] - Train, Round 200: loss=1.9109, accuracy=0.3196, gradient_norm=2.9019, 
[2025-09-12 21:14:23,477][__main__][INFO] - Train, Round 201: loss=1.8972, accuracy=0.3254, gradient_norm=2.8616, 
[2025-09-12 21:14:23,477][__main__][INFO] - Train, Round 202: loss=1.9010, accuracy=0.3136, gradient_norm=2.5540, 
[2025-09-12 21:14:23,477][__main__][INFO] - Train, Round 203: loss=1.8208, accuracy=0.3470, gradient_norm=2.1006, 
[2025-09-12 21:14:23,477][__main__][INFO] - Train, Round 204: loss=1.7895, accuracy=0.3616, gradient_norm=1.9232, 
[2025-09-12 21:14:23,477][__main__][INFO] - Train, Round 205: loss=1.7776, accuracy=0.3665, gradient_norm=1.9438, 
[2025-09-12 21:14:23,477][__main__][INFO] - Train, Round 206: loss=1.7767, accuracy=0.3615, gradient_norm=2.1080, 
[2025-09-12 21:14:23,477][__main__][INFO] - Train, Round 207: loss=1.7849, accuracy=0.3600, gradient_norm=2.2829, 
[2025-09-12 21:14:23,477][__main__][INFO] - Train, Round 208: loss=1.7909, accuracy=0.3522, gradient_norm=2.3754, 
[2025-09-12 21:14:23,477][__main__][INFO] - Train, Round 209: loss=1.7989, accuracy=0.3569, gradient_norm=2.4918, 
[2025-09-12 21:14:23,477][__main__][INFO] - Train, Round 210: loss=1.8031, accuracy=0.3466, gradient_norm=2.3735, 
[2025-09-12 21:14:23,477][__main__][INFO] - Train, Round 211: loss=1.7923, accuracy=0.3565, gradient_norm=2.4332, 
[2025-09-12 21:14:23,477][__main__][INFO] - Train, Round 212: loss=1.7938, accuracy=0.3550, gradient_norm=2.3994, 
[2025-09-12 21:14:23,477][__main__][INFO] - Train, Round 213: loss=1.7901, accuracy=0.3609, gradient_norm=2.3876, 
[2025-09-12 21:14:23,477][__main__][INFO] - Train, Round 214: loss=1.7805, accuracy=0.3601, gradient_norm=2.3459, 
[2025-09-12 21:14:23,477][__main__][INFO] - Train, Round 215: loss=1.7785, accuracy=0.3664, gradient_norm=2.3904, 
[2025-09-12 21:14:23,477][__main__][INFO] - Train, Round 216: loss=1.7705, accuracy=0.3644, gradient_norm=2.2782, 
[2025-09-12 21:14:23,477][__main__][INFO] - Train, Round 217: loss=1.7646, accuracy=0.3695, gradient_norm=2.3668, 
[2025-09-12 21:14:23,477][__main__][INFO] - Train, Round 218: loss=1.7603, accuracy=0.3720, gradient_norm=2.4092, 
[2025-09-12 21:14:23,477][__main__][INFO] - Train, Round 219: loss=1.7853, accuracy=0.3641, gradient_norm=2.7643, 
[2025-09-12 21:14:23,477][__main__][INFO] - Train, Round 220: loss=1.8498, accuracy=0.3423, gradient_norm=3.1040, 
[2025-09-12 21:14:23,477][__main__][INFO] - Train, Round 221: loss=1.8956, accuracy=0.3266, gradient_norm=3.3295, 
[2025-09-12 21:14:23,477][__main__][INFO] - Train, Round 222: loss=1.9057, accuracy=0.3140, gradient_norm=2.9012, 
[2025-09-12 21:14:23,477][__main__][INFO] - Train, Round 223: loss=1.7746, accuracy=0.3688, gradient_norm=2.1374, 
[2025-09-12 21:14:23,477][__main__][INFO] - Train, Round 224: loss=1.7343, accuracy=0.3888, gradient_norm=2.0832, 
[2025-09-12 21:14:23,477][__main__][INFO] - Train, Round 225: loss=1.7304, accuracy=0.3843, gradient_norm=2.0950, 
[2025-09-12 21:14:23,477][__main__][INFO] - Train, Round 226: loss=1.7323, accuracy=0.3819, gradient_norm=2.3195, 
[2025-09-12 21:14:23,477][__main__][INFO] - Train, Round 227: loss=1.7540, accuracy=0.3728, gradient_norm=2.3804, 
[2025-09-12 21:14:23,477][__main__][INFO] - Train, Round 228: loss=1.7550, accuracy=0.3739, gradient_norm=2.6607, 
[2025-09-12 21:14:23,477][__main__][INFO] - Train, Round 229: loss=1.8013, accuracy=0.3531, gradient_norm=2.6956, 
[2025-09-12 21:14:23,477][__main__][INFO] - Train, Round 230: loss=1.7693, accuracy=0.3666, gradient_norm=2.6342, 
[2025-09-12 21:14:23,477][__main__][INFO] - Train, Round 231: loss=1.7725, accuracy=0.3581, gradient_norm=2.5970, 
[2025-09-12 21:14:23,477][__main__][INFO] - Train, Round 232: loss=1.7699, accuracy=0.3596, gradient_norm=2.6198, 
[2025-09-12 21:14:23,477][__main__][INFO] - Train, Round 233: loss=1.7603, accuracy=0.3577, gradient_norm=2.4210, 
[2025-09-12 21:14:23,477][__main__][INFO] - Train, Round 234: loss=1.7415, accuracy=0.3715, gradient_norm=2.3706, 
[2025-09-12 21:14:23,477][__main__][INFO] - Train, Round 235: loss=1.7325, accuracy=0.3765, gradient_norm=2.4196, 
[2025-09-12 21:14:23,477][__main__][INFO] - Train, Round 236: loss=1.7400, accuracy=0.3713, gradient_norm=2.4641, 
[2025-09-12 21:14:23,477][__main__][INFO] - Train, Round 237: loss=1.7344, accuracy=0.3764, gradient_norm=2.3854, 
[2025-09-12 21:14:23,477][__main__][INFO] - Train, Round 238: loss=1.7179, accuracy=0.3850, gradient_norm=2.4212, 
[2025-09-12 21:14:23,477][__main__][INFO] - Train, Round 239: loss=1.7340, accuracy=0.3766, gradient_norm=2.4968, 
[2025-09-12 21:14:23,478][__main__][INFO] - Train, Round 240: loss=1.7213, accuracy=0.3848, gradient_norm=2.5840, 
[2025-09-12 21:14:23,478][__main__][INFO] - Train, Round 241: loss=1.7500, accuracy=0.3780, gradient_norm=2.6916, 
[2025-09-12 21:14:23,478][__main__][INFO] - Train, Round 242: loss=1.7509, accuracy=0.3755, gradient_norm=2.7692, 
[2025-09-12 21:14:23,478][__main__][INFO] - Train, Round 243: loss=1.7618, accuracy=0.3747, gradient_norm=2.7440, 
[2025-09-12 21:14:23,478][__main__][INFO] - Train, Round 244: loss=1.7455, accuracy=0.3804, gradient_norm=2.8374, 
[2025-09-12 21:14:23,478][__main__][INFO] - Train, Round 245: loss=1.7830, accuracy=0.3684, gradient_norm=2.9646, 
[2025-09-12 21:14:23,478][__main__][INFO] - Train, Round 246: loss=1.7694, accuracy=0.3771, gradient_norm=2.9528, 
[2025-09-12 21:14:23,478][__main__][INFO] - Train, Round 247: loss=1.7774, accuracy=0.3690, gradient_norm=2.8178, 
[2025-09-12 21:14:23,478][__main__][INFO] - Train, Round 248: loss=1.7383, accuracy=0.3844, gradient_norm=2.5663, 
[2025-09-12 21:14:23,478][__main__][INFO] - Train, Round 249: loss=1.7021, accuracy=0.3956, gradient_norm=2.3566, 
[2025-09-12 21:14:23,478][__main__][INFO] - Train, Round 250: loss=1.6799, accuracy=0.4043, gradient_norm=2.2583, 
[2025-09-12 21:14:23,478][__main__][INFO] - Train, Round 251: loss=1.6620, accuracy=0.4064, gradient_norm=2.3282, 
[2025-09-12 21:14:23,478][__main__][INFO] - Train, Round 252: loss=1.6617, accuracy=0.4102, gradient_norm=2.2515, 
[2025-09-12 21:14:23,478][__main__][INFO] - Train, Round 253: loss=1.6668, accuracy=0.4044, gradient_norm=2.5335, 
[2025-09-12 21:14:23,478][__main__][INFO] - Train, Round 254: loss=1.6959, accuracy=0.3930, gradient_norm=2.6324, 
[2025-09-12 21:14:23,478][__main__][INFO] - Train, Round 255: loss=1.7058, accuracy=0.3849, gradient_norm=2.8083, 
[2025-09-12 21:14:23,478][__main__][INFO] - Train, Round 256: loss=1.7222, accuracy=0.3777, gradient_norm=2.8896, 
[2025-09-12 21:14:23,478][__main__][INFO] - Train, Round 257: loss=1.7563, accuracy=0.3656, gradient_norm=3.0172, 
[2025-09-12 21:14:23,478][__main__][INFO] - Train, Round 258: loss=1.7283, accuracy=0.3760, gradient_norm=2.7309, 
[2025-09-12 21:14:23,478][__main__][INFO] - Train, Round 259: loss=1.7013, accuracy=0.3862, gradient_norm=2.6562, 
[2025-09-12 21:14:23,478][__main__][INFO] - Train, Round 260: loss=1.7011, accuracy=0.3888, gradient_norm=2.5962, 
[2025-09-12 21:14:23,478][__main__][INFO] - Train, Round 261: loss=1.7003, accuracy=0.3815, gradient_norm=2.7484, 
[2025-09-12 21:14:23,478][__main__][INFO] - Train, Round 262: loss=1.7108, accuracy=0.3802, gradient_norm=2.7311, 
[2025-09-12 21:14:23,478][__main__][INFO] - Train, Round 263: loss=1.7023, accuracy=0.3865, gradient_norm=2.6683, 
[2025-09-12 21:14:23,478][__main__][INFO] - Train, Round 264: loss=1.6669, accuracy=0.4060, gradient_norm=2.4718, 
[2025-09-12 21:14:23,478][__main__][INFO] - Train, Round 265: loss=1.6593, accuracy=0.4021, gradient_norm=2.6532, 
[2025-09-12 21:14:23,478][__main__][INFO] - Train, Round 266: loss=1.6924, accuracy=0.3991, gradient_norm=2.6495, 
[2025-09-12 21:14:23,478][__main__][INFO] - Train, Round 267: loss=1.6734, accuracy=0.3961, gradient_norm=2.8256, 
[2025-09-12 21:14:23,478][__main__][INFO] - Train, Round 268: loss=1.7158, accuracy=0.3918, gradient_norm=2.9809, 
[2025-09-12 21:14:23,478][__main__][INFO] - Train, Round 269: loss=1.7124, accuracy=0.3882, gradient_norm=2.9700, 
[2025-09-12 21:14:23,478][__main__][INFO] - Train, Round 270: loss=1.7117, accuracy=0.3966, gradient_norm=2.9049, 
[2025-09-12 21:14:23,478][__main__][INFO] - Train, Round 271: loss=1.6879, accuracy=0.3964, gradient_norm=2.7535, 
[2025-09-12 21:14:23,478][__main__][INFO] - Train, Round 272: loss=1.6647, accuracy=0.4127, gradient_norm=2.6819, 
[2025-09-12 21:14:23,478][__main__][INFO] - Train, Round 273: loss=1.6560, accuracy=0.4080, gradient_norm=2.6561, 
[2025-09-12 21:14:23,478][__main__][INFO] - Train, Round 274: loss=1.6580, accuracy=0.4148, gradient_norm=2.6471, 
[2025-09-12 21:14:23,478][__main__][INFO] - Train, Round 275: loss=1.6473, accuracy=0.4138, gradient_norm=2.6764, 
[2025-09-12 21:14:23,478][__main__][INFO] - Train, Round 276: loss=1.6567, accuracy=0.4116, gradient_norm=2.8495, 
[2025-09-12 21:14:23,478][__main__][INFO] - Train, Round 277: loss=1.6648, accuracy=0.4086, gradient_norm=2.8372, 
[2025-09-12 21:14:23,478][__main__][INFO] - Train, Round 278: loss=1.6632, accuracy=0.4034, gradient_norm=2.7977, 
[2025-09-12 21:14:23,478][__main__][INFO] - Train, Round 279: loss=1.6421, accuracy=0.4152, gradient_norm=2.8285, 
[2025-09-12 21:14:23,478][__main__][INFO] - Train, Round 280: loss=1.6509, accuracy=0.4118, gradient_norm=2.9407, 
[2025-09-12 21:14:23,478][__main__][INFO] - Train, Round 281: loss=1.6655, accuracy=0.4097, gradient_norm=2.8836, 
[2025-09-12 21:14:23,478][__main__][INFO] - Train, Round 282: loss=1.6403, accuracy=0.4151, gradient_norm=2.6900, 
[2025-09-12 21:14:23,478][__main__][INFO] - Train, Round 283: loss=1.6177, accuracy=0.4259, gradient_norm=2.6948, 
[2025-09-12 21:14:23,478][__main__][INFO] - Train, Round 284: loss=1.6126, accuracy=0.4275, gradient_norm=2.6732, 
[2025-09-12 21:14:23,478][__main__][INFO] - Train, Round 285: loss=1.6094, accuracy=0.4280, gradient_norm=2.7635, 
[2025-09-12 21:14:23,478][__main__][INFO] - Train, Round 286: loss=1.6320, accuracy=0.4248, gradient_norm=2.9405, 
[2025-09-12 21:14:23,478][__main__][INFO] - Train, Round 287: loss=1.6597, accuracy=0.4064, gradient_norm=3.1032, 
[2025-09-12 21:14:23,478][__main__][INFO] - Train, Round 288: loss=1.6900, accuracy=0.4066, gradient_norm=3.2325, 
[2025-09-12 21:14:23,478][__main__][INFO] - Train, Round 289: loss=1.6813, accuracy=0.3939, gradient_norm=3.0842, 
[2025-09-12 21:14:23,478][__main__][INFO] - Train, Round 290: loss=1.6615, accuracy=0.4107, gradient_norm=2.9137, 
[2025-09-12 21:14:23,478][__main__][INFO] - Train, Round 291: loss=1.6296, accuracy=0.4137, gradient_norm=2.6582, 
[2025-09-12 21:14:23,478][__main__][INFO] - Train, Round 292: loss=1.5917, accuracy=0.4323, gradient_norm=2.5373, 
[2025-09-12 21:14:23,478][__main__][INFO] - Train, Round 293: loss=1.5874, accuracy=0.4336, gradient_norm=2.5746, 
[2025-09-12 21:14:23,478][__main__][INFO] - Train, Round 294: loss=1.5960, accuracy=0.4288, gradient_norm=2.8719, 
[2025-09-12 21:14:23,478][__main__][INFO] - Train, Round 295: loss=1.6701, accuracy=0.4001, gradient_norm=3.4240, 
[2025-09-12 21:14:23,478][__main__][INFO] - Train, Round 296: loss=1.8216, accuracy=0.3674, gradient_norm=3.9377, 
[2025-09-12 21:14:23,478][__main__][INFO] - Train, Round 297: loss=1.8052, accuracy=0.3334, gradient_norm=3.6450, 
[2025-09-12 21:14:23,478][__main__][INFO] - Train, Round 298: loss=1.7267, accuracy=0.3776, gradient_norm=2.9806, 
[2025-09-12 21:14:23,478][__main__][INFO] - Train, Round 299: loss=1.6316, accuracy=0.4135, gradient_norm=2.4470, 
[2025-09-12 21:14:23,478][__main__][INFO] - Train, Round 300: loss=1.5917, accuracy=0.4284, gradient_norm=2.4233, 
[2025-09-12 21:14:23,478][__main__][INFO] - Train, Round 301: loss=1.5796, accuracy=0.4319, gradient_norm=2.4613, 
[2025-09-12 21:14:23,478][__main__][INFO] - Train, Round 302: loss=1.5822, accuracy=0.4302, gradient_norm=2.5721, 
[2025-09-12 21:14:23,478][__main__][INFO] - Train, Round 303: loss=1.5827, accuracy=0.4292, gradient_norm=2.6743, 
[2025-09-12 21:14:23,478][__main__][INFO] - Train, Round 304: loss=1.5870, accuracy=0.4265, gradient_norm=2.7740, 
[2025-09-12 21:14:23,478][__main__][INFO] - Train, Round 305: loss=1.5999, accuracy=0.4217, gradient_norm=2.8987, 
[2025-09-12 21:14:23,478][__main__][INFO] - Train, Round 306: loss=1.6086, accuracy=0.4137, gradient_norm=2.9646, 
[2025-09-12 21:14:23,478][__main__][INFO] - Train, Round 307: loss=1.6028, accuracy=0.4187, gradient_norm=2.7971, 
[2025-09-12 21:14:23,478][__main__][INFO] - Train, Round 308: loss=1.5936, accuracy=0.4194, gradient_norm=3.0074, 
[2025-09-12 21:14:23,478][__main__][INFO] - Train, Round 309: loss=1.6302, accuracy=0.4086, gradient_norm=3.1098, 
[2025-09-12 21:14:23,478][__main__][INFO] - Train, Round 310: loss=1.6342, accuracy=0.3979, gradient_norm=3.1610, 
[2025-09-12 21:14:23,478][__main__][INFO] - Train, Round 311: loss=1.6340, accuracy=0.4111, gradient_norm=3.0217, 
[2025-09-12 21:14:23,478][__main__][INFO] - Train, Round 312: loss=1.6009, accuracy=0.4086, gradient_norm=2.8240, 
[2025-09-12 21:14:23,478][__main__][INFO] - Train, Round 313: loss=1.5915, accuracy=0.4305, gradient_norm=2.7952, 
[2025-09-12 21:14:23,479][__main__][INFO] - Train, Round 314: loss=1.5694, accuracy=0.4282, gradient_norm=2.8773, 
[2025-09-12 21:14:23,479][__main__][INFO] - Train, Round 315: loss=1.5960, accuracy=0.4288, gradient_norm=2.7755, 
[2025-09-12 21:14:23,479][__main__][INFO] - Train, Round 316: loss=1.5584, accuracy=0.4392, gradient_norm=2.7485, 
[2025-09-12 21:14:23,479][__main__][INFO] - Train, Round 317: loss=1.5742, accuracy=0.4373, gradient_norm=2.9275, 
[2025-09-12 21:14:23,479][__main__][INFO] - Train, Round 318: loss=1.6062, accuracy=0.4289, gradient_norm=3.0473, 
[2025-09-12 21:14:23,479][__main__][INFO] - Train, Round 319: loss=1.6137, accuracy=0.4288, gradient_norm=3.1735, 
[2025-09-12 21:14:23,479][__main__][INFO] - Train, Round 320: loss=1.6285, accuracy=0.4265, gradient_norm=3.3405, 
[2025-09-12 21:14:23,479][__main__][INFO] - Train, Round 321: loss=1.6719, accuracy=0.4159, gradient_norm=3.4790, 
[2025-09-12 21:14:23,479][__main__][INFO] - Train, Round 322: loss=1.6596, accuracy=0.4156, gradient_norm=3.4108, 
[2025-09-12 21:14:23,479][__main__][INFO] - Train, Round 323: loss=1.6738, accuracy=0.4017, gradient_norm=3.2898, 
[2025-09-12 21:14:23,479][__main__][INFO] - Train, Round 324: loss=1.5947, accuracy=0.4330, gradient_norm=2.7238, 
[2025-09-12 21:14:23,479][__main__][INFO] - Train, Round 325: loss=1.5375, accuracy=0.4530, gradient_norm=2.4604, 
[2025-09-12 21:14:23,479][__main__][INFO] - Train, Round 326: loss=1.5090, accuracy=0.4671, gradient_norm=2.5081, 
[2025-09-12 21:14:23,479][__main__][INFO] - Train, Round 327: loss=1.5030, accuracy=0.4628, gradient_norm=2.5329, 
[2025-09-12 21:14:23,479][__main__][INFO] - Train, Round 328: loss=1.4906, accuracy=0.4698, gradient_norm=2.6093, 
[2025-09-12 21:14:23,479][__main__][INFO] - Train, Round 329: loss=1.4958, accuracy=0.4655, gradient_norm=2.6691, 
[2025-09-12 21:14:23,479][__main__][INFO] - Train, Round 330: loss=1.4987, accuracy=0.4638, gradient_norm=2.8562, 
[2025-09-12 21:14:23,479][__main__][INFO] - Train, Round 331: loss=1.5301, accuracy=0.4557, gradient_norm=3.0185, 
[2025-09-12 21:14:23,479][__main__][INFO] - Train, Round 332: loss=1.5749, accuracy=0.4366, gradient_norm=3.4196, 
[2025-09-12 21:14:23,479][__main__][INFO] - Train, Round 333: loss=1.6444, accuracy=0.4187, gradient_norm=3.5625, 
[2025-09-12 21:14:23,479][__main__][INFO] - Train, Round 334: loss=1.6222, accuracy=0.4069, gradient_norm=3.4264, 
[2025-09-12 21:14:23,479][__main__][INFO] - Train, Round 335: loss=1.6110, accuracy=0.4196, gradient_norm=3.0739, 
[2025-09-12 21:14:23,479][__main__][INFO] - Train, Round 336: loss=1.5338, accuracy=0.4431, gradient_norm=2.8714, 
[2025-09-12 21:14:23,479][__main__][INFO] - Train, Round 337: loss=1.5245, accuracy=0.4541, gradient_norm=2.7937, 
[2025-09-12 21:14:23,479][__main__][INFO] - Train, Round 338: loss=1.5333, accuracy=0.4454, gradient_norm=3.0836, 
[2025-09-12 21:14:23,479][__main__][INFO] - Train, Round 339: loss=1.5681, accuracy=0.4363, gradient_norm=3.2013, 
[2025-09-12 21:14:23,479][__main__][INFO] - Train, Round 340: loss=1.5840, accuracy=0.4274, gradient_norm=3.2787, 
[2025-09-12 21:14:23,479][__main__][INFO] - Train, Round 341: loss=1.5722, accuracy=0.4379, gradient_norm=3.1507, 
[2025-09-12 21:14:23,479][__main__][INFO] - Train, Round 342: loss=1.5512, accuracy=0.4415, gradient_norm=3.0783, 
[2025-09-12 21:14:23,479][__main__][INFO] - Train, Round 343: loss=1.5428, accuracy=0.4513, gradient_norm=2.9902, 
[2025-09-12 21:14:23,479][__main__][INFO] - Train, Round 344: loss=1.5289, accuracy=0.4487, gradient_norm=3.0422, 
[2025-09-12 21:14:23,479][__main__][INFO] - Train, Round 345: loss=1.5416, accuracy=0.4459, gradient_norm=3.1472, 
[2025-09-12 21:14:23,479][__main__][INFO] - Train, Round 346: loss=1.5659, accuracy=0.4446, gradient_norm=3.2533, 
[2025-09-12 21:14:23,479][__main__][INFO] - Train, Round 347: loss=1.5734, accuracy=0.4392, gradient_norm=3.2661, 
[2025-09-12 21:14:23,479][__main__][INFO] - Train, Round 348: loss=1.5824, accuracy=0.4438, gradient_norm=3.2967, 
[2025-09-12 21:14:23,479][__main__][INFO] - Train, Round 349: loss=1.5483, accuracy=0.4476, gradient_norm=3.0978, 
[2025-09-12 21:14:23,479][__main__][INFO] - Train, Round 350: loss=1.5222, accuracy=0.4641, gradient_norm=2.8935, 
[2025-09-12 21:14:23,479][__main__][INFO] - Train, Round 351: loss=1.4815, accuracy=0.4663, gradient_norm=2.8022, 
[2025-09-12 21:14:23,479][__main__][INFO] - Train, Round 352: loss=1.4915, accuracy=0.4706, gradient_norm=2.8627, 
[2025-09-12 21:14:23,479][__main__][INFO] - Train, Round 353: loss=1.4689, accuracy=0.4697, gradient_norm=2.8885, 
[2025-09-12 21:14:23,479][__main__][INFO] - Train, Round 354: loss=1.4828, accuracy=0.4715, gradient_norm=2.9689, 
[2025-09-12 21:14:23,479][__main__][INFO] - Train, Round 355: loss=1.4878, accuracy=0.4579, gradient_norm=3.2076, 
[2025-09-12 21:14:23,479][__main__][INFO] - Train, Round 356: loss=1.5578, accuracy=0.4407, gradient_norm=3.5462, 
[2025-09-12 21:14:23,479][__main__][INFO] - Train, Round 357: loss=1.5718, accuracy=0.4228, gradient_norm=3.4070, 
[2025-09-12 21:14:23,479][__main__][INFO] - Train, Round 358: loss=1.5557, accuracy=0.4358, gradient_norm=3.1558, 
[2025-09-12 21:14:23,479][__main__][INFO] - Train, Round 359: loss=1.4955, accuracy=0.4496, gradient_norm=3.0350, 
[2025-09-12 21:14:23,479][__main__][INFO] - Train, Round 360: loss=1.5105, accuracy=0.4568, gradient_norm=2.9909, 
[2025-09-12 21:14:23,479][__main__][INFO] - Train, Round 361: loss=1.4675, accuracy=0.4642, gradient_norm=2.8640, 
[2025-09-12 21:14:23,479][__main__][INFO] - Train, Round 362: loss=1.4924, accuracy=0.4664, gradient_norm=2.9993, 
[2025-09-12 21:14:23,479][__main__][INFO] - Train, Round 363: loss=1.4695, accuracy=0.4654, gradient_norm=3.0168, 
[2025-09-12 21:14:23,479][__main__][INFO] - Train, Round 364: loss=1.4766, accuracy=0.4696, gradient_norm=3.0722, 
[2025-09-12 21:14:23,479][__main__][INFO] - Train, Round 365: loss=1.4823, accuracy=0.4628, gradient_norm=3.1404, 
[2025-09-12 21:14:23,479][__main__][INFO] - Train, Round 366: loss=1.4907, accuracy=0.4619, gradient_norm=3.2338, 
[2025-09-12 21:14:23,479][__main__][INFO] - Train, Round 367: loss=1.5034, accuracy=0.4561, gradient_norm=3.3542, 
[2025-09-12 21:14:23,479][__main__][INFO] - Train, Round 368: loss=1.5196, accuracy=0.4487, gradient_norm=3.2651, 
[2025-09-12 21:14:23,479][__main__][INFO] - Train, Round 369: loss=1.4961, accuracy=0.4569, gradient_norm=3.3186, 
[2025-09-12 21:14:23,479][__main__][INFO] - Train, Round 370: loss=1.5180, accuracy=0.4556, gradient_norm=3.1803, 
[2025-09-12 21:14:23,479][__main__][INFO] - Train, Round 371: loss=1.4852, accuracy=0.4632, gradient_norm=3.2058, 
[2025-09-12 21:14:23,479][__main__][INFO] - Train, Round 372: loss=1.5195, accuracy=0.4552, gradient_norm=3.3110, 
[2025-09-12 21:14:23,479][__main__][INFO] - Train, Round 373: loss=1.5165, accuracy=0.4472, gradient_norm=3.4788, 
[2025-09-12 21:14:23,479][__main__][INFO] - Train, Round 374: loss=1.5541, accuracy=0.4354, gradient_norm=3.4926, 
[2025-09-12 21:14:23,479][__main__][INFO] - Train, Round 375: loss=1.5178, accuracy=0.4450, gradient_norm=3.2329, 
[2025-09-12 21:14:23,479][__main__][INFO] - Train, Round 376: loss=1.4686, accuracy=0.4713, gradient_norm=2.9185, 
[2025-09-12 21:14:23,479][__main__][INFO] - Train, Round 377: loss=1.4501, accuracy=0.4820, gradient_norm=3.0646, 
[2025-09-12 21:14:23,479][__main__][INFO] - Train, Round 378: loss=1.4979, accuracy=0.4700, gradient_norm=3.4153, 
[2025-09-12 21:14:23,479][__main__][INFO] - Train, Round 379: loss=1.5605, accuracy=0.4512, gradient_norm=3.8602, 
[2025-09-12 21:14:23,479][__main__][INFO] - Train, Round 380: loss=1.6449, accuracy=0.4237, gradient_norm=4.3659, 
[2025-09-12 21:14:23,479][__main__][INFO] - Train, Round 381: loss=1.7447, accuracy=0.4068, gradient_norm=4.0821, 
[2025-09-12 21:14:23,479][__main__][INFO] - Train, Round 382: loss=1.5242, accuracy=0.4632, gradient_norm=2.8912, 
[2025-09-12 21:14:23,479][__main__][INFO] - Train, Round 383: loss=1.4362, accuracy=0.4953, gradient_norm=2.5997, 
[2025-09-12 21:14:23,479][__main__][INFO] - Train, Round 384: loss=1.3870, accuracy=0.5130, gradient_norm=2.5087, 
[2025-09-12 21:14:23,479][__main__][INFO] - Train, Round 385: loss=1.3767, accuracy=0.5083, gradient_norm=2.7302, 
[2025-09-12 21:14:23,479][__main__][INFO] - Train, Round 386: loss=1.3824, accuracy=0.5097, gradient_norm=2.8431, 
[2025-09-12 21:14:23,479][__main__][INFO] - Train, Round 387: loss=1.4249, accuracy=0.4828, gradient_norm=3.3808, 
[2025-09-12 21:14:23,479][__main__][INFO] - Train, Round 388: loss=1.5223, accuracy=0.4506, gradient_norm=3.7443, 
[2025-09-12 21:14:23,479][__main__][INFO] - Train, Round 389: loss=1.5315, accuracy=0.4361, gradient_norm=3.7832, 
[2025-09-12 21:14:23,479][__main__][INFO] - Train, Round 390: loss=1.5301, accuracy=0.4381, gradient_norm=3.4032, 
[2025-09-12 21:14:23,479][__main__][INFO] - Train, Round 391: loss=1.4361, accuracy=0.4695, gradient_norm=2.9495, 
[2025-09-12 21:14:23,479][__main__][INFO] - Train, Round 392: loss=1.4084, accuracy=0.4995, gradient_norm=2.7491, 
[2025-09-12 21:14:23,479][__main__][INFO] - Train, Round 393: loss=1.3783, accuracy=0.5021, gradient_norm=2.8116, 
[2025-09-12 21:14:23,480][__main__][INFO] - Train, Round 394: loss=1.3835, accuracy=0.5043, gradient_norm=2.9851, 
[2025-09-12 21:14:23,480][__main__][INFO] - Train, Round 395: loss=1.4192, accuracy=0.4795, gradient_norm=3.4687, 
[2025-09-12 21:14:23,480][__main__][INFO] - Train, Round 396: loss=1.5289, accuracy=0.4493, gradient_norm=3.7404, 
[2025-09-12 21:14:23,480][__main__][INFO] - Train, Round 397: loss=1.5015, accuracy=0.4478, gradient_norm=3.4663, 
[2025-09-12 21:14:23,480][__main__][INFO] - Train, Round 398: loss=1.4421, accuracy=0.4714, gradient_norm=3.0505, 
[2025-09-12 21:14:23,480][__main__][INFO] - Train, Round 399: loss=1.3956, accuracy=0.4973, gradient_norm=3.0003, 
[2025-09-12 21:14:23,480][__main__][INFO] - Train, Round 400: loss=1.3875, accuracy=0.4930, gradient_norm=3.0596, 
[2025-09-12 21:14:23,480][__main__][INFO] - Train, Round 401: loss=1.4127, accuracy=0.4918, gradient_norm=3.3178, 
[2025-09-12 21:14:23,480][__main__][INFO] - Train, Round 402: loss=1.4438, accuracy=0.4725, gradient_norm=3.4879, 
[2025-09-12 21:14:23,480][__main__][INFO] - Train, Round 403: loss=1.4877, accuracy=0.4733, gradient_norm=3.5611, 
[2025-09-12 21:14:23,480][__main__][INFO] - Train, Round 404: loss=1.4713, accuracy=0.4732, gradient_norm=3.6540, 
[2025-09-12 21:14:23,480][__main__][INFO] - Train, Round 405: loss=1.5096, accuracy=0.4740, gradient_norm=3.7662, 
[2025-09-12 21:14:23,480][__main__][INFO] - Train, Round 406: loss=1.5075, accuracy=0.4644, gradient_norm=3.8262, 
[2025-09-12 21:14:23,480][__main__][INFO] - Train, Round 407: loss=1.5336, accuracy=0.4656, gradient_norm=3.7403, 
[2025-09-12 21:14:23,480][__main__][INFO] - Train, Round 408: loss=1.4551, accuracy=0.4879, gradient_norm=3.3102, 
[2025-09-12 21:14:23,480][__main__][INFO] - Train, Round 409: loss=1.4105, accuracy=0.5058, gradient_norm=3.0828, 
[2025-09-12 21:14:23,480][__main__][INFO] - Train, Round 410: loss=1.3637, accuracy=0.5186, gradient_norm=2.8853, 
[2025-09-12 21:14:23,480][__main__][INFO] - Train, Round 411: loss=1.3423, accuracy=0.5237, gradient_norm=2.9970, 
[2025-09-12 21:14:23,480][__main__][INFO] - Train, Round 412: loss=1.3647, accuracy=0.5142, gradient_norm=3.1579, 
[2025-09-12 21:14:23,480][__main__][INFO] - Train, Round 413: loss=1.4040, accuracy=0.4899, gradient_norm=3.7652, 
[2025-09-12 21:14:23,480][__main__][INFO] - Train, Round 414: loss=1.5270, accuracy=0.4509, gradient_norm=4.0920, 
[2025-09-12 21:14:23,480][__main__][INFO] - Train, Round 415: loss=1.5171, accuracy=0.4468, gradient_norm=3.8280, 
[2025-09-12 21:14:23,480][__main__][INFO] - Train, Round 416: loss=1.4420, accuracy=0.4844, gradient_norm=3.1291, 
[2025-09-12 21:14:23,480][__main__][INFO] - Train, Round 417: loss=1.3469, accuracy=0.5227, gradient_norm=2.7785, 
[2025-09-12 21:14:23,480][__main__][INFO] - Train, Round 418: loss=1.3183, accuracy=0.5334, gradient_norm=2.8682, 
[2025-09-12 21:14:23,480][__main__][INFO] - Train, Round 419: loss=1.3345, accuracy=0.5232, gradient_norm=3.1534, 
[2025-09-12 21:14:23,480][__main__][INFO] - Train, Round 420: loss=1.3801, accuracy=0.5038, gradient_norm=3.4695, 
[2025-09-12 21:14:23,480][__main__][INFO] - Train, Round 421: loss=1.4354, accuracy=0.4814, gradient_norm=3.7637, 
[2025-09-12 21:14:23,480][__main__][INFO] - Train, Round 422: loss=1.4851, accuracy=0.4595, gradient_norm=3.8871, 
[2025-09-12 21:14:23,480][__main__][INFO] - Train, Round 423: loss=1.4828, accuracy=0.4518, gradient_norm=3.9618, 
[2025-09-12 21:14:23,480][__main__][INFO] - Train, Round 424: loss=1.4832, accuracy=0.4687, gradient_norm=3.5167, 
[2025-09-12 21:14:23,480][__main__][INFO] - Train, Round 425: loss=1.3390, accuracy=0.5185, gradient_norm=2.8433, 
[2025-09-12 21:14:23,480][__main__][INFO] - Train, Round 426: loss=1.3108, accuracy=0.5341, gradient_norm=2.8851, 
[2025-09-12 21:14:23,480][__main__][INFO] - Train, Round 427: loss=1.3068, accuracy=0.5267, gradient_norm=3.1517, 
[2025-09-12 21:14:23,480][__main__][INFO] - Train, Round 428: loss=1.3648, accuracy=0.5047, gradient_norm=3.3559, 
[2025-09-12 21:14:23,480][__main__][INFO] - Train, Round 429: loss=1.3834, accuracy=0.4949, gradient_norm=3.7154, 
[2025-09-12 21:14:23,480][__main__][INFO] - Train, Round 430: loss=1.4657, accuracy=0.4619, gradient_norm=3.9603, 
[2025-09-12 21:14:23,480][__main__][INFO] - Train, Round 431: loss=1.4782, accuracy=0.4722, gradient_norm=3.7947, 
[2025-09-12 21:14:23,480][__main__][INFO] - Train, Round 432: loss=1.4196, accuracy=0.4942, gradient_norm=3.4121, 
[2025-09-12 21:14:23,480][__main__][INFO] - Train, Round 433: loss=1.3688, accuracy=0.5165, gradient_norm=3.3338, 
[2025-09-12 21:14:23,480][__main__][INFO] - Train, Round 434: loss=1.3512, accuracy=0.5227, gradient_norm=3.2595, 
[2025-09-12 21:14:23,480][__main__][INFO] - Train, Round 435: loss=1.3618, accuracy=0.5203, gradient_norm=3.4920, 
[2025-09-12 21:14:23,480][__main__][INFO] - Train, Round 436: loss=1.3850, accuracy=0.5119, gradient_norm=3.8812, 
[2025-09-12 21:14:23,480][__main__][INFO] - Train, Round 437: loss=1.5237, accuracy=0.4714, gradient_norm=4.4962, 
[2025-09-12 21:14:23,480][__main__][INFO] - Train, Round 438: loss=1.5978, accuracy=0.4420, gradient_norm=4.5730, 
[2025-09-12 21:14:23,480][__main__][INFO] - Train, Round 439: loss=1.5667, accuracy=0.4551, gradient_norm=3.7961, 
[2025-09-12 21:14:23,480][__main__][INFO] - Train, Round 440: loss=1.3679, accuracy=0.5097, gradient_norm=3.0271, 
[2025-09-12 21:14:23,480][__main__][INFO] - Train, Round 441: loss=1.3247, accuracy=0.5349, gradient_norm=2.9304, 
[2025-09-12 21:14:23,480][__main__][INFO] - Train, Round 442: loss=1.2920, accuracy=0.5342, gradient_norm=3.0479, 
[2025-09-12 21:14:23,480][__main__][INFO] - Train, Round 443: loss=1.2993, accuracy=0.5345, gradient_norm=3.2096, 
[2025-09-12 21:14:23,480][__main__][INFO] - Train, Round 444: loss=1.3346, accuracy=0.5119, gradient_norm=3.6168, 
[2025-09-12 21:14:23,480][__main__][INFO] - Train, Round 445: loss=1.4152, accuracy=0.4838, gradient_norm=3.9412, 
[2025-09-12 21:14:23,480][__main__][INFO] - Train, Round 446: loss=1.4215, accuracy=0.4822, gradient_norm=3.9394, 
[2025-09-12 21:14:23,480][__main__][INFO] - Train, Round 447: loss=1.4090, accuracy=0.4854, gradient_norm=3.5287, 
[2025-09-12 21:14:23,480][__main__][INFO] - Train, Round 448: loss=1.3236, accuracy=0.5229, gradient_norm=3.3952, 
[2025-09-12 21:14:23,480][__main__][INFO] - Train, Round 449: loss=1.3584, accuracy=0.5142, gradient_norm=3.5539, 
[2025-09-12 21:14:23,480][__main__][INFO] - Train, Round 450: loss=1.3696, accuracy=0.5049, gradient_norm=3.8520, 
[2025-09-12 21:14:23,480][__main__][INFO] - Train, Round 451: loss=1.4454, accuracy=0.4810, gradient_norm=4.1477, 
[2025-09-12 21:14:23,480][__main__][INFO] - Train, Round 452: loss=1.4765, accuracy=0.4661, gradient_norm=4.1574, 
[2025-09-12 21:14:23,480][__main__][INFO] - Train, Round 453: loss=1.3944, accuracy=0.5000, gradient_norm=3.3619, 
[2025-09-12 21:14:23,480][__main__][INFO] - Train, Round 454: loss=1.2999, accuracy=0.5331, gradient_norm=3.1398, 
[2025-09-12 21:14:23,480][__main__][INFO] - Train, Round 455: loss=1.2834, accuracy=0.5445, gradient_norm=3.1511, 
[2025-09-12 21:14:23,480][__main__][INFO] - Train, Round 456: loss=1.2845, accuracy=0.5391, gradient_norm=3.3720, 
[2025-09-12 21:14:23,480][__main__][INFO] - Train, Round 457: loss=1.3253, accuracy=0.5327, gradient_norm=3.7557, 
[2025-09-12 21:14:23,480][__main__][INFO] - Train, Round 458: loss=1.3953, accuracy=0.4943, gradient_norm=4.0787, 
[2025-09-12 21:14:23,480][__main__][INFO] - Train, Round 459: loss=1.4427, accuracy=0.5002, gradient_norm=4.0667, 
[2025-09-12 21:14:23,480][__main__][INFO] - Train, Round 460: loss=1.3742, accuracy=0.5110, gradient_norm=3.6710, 
[2025-09-12 21:14:23,480][__main__][INFO] - Train, Round 461: loss=1.3278, accuracy=0.5383, gradient_norm=3.6531, 
[2025-09-12 21:14:23,480][__main__][INFO] - Train, Round 462: loss=1.3300, accuracy=0.5350, gradient_norm=3.6336, 
[2025-09-12 21:14:23,480][__main__][INFO] - Train, Round 463: loss=1.3267, accuracy=0.5365, gradient_norm=3.7297, 
[2025-09-12 21:14:23,480][__main__][INFO] - Train, Round 464: loss=1.3466, accuracy=0.5270, gradient_norm=3.8083, 
[2025-09-12 21:14:23,480][__main__][INFO] - Train, Round 465: loss=1.3621, accuracy=0.5169, gradient_norm=3.8681, 
[2025-09-12 21:14:23,480][__main__][INFO] - Train, Round 466: loss=1.3515, accuracy=0.5181, gradient_norm=3.8243, 
[2025-09-12 21:14:23,480][__main__][INFO] - Train, Round 467: loss=1.3426, accuracy=0.5136, gradient_norm=3.8384, 
[2025-09-12 21:14:23,480][__main__][INFO] - Train, Round 468: loss=1.3387, accuracy=0.5195, gradient_norm=3.6825, 
[2025-09-12 21:14:23,480][__main__][INFO] - Train, Round 469: loss=1.3090, accuracy=0.5215, gradient_norm=3.6297, 
[2025-09-12 21:14:23,480][__main__][INFO] - Train, Round 470: loss=1.2972, accuracy=0.5329, gradient_norm=3.6169, 
[2025-09-12 21:14:23,480][__main__][INFO] - Train, Round 471: loss=1.3010, accuracy=0.5218, gradient_norm=3.6321, 
[2025-09-12 21:14:23,480][__main__][INFO] - Train, Round 472: loss=1.2744, accuracy=0.5402, gradient_norm=3.5239, 
[2025-09-12 21:14:23,480][__main__][INFO] - Train, Round 473: loss=1.2750, accuracy=0.5242, gradient_norm=3.6484, 
[2025-09-12 21:14:23,480][__main__][INFO] - Train, Round 474: loss=1.3061, accuracy=0.5246, gradient_norm=3.7151, 
[2025-09-12 21:14:23,480][__main__][INFO] - Train, Round 475: loss=1.2947, accuracy=0.5241, gradient_norm=3.7419, 
[2025-09-12 21:14:23,480][__main__][INFO] - Train, Round 476: loss=1.3077, accuracy=0.5280, gradient_norm=3.7839, 
[2025-09-12 21:14:23,481][__main__][INFO] - Train, Round 477: loss=1.3287, accuracy=0.5096, gradient_norm=4.1182, 
[2025-09-12 21:14:23,481][__main__][INFO] - Train, Round 478: loss=1.4212, accuracy=0.4885, gradient_norm=4.2006, 
[2025-09-12 21:14:23,481][__main__][INFO] - Train, Round 479: loss=1.3277, accuracy=0.5070, gradient_norm=4.0598, 
[2025-09-12 21:14:23,481][__main__][INFO] - Train, Round 480: loss=1.3732, accuracy=0.5237, gradient_norm=3.6422, 
[2025-09-12 21:14:23,481][__main__][INFO] - Train, Round 481: loss=1.2012, accuracy=0.5691, gradient_norm=3.0682, 
[2025-09-12 21:14:23,481][__main__][INFO] - Train, Round 482: loss=1.1968, accuracy=0.5814, gradient_norm=3.1827, 
[2025-09-12 21:14:23,481][__main__][INFO] - Train, Round 483: loss=1.1955, accuracy=0.5676, gradient_norm=3.5005, 
[2025-09-12 21:14:23,481][__main__][INFO] - Train, Round 484: loss=1.2525, accuracy=0.5431, gradient_norm=3.8777, 
[2025-09-12 21:14:23,481][__main__][INFO] - Train, Round 485: loss=1.3519, accuracy=0.5043, gradient_norm=4.4693, 
[2025-09-12 21:14:23,481][__main__][INFO] - Train, Round 486: loss=1.4234, accuracy=0.4683, gradient_norm=4.3928, 
[2025-09-12 21:14:23,481][__main__][INFO] - Train, Round 487: loss=1.3111, accuracy=0.5252, gradient_norm=3.7054, 
[2025-09-12 21:14:23,481][__main__][INFO] - Train, Round 488: loss=1.2364, accuracy=0.5560, gradient_norm=3.4212, 
[2025-09-12 21:14:23,481][__main__][INFO] - Train, Round 489: loss=1.2292, accuracy=0.5572, gradient_norm=3.6287, 
[2025-09-12 21:14:23,481][__main__][INFO] - Train, Round 490: loss=1.2756, accuracy=0.5303, gradient_norm=3.9892, 
[2025-09-12 21:14:23,481][__main__][INFO] - Train, Round 491: loss=1.3906, accuracy=0.4986, gradient_norm=4.3578, 
[2025-09-12 21:14:23,481][__main__][INFO] - Train, Round 492: loss=1.3889, accuracy=0.4801, gradient_norm=4.5152, 
[2025-09-12 21:14:23,481][__main__][INFO] - Train, Round 493: loss=1.3968, accuracy=0.4912, gradient_norm=3.9894, 
[2025-09-12 21:14:23,481][__main__][INFO] - Train, Round 494: loss=1.2648, accuracy=0.5426, gradient_norm=3.3247, 
[2025-09-12 21:14:23,481][__main__][INFO] - Train, Round 495: loss=1.2018, accuracy=0.5770, gradient_norm=3.1992, 
[2025-09-12 21:14:23,481][__main__][INFO] - Train, Round 496: loss=1.2007, accuracy=0.5672, gradient_norm=3.3965, 
[2025-09-12 21:14:23,481][__main__][INFO] - Train, Round 497: loss=1.2218, accuracy=0.5612, gradient_norm=3.7859, 
[2025-09-12 21:14:23,481][__main__][INFO] - Train, Round 498: loss=1.3390, accuracy=0.5233, gradient_norm=4.4581, 
[2025-09-12 21:14:23,481][__main__][INFO] - Train, Round 499: loss=1.4237, accuracy=0.4928, gradient_norm=4.3519, 
[2025-09-12 21:14:23,481][__main__][INFO] - Train, Round 500: loss=1.2745, accuracy=0.5393, gradient_norm=3.7491, 
[2025-09-12 21:14:23,481][__main__][INFO] - Test, Round 001: loss=2.3056, accuracy=0.1018, 
[2025-09-12 21:14:23,481][__main__][INFO] - Test, Round 002: loss=2.3053, accuracy=0.1032, 
[2025-09-12 21:14:23,481][__main__][INFO] - Test, Round 003: loss=2.3051, accuracy=0.1031, 
[2025-09-12 21:14:23,481][__main__][INFO] - Test, Round 004: loss=2.3049, accuracy=0.1025, 
[2025-09-12 21:14:23,481][__main__][INFO] - Test, Round 005: loss=2.3047, accuracy=0.1032, 
[2025-09-12 21:14:23,481][__main__][INFO] - Test, Round 006: loss=2.3045, accuracy=0.1036, 
[2025-09-12 21:14:23,481][__main__][INFO] - Test, Round 007: loss=2.3043, accuracy=0.1036, 
[2025-09-12 21:14:23,481][__main__][INFO] - Test, Round 008: loss=2.3041, accuracy=0.1040, 
[2025-09-12 21:14:23,481][__main__][INFO] - Test, Round 009: loss=2.3040, accuracy=0.1050, 
[2025-09-12 21:14:23,481][__main__][INFO] - Test, Round 010: loss=2.3038, accuracy=0.1064, 
[2025-09-12 21:14:23,481][__main__][INFO] - Test, Round 011: loss=2.3036, accuracy=0.1054, 
[2025-09-12 21:14:23,481][__main__][INFO] - Test, Round 012: loss=2.3034, accuracy=0.1055, 
[2025-09-12 21:14:23,481][__main__][INFO] - Test, Round 013: loss=2.3032, accuracy=0.1066, 
[2025-09-12 21:14:23,481][__main__][INFO] - Test, Round 014: loss=2.3030, accuracy=0.1095, 
[2025-09-12 21:14:23,481][__main__][INFO] - Test, Round 015: loss=2.3028, accuracy=0.1120, 
[2025-09-12 21:14:23,481][__main__][INFO] - Test, Round 016: loss=2.3026, accuracy=0.1144, 
[2025-09-12 21:14:23,481][__main__][INFO] - Test, Round 017: loss=2.3024, accuracy=0.1197, 
[2025-09-12 21:14:23,481][__main__][INFO] - Test, Round 018: loss=2.3022, accuracy=0.1221, 
[2025-09-12 21:14:23,481][__main__][INFO] - Test, Round 019: loss=2.3020, accuracy=0.1230, 
[2025-09-12 21:14:23,481][__main__][INFO] - Test, Round 020: loss=2.3018, accuracy=0.1263, 
[2025-09-12 21:14:23,481][__main__][INFO] - Test, Round 021: loss=2.3016, accuracy=0.1315, 
[2025-09-12 21:14:23,481][__main__][INFO] - Test, Round 022: loss=2.3014, accuracy=0.1340, 
[2025-09-12 21:14:23,481][__main__][INFO] - Test, Round 023: loss=2.3012, accuracy=0.1370, 
[2025-09-12 21:14:23,481][__main__][INFO] - Test, Round 024: loss=2.3009, accuracy=0.1391, 
[2025-09-12 21:14:23,481][__main__][INFO] - Test, Round 025: loss=2.3007, accuracy=0.1389, 
[2025-09-12 21:14:23,481][__main__][INFO] - Test, Round 026: loss=2.3005, accuracy=0.1433, 
[2025-09-12 21:14:23,481][__main__][INFO] - Test, Round 027: loss=2.3003, accuracy=0.1467, 
[2025-09-12 21:14:23,481][__main__][INFO] - Test, Round 028: loss=2.3000, accuracy=0.1464, 
[2025-09-12 21:14:23,481][__main__][INFO] - Test, Round 029: loss=2.2998, accuracy=0.1466, 
[2025-09-12 21:14:23,481][__main__][INFO] - Test, Round 030: loss=2.2995, accuracy=0.1490, 
[2025-09-12 21:14:23,481][__main__][INFO] - Test, Round 031: loss=2.2992, accuracy=0.1512, 
[2025-09-12 21:14:23,481][__main__][INFO] - Test, Round 032: loss=2.2990, accuracy=0.1505, 
[2025-09-12 21:14:23,481][__main__][INFO] - Test, Round 033: loss=2.2987, accuracy=0.1531, 
[2025-09-12 21:14:23,481][__main__][INFO] - Test, Round 034: loss=2.2985, accuracy=0.1523, 
[2025-09-12 21:14:23,481][__main__][INFO] - Test, Round 035: loss=2.2982, accuracy=0.1535, 
[2025-09-12 21:14:23,481][__main__][INFO] - Test, Round 036: loss=2.2979, accuracy=0.1551, 
[2025-09-12 21:14:23,481][__main__][INFO] - Test, Round 037: loss=2.2976, accuracy=0.1558, 
[2025-09-12 21:14:23,481][__main__][INFO] - Test, Round 038: loss=2.2972, accuracy=0.1570, 
[2025-09-12 21:14:23,481][__main__][INFO] - Test, Round 039: loss=2.2969, accuracy=0.1589, 
[2025-09-12 21:14:23,481][__main__][INFO] - Test, Round 040: loss=2.2965, accuracy=0.1595, 
[2025-09-12 21:14:23,481][__main__][INFO] - Test, Round 041: loss=2.2962, accuracy=0.1606, 
[2025-09-12 21:14:23,481][__main__][INFO] - Test, Round 042: loss=2.2958, accuracy=0.1597, 
[2025-09-12 21:14:23,481][__main__][INFO] - Test, Round 043: loss=2.2954, accuracy=0.1603, 
[2025-09-12 21:14:23,481][__main__][INFO] - Test, Round 044: loss=2.2949, accuracy=0.1620, 
[2025-09-12 21:14:23,481][__main__][INFO] - Test, Round 045: loss=2.2945, accuracy=0.1633, 
[2025-09-12 21:14:23,481][__main__][INFO] - Test, Round 046: loss=2.2940, accuracy=0.1630, 
[2025-09-12 21:14:23,481][__main__][INFO] - Test, Round 047: loss=2.2935, accuracy=0.1631, 
[2025-09-12 21:14:23,481][__main__][INFO] - Test, Round 048: loss=2.2930, accuracy=0.1617, 
[2025-09-12 21:14:23,481][__main__][INFO] - Test, Round 049: loss=2.2924, accuracy=0.1622, 
[2025-09-12 21:14:23,481][__main__][INFO] - Test, Round 050: loss=2.2919, accuracy=0.1629, 
[2025-09-12 21:14:23,481][__main__][INFO] - Test, Round 051: loss=2.2913, accuracy=0.1642, 
[2025-09-12 21:14:23,481][__main__][INFO] - Test, Round 052: loss=2.2906, accuracy=0.1659, 
[2025-09-12 21:14:23,481][__main__][INFO] - Test, Round 053: loss=2.2899, accuracy=0.1675, 
[2025-09-12 21:14:23,481][__main__][INFO] - Test, Round 054: loss=2.2892, accuracy=0.1709, 
[2025-09-12 21:14:23,481][__main__][INFO] - Test, Round 055: loss=2.2884, accuracy=0.1709, 
[2025-09-12 21:14:23,481][__main__][INFO] - Test, Round 056: loss=2.2876, accuracy=0.1707, 
[2025-09-12 21:14:23,481][__main__][INFO] - Test, Round 057: loss=2.2867, accuracy=0.1720, 
[2025-09-12 21:14:23,481][__main__][INFO] - Test, Round 058: loss=2.2858, accuracy=0.1735, 
[2025-09-12 21:14:23,481][__main__][INFO] - Test, Round 059: loss=2.2848, accuracy=0.1766, 
[2025-09-12 21:14:23,481][__main__][INFO] - Test, Round 060: loss=2.2837, accuracy=0.1777, 
[2025-09-12 21:14:23,482][__main__][INFO] - Test, Round 061: loss=2.2826, accuracy=0.1780, 
[2025-09-12 21:14:23,482][__main__][INFO] - Test, Round 062: loss=2.2813, accuracy=0.1780, 
[2025-09-12 21:14:23,482][__main__][INFO] - Test, Round 063: loss=2.2800, accuracy=0.1754, 
[2025-09-12 21:14:23,482][__main__][INFO] - Test, Round 064: loss=2.2785, accuracy=0.1753, 
[2025-09-12 21:14:23,482][__main__][INFO] - Test, Round 065: loss=2.2769, accuracy=0.1753, 
[2025-09-12 21:14:23,482][__main__][INFO] - Test, Round 066: loss=2.2752, accuracy=0.1724, 
[2025-09-12 21:14:23,482][__main__][INFO] - Test, Round 067: loss=2.2734, accuracy=0.1744, 
[2025-09-12 21:14:23,482][__main__][INFO] - Test, Round 068: loss=2.2713, accuracy=0.1750, 
[2025-09-12 21:14:23,482][__main__][INFO] - Test, Round 069: loss=2.2692, accuracy=0.1757, 
[2025-09-12 21:14:23,482][__main__][INFO] - Test, Round 070: loss=2.2669, accuracy=0.1751, 
[2025-09-12 21:14:23,482][__main__][INFO] - Test, Round 071: loss=2.2644, accuracy=0.1768, 
[2025-09-12 21:14:23,482][__main__][INFO] - Test, Round 072: loss=2.2617, accuracy=0.1764, 
[2025-09-12 21:14:23,482][__main__][INFO] - Test, Round 073: loss=2.2588, accuracy=0.1813, 
[2025-09-12 21:14:23,482][__main__][INFO] - Test, Round 074: loss=2.2554, accuracy=0.1803, 
[2025-09-12 21:14:23,482][__main__][INFO] - Test, Round 075: loss=2.2518, accuracy=0.1798, 
[2025-09-12 21:14:23,482][__main__][INFO] - Test, Round 076: loss=2.2482, accuracy=0.1818, 
[2025-09-12 21:14:23,482][__main__][INFO] - Test, Round 077: loss=2.2443, accuracy=0.1834, 
[2025-09-12 21:14:23,482][__main__][INFO] - Test, Round 078: loss=2.2400, accuracy=0.1848, 
[2025-09-12 21:14:23,482][__main__][INFO] - Test, Round 079: loss=2.2353, accuracy=0.1870, 
[2025-09-12 21:14:23,482][__main__][INFO] - Test, Round 080: loss=2.2302, accuracy=0.1891, 
[2025-09-12 21:14:23,482][__main__][INFO] - Test, Round 081: loss=2.2247, accuracy=0.1893, 
[2025-09-12 21:14:23,482][__main__][INFO] - Test, Round 082: loss=2.2193, accuracy=0.1889, 
[2025-09-12 21:14:23,482][__main__][INFO] - Test, Round 083: loss=2.2136, accuracy=0.1889, 
[2025-09-12 21:14:23,482][__main__][INFO] - Test, Round 084: loss=2.2071, accuracy=0.1891, 
[2025-09-12 21:14:23,482][__main__][INFO] - Test, Round 085: loss=2.2009, accuracy=0.1868, 
[2025-09-12 21:14:23,482][__main__][INFO] - Test, Round 086: loss=2.1944, accuracy=0.1944, 
[2025-09-12 21:14:23,482][__main__][INFO] - Test, Round 087: loss=2.1873, accuracy=0.1953, 
[2025-09-12 21:14:23,482][__main__][INFO] - Test, Round 088: loss=2.1806, accuracy=0.1975, 
[2025-09-12 21:14:23,482][__main__][INFO] - Test, Round 089: loss=2.1740, accuracy=0.1929, 
[2025-09-12 21:14:23,482][__main__][INFO] - Test, Round 090: loss=2.1665, accuracy=0.1993, 
[2025-09-12 21:14:23,482][__main__][INFO] - Test, Round 091: loss=2.1594, accuracy=0.1981, 
[2025-09-12 21:14:23,482][__main__][INFO] - Test, Round 092: loss=2.1526, accuracy=0.2006, 
[2025-09-12 21:14:23,482][__main__][INFO] - Test, Round 093: loss=2.1463, accuracy=0.2044, 
[2025-09-12 21:14:23,482][__main__][INFO] - Test, Round 094: loss=2.1393, accuracy=0.2093, 
[2025-09-12 21:14:23,482][__main__][INFO] - Test, Round 095: loss=2.1344, accuracy=0.2080, 
[2025-09-12 21:14:23,482][__main__][INFO] - Test, Round 096: loss=2.1276, accuracy=0.2134, 
[2025-09-12 21:14:23,482][__main__][INFO] - Test, Round 097: loss=2.1212, accuracy=0.2193, 
[2025-09-12 21:14:23,482][__main__][INFO] - Test, Round 098: loss=2.1143, accuracy=0.2227, 
[2025-09-12 21:14:23,482][__main__][INFO] - Test, Round 099: loss=2.1079, accuracy=0.2271, 
[2025-09-12 21:14:23,482][__main__][INFO] - Test, Round 100: loss=2.1033, accuracy=0.2318, 
[2025-09-12 21:14:23,482][__main__][INFO] - Test, Round 101: loss=2.0978, accuracy=0.2353, 
[2025-09-12 21:14:23,482][__main__][INFO] - Test, Round 102: loss=2.0915, accuracy=0.2404, 
[2025-09-12 21:14:23,482][__main__][INFO] - Test, Round 103: loss=2.0881, accuracy=0.2414, 
[2025-09-12 21:14:23,482][__main__][INFO] - Test, Round 104: loss=2.0937, accuracy=0.2450, 
[2025-09-12 21:14:23,482][__main__][INFO] - Test, Round 105: loss=2.1581, accuracy=0.2221, 
[2025-09-12 21:14:23,482][__main__][INFO] - Test, Round 106: loss=2.4051, accuracy=0.1638, 
[2025-09-12 21:14:23,482][__main__][INFO] - Test, Round 107: loss=2.4964, accuracy=0.1654, 
[2025-09-12 21:14:23,482][__main__][INFO] - Test, Round 108: loss=2.2797, accuracy=0.1929, 
[2025-09-12 21:14:23,482][__main__][INFO] - Test, Round 109: loss=2.1519, accuracy=0.2599, 
[2025-09-12 21:14:23,482][__main__][INFO] - Test, Round 110: loss=2.1188, accuracy=0.2782, 
[2025-09-12 21:14:23,482][__main__][INFO] - Test, Round 111: loss=2.0996, accuracy=0.2810, 
[2025-09-12 21:14:23,482][__main__][INFO] - Test, Round 112: loss=2.0846, accuracy=0.2752, 
[2025-09-12 21:14:23,482][__main__][INFO] - Test, Round 113: loss=2.0727, accuracy=0.2731, 
[2025-09-12 21:14:23,482][__main__][INFO] - Test, Round 114: loss=2.0650, accuracy=0.2714, 
[2025-09-12 21:14:23,482][__main__][INFO] - Test, Round 115: loss=2.0595, accuracy=0.2699, 
[2025-09-12 21:14:23,482][__main__][INFO] - Test, Round 116: loss=2.0568, accuracy=0.2769, 
[2025-09-12 21:14:23,482][__main__][INFO] - Test, Round 117: loss=2.0678, accuracy=0.2643, 
[2025-09-12 21:14:23,482][__main__][INFO] - Test, Round 118: loss=2.0884, accuracy=0.2608, 
[2025-09-12 21:14:23,482][__main__][INFO] - Test, Round 119: loss=2.1651, accuracy=0.2307, 
[2025-09-12 21:14:23,482][__main__][INFO] - Test, Round 120: loss=2.1792, accuracy=0.2349, 
[2025-09-12 21:14:23,482][__main__][INFO] - Test, Round 121: loss=2.1957, accuracy=0.2214, 
[2025-09-12 21:14:23,482][__main__][INFO] - Test, Round 122: loss=2.0713, accuracy=0.2743, 
[2025-09-12 21:14:23,482][__main__][INFO] - Test, Round 123: loss=2.0495, accuracy=0.2804, 
[2025-09-12 21:14:23,482][__main__][INFO] - Test, Round 124: loss=2.0348, accuracy=0.2884, 
[2025-09-12 21:14:23,482][__main__][INFO] - Test, Round 125: loss=2.0281, accuracy=0.2920, 
[2025-09-12 21:14:23,482][__main__][INFO] - Test, Round 126: loss=2.0227, accuracy=0.2896, 
[2025-09-12 21:14:23,482][__main__][INFO] - Test, Round 127: loss=2.0356, accuracy=0.2783, 
[2025-09-12 21:14:23,482][__main__][INFO] - Test, Round 128: loss=2.0729, accuracy=0.2709, 
[2025-09-12 21:14:23,482][__main__][INFO] - Test, Round 129: loss=2.1480, accuracy=0.2377, 
[2025-09-12 21:14:23,482][__main__][INFO] - Test, Round 130: loss=2.1418, accuracy=0.2422, 
[2025-09-12 21:14:23,482][__main__][INFO] - Test, Round 131: loss=2.1752, accuracy=0.2337, 
[2025-09-12 21:14:23,482][__main__][INFO] - Test, Round 132: loss=2.0317, accuracy=0.2872, 
[2025-09-12 21:14:23,482][__main__][INFO] - Test, Round 133: loss=2.0057, accuracy=0.3002, 
[2025-09-12 21:14:23,482][__main__][INFO] - Test, Round 134: loss=1.9898, accuracy=0.3045, 
[2025-09-12 21:14:23,482][__main__][INFO] - Test, Round 135: loss=1.9849, accuracy=0.3086, 
[2025-09-12 21:14:23,482][__main__][INFO] - Test, Round 136: loss=1.9868, accuracy=0.3013, 
[2025-09-12 21:14:23,482][__main__][INFO] - Test, Round 137: loss=1.9915, accuracy=0.3006, 
[2025-09-12 21:14:23,482][__main__][INFO] - Test, Round 138: loss=2.0133, accuracy=0.2923, 
[2025-09-12 21:14:23,482][__main__][INFO] - Test, Round 139: loss=2.0823, accuracy=0.2640, 
[2025-09-12 21:14:23,482][__main__][INFO] - Test, Round 140: loss=2.0971, accuracy=0.2571, 
[2025-09-12 21:14:23,482][__main__][INFO] - Test, Round 141: loss=2.1045, accuracy=0.2583, 
[2025-09-12 21:14:23,482][__main__][INFO] - Test, Round 142: loss=2.0066, accuracy=0.2866, 
[2025-09-12 21:14:23,482][__main__][INFO] - Test, Round 143: loss=1.9823, accuracy=0.3008, 
[2025-09-12 21:14:23,482][__main__][INFO] - Test, Round 144: loss=1.9657, accuracy=0.3071, 
[2025-09-12 21:14:23,483][__main__][INFO] - Test, Round 145: loss=1.9632, accuracy=0.3054, 
[2025-09-12 21:14:23,483][__main__][INFO] - Test, Round 146: loss=1.9745, accuracy=0.2950, 
[2025-09-12 21:14:23,483][__main__][INFO] - Test, Round 147: loss=1.9994, accuracy=0.2914, 
[2025-09-12 21:14:23,483][__main__][INFO] - Test, Round 148: loss=1.9961, accuracy=0.2870, 
[2025-09-12 21:14:23,483][__main__][INFO] - Test, Round 149: loss=2.0087, accuracy=0.2861, 
[2025-09-12 21:14:23,483][__main__][INFO] - Test, Round 150: loss=2.0034, accuracy=0.2845, 
[2025-09-12 21:14:23,483][__main__][INFO] - Test, Round 151: loss=2.0366, accuracy=0.2828, 
[2025-09-12 21:14:23,483][__main__][INFO] - Test, Round 152: loss=1.9667, accuracy=0.2977, 
[2025-09-12 21:14:23,483][__main__][INFO] - Test, Round 153: loss=1.9643, accuracy=0.3068, 
[2025-09-12 21:14:23,483][__main__][INFO] - Test, Round 154: loss=1.9434, accuracy=0.3092, 
[2025-09-12 21:14:23,483][__main__][INFO] - Test, Round 155: loss=1.9408, accuracy=0.3164, 
[2025-09-12 21:14:23,483][__main__][INFO] - Test, Round 156: loss=1.9306, accuracy=0.3103, 
[2025-09-12 21:14:23,483][__main__][INFO] - Test, Round 157: loss=1.9553, accuracy=0.3098, 
[2025-09-12 21:14:23,483][__main__][INFO] - Test, Round 158: loss=1.9488, accuracy=0.2988, 
[2025-09-12 21:14:23,483][__main__][INFO] - Test, Round 159: loss=1.9704, accuracy=0.2988, 
[2025-09-12 21:14:23,483][__main__][INFO] - Test, Round 160: loss=1.9456, accuracy=0.3051, 
[2025-09-12 21:14:23,483][__main__][INFO] - Test, Round 161: loss=1.9531, accuracy=0.3091, 
[2025-09-12 21:14:23,483][__main__][INFO] - Test, Round 162: loss=1.9250, accuracy=0.3088, 
[2025-09-12 21:14:23,483][__main__][INFO] - Test, Round 163: loss=1.9476, accuracy=0.3025, 
[2025-09-12 21:14:23,483][__main__][INFO] - Test, Round 164: loss=1.9362, accuracy=0.3043, 
[2025-09-12 21:14:23,483][__main__][INFO] - Test, Round 165: loss=1.9659, accuracy=0.2886, 
[2025-09-12 21:14:23,483][__main__][INFO] - Test, Round 166: loss=1.9631, accuracy=0.2949, 
[2025-09-12 21:14:23,483][__main__][INFO] - Test, Round 167: loss=1.9455, accuracy=0.2975, 
[2025-09-12 21:14:23,483][__main__][INFO] - Test, Round 168: loss=1.8963, accuracy=0.3172, 
[2025-09-12 21:14:23,483][__main__][INFO] - Test, Round 169: loss=1.8905, accuracy=0.3310, 
[2025-09-12 21:14:23,483][__main__][INFO] - Test, Round 170: loss=1.8786, accuracy=0.3295, 
[2025-09-12 21:14:23,483][__main__][INFO] - Test, Round 171: loss=1.8809, accuracy=0.3286, 
[2025-09-12 21:14:23,483][__main__][INFO] - Test, Round 172: loss=1.8853, accuracy=0.3304, 
[2025-09-12 21:14:23,483][__main__][INFO] - Test, Round 173: loss=1.9255, accuracy=0.3191, 
[2025-09-12 21:14:23,483][__main__][INFO] - Test, Round 174: loss=1.9466, accuracy=0.3109, 
[2025-09-12 21:14:23,483][__main__][INFO] - Test, Round 175: loss=2.0085, accuracy=0.2923, 
[2025-09-12 21:14:23,483][__main__][INFO] - Test, Round 176: loss=2.0109, accuracy=0.2807, 
[2025-09-12 21:14:23,483][__main__][INFO] - Test, Round 177: loss=2.0049, accuracy=0.2876, 
[2025-09-12 21:14:23,483][__main__][INFO] - Test, Round 178: loss=1.9227, accuracy=0.3055, 
[2025-09-12 21:14:23,483][__main__][INFO] - Test, Round 179: loss=1.8876, accuracy=0.3301, 
[2025-09-12 21:14:23,483][__main__][INFO] - Test, Round 180: loss=1.8698, accuracy=0.3347, 
[2025-09-12 21:14:23,483][__main__][INFO] - Test, Round 181: loss=1.8579, accuracy=0.3415, 
[2025-09-12 21:14:23,483][__main__][INFO] - Test, Round 182: loss=1.8585, accuracy=0.3414, 
[2025-09-12 21:14:23,483][__main__][INFO] - Test, Round 183: loss=1.8630, accuracy=0.3356, 
[2025-09-12 21:14:23,483][__main__][INFO] - Test, Round 184: loss=1.8917, accuracy=0.3130, 
[2025-09-12 21:14:23,483][__main__][INFO] - Test, Round 185: loss=1.9227, accuracy=0.3076, 
[2025-09-12 21:14:23,483][__main__][INFO] - Test, Round 186: loss=1.9323, accuracy=0.2948, 
[2025-09-12 21:14:23,483][__main__][INFO] - Test, Round 187: loss=1.9113, accuracy=0.3130, 
[2025-09-12 21:14:23,483][__main__][INFO] - Test, Round 188: loss=1.8998, accuracy=0.3066, 
[2025-09-12 21:14:23,483][__main__][INFO] - Test, Round 189: loss=1.8746, accuracy=0.3224, 
[2025-09-12 21:14:23,483][__main__][INFO] - Test, Round 190: loss=1.8767, accuracy=0.3217, 
[2025-09-12 21:14:23,483][__main__][INFO] - Test, Round 191: loss=1.8639, accuracy=0.3292, 
[2025-09-12 21:14:23,483][__main__][INFO] - Test, Round 192: loss=1.8724, accuracy=0.3209, 
[2025-09-12 21:14:23,483][__main__][INFO] - Test, Round 193: loss=1.8515, accuracy=0.3368, 
[2025-09-12 21:14:23,483][__main__][INFO] - Test, Round 194: loss=1.8689, accuracy=0.3267, 
[2025-09-12 21:14:23,483][__main__][INFO] - Test, Round 195: loss=1.8628, accuracy=0.3351, 
[2025-09-12 21:14:23,483][__main__][INFO] - Test, Round 196: loss=1.8750, accuracy=0.3325, 
[2025-09-12 21:14:23,483][__main__][INFO] - Test, Round 197: loss=1.9098, accuracy=0.3287, 
[2025-09-12 21:14:23,483][__main__][INFO] - Test, Round 198: loss=1.9150, accuracy=0.3250, 
[2025-09-12 21:14:23,483][__main__][INFO] - Test, Round 199: loss=1.9705, accuracy=0.3029, 
[2025-09-12 21:14:23,483][__main__][INFO] - Test, Round 200: loss=1.9506, accuracy=0.3075, 
[2025-09-12 21:14:23,483][__main__][INFO] - Test, Round 201: loss=1.9549, accuracy=0.2932, 
[2025-09-12 21:14:23,483][__main__][INFO] - Test, Round 202: loss=1.8616, accuracy=0.3321, 
[2025-09-12 21:14:23,483][__main__][INFO] - Test, Round 203: loss=1.8359, accuracy=0.3465, 
[2025-09-12 21:14:23,483][__main__][INFO] - Test, Round 204: loss=1.8209, accuracy=0.3503, 
[2025-09-12 21:14:23,483][__main__][INFO] - Test, Round 205: loss=1.8281, accuracy=0.3399, 
[2025-09-12 21:14:23,483][__main__][INFO] - Test, Round 206: loss=1.8319, accuracy=0.3428, 
[2025-09-12 21:14:23,483][__main__][INFO] - Test, Round 207: loss=1.8477, accuracy=0.3324, 
[2025-09-12 21:14:23,483][__main__][INFO] - Test, Round 208: loss=1.8505, accuracy=0.3404, 
[2025-09-12 21:14:23,483][__main__][INFO] - Test, Round 209: loss=1.8625, accuracy=0.3283, 
[2025-09-12 21:14:23,483][__main__][INFO] - Test, Round 210: loss=1.8441, accuracy=0.3386, 
[2025-09-12 21:14:23,483][__main__][INFO] - Test, Round 211: loss=1.8527, accuracy=0.3355, 
[2025-09-12 21:14:23,483][__main__][INFO] - Test, Round 212: loss=1.8435, accuracy=0.3413, 
[2025-09-12 21:14:23,483][__main__][INFO] - Test, Round 213: loss=1.8405, accuracy=0.3399, 
[2025-09-12 21:14:23,483][__main__][INFO] - Test, Round 214: loss=1.8339, accuracy=0.3482, 
[2025-09-12 21:14:23,483][__main__][INFO] - Test, Round 215: loss=1.8304, accuracy=0.3468, 
[2025-09-12 21:14:23,483][__main__][INFO] - Test, Round 216: loss=1.8229, accuracy=0.3499, 
[2025-09-12 21:14:23,483][__main__][INFO] - Test, Round 217: loss=1.8227, accuracy=0.3533, 
[2025-09-12 21:14:23,483][__main__][INFO] - Test, Round 218: loss=1.8531, accuracy=0.3432, 
[2025-09-12 21:14:23,483][__main__][INFO] - Test, Round 219: loss=1.9247, accuracy=0.3246, 
[2025-09-12 21:14:23,483][__main__][INFO] - Test, Round 220: loss=1.9844, accuracy=0.3014, 
[2025-09-12 21:14:23,483][__main__][INFO] - Test, Round 221: loss=1.9760, accuracy=0.2987, 
[2025-09-12 21:14:23,483][__main__][INFO] - Test, Round 222: loss=1.8342, accuracy=0.3481, 
[2025-09-12 21:14:23,483][__main__][INFO] - Test, Round 223: loss=1.7908, accuracy=0.3674, 
[2025-09-12 21:14:23,483][__main__][INFO] - Test, Round 224: loss=1.7912, accuracy=0.3623, 
[2025-09-12 21:14:23,483][__main__][INFO] - Test, Round 225: loss=1.7951, accuracy=0.3570, 
[2025-09-12 21:14:23,484][__main__][INFO] - Test, Round 226: loss=1.8228, accuracy=0.3479, 
[2025-09-12 21:14:23,484][__main__][INFO] - Test, Round 227: loss=1.8263, accuracy=0.3450, 
[2025-09-12 21:14:23,484][__main__][INFO] - Test, Round 228: loss=1.8776, accuracy=0.3315, 
[2025-09-12 21:14:23,484][__main__][INFO] - Test, Round 229: loss=1.8382, accuracy=0.3406, 
[2025-09-12 21:14:23,484][__main__][INFO] - Test, Round 230: loss=1.8456, accuracy=0.3329, 
[2025-09-12 21:14:23,484][__main__][INFO] - Test, Round 231: loss=1.8355, accuracy=0.3348, 
[2025-09-12 21:14:23,484][__main__][INFO] - Test, Round 232: loss=1.8331, accuracy=0.3318, 
[2025-09-12 21:14:23,484][__main__][INFO] - Test, Round 233: loss=1.8048, accuracy=0.3456, 
[2025-09-12 21:14:23,484][__main__][INFO] - Test, Round 234: loss=1.8034, accuracy=0.3485, 
[2025-09-12 21:14:23,484][__main__][INFO] - Test, Round 235: loss=1.8087, accuracy=0.3452, 
[2025-09-12 21:14:23,484][__main__][INFO] - Test, Round 236: loss=1.8087, accuracy=0.3471, 
[2025-09-12 21:14:23,484][__main__][INFO] - Test, Round 237: loss=1.7877, accuracy=0.3561, 
[2025-09-12 21:14:23,484][__main__][INFO] - Test, Round 238: loss=1.8118, accuracy=0.3519, 
[2025-09-12 21:14:23,484][__main__][INFO] - Test, Round 239: loss=1.7950, accuracy=0.3596, 
[2025-09-12 21:14:23,484][__main__][INFO] - Test, Round 240: loss=1.8311, accuracy=0.3515, 
[2025-09-12 21:14:23,484][__main__][INFO] - Test, Round 241: loss=1.8314, accuracy=0.3493, 
[2025-09-12 21:14:23,484][__main__][INFO] - Test, Round 242: loss=1.8456, accuracy=0.3501, 
[2025-09-12 21:14:23,484][__main__][INFO] - Test, Round 243: loss=1.8295, accuracy=0.3544, 
[2025-09-12 21:14:23,484][__main__][INFO] - Test, Round 244: loss=1.8716, accuracy=0.3434, 
[2025-09-12 21:14:23,484][__main__][INFO] - Test, Round 245: loss=1.8574, accuracy=0.3495, 
[2025-09-12 21:14:23,484][__main__][INFO] - Test, Round 246: loss=1.8618, accuracy=0.3453, 
[2025-09-12 21:14:23,484][__main__][INFO] - Test, Round 247: loss=1.8196, accuracy=0.3592, 
[2025-09-12 21:14:23,484][__main__][INFO] - Test, Round 248: loss=1.7816, accuracy=0.3663, 
[2025-09-12 21:14:23,484][__main__][INFO] - Test, Round 249: loss=1.7602, accuracy=0.3751, 
[2025-09-12 21:14:23,484][__main__][INFO] - Test, Round 250: loss=1.7425, accuracy=0.3749, 
[2025-09-12 21:14:23,484][__main__][INFO] - Test, Round 251: loss=1.7440, accuracy=0.3779, 
[2025-09-12 21:14:23,484][__main__][INFO] - Test, Round 252: loss=1.7537, accuracy=0.3749, 
[2025-09-12 21:14:23,484][__main__][INFO] - Test, Round 253: loss=1.7885, accuracy=0.3610, 
[2025-09-12 21:14:23,484][__main__][INFO] - Test, Round 254: loss=1.8005, accuracy=0.3567, 
[2025-09-12 21:14:23,484][__main__][INFO] - Test, Round 255: loss=1.8195, accuracy=0.3435, 
[2025-09-12 21:14:23,484][__main__][INFO] - Test, Round 256: loss=1.8548, accuracy=0.3391, 
[2025-09-12 21:14:23,484][__main__][INFO] - Test, Round 257: loss=1.8167, accuracy=0.3432, 
[2025-09-12 21:14:23,484][__main__][INFO] - Test, Round 258: loss=1.7894, accuracy=0.3537, 
[2025-09-12 21:14:23,484][__main__][INFO] - Test, Round 259: loss=1.7889, accuracy=0.3554, 
[2025-09-12 21:14:23,484][__main__][INFO] - Test, Round 260: loss=1.7911, accuracy=0.3472, 
[2025-09-12 21:14:23,484][__main__][INFO] - Test, Round 261: loss=1.8016, accuracy=0.3505, 
[2025-09-12 21:14:23,484][__main__][INFO] - Test, Round 262: loss=1.7938, accuracy=0.3499, 
[2025-09-12 21:14:23,484][__main__][INFO] - Test, Round 263: loss=1.7588, accuracy=0.3709, 
[2025-09-12 21:14:23,484][__main__][INFO] - Test, Round 264: loss=1.7544, accuracy=0.3681, 
[2025-09-12 21:14:23,484][__main__][INFO] - Test, Round 265: loss=1.7944, accuracy=0.3647, 
[2025-09-12 21:14:23,484][__main__][INFO] - Test, Round 266: loss=1.7745, accuracy=0.3625, 
[2025-09-12 21:14:23,484][__main__][INFO] - Test, Round 267: loss=1.8244, accuracy=0.3593, 
[2025-09-12 21:14:23,484][__main__][INFO] - Test, Round 268: loss=1.8177, accuracy=0.3570, 
[2025-09-12 21:14:23,484][__main__][INFO] - Test, Round 269: loss=1.8176, accuracy=0.3605, 
[2025-09-12 21:14:23,484][__main__][INFO] - Test, Round 270: loss=1.7880, accuracy=0.3645, 
[2025-09-12 21:14:23,484][__main__][INFO] - Test, Round 271: loss=1.7677, accuracy=0.3740, 
[2025-09-12 21:14:23,484][__main__][INFO] - Test, Round 272: loss=1.7582, accuracy=0.3762, 
[2025-09-12 21:14:23,484][__main__][INFO] - Test, Round 273: loss=1.7657, accuracy=0.3736, 
[2025-09-12 21:14:23,484][__main__][INFO] - Test, Round 274: loss=1.7530, accuracy=0.3781, 
[2025-09-12 21:14:23,484][__main__][INFO] - Test, Round 275: loss=1.7685, accuracy=0.3716, 
[2025-09-12 21:14:23,484][__main__][INFO] - Test, Round 276: loss=1.7733, accuracy=0.3737, 
[2025-09-12 21:14:23,484][__main__][INFO] - Test, Round 277: loss=1.7789, accuracy=0.3635, 
[2025-09-12 21:14:23,484][__main__][INFO] - Test, Round 278: loss=1.7515, accuracy=0.3772, 
[2025-09-12 21:14:23,484][__main__][INFO] - Test, Round 279: loss=1.7700, accuracy=0.3701, 
[2025-09-12 21:14:23,484][__main__][INFO] - Test, Round 280: loss=1.7790, accuracy=0.3716, 
[2025-09-12 21:14:23,484][__main__][INFO] - Test, Round 281: loss=1.7581, accuracy=0.3750, 
[2025-09-12 21:14:23,484][__main__][INFO] - Test, Round 282: loss=1.7296, accuracy=0.3850, 
[2025-09-12 21:14:23,484][__main__][INFO] - Test, Round 283: loss=1.7311, accuracy=0.3889, 
[2025-09-12 21:14:23,484][__main__][INFO] - Test, Round 284: loss=1.7281, accuracy=0.3878, 
[2025-09-12 21:14:23,484][__main__][INFO] - Test, Round 285: loss=1.7579, accuracy=0.3849, 
[2025-09-12 21:14:23,484][__main__][INFO] - Test, Round 286: loss=1.7877, accuracy=0.3651, 
[2025-09-12 21:14:23,484][__main__][INFO] - Test, Round 287: loss=1.8180, accuracy=0.3645, 
[2025-09-12 21:14:23,484][__main__][INFO] - Test, Round 288: loss=1.8067, accuracy=0.3556, 
[2025-09-12 21:14:23,484][__main__][INFO] - Test, Round 289: loss=1.7811, accuracy=0.3686, 
[2025-09-12 21:14:23,484][__main__][INFO] - Test, Round 290: loss=1.7495, accuracy=0.3654, 
[2025-09-12 21:14:23,484][__main__][INFO] - Test, Round 291: loss=1.7103, accuracy=0.3957, 
[2025-09-12 21:14:23,484][__main__][INFO] - Test, Round 292: loss=1.7165, accuracy=0.3846, 
[2025-09-12 21:14:23,484][__main__][INFO] - Test, Round 293: loss=1.7286, accuracy=0.3792, 
[2025-09-12 21:14:23,484][__main__][INFO] - Test, Round 294: loss=1.8290, accuracy=0.3470, 
[2025-09-12 21:14:23,484][__main__][INFO] - Test, Round 295: loss=1.9778, accuracy=0.3305, 
[2025-09-12 21:14:23,484][__main__][INFO] - Test, Round 296: loss=1.9527, accuracy=0.2909, 
[2025-09-12 21:14:23,484][__main__][INFO] - Test, Round 297: loss=1.8458, accuracy=0.3415, 
[2025-09-12 21:14:23,484][__main__][INFO] - Test, Round 298: loss=1.7418, accuracy=0.3731, 
[2025-09-12 21:14:23,484][__main__][INFO] - Test, Round 299: loss=1.7086, accuracy=0.3809, 
[2025-09-12 21:14:23,484][__main__][INFO] - Test, Round 300: loss=1.6988, accuracy=0.3907, 
[2025-09-12 21:14:23,484][__main__][INFO] - Test, Round 301: loss=1.7119, accuracy=0.3812, 
[2025-09-12 21:14:23,484][__main__][INFO] - Test, Round 302: loss=1.7125, accuracy=0.3845, 
[2025-09-12 21:14:23,484][__main__][INFO] - Test, Round 303: loss=1.7232, accuracy=0.3776, 
[2025-09-12 21:14:23,484][__main__][INFO] - Test, Round 304: loss=1.7353, accuracy=0.3769, 
[2025-09-12 21:14:23,484][__main__][INFO] - Test, Round 305: loss=1.7511, accuracy=0.3650, 
[2025-09-12 21:14:23,484][__main__][INFO] - Test, Round 306: loss=1.7422, accuracy=0.3727, 
[2025-09-12 21:14:23,484][__main__][INFO] - Test, Round 307: loss=1.7376, accuracy=0.3729, 
[2025-09-12 21:14:23,484][__main__][INFO] - Test, Round 308: loss=1.7765, accuracy=0.3646, 
[2025-09-12 21:14:23,484][__main__][INFO] - Test, Round 309: loss=1.7800, accuracy=0.3500, 
[2025-09-12 21:14:23,484][__main__][INFO] - Test, Round 310: loss=1.7766, accuracy=0.3641, 
[2025-09-12 21:14:23,485][__main__][INFO] - Test, Round 311: loss=1.7396, accuracy=0.3641, 
[2025-09-12 21:14:23,485][__main__][INFO] - Test, Round 312: loss=1.7316, accuracy=0.3813, 
[2025-09-12 21:14:23,485][__main__][INFO] - Test, Round 313: loss=1.7091, accuracy=0.3793, 
[2025-09-12 21:14:23,485][__main__][INFO] - Test, Round 314: loss=1.7448, accuracy=0.3749, 
[2025-09-12 21:14:23,485][__main__][INFO] - Test, Round 315: loss=1.6975, accuracy=0.3939, 
[2025-09-12 21:14:23,485][__main__][INFO] - Test, Round 316: loss=1.7317, accuracy=0.3805, 
[2025-09-12 21:14:23,485][__main__][INFO] - Test, Round 317: loss=1.7565, accuracy=0.3828, 
[2025-09-12 21:14:23,485][__main__][INFO] - Test, Round 318: loss=1.7748, accuracy=0.3743, 
[2025-09-12 21:14:23,485][__main__][INFO] - Test, Round 319: loss=1.7835, accuracy=0.3776, 
[2025-09-12 21:14:23,485][__main__][INFO] - Test, Round 320: loss=1.8352, accuracy=0.3690, 
[2025-09-12 21:14:23,485][__main__][INFO] - Test, Round 321: loss=1.8170, accuracy=0.3655, 
[2025-09-12 21:14:23,485][__main__][INFO] - Test, Round 322: loss=1.8266, accuracy=0.3572, 
[2025-09-12 21:14:23,485][__main__][INFO] - Test, Round 323: loss=1.7390, accuracy=0.3799, 
[2025-09-12 21:14:23,485][__main__][INFO] - Test, Round 324: loss=1.6829, accuracy=0.3980, 
[2025-09-12 21:14:23,485][__main__][INFO] - Test, Round 325: loss=1.6572, accuracy=0.4081, 
[2025-09-12 21:14:23,485][__main__][INFO] - Test, Round 326: loss=1.6610, accuracy=0.4061, 
[2025-09-12 21:14:23,485][__main__][INFO] - Test, Round 327: loss=1.6510, accuracy=0.4106, 
[2025-09-12 21:14:23,485][__main__][INFO] - Test, Round 328: loss=1.6652, accuracy=0.4070, 
[2025-09-12 21:14:23,485][__main__][INFO] - Test, Round 329: loss=1.6706, accuracy=0.4103, 
[2025-09-12 21:14:23,485][__main__][INFO] - Test, Round 330: loss=1.7087, accuracy=0.3969, 
[2025-09-12 21:14:23,485][__main__][INFO] - Test, Round 331: loss=1.7574, accuracy=0.3771, 
[2025-09-12 21:14:23,485][__main__][INFO] - Test, Round 332: loss=1.8306, accuracy=0.3594, 
[2025-09-12 21:14:23,485][__main__][INFO] - Test, Round 333: loss=1.8004, accuracy=0.3512, 
[2025-09-12 21:14:23,485][__main__][INFO] - Test, Round 334: loss=1.7811, accuracy=0.3654, 
[2025-09-12 21:14:23,485][__main__][INFO] - Test, Round 335: loss=1.6997, accuracy=0.3862, 
[2025-09-12 21:14:23,485][__main__][INFO] - Test, Round 336: loss=1.7005, accuracy=0.3908, 
[2025-09-12 21:14:23,485][__main__][INFO] - Test, Round 337: loss=1.7118, accuracy=0.3900, 
[2025-09-12 21:14:23,485][__main__][INFO] - Test, Round 338: loss=1.7562, accuracy=0.3729, 
[2025-09-12 21:14:23,485][__main__][INFO] - Test, Round 339: loss=1.7736, accuracy=0.3706, 
[2025-09-12 21:14:23,485][__main__][INFO] - Test, Round 340: loss=1.7519, accuracy=0.3763, 
[2025-09-12 21:14:23,485][__main__][INFO] - Test, Round 341: loss=1.7345, accuracy=0.3813, 
[2025-09-12 21:14:23,485][__main__][INFO] - Test, Round 342: loss=1.7191, accuracy=0.3928, 
[2025-09-12 21:14:23,485][__main__][INFO] - Test, Round 343: loss=1.7129, accuracy=0.3816, 
[2025-09-12 21:14:23,485][__main__][INFO] - Test, Round 344: loss=1.7239, accuracy=0.3875, 
[2025-09-12 21:14:23,485][__main__][INFO] - Test, Round 345: loss=1.7532, accuracy=0.3840, 
[2025-09-12 21:14:23,485][__main__][INFO] - Test, Round 346: loss=1.7589, accuracy=0.3808, 
[2025-09-12 21:14:23,485][__main__][INFO] - Test, Round 347: loss=1.7668, accuracy=0.3832, 
[2025-09-12 21:14:23,485][__main__][INFO] - Test, Round 348: loss=1.7299, accuracy=0.3840, 
[2025-09-12 21:14:23,485][__main__][INFO] - Test, Round 349: loss=1.7039, accuracy=0.4004, 
[2025-09-12 21:14:23,485][__main__][INFO] - Test, Round 350: loss=1.6657, accuracy=0.4040, 
[2025-09-12 21:14:23,485][__main__][INFO] - Test, Round 351: loss=1.6857, accuracy=0.4014, 
[2025-09-12 21:14:23,485][__main__][INFO] - Test, Round 352: loss=1.6622, accuracy=0.4055, 
[2025-09-12 21:14:23,485][__main__][INFO] - Test, Round 353: loss=1.6860, accuracy=0.4008, 
[2025-09-12 21:14:23,485][__main__][INFO] - Test, Round 354: loss=1.6903, accuracy=0.3968, 
[2025-09-12 21:14:23,485][__main__][INFO] - Test, Round 355: loss=1.7754, accuracy=0.3717, 
[2025-09-12 21:14:23,485][__main__][INFO] - Test, Round 356: loss=1.7758, accuracy=0.3691, 
[2025-09-12 21:14:23,485][__main__][INFO] - Test, Round 357: loss=1.7625, accuracy=0.3627, 
[2025-09-12 21:14:23,485][__main__][INFO] - Test, Round 358: loss=1.6857, accuracy=0.3867, 
[2025-09-12 21:14:23,485][__main__][INFO] - Test, Round 359: loss=1.7123, accuracy=0.3792, 
[2025-09-12 21:14:23,485][__main__][INFO] - Test, Round 360: loss=1.6633, accuracy=0.4011, 
[2025-09-12 21:14:23,485][__main__][INFO] - Test, Round 361: loss=1.7021, accuracy=0.3885, 
[2025-09-12 21:14:23,485][__main__][INFO] - Test, Round 362: loss=1.6731, accuracy=0.3981, 
[2025-09-12 21:14:23,485][__main__][INFO] - Test, Round 363: loss=1.6918, accuracy=0.3925, 
[2025-09-12 21:14:23,485][__main__][INFO] - Test, Round 364: loss=1.6923, accuracy=0.3969, 
[2025-09-12 21:14:23,485][__main__][INFO] - Test, Round 365: loss=1.7139, accuracy=0.3846, 
[2025-09-12 21:14:23,485][__main__][INFO] - Test, Round 366: loss=1.7192, accuracy=0.3917, 
[2025-09-12 21:14:23,485][__main__][INFO] - Test, Round 367: loss=1.7442, accuracy=0.3774, 
[2025-09-12 21:14:23,485][__main__][INFO] - Test, Round 368: loss=1.7102, accuracy=0.3883, 
[2025-09-12 21:14:23,485][__main__][INFO] - Test, Round 369: loss=1.7359, accuracy=0.3840, 
[2025-09-12 21:14:23,485][__main__][INFO] - Test, Round 370: loss=1.7029, accuracy=0.3847, 
[2025-09-12 21:14:23,485][__main__][INFO] - Test, Round 371: loss=1.7437, accuracy=0.3779, 
[2025-09-12 21:14:23,485][__main__][INFO] - Test, Round 372: loss=1.7432, accuracy=0.3807, 
[2025-09-12 21:14:23,485][__main__][INFO] - Test, Round 373: loss=1.7730, accuracy=0.3598, 
[2025-09-12 21:14:23,485][__main__][INFO] - Test, Round 374: loss=1.7389, accuracy=0.3787, 
[2025-09-12 21:14:23,485][__main__][INFO] - Test, Round 375: loss=1.6755, accuracy=0.3939, 
[2025-09-12 21:14:23,485][__main__][INFO] - Test, Round 376: loss=1.6757, accuracy=0.4050, 
[2025-09-12 21:14:23,485][__main__][INFO] - Test, Round 377: loss=1.7268, accuracy=0.3980, 
[2025-09-12 21:14:23,485][__main__][INFO] - Test, Round 378: loss=1.8106, accuracy=0.3775, 
[2025-09-12 21:14:23,485][__main__][INFO] - Test, Round 379: loss=1.8952, accuracy=0.3597, 
[2025-09-12 21:14:23,485][__main__][INFO] - Test, Round 380: loss=1.9694, accuracy=0.3447, 
[2025-09-12 21:14:23,485][__main__][INFO] - Test, Round 381: loss=1.7248, accuracy=0.3869, 
[2025-09-12 21:14:23,485][__main__][INFO] - Test, Round 382: loss=1.6442, accuracy=0.4175, 
[2025-09-12 21:14:23,485][__main__][INFO] - Test, Round 383: loss=1.6072, accuracy=0.4242, 
[2025-09-12 21:14:23,485][__main__][INFO] - Test, Round 384: loss=1.6088, accuracy=0.4252, 
[2025-09-12 21:14:23,485][__main__][INFO] - Test, Round 385: loss=1.6258, accuracy=0.4214, 
[2025-09-12 21:14:23,485][__main__][INFO] - Test, Round 386: loss=1.6805, accuracy=0.4090, 
[2025-09-12 21:14:23,485][__main__][INFO] - Test, Round 387: loss=1.7854, accuracy=0.3695, 
[2025-09-12 21:14:23,485][__main__][INFO] - Test, Round 388: loss=1.7839, accuracy=0.3745, 
[2025-09-12 21:14:23,485][__main__][INFO] - Test, Round 389: loss=1.7714, accuracy=0.3595, 
[2025-09-12 21:14:23,485][__main__][INFO] - Test, Round 390: loss=1.6659, accuracy=0.3938, 
[2025-09-12 21:14:23,485][__main__][INFO] - Test, Round 391: loss=1.6431, accuracy=0.4095, 
[2025-09-12 21:14:23,485][__main__][INFO] - Test, Round 392: loss=1.6227, accuracy=0.4135, 
[2025-09-12 21:14:23,485][__main__][INFO] - Test, Round 393: loss=1.6364, accuracy=0.4141, 
[2025-09-12 21:14:23,485][__main__][INFO] - Test, Round 394: loss=1.6938, accuracy=0.3928, 
[2025-09-12 21:14:23,486][__main__][INFO] - Test, Round 395: loss=1.7982, accuracy=0.3735, 
[2025-09-12 21:14:23,486][__main__][INFO] - Test, Round 396: loss=1.7630, accuracy=0.3731, 
[2025-09-12 21:14:23,486][__main__][INFO] - Test, Round 397: loss=1.6849, accuracy=0.4003, 
[2025-09-12 21:14:23,486][__main__][INFO] - Test, Round 398: loss=1.6538, accuracy=0.4027, 
[2025-09-12 21:14:23,486][__main__][INFO] - Test, Round 399: loss=1.6467, accuracy=0.4111, 
[2025-09-12 21:14:23,486][__main__][INFO] - Test, Round 400: loss=1.6886, accuracy=0.4052, 
[2025-09-12 21:14:23,486][__main__][INFO] - Test, Round 401: loss=1.7144, accuracy=0.3984, 
[2025-09-12 21:14:23,486][__main__][INFO] - Test, Round 402: loss=1.7671, accuracy=0.3865, 
[2025-09-12 21:14:23,486][__main__][INFO] - Test, Round 403: loss=1.7391, accuracy=0.3936, 
[2025-09-12 21:14:23,486][__main__][INFO] - Test, Round 404: loss=1.7828, accuracy=0.3845, 
[2025-09-12 21:14:23,486][__main__][INFO] - Test, Round 405: loss=1.7770, accuracy=0.3816, 
[2025-09-12 21:14:23,486][__main__][INFO] - Test, Round 406: loss=1.8008, accuracy=0.3811, 
[2025-09-12 21:14:23,486][__main__][INFO] - Test, Round 407: loss=1.7147, accuracy=0.3960, 
[2025-09-12 21:14:23,486][__main__][INFO] - Test, Round 408: loss=1.6783, accuracy=0.4117, 
[2025-09-12 21:14:23,486][__main__][INFO] - Test, Round 409: loss=1.6357, accuracy=0.4218, 
[2025-09-12 21:14:23,486][__main__][INFO] - Test, Round 410: loss=1.6273, accuracy=0.4240, 
[2025-09-12 21:14:23,486][__main__][INFO] - Test, Round 411: loss=1.6594, accuracy=0.4121, 
[2025-09-12 21:14:23,486][__main__][INFO] - Test, Round 412: loss=1.7191, accuracy=0.3989, 
[2025-09-12 21:14:23,486][__main__][INFO] - Test, Round 413: loss=1.8320, accuracy=0.3645, 
[2025-09-12 21:14:23,486][__main__][INFO] - Test, Round 414: loss=1.8135, accuracy=0.3666, 
[2025-09-12 21:14:23,486][__main__][INFO] - Test, Round 415: loss=1.7058, accuracy=0.3959, 
[2025-09-12 21:14:23,486][__main__][INFO] - Test, Round 416: loss=1.6191, accuracy=0.4159, 
[2025-09-12 21:14:23,486][__main__][INFO] - Test, Round 417: loss=1.6020, accuracy=0.4278, 
[2025-09-12 21:14:23,486][__main__][INFO] - Test, Round 418: loss=1.6390, accuracy=0.4175, 
[2025-09-12 21:14:23,486][__main__][INFO] - Test, Round 419: loss=1.6911, accuracy=0.4019, 
[2025-09-12 21:14:23,486][__main__][INFO] - Test, Round 420: loss=1.7552, accuracy=0.3866, 
[2025-09-12 21:14:23,486][__main__][INFO] - Test, Round 421: loss=1.7995, accuracy=0.3691, 
[2025-09-12 21:14:23,486][__main__][INFO] - Test, Round 422: loss=1.8026, accuracy=0.3608, 
[2025-09-12 21:14:23,486][__main__][INFO] - Test, Round 423: loss=1.7756, accuracy=0.3844, 
[2025-09-12 21:14:23,486][__main__][INFO] - Test, Round 424: loss=1.6282, accuracy=0.4185, 
[2025-09-12 21:14:23,486][__main__][INFO] - Test, Round 425: loss=1.6133, accuracy=0.4256, 
[2025-09-12 21:14:23,486][__main__][INFO] - Test, Round 426: loss=1.6255, accuracy=0.4199, 
[2025-09-12 21:14:23,486][__main__][INFO] - Test, Round 427: loss=1.6864, accuracy=0.4072, 
[2025-09-12 21:14:23,486][__main__][INFO] - Test, Round 428: loss=1.7210, accuracy=0.4021, 
[2025-09-12 21:14:23,486][__main__][INFO] - Test, Round 429: loss=1.7871, accuracy=0.3816, 
[2025-09-12 21:14:23,486][__main__][INFO] - Test, Round 430: loss=1.7968, accuracy=0.3812, 
[2025-09-12 21:14:23,486][__main__][INFO] - Test, Round 431: loss=1.7109, accuracy=0.3986, 
[2025-09-12 21:14:23,486][__main__][INFO] - Test, Round 432: loss=1.6766, accuracy=0.4100, 
[2025-09-12 21:14:23,486][__main__][INFO] - Test, Round 433: loss=1.6585, accuracy=0.4131, 
[2025-09-12 21:14:23,486][__main__][INFO] - Test, Round 434: loss=1.6902, accuracy=0.4130, 
[2025-09-12 21:14:23,486][__main__][INFO] - Test, Round 435: loss=1.7196, accuracy=0.4055, 
[2025-09-12 21:14:23,486][__main__][INFO] - Test, Round 436: loss=1.8738, accuracy=0.3715, 
[2025-09-12 21:14:23,486][__main__][INFO] - Test, Round 437: loss=1.9432, accuracy=0.3534, 
[2025-09-12 21:14:23,486][__main__][INFO] - Test, Round 438: loss=1.8796, accuracy=0.3567, 
[2025-09-12 21:14:23,486][__main__][INFO] - Test, Round 439: loss=1.6593, accuracy=0.4051, 
[2025-09-12 21:14:23,486][__main__][INFO] - Test, Round 440: loss=1.6381, accuracy=0.4146, 
[2025-09-12 21:14:23,486][__main__][INFO] - Test, Round 441: loss=1.6201, accuracy=0.4206, 
[2025-09-12 21:14:23,486][__main__][INFO] - Test, Round 442: loss=1.6498, accuracy=0.4158, 
[2025-09-12 21:14:23,486][__main__][INFO] - Test, Round 443: loss=1.6916, accuracy=0.4081, 
[2025-09-12 21:14:23,486][__main__][INFO] - Test, Round 444: loss=1.7895, accuracy=0.3771, 
[2025-09-12 21:14:23,486][__main__][INFO] - Test, Round 445: loss=1.7729, accuracy=0.3893, 
[2025-09-12 21:14:23,486][__main__][INFO] - Test, Round 446: loss=1.7556, accuracy=0.3779, 
[2025-09-12 21:14:23,486][__main__][INFO] - Test, Round 447: loss=1.6698, accuracy=0.4092, 
[2025-09-12 21:14:23,486][__main__][INFO] - Test, Round 448: loss=1.7167, accuracy=0.3887, 
[2025-09-12 21:14:23,486][__main__][INFO] - Test, Round 449: loss=1.7411, accuracy=0.3906, 
[2025-09-12 21:14:23,486][__main__][INFO] - Test, Round 450: loss=1.8161, accuracy=0.3650, 
[2025-09-12 21:14:23,486][__main__][INFO] - Test, Round 451: loss=1.8467, accuracy=0.3711, 
[2025-09-12 21:14:23,486][__main__][INFO] - Test, Round 452: loss=1.7244, accuracy=0.3872, 
[2025-09-12 21:14:23,486][__main__][INFO] - Test, Round 453: loss=1.6395, accuracy=0.4133, 
[2025-09-12 21:14:23,486][__main__][INFO] - Test, Round 454: loss=1.6377, accuracy=0.4214, 
[2025-09-12 21:14:23,486][__main__][INFO] - Test, Round 455: loss=1.6502, accuracy=0.4207, 
[2025-09-12 21:14:23,486][__main__][INFO] - Test, Round 456: loss=1.7124, accuracy=0.4097, 
[2025-09-12 21:14:23,486][__main__][INFO] - Test, Round 457: loss=1.7743, accuracy=0.3928, 
[2025-09-12 21:14:23,486][__main__][INFO] - Test, Round 458: loss=1.8304, accuracy=0.3853, 
[2025-09-12 21:14:23,486][__main__][INFO] - Test, Round 459: loss=1.7353, accuracy=0.4005, 
[2025-09-12 21:14:23,486][__main__][INFO] - Test, Round 460: loss=1.7042, accuracy=0.4096, 
[2025-09-12 21:14:23,486][__main__][INFO] - Test, Round 461: loss=1.7034, accuracy=0.4143, 
[2025-09-12 21:14:23,486][__main__][INFO] - Test, Round 462: loss=1.7205, accuracy=0.4080, 
[2025-09-12 21:14:23,486][__main__][INFO] - Test, Round 463: loss=1.7370, accuracy=0.4024, 
[2025-09-12 21:14:23,486][__main__][INFO] - Test, Round 464: loss=1.7669, accuracy=0.3947, 
[2025-09-12 21:14:23,486][__main__][INFO] - Test, Round 465: loss=1.7421, accuracy=0.3948, 
[2025-09-12 21:14:23,486][__main__][INFO] - Test, Round 466: loss=1.7452, accuracy=0.3955, 
[2025-09-12 21:14:23,486][__main__][INFO] - Test, Round 467: loss=1.7291, accuracy=0.3945, 
[2025-09-12 21:14:23,486][__main__][INFO] - Test, Round 468: loss=1.7067, accuracy=0.3997, 
[2025-09-12 21:14:23,486][__main__][INFO] - Test, Round 469: loss=1.7003, accuracy=0.4058, 
[2025-09-12 21:14:23,486][__main__][INFO] - Test, Round 470: loss=1.7042, accuracy=0.4033, 
[2025-09-12 21:14:23,486][__main__][INFO] - Test, Round 471: loss=1.6816, accuracy=0.4118, 
[2025-09-12 21:14:23,486][__main__][INFO] - Test, Round 472: loss=1.6887, accuracy=0.4067, 
[2025-09-12 21:14:23,486][__main__][INFO] - Test, Round 473: loss=1.7294, accuracy=0.3981, 
[2025-09-12 21:14:23,486][__main__][INFO] - Test, Round 474: loss=1.7064, accuracy=0.4089, 
[2025-09-12 21:14:23,486][__main__][INFO] - Test, Round 475: loss=1.7339, accuracy=0.3913, 
[2025-09-12 21:14:23,486][__main__][INFO] - Test, Round 476: loss=1.7630, accuracy=0.3932, 
[2025-09-12 21:14:23,486][__main__][INFO] - Test, Round 477: loss=1.8590, accuracy=0.3647, 
[2025-09-12 21:14:23,486][__main__][INFO] - Test, Round 478: loss=1.7560, accuracy=0.3868, 
[2025-09-12 21:14:23,487][__main__][INFO] - Test, Round 479: loss=1.7802, accuracy=0.3904, 
[2025-09-12 21:14:23,487][__main__][INFO] - Test, Round 480: loss=1.6041, accuracy=0.4322, 
[2025-09-12 21:14:23,487][__main__][INFO] - Test, Round 481: loss=1.6266, accuracy=0.4222, 
[2025-09-12 21:14:23,487][__main__][INFO] - Test, Round 482: loss=1.6459, accuracy=0.4213, 
[2025-09-12 21:14:23,487][__main__][INFO] - Test, Round 483: loss=1.7195, accuracy=0.4008, 
[2025-09-12 21:14:23,487][__main__][INFO] - Test, Round 484: loss=1.8320, accuracy=0.3866, 
[2025-09-12 21:14:23,487][__main__][INFO] - Test, Round 485: loss=1.8826, accuracy=0.3585, 
[2025-09-12 21:14:23,487][__main__][INFO] - Test, Round 486: loss=1.7335, accuracy=0.3989, 
[2025-09-12 21:14:23,487][__main__][INFO] - Test, Round 487: loss=1.6621, accuracy=0.4109, 
[2025-09-12 21:14:23,487][__main__][INFO] - Test, Round 488: loss=1.6757, accuracy=0.4165, 
[2025-09-12 21:14:23,487][__main__][INFO] - Test, Round 489: loss=1.7468, accuracy=0.3929, 
[2025-09-12 21:14:23,487][__main__][INFO] - Test, Round 490: loss=1.8630, accuracy=0.3829, 
[2025-09-12 21:14:23,487][__main__][INFO] - Test, Round 491: loss=1.8561, accuracy=0.3625, 
[2025-09-12 21:14:23,487][__main__][INFO] - Test, Round 492: loss=1.8124, accuracy=0.3852, 
[2025-09-12 21:14:23,487][__main__][INFO] - Test, Round 493: loss=1.6763, accuracy=0.4066, 
[2025-09-12 21:14:23,487][__main__][INFO] - Test, Round 494: loss=1.6203, accuracy=0.4219, 
[2025-09-12 21:14:23,487][__main__][INFO] - Test, Round 495: loss=1.6538, accuracy=0.4247, 
[2025-09-12 21:14:23,487][__main__][INFO] - Test, Round 496: loss=1.6920, accuracy=0.4071, 
[2025-09-12 21:14:23,487][__main__][INFO] - Test, Round 497: loss=1.8341, accuracy=0.3948, 
[2025-09-12 21:14:23,487][__main__][INFO] - Test, Round 498: loss=1.8847, accuracy=0.3624, 
[2025-09-12 21:14:23,487][__main__][INFO] - Test, Round 499: loss=1.7208, accuracy=0.4034, 
[2025-09-12 21:14:23,487][__main__][INFO] - Test, Round 500: loss=1.6727, accuracy=0.4133, 
