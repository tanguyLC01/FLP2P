[2025-09-12 17:28:35,282][flp2p.graph_runner][INFO] - Train, Round 0 : loss => 2.305283138155937,  accuracy: 0.10242, gradient_norm : 0.1796742022356441
[2025-09-12 17:28:44,751][flp2p.graph_runner][INFO] - Test, Round 0 : loss => 2.305250248396397,  accuracy: 0.1011
[2025-09-12 17:28:54,184][flp2p.graph_runner][INFO] - Train, Round 1 : loss => 2.304766075313091,  accuracy: 0.10482, gradient_norm : 0.17420500497879005
[2025-09-12 17:29:03,832][flp2p.graph_runner][INFO] - Test, Round 1 : loss => 2.304730852496624,  accuracy: 0.1036
[2025-09-12 17:29:14,000][flp2p.graph_runner][INFO] - Train, Round 2 : loss => 2.3041883277893067,  accuracy: 0.1069, gradient_norm : 0.17321378239390128
[2025-09-12 17:29:24,201][flp2p.graph_runner][INFO] - Test, Round 2 : loss => 2.3042407679438592,  accuracy: 0.11
[2025-09-12 17:29:34,067][flp2p.graph_runner][INFO] - Train, Round 3 : loss => 2.303682977557182,  accuracy: 0.10946, gradient_norm : 0.1733060843254307
[2025-09-12 17:29:44,333][flp2p.graph_runner][INFO] - Test, Round 3 : loss => 2.303774310219288,  accuracy: 0.1131
[2025-09-12 17:29:54,112][flp2p.graph_runner][INFO] - Train, Round 4 : loss => 2.303235594034195,  accuracy: 0.11184, gradient_norm : 0.18075796949522022
[2025-09-12 17:30:04,062][flp2p.graph_runner][INFO] - Test, Round 4 : loss => 2.3032864800572397,  accuracy: 0.1155
[2025-09-12 17:30:13,736][flp2p.graph_runner][INFO] - Train, Round 5 : loss => 2.3026857194304466,  accuracy: 0.11492, gradient_norm : 0.1754170479137736
[2025-09-12 17:30:23,369][flp2p.graph_runner][INFO] - Test, Round 5 : loss => 2.3028365908622743,  accuracy: 0.1155
[2025-09-12 17:30:33,131][flp2p.graph_runner][INFO] - Train, Round 6 : loss => 2.3022151589393616,  accuracy: 0.11706, gradient_norm : 0.17520639740827013
[2025-09-12 17:30:42,853][flp2p.graph_runner][INFO] - Test, Round 6 : loss => 2.3023773037552835,  accuracy: 0.1167
[2025-09-12 17:30:52,540][flp2p.graph_runner][INFO] - Train, Round 7 : loss => 2.301730181872845,  accuracy: 0.11902, gradient_norm : 0.18099719662474406
[2025-09-12 17:31:02,474][flp2p.graph_runner][INFO] - Test, Round 7 : loss => 2.3019229439377784,  accuracy: 0.1192
[2025-09-12 17:31:12,269][flp2p.graph_runner][INFO] - Train, Round 8 : loss => 2.3012241137027742,  accuracy: 0.12124, gradient_norm : 0.1720704896252975
[2025-09-12 17:31:22,203][flp2p.graph_runner][INFO] - Test, Round 8 : loss => 2.3014756295323373,  accuracy: 0.1228
[2025-09-12 17:31:31,932][flp2p.graph_runner][INFO] - Train, Round 9 : loss => 2.3007619240880013,  accuracy: 0.12166, gradient_norm : 0.18044225737971945
[2025-09-12 17:31:41,633][flp2p.graph_runner][INFO] - Test, Round 9 : loss => 2.3010344544768335,  accuracy: 0.1248
[2025-09-12 17:31:51,304][flp2p.graph_runner][INFO] - Train, Round 10 : loss => 2.3002917185425757,  accuracy: 0.12344, gradient_norm : 0.1765872339612161
[2025-09-12 17:32:00,930][flp2p.graph_runner][INFO] - Test, Round 10 : loss => 2.3005794632434844,  accuracy: 0.1259
[2025-09-12 17:32:10,682][flp2p.graph_runner][INFO] - Train, Round 11 : loss => 2.2998410347104072,  accuracy: 0.12504, gradient_norm : 0.17895865803124164
[2025-09-12 17:32:20,327][flp2p.graph_runner][INFO] - Test, Round 11 : loss => 2.300105261671543,  accuracy: 0.1265
[2025-09-12 17:32:30,005][flp2p.graph_runner][INFO] - Train, Round 12 : loss => 2.299327205121517,  accuracy: 0.12582, gradient_norm : 0.18153450826544773
[2025-09-12 17:32:39,750][flp2p.graph_runner][INFO] - Test, Round 12 : loss => 2.2996226083517075,  accuracy: 0.1284
[2025-09-12 17:32:49,491][flp2p.graph_runner][INFO] - Train, Round 13 : loss => 2.298809960782528,  accuracy: 0.12724, gradient_norm : 0.1770375681562123
[2025-09-12 17:32:59,391][flp2p.graph_runner][INFO] - Test, Round 13 : loss => 2.299150853276253,  accuracy: 0.1303
[2025-09-12 17:33:09,152][flp2p.graph_runner][INFO] - Train, Round 14 : loss => 2.2983020550012587,  accuracy: 0.1281, gradient_norm : 0.17903541433243625
[2025-09-12 17:33:18,926][flp2p.graph_runner][INFO] - Test, Round 14 : loss => 2.298657404065132,  accuracy: 0.1309
[2025-09-12 17:33:28,663][flp2p.graph_runner][INFO] - Train, Round 15 : loss => 2.2977738898992537,  accuracy: 0.12958, gradient_norm : 0.18548791961073474
[2025-09-12 17:33:38,306][flp2p.graph_runner][INFO] - Test, Round 15 : loss => 2.298147856557369,  accuracy: 0.1339
[2025-09-12 17:33:48,098][flp2p.graph_runner][INFO] - Train, Round 16 : loss => 2.2972655439376832,  accuracy: 0.13142, gradient_norm : 0.18323163523139496
[2025-09-12 17:33:58,036][flp2p.graph_runner][INFO] - Test, Round 16 : loss => 2.297615185701847,  accuracy: 0.1323
[2025-09-12 17:34:07,868][flp2p.graph_runner][INFO] - Train, Round 17 : loss => 2.2966658592224123,  accuracy: 0.1317, gradient_norm : 0.187547426177966
[2025-09-12 17:34:17,522][flp2p.graph_runner][INFO] - Test, Round 17 : loss => 2.297083939266205,  accuracy: 0.1325
[2025-09-12 17:34:27,240][flp2p.graph_runner][INFO] - Train, Round 18 : loss => 2.2961034801602365,  accuracy: 0.1327, gradient_norm : 0.18812389648646666
[2025-09-12 17:34:36,940][flp2p.graph_runner][INFO] - Test, Round 18 : loss => 2.2965329115748405,  accuracy: 0.1329
[2025-09-12 17:34:46,638][flp2p.graph_runner][INFO] - Train, Round 19 : loss => 2.295485998690128,  accuracy: 0.13458, gradient_norm : 0.18986962298744003
[2025-09-12 17:34:56,567][flp2p.graph_runner][INFO] - Test, Round 19 : loss => 2.2959717813968656,  accuracy: 0.1335
[2025-09-12 17:35:06,366][flp2p.graph_runner][INFO] - Train, Round 20 : loss => 2.2948857659101485,  accuracy: 0.13604, gradient_norm : 0.1881130665938729
[2025-09-12 17:35:16,264][flp2p.graph_runner][INFO] - Test, Round 20 : loss => 2.295370859527588,  accuracy: 0.1343
[2025-09-12 17:35:25,967][flp2p.graph_runner][INFO] - Train, Round 21 : loss => 2.2942053887248037,  accuracy: 0.13804, gradient_norm : 0.19585973263818296
[2025-09-12 17:35:35,837][flp2p.graph_runner][INFO] - Test, Round 21 : loss => 2.2947444869160654,  accuracy: 0.1339
[2025-09-12 17:35:45,546][flp2p.graph_runner][INFO] - Train, Round 22 : loss => 2.2935271349549295,  accuracy: 0.13992, gradient_norm : 0.19110214105760512
[2025-09-12 17:35:55,521][flp2p.graph_runner][INFO] - Test, Round 22 : loss => 2.2940882185459137,  accuracy: 0.1353
[2025-09-12 17:36:05,203][flp2p.graph_runner][INFO] - Train, Round 23 : loss => 2.2928215575218203,  accuracy: 0.14066, gradient_norm : 0.20018861173706487
[2025-09-12 17:36:15,061][flp2p.graph_runner][INFO] - Test, Round 23 : loss => 2.293345990717411,  accuracy: 0.1373
[2025-09-12 17:36:24,808][flp2p.graph_runner][INFO] - Train, Round 24 : loss => 2.291974692940712,  accuracy: 0.14224, gradient_norm : 0.20452340877398506
[2025-09-12 17:36:34,569][flp2p.graph_runner][INFO] - Test, Round 24 : loss => 2.292548380970955,  accuracy: 0.1389
[2025-09-12 17:36:44,219][flp2p.graph_runner][INFO] - Train, Round 25 : loss => 2.2911158058047296,  accuracy: 0.14322, gradient_norm : 0.21279911452052772
[2025-09-12 17:36:54,051][flp2p.graph_runner][INFO] - Test, Round 25 : loss => 2.2917009882092474,  accuracy: 0.1411
[2025-09-12 17:37:03,704][flp2p.graph_runner][INFO] - Train, Round 26 : loss => 2.290189619362354,  accuracy: 0.14574, gradient_norm : 0.2120174072259043
[2025-09-12 17:37:13,435][flp2p.graph_runner][INFO] - Test, Round 26 : loss => 2.2907823644280434,  accuracy: 0.1419
[2025-09-12 17:37:23,162][flp2p.graph_runner][INFO] - Train, Round 27 : loss => 2.289189543426037,  accuracy: 0.1478, gradient_norm : 0.22178017538446393
[2025-09-12 17:37:32,825][flp2p.graph_runner][INFO] - Test, Round 27 : loss => 2.2897804858088495,  accuracy: 0.1461
[2025-09-12 17:37:42,413][flp2p.graph_runner][INFO] - Train, Round 28 : loss => 2.2880850625038147,  accuracy: 0.15016, gradient_norm : 0.2253808305169314
[2025-09-12 17:37:52,121][flp2p.graph_runner][INFO] - Test, Round 28 : loss => 2.28870649138689,  accuracy: 0.1486
[2025-09-12 17:38:01,853][flp2p.graph_runner][INFO] - Train, Round 29 : loss => 2.286904377639294,  accuracy: 0.15166, gradient_norm : 0.2307697638532594
[2025-09-12 17:38:11,623][flp2p.graph_runner][INFO] - Test, Round 29 : loss => 2.2875293070673943,  accuracy: 0.1485
[2025-09-12 17:38:21,437][flp2p.graph_runner][INFO] - Train, Round 30 : loss => 2.285644628405571,  accuracy: 0.1541, gradient_norm : 0.23302551290415457
[2025-09-12 17:38:31,368][flp2p.graph_runner][INFO] - Test, Round 30 : loss => 2.2862900081634523,  accuracy: 0.1507
[2025-09-12 17:38:41,190][flp2p.graph_runner][INFO] - Train, Round 31 : loss => 2.284185853600502,  accuracy: 0.1561, gradient_norm : 0.2410631692507322
[2025-09-12 17:38:51,123][flp2p.graph_runner][INFO] - Test, Round 31 : loss => 2.284887503325939,  accuracy: 0.1508
[2025-09-12 17:39:00,816][flp2p.graph_runner][INFO] - Train, Round 32 : loss => 2.2825861370563505,  accuracy: 0.15746, gradient_norm : 0.25301855276186286
[2025-09-12 17:39:11,247][flp2p.graph_runner][INFO] - Test, Round 32 : loss => 2.283313383901119,  accuracy: 0.1519
[2025-09-12 17:39:21,581][flp2p.graph_runner][INFO] - Train, Round 33 : loss => 2.280897979736328,  accuracy: 0.15906, gradient_norm : 0.26852108111387524
[2025-09-12 17:39:38,894][flp2p.graph_runner][INFO] - Test, Round 33 : loss => 2.281620679104328,  accuracy: 0.1538
[2025-09-12 17:39:49,468][flp2p.graph_runner][INFO] - Train, Round 34 : loss => 2.2789650985598566,  accuracy: 0.16108, gradient_norm : 0.2761445608071909
[2025-09-12 17:40:20,332][flp2p.graph_runner][INFO] - Test, Round 34 : loss => 2.279649673295021,  accuracy: 0.1565
[2025-09-12 17:40:31,094][flp2p.graph_runner][INFO] - Train, Round 35 : loss => 2.2767541533708573,  accuracy: 0.16282, gradient_norm : 0.2830252621409516
[2025-09-12 17:41:01,123][flp2p.graph_runner][INFO] - Test, Round 35 : loss => 2.277472228193283,  accuracy: 0.157
[2025-09-12 17:41:11,956][flp2p.graph_runner][INFO] - Train, Round 36 : loss => 2.2742741480469704,  accuracy: 0.16432, gradient_norm : 0.3037143992661945
[2025-09-12 17:41:34,125][flp2p.graph_runner][INFO] - Test, Round 36 : loss => 2.2749563571929934,  accuracy: 0.1588
[2025-09-12 17:41:45,183][flp2p.graph_runner][INFO] - Train, Round 37 : loss => 2.2715248373150825,  accuracy: 0.16572, gradient_norm : 0.3227133935089867
[2025-09-12 17:42:04,778][flp2p.graph_runner][INFO] - Test, Round 37 : loss => 2.2722301701426506,  accuracy: 0.159
[2025-09-12 17:42:15,335][flp2p.graph_runner][INFO] - Train, Round 38 : loss => 2.268432961702347,  accuracy: 0.16776, gradient_norm : 0.32954559399078576
[2025-09-12 17:42:38,447][flp2p.graph_runner][INFO] - Test, Round 38 : loss => 2.269114935660362,  accuracy: 0.1602
[2025-09-12 17:42:49,150][flp2p.graph_runner][INFO] - Train, Round 39 : loss => 2.264920082092285,  accuracy: 0.16946, gradient_norm : 0.3663659336146744
[2025-09-12 17:43:19,925][flp2p.graph_runner][INFO] - Test, Round 39 : loss => 2.265612815618515,  accuracy: 0.1618
[2025-09-12 17:43:30,563][flp2p.graph_runner][INFO] - Train, Round 40 : loss => 2.260986876785755,  accuracy: 0.17216, gradient_norm : 0.3912753081438007
[2025-09-12 17:44:03,411][flp2p.graph_runner][INFO] - Test, Round 40 : loss => 2.2616817565679552,  accuracy: 0.1639
[2025-09-12 17:44:13,960][flp2p.graph_runner][INFO] - Train, Round 41 : loss => 2.256519547700882,  accuracy: 0.17412, gradient_norm : 0.4021169932533212
[2025-09-12 17:44:43,198][flp2p.graph_runner][INFO] - Test, Round 41 : loss => 2.2571958238124847,  accuracy: 0.167
[2025-09-12 17:44:53,817][flp2p.graph_runner][INFO] - Train, Round 42 : loss => 2.2513861614465713,  accuracy: 0.17562, gradient_norm : 0.441569886963962
[2025-09-12 17:45:14,304][flp2p.graph_runner][INFO] - Test, Round 42 : loss => 2.251986191689968,  accuracy: 0.1685
[2025-09-12 17:45:24,849][flp2p.graph_runner][INFO] - Train, Round 43 : loss => 2.245473473072052,  accuracy: 0.17854, gradient_norm : 0.47242120760775436
[2025-09-12 17:45:50,046][flp2p.graph_runner][INFO] - Test, Round 43 : loss => 2.2461724163889887,  accuracy: 0.1705
[2025-09-12 17:46:00,695][flp2p.graph_runner][INFO] - Train, Round 44 : loss => 2.238975077867508,  accuracy: 0.18046, gradient_norm : 0.5001176628101509
[2025-09-12 17:46:32,025][flp2p.graph_runner][INFO] - Test, Round 44 : loss => 2.239820694887638,  accuracy: 0.1734
[2025-09-12 17:46:42,729][flp2p.graph_runner][INFO] - Train, Round 45 : loss => 2.231734379827976,  accuracy: 0.18428, gradient_norm : 0.5408735148353928
[2025-09-12 17:47:13,060][flp2p.graph_runner][INFO] - Test, Round 45 : loss => 2.2326256499171255,  accuracy: 0.175
[2025-09-12 17:47:23,855][flp2p.graph_runner][INFO] - Train, Round 46 : loss => 2.223424870967865,  accuracy: 0.18698, gradient_norm : 0.5923043750156917
[2025-09-12 17:47:52,319][flp2p.graph_runner][INFO] - Test, Round 46 : loss => 2.2243815939188005,  accuracy: 0.179
[2025-09-12 17:48:03,211][flp2p.graph_runner][INFO] - Train, Round 47 : loss => 2.2140171638131143,  accuracy: 0.18942, gradient_norm : 0.6262721989921092
[2025-09-12 17:48:22,365][flp2p.graph_runner][INFO] - Test, Round 47 : loss => 2.2154914592146873,  accuracy: 0.1806
[2025-09-12 17:48:33,087][flp2p.graph_runner][INFO] - Train, Round 48 : loss => 2.203749331533909,  accuracy: 0.19332, gradient_norm : 0.6881337209057538
[2025-09-12 17:48:51,662][flp2p.graph_runner][INFO] - Test, Round 48 : loss => 2.205733499610424,  accuracy: 0.1841
[2025-09-12 17:49:02,313][flp2p.graph_runner][INFO] - Train, Round 49 : loss => 2.1927200800180433,  accuracy: 0.19474, gradient_norm : 0.7431860636885662
[2025-09-12 17:49:28,205][flp2p.graph_runner][INFO] - Test, Round 49 : loss => 2.195715399336815,  accuracy: 0.1873
[2025-09-12 17:49:38,843][flp2p.graph_runner][INFO] - Train, Round 50 : loss => 2.1810579749941827,  accuracy: 0.19892, gradient_norm : 0.7907698569042444
[2025-09-12 17:50:03,604][flp2p.graph_runner][INFO] - Test, Round 50 : loss => 2.1851190945625305,  accuracy: 0.1879
[2025-09-12 17:50:14,182][flp2p.graph_runner][INFO] - Train, Round 51 : loss => 2.1686562784016132,  accuracy: 0.20136, gradient_norm : 0.90552193768007
[2025-09-12 17:50:26,486][flp2p.graph_runner][INFO] - Test, Round 51 : loss => 2.1742293744087218,  accuracy: 0.1906
[2025-09-12 17:50:37,236][flp2p.graph_runner][INFO] - Train, Round 52 : loss => 2.1562172290682793,  accuracy: 0.20444, gradient_norm : 0.9501793204544811
[2025-09-12 17:50:48,749][flp2p.graph_runner][INFO] - Test, Round 52 : loss => 2.1630804193854334,  accuracy: 0.1964
[2025-09-12 17:50:59,841][flp2p.graph_runner][INFO] - Train, Round 53 : loss => 2.1436183293163777,  accuracy: 0.21054, gradient_norm : 1.0324853663420286
[2025-09-12 17:51:18,806][flp2p.graph_runner][INFO] - Test, Round 53 : loss => 2.1532135340332985,  accuracy: 0.2029
[2025-09-12 17:51:29,791][flp2p.graph_runner][INFO] - Train, Round 54 : loss => 2.131346288919449,  accuracy: 0.2179, gradient_norm : 1.0570825460088136
[2025-09-12 17:51:53,379][flp2p.graph_runner][INFO] - Test, Round 54 : loss => 2.143399133324623,  accuracy: 0.2024
[2025-09-12 17:52:04,384][flp2p.graph_runner][INFO] - Train, Round 55 : loss => 2.1196600648760797,  accuracy: 0.2193, gradient_norm : 1.1793707676646266
[2025-09-12 17:52:22,511][flp2p.graph_runner][INFO] - Test, Round 55 : loss => 2.1338517593443393,  accuracy: 0.2145
[2025-09-12 17:52:33,271][flp2p.graph_runner][INFO] - Train, Round 56 : loss => 2.107612970471382,  accuracy: 0.22926, gradient_norm : 1.281503466858179
[2025-09-12 17:52:52,236][flp2p.graph_runner][INFO] - Test, Round 56 : loss => 2.1247588528990744,  accuracy: 0.2191
[2025-09-12 17:53:02,848][flp2p.graph_runner][INFO] - Train, Round 57 : loss => 2.0962539418041706,  accuracy: 0.23608, gradient_norm : 1.3063224012508874
[2025-09-12 17:53:31,774][flp2p.graph_runner][INFO] - Test, Round 57 : loss => 2.11545817784667,  accuracy: 0.2233
[2025-09-12 17:53:42,391][flp2p.graph_runner][INFO] - Train, Round 58 : loss => 2.0853883926570416,  accuracy: 0.24252, gradient_norm : 1.4589896798950515
[2025-09-12 17:54:16,424][flp2p.graph_runner][INFO] - Test, Round 58 : loss => 2.107008400052786,  accuracy: 0.23
[2025-09-12 17:54:27,106][flp2p.graph_runner][INFO] - Train, Round 59 : loss => 2.0742494802176954,  accuracy: 0.25044, gradient_norm : 1.4732624238574792
[2025-09-12 17:55:04,962][flp2p.graph_runner][INFO] - Test, Round 59 : loss => 2.097559951937199,  accuracy: 0.2358
[2025-09-12 17:55:15,816][flp2p.graph_runner][INFO] - Train, Round 60 : loss => 2.062706206291914,  accuracy: 0.25854, gradient_norm : 1.536425368303213
[2025-09-12 17:55:41,814][flp2p.graph_runner][INFO] - Test, Round 60 : loss => 2.0899220690071583,  accuracy: 0.2444
[2025-09-12 17:55:52,786][flp2p.graph_runner][INFO] - Train, Round 61 : loss => 2.0507062429189684,  accuracy: 0.2672, gradient_norm : 1.5907453545966255
[2025-09-12 17:56:11,313][flp2p.graph_runner][INFO] - Test, Round 61 : loss => 2.104380246847868,  accuracy: 0.2403
[2025-09-12 17:56:22,092][flp2p.graph_runner][INFO] - Train, Round 62 : loss => 2.0440961849689483,  accuracy: 0.27254, gradient_norm : 1.6985354683262606
[2025-09-12 17:56:41,462][flp2p.graph_runner][INFO] - Test, Round 62 : loss => 2.070726849848032,  accuracy: 0.2571
[2025-09-12 17:56:52,056][flp2p.graph_runner][INFO] - Train, Round 63 : loss => 2.0271332642436026,  accuracy: 0.28092, gradient_norm : 1.786816752923397
[2025-09-12 17:57:16,867][flp2p.graph_runner][INFO] - Test, Round 63 : loss => 2.063977905768156,  accuracy: 0.2598
[2025-09-12 17:57:27,428][flp2p.graph_runner][INFO] - Train, Round 64 : loss => 2.015034200698137,  accuracy: 0.29002, gradient_norm : 1.8482604591571887
[2025-09-12 17:57:43,016][flp2p.graph_runner][INFO] - Test, Round 64 : loss => 2.0624894146203996,  accuracy: 0.2593
[2025-09-12 17:57:53,662][flp2p.graph_runner][INFO] - Train, Round 65 : loss => 2.003939451277256,  accuracy: 0.2905, gradient_norm : 1.9566991176639639
[2025-09-12 17:58:05,629][flp2p.graph_runner][INFO] - Test, Round 65 : loss => 2.0446125014364718,  accuracy: 0.2675
[2025-09-12 17:58:16,827][flp2p.graph_runner][INFO] - Train, Round 66 : loss => 1.9909521858394146,  accuracy: 0.29624, gradient_norm : 1.9772383591779694
[2025-09-12 17:58:28,114][flp2p.graph_runner][INFO] - Test, Round 66 : loss => 2.0370043519496916,  accuracy: 0.2752
[2025-09-12 17:58:38,982][flp2p.graph_runner][INFO] - Train, Round 67 : loss => 1.9800912608206271,  accuracy: 0.30352, gradient_norm : 2.166546648020797
[2025-09-12 17:59:06,122][flp2p.graph_runner][INFO] - Test, Round 67 : loss => 2.0444904498636722,  accuracy: 0.2759
[2025-09-12 17:59:16,899][flp2p.graph_runner][INFO] - Train, Round 68 : loss => 1.971001101732254,  accuracy: 0.30482, gradient_norm : 2.1779097249209345
[2025-09-12 17:59:44,986][flp2p.graph_runner][INFO] - Test, Round 68 : loss => 2.023809490519762,  accuracy: 0.2768
[2025-09-12 17:59:55,706][flp2p.graph_runner][INFO] - Train, Round 69 : loss => 1.9568950821459294,  accuracy: 0.31016, gradient_norm : 2.3164241408631834
[2025-09-12 18:00:21,201][flp2p.graph_runner][INFO] - Test, Round 69 : loss => 2.011997482776642,  accuracy: 0.2847
[2025-09-12 18:00:32,276][flp2p.graph_runner][INFO] - Train, Round 70 : loss => 1.9456107887625693,  accuracy: 0.31382, gradient_norm : 2.421385402394327
[2025-09-12 18:00:51,128][flp2p.graph_runner][INFO] - Test, Round 70 : loss => 1.9990148710012436,  accuracy: 0.2853
[2025-09-12 18:01:02,049][flp2p.graph_runner][INFO] - Train, Round 71 : loss => 1.933364644050598,  accuracy: 0.31822, gradient_norm : 2.439105259943564
[2025-09-12 18:01:20,807][flp2p.graph_runner][INFO] - Test, Round 71 : loss => 2.005073427593708,  accuracy: 0.2871
[2025-09-12 18:01:31,406][flp2p.graph_runner][INFO] - Train, Round 72 : loss => 1.9266608367860316,  accuracy: 0.31906, gradient_norm : 2.447936578212148
[2025-09-12 18:01:55,707][flp2p.graph_runner][INFO] - Test, Round 72 : loss => 1.988522742652893,  accuracy: 0.2952
[2025-09-12 18:02:06,242][flp2p.graph_runner][INFO] - Train, Round 73 : loss => 1.9125841309130192,  accuracy: 0.3217, gradient_norm : 2.5795373297547894
[2025-09-12 18:02:25,566][flp2p.graph_runner][INFO] - Test, Round 73 : loss => 1.9985168021738529,  accuracy: 0.284
[2025-09-12 18:02:36,104][flp2p.graph_runner][INFO] - Train, Round 74 : loss => 1.9068158498406411,  accuracy: 0.32262, gradient_norm : 2.5457355475388117
[2025-09-12 18:02:47,984][flp2p.graph_runner][INFO] - Test, Round 74 : loss => 1.9826269879281522,  accuracy: 0.2841
[2025-09-12 18:02:59,106][flp2p.graph_runner][INFO] - Train, Round 75 : loss => 1.8964787490665913,  accuracy: 0.3244, gradient_norm : 2.899428839801164
[2025-09-12 18:03:10,546][flp2p.graph_runner][INFO] - Test, Round 75 : loss => 1.9868984074354172,  accuracy: 0.2906
[2025-09-12 18:03:21,317][flp2p.graph_runner][INFO] - Train, Round 76 : loss => 1.8891704535484315,  accuracy: 0.32766, gradient_norm : 2.871092662632362
[2025-09-12 18:03:43,902][flp2p.graph_runner][INFO] - Test, Round 76 : loss => 1.999315421628952,  accuracy: 0.2806
[2025-09-12 18:03:54,677][flp2p.graph_runner][INFO] - Train, Round 77 : loss => 1.883957144021988,  accuracy: 0.32872, gradient_norm : 3.1349091178332156
[2025-09-12 18:04:26,426][flp2p.graph_runner][INFO] - Test, Round 77 : loss => 1.996021404325962,  accuracy: 0.2924
[2025-09-12 18:04:37,243][flp2p.graph_runner][INFO] - Train, Round 78 : loss => 1.873862306922674,  accuracy: 0.33226, gradient_norm : 3.1286403619212617
[2025-09-12 18:05:07,645][flp2p.graph_runner][INFO] - Test, Round 78 : loss => 2.0786608590602875,  accuracy: 0.2713
[2025-09-12 18:05:18,434][flp2p.graph_runner][INFO] - Train, Round 79 : loss => 1.8775774195790291,  accuracy: 0.33124, gradient_norm : 2.9395846971697694
[2025-09-12 18:05:41,018][flp2p.graph_runner][INFO] - Test, Round 79 : loss => 1.9727221953749656,  accuracy: 0.3012
[2025-09-12 18:05:51,956][flp2p.graph_runner][INFO] - Train, Round 80 : loss => 1.8544501496851444,  accuracy: 0.33852, gradient_norm : 3.1659352419139735
[2025-09-12 18:06:10,795][flp2p.graph_runner][INFO] - Test, Round 80 : loss => 2.0177038972258567,  accuracy: 0.282
[2025-09-12 18:06:21,427][flp2p.graph_runner][INFO] - Train, Round 81 : loss => 1.8551242907345296,  accuracy: 0.33884, gradient_norm : 3.036984495782252
[2025-09-12 18:06:39,777][flp2p.graph_runner][INFO] - Test, Round 81 : loss => 1.9475514153480529,  accuracy: 0.2996
[2025-09-12 18:06:50,345][flp2p.graph_runner][INFO] - Train, Round 82 : loss => 1.840311065763235,  accuracy: 0.34234, gradient_norm : 3.159447485189561
[2025-09-12 18:07:08,035][flp2p.graph_runner][INFO] - Test, Round 82 : loss => 1.969993351250887,  accuracy: 0.2971
[2025-09-12 18:07:18,579][flp2p.graph_runner][INFO] - Train, Round 83 : loss => 1.834386260062456,  accuracy: 0.34532, gradient_norm : 3.218440883769414
[2025-09-12 18:07:30,425][flp2p.graph_runner][INFO] - Test, Round 83 : loss => 1.9823857197642327,  accuracy: 0.2938
[2025-09-12 18:07:41,391][flp2p.graph_runner][INFO] - Train, Round 84 : loss => 1.8305573165416718,  accuracy: 0.346, gradient_norm : 3.3460268541947253
[2025-09-12 18:07:53,054][flp2p.graph_runner][INFO] - Test, Round 84 : loss => 2.1243444078981875,  accuracy: 0.2626
[2025-09-12 18:08:03,816][flp2p.graph_runner][INFO] - Train, Round 85 : loss => 1.8417684192955495,  accuracy: 0.34422, gradient_norm : 3.0926304248070062
[2025-09-12 18:08:21,144][flp2p.graph_runner][INFO] - Test, Round 85 : loss => 2.021169429308176,  accuracy: 0.2864
[2025-09-12 18:08:31,716][flp2p.graph_runner][INFO] - Train, Round 86 : loss => 1.819204386472702,  accuracy: 0.35012, gradient_norm : 3.2358109424741106
[2025-09-12 18:08:49,330][flp2p.graph_runner][INFO] - Test, Round 86 : loss => 1.9614724723637105,  accuracy: 0.3003
[2025-09-12 18:09:00,009][flp2p.graph_runner][INFO] - Train, Round 87 : loss => 1.8058507107198238,  accuracy: 0.3561, gradient_norm : 3.5066324010675345
[2025-09-12 18:09:19,897][flp2p.graph_runner][INFO] - Test, Round 87 : loss => 1.9742393997848033,  accuracy: 0.3037
[2025-09-12 18:09:30,575][flp2p.graph_runner][INFO] - Train, Round 88 : loss => 1.7995617745816708,  accuracy: 0.3572, gradient_norm : 3.486263397948872
[2025-09-12 18:09:58,483][flp2p.graph_runner][INFO] - Test, Round 88 : loss => 2.0955690782845022,  accuracy: 0.2698
[2025-09-12 18:10:09,162][flp2p.graph_runner][INFO] - Train, Round 89 : loss => 1.8131114591658115,  accuracy: 0.3551, gradient_norm : 3.421674010396615
[2025-09-12 18:10:36,424][flp2p.graph_runner][INFO] - Test, Round 89 : loss => 1.958857718807459,  accuracy: 0.3052
[2025-09-12 18:10:47,356][flp2p.graph_runner][INFO] - Train, Round 90 : loss => 1.783640677779913,  accuracy: 0.36184, gradient_norm : 3.6024987287021015
[2025-09-12 18:11:06,470][flp2p.graph_runner][INFO] - Test, Round 90 : loss => 1.9405140334784985,  accuracy: 0.3093
[2025-09-12 18:11:17,374][flp2p.graph_runner][INFO] - Train, Round 91 : loss => 1.7744851385056972,  accuracy: 0.3661, gradient_norm : 3.647709133296359
[2025-09-12 18:11:36,086][flp2p.graph_runner][INFO] - Test, Round 91 : loss => 1.9380074051856995,  accuracy: 0.3072
[2025-09-12 18:11:46,675][flp2p.graph_runner][INFO] - Train, Round 92 : loss => 1.7651398108899594,  accuracy: 0.36828, gradient_norm : 3.5992882933269335
[2025-09-12 18:12:07,856][flp2p.graph_runner][INFO] - Test, Round 92 : loss => 1.9792388281166553,  accuracy: 0.299
[2025-09-12 18:12:18,416][flp2p.graph_runner][INFO] - Train, Round 93 : loss => 1.7658094914257527,  accuracy: 0.3711, gradient_norm : 3.69679442573127
[2025-09-12 18:12:30,773][flp2p.graph_runner][INFO] - Test, Round 93 : loss => 1.9900882812082767,  accuracy: 0.2957
[2025-09-12 18:12:41,535][flp2p.graph_runner][INFO] - Train, Round 94 : loss => 1.760480995774269,  accuracy: 0.37448, gradient_norm : 3.9484288427124126
[2025-09-12 18:12:53,371][flp2p.graph_runner][INFO] - Test, Round 94 : loss => 1.9743012746095658,  accuracy: 0.3
[2025-09-12 18:13:04,623][flp2p.graph_runner][INFO] - Train, Round 95 : loss => 1.7495841313898564,  accuracy: 0.37506, gradient_norm : 3.726376724442533
[2025-09-12 18:13:16,153][flp2p.graph_runner][INFO] - Test, Round 95 : loss => 1.9466135110735894,  accuracy: 0.3029
[2025-09-12 18:13:27,112][flp2p.graph_runner][INFO] - Train, Round 96 : loss => 1.7362857288122178,  accuracy: 0.38114, gradient_norm : 3.5797528803818146
[2025-09-12 18:13:43,717][flp2p.graph_runner][INFO] - Test, Round 96 : loss => 1.9211264328956603,  accuracy: 0.3098
[2025-09-12 18:13:54,255][flp2p.graph_runner][INFO] - Train, Round 97 : loss => 1.7254678593575954,  accuracy: 0.38388, gradient_norm : 3.74230508784939
[2025-09-12 18:14:12,816][flp2p.graph_runner][INFO] - Test, Round 97 : loss => 1.92380184212327,  accuracy: 0.3078
[2025-09-12 18:14:23,469][flp2p.graph_runner][INFO] - Train, Round 98 : loss => 1.721662687957287,  accuracy: 0.38368, gradient_norm : 3.922214457235642
[2025-09-12 18:14:45,956][flp2p.graph_runner][INFO] - Test, Round 98 : loss => 1.9568350630521774,  accuracy: 0.3068
[2025-09-12 18:14:56,766][flp2p.graph_runner][INFO] - Train, Round 99 : loss => 1.7158512222766875,  accuracy: 0.39028, gradient_norm : 3.9240000248931968
[2025-09-12 18:15:20,820][flp2p.graph_runner][INFO] - Test, Round 99 : loss => 1.9760933117687702,  accuracy: 0.3043
[2025-09-12 18:15:31,771][flp2p.graph_runner][INFO] - Train, Round 100 : loss => 1.7099078911542893,  accuracy: 0.39206, gradient_norm : 3.976088056798614
[2025-09-12 18:15:49,917][flp2p.graph_runner][INFO] - Test, Round 100 : loss => 1.9301484258830548,  accuracy: 0.307
[2025-09-12 18:16:00,857][flp2p.graph_runner][INFO] - Train, Round 101 : loss => 1.692944465279579,  accuracy: 0.39646, gradient_norm : 3.9790104537225517
[2025-09-12 18:16:19,569][flp2p.graph_runner][INFO] - Test, Round 101 : loss => 1.9299146251022816,  accuracy: 0.3156
[2025-09-12 18:16:30,212][flp2p.graph_runner][INFO] - Train, Round 102 : loss => 1.6873443721234798,  accuracy: 0.40048, gradient_norm : 4.01936081607387
[2025-09-12 18:16:56,891][flp2p.graph_runner][INFO] - Test, Round 102 : loss => 1.911069579809904,  accuracy: 0.3109
[2025-09-12 18:17:07,566][flp2p.graph_runner][INFO] - Train, Round 103 : loss => 1.676248391866684,  accuracy: 0.4048, gradient_norm : 4.233784508375922
[2025-09-12 18:17:28,217][flp2p.graph_runner][INFO] - Test, Round 103 : loss => 1.9706256570398808,  accuracy: 0.3028
[2025-09-12 18:17:38,858][flp2p.graph_runner][INFO] - Train, Round 104 : loss => 1.678947950899601,  accuracy: 0.40274, gradient_norm : 4.2590396160114015
[2025-09-12 18:17:50,821][flp2p.graph_runner][INFO] - Test, Round 104 : loss => 1.9094966522991657,  accuracy: 0.3223
[2025-09-12 18:18:02,004][flp2p.graph_runner][INFO] - Train, Round 105 : loss => 1.66141994073987,  accuracy: 0.41102, gradient_norm : 4.160482198284064
[2025-09-12 18:18:13,491][flp2p.graph_runner][INFO] - Test, Round 105 : loss => 1.9531568276762963,  accuracy: 0.3094
[2025-09-12 18:18:26,943][flp2p.graph_runner][INFO] - Train, Round 106 : loss => 1.6577102898061276,  accuracy: 0.41016, gradient_norm : 4.177418518442174
[2025-09-12 18:18:46,579][flp2p.graph_runner][INFO] - Test, Round 106 : loss => 1.908910866343975,  accuracy: 0.3201
[2025-09-12 18:18:57,435][flp2p.graph_runner][INFO] - Train, Round 107 : loss => 1.6413710176944734,  accuracy: 0.41716, gradient_norm : 4.404782855782075
[2025-09-12 18:19:26,236][flp2p.graph_runner][INFO] - Test, Round 107 : loss => 1.9097355553388595,  accuracy: 0.3203
[2025-09-12 18:19:37,044][flp2p.graph_runner][INFO] - Train, Round 108 : loss => 1.6314968682825566,  accuracy: 0.4205, gradient_norm : 4.156129002736799
[2025-09-12 18:20:02,625][flp2p.graph_runner][INFO] - Test, Round 108 : loss => 1.9620030245482922,  accuracy: 0.3113
[2025-09-12 18:20:13,573][flp2p.graph_runner][INFO] - Train, Round 109 : loss => 1.6298975747823716,  accuracy: 0.42246, gradient_norm : 4.477151411748035
[2025-09-12 18:20:32,214][flp2p.graph_runner][INFO] - Test, Round 109 : loss => 1.920169128847122,  accuracy: 0.3177
[2025-09-12 18:20:43,063][flp2p.graph_runner][INFO] - Train, Round 110 : loss => 1.6120934490859509,  accuracy: 0.42884, gradient_norm : 4.348114374076933
[2025-09-12 18:21:01,707][flp2p.graph_runner][INFO] - Test, Round 110 : loss => 2.123821483206749,  accuracy: 0.276
[2025-09-12 18:21:12,362][flp2p.graph_runner][INFO] - Train, Round 111 : loss => 1.6373912793397905,  accuracy: 0.41906, gradient_norm : 4.471538159265806
[2025-09-12 18:21:35,778][flp2p.graph_runner][INFO] - Test, Round 111 : loss => 1.911594729936123,  accuracy: 0.3222
[2025-09-12 18:21:46,462][flp2p.graph_runner][INFO] - Train, Round 112 : loss => 1.5946584011614322,  accuracy: 0.43368, gradient_norm : 4.226194955793405
[2025-09-12 18:22:09,878][flp2p.graph_runner][INFO] - Test, Round 112 : loss => 1.9262435514688492,  accuracy: 0.3156
[2025-09-12 18:22:20,508][flp2p.graph_runner][INFO] - Train, Round 113 : loss => 1.5856665793061255,  accuracy: 0.43696, gradient_norm : 4.508060590823074
[2025-09-12 18:22:32,357][flp2p.graph_runner][INFO] - Test, Round 113 : loss => 1.961643666601181,  accuracy: 0.3106
[2025-09-12 18:22:43,273][flp2p.graph_runner][INFO] - Train, Round 114 : loss => 1.580916942358017,  accuracy: 0.44208, gradient_norm : 4.60263405852872
[2025-09-12 18:22:54,748][flp2p.graph_runner][INFO] - Test, Round 114 : loss => 1.9078221089601517,  accuracy: 0.3172
[2025-09-12 18:23:05,796][flp2p.graph_runner][INFO] - Train, Round 115 : loss => 1.5615733876824378,  accuracy: 0.44366, gradient_norm : 4.638515270973438
[2025-09-12 18:23:24,640][flp2p.graph_runner][INFO] - Test, Round 115 : loss => 1.957753487443924,  accuracy: 0.3083
[2025-09-12 18:23:35,465][flp2p.graph_runner][INFO] - Train, Round 116 : loss => 1.5608572685718536,  accuracy: 0.4469, gradient_norm : 4.859081381733526
[2025-09-12 18:24:04,778][flp2p.graph_runner][INFO] - Test, Round 116 : loss => 1.8891133995711804,  accuracy: 0.3304
[2025-09-12 18:24:15,684][flp2p.graph_runner][INFO] - Train, Round 117 : loss => 1.5328868967294693,  accuracy: 0.45494, gradient_norm : 4.682229722575302
[2025-09-12 18:24:38,225][flp2p.graph_runner][INFO] - Test, Round 117 : loss => 1.955154524999857,  accuracy: 0.3189
[2025-09-12 18:24:49,249][flp2p.graph_runner][INFO] - Train, Round 118 : loss => 1.542814581245184,  accuracy: 0.45352, gradient_norm : 4.858051286680082
[2025-09-12 18:25:07,821][flp2p.graph_runner][INFO] - Test, Round 118 : loss => 1.9590740945756435,  accuracy: 0.3134
[2025-09-12 18:25:18,557][flp2p.graph_runner][INFO] - Train, Round 119 : loss => 1.529450239241123,  accuracy: 0.45852, gradient_norm : 4.928343017447084
[2025-09-12 18:25:40,384][flp2p.graph_runner][INFO] - Test, Round 119 : loss => 1.9488554980576038,  accuracy: 0.3164
[2025-09-12 18:25:50,999][flp2p.graph_runner][INFO] - Train, Round 120 : loss => 1.517335301041603,  accuracy: 0.46308, gradient_norm : 4.9320594816636385
[2025-09-12 18:26:16,487][flp2p.graph_runner][INFO] - Test, Round 120 : loss => 1.9241567920625209,  accuracy: 0.3205
[2025-09-12 18:26:27,079][flp2p.graph_runner][INFO] - Train, Round 121 : loss => 1.5018586914241314,  accuracy: 0.46732, gradient_norm : 4.955156544921007
[2025-09-12 18:26:40,857][flp2p.graph_runner][INFO] - Test, Round 121 : loss => 1.9278220917761326,  accuracy: 0.3279
[2025-09-12 18:26:51,550][flp2p.graph_runner][INFO] - Train, Round 122 : loss => 1.4928980252146722,  accuracy: 0.47142, gradient_norm : 5.008919431516866
[2025-09-12 18:27:03,630][flp2p.graph_runner][INFO] - Test, Round 122 : loss => 2.3014941308140755,  accuracy: 0.2777
[2025-09-12 18:27:14,835][flp2p.graph_runner][INFO] - Train, Round 123 : loss => 1.5397487729787827,  accuracy: 0.45738, gradient_norm : 4.908979513639915
[2025-09-12 18:27:27,205][flp2p.graph_runner][INFO] - Test, Round 123 : loss => 2.015499783360958,  accuracy: 0.3165
[2025-09-12 18:27:38,026][flp2p.graph_runner][INFO] - Train, Round 124 : loss => 1.4806289553642273,  accuracy: 0.47522, gradient_norm : 4.948426644123629
[2025-09-12 18:28:08,113][flp2p.graph_runner][INFO] - Test, Round 124 : loss => 2.0022107678711416,  accuracy: 0.3053
[2025-09-12 18:28:18,925][flp2p.graph_runner][INFO] - Train, Round 125 : loss => 1.4690066832304,  accuracy: 0.47834, gradient_norm : 5.145282438498036
[2025-09-12 18:28:48,526][flp2p.graph_runner][INFO] - Test, Round 125 : loss => 1.920720822018385,  accuracy: 0.3232
[2025-09-12 18:28:59,538][flp2p.graph_runner][INFO] - Train, Round 126 : loss => 1.4420462967455387,  accuracy: 0.49036, gradient_norm : 5.313322779283939
[2025-09-12 18:29:22,175][flp2p.graph_runner][INFO] - Test, Round 126 : loss => 1.8870718813180924,  accuracy: 0.3353
[2025-09-12 18:29:33,196][flp2p.graph_runner][INFO] - Train, Round 127 : loss => 1.4308637776970863,  accuracy: 0.4904, gradient_norm : 5.066305603658048
[2025-09-12 18:29:51,523][flp2p.graph_runner][INFO] - Test, Round 127 : loss => 1.984603676569462,  accuracy: 0.3227
[2025-09-12 18:30:02,275][flp2p.graph_runner][INFO] - Train, Round 128 : loss => 1.4266043263673782,  accuracy: 0.49414, gradient_norm : 5.273133271826381
[2025-09-12 18:30:22,612][flp2p.graph_runner][INFO] - Test, Round 128 : loss => 2.1625162554323674,  accuracy: 0.2873
[2025-09-12 18:30:33,283][flp2p.graph_runner][INFO] - Train, Round 129 : loss => 1.4511263845860958,  accuracy: 0.4841, gradient_norm : 5.229140079048369
[2025-09-12 18:30:59,472][flp2p.graph_runner][INFO] - Test, Round 129 : loss => 2.09587059122324,  accuracy: 0.2993
[2025-09-12 18:31:10,135][flp2p.graph_runner][INFO] - Train, Round 130 : loss => 1.4217464484274387,  accuracy: 0.4959, gradient_norm : 5.255991116112323
[2025-09-12 18:31:35,102][flp2p.graph_runner][INFO] - Test, Round 130 : loss => 1.9943976884484291,  accuracy: 0.321
[2025-09-12 18:31:45,719][flp2p.graph_runner][INFO] - Train, Round 131 : loss => 1.3994963836669922,  accuracy: 0.49962, gradient_norm : 5.562458603714405
[2025-09-12 18:31:57,530][flp2p.graph_runner][INFO] - Test, Round 131 : loss => 2.026667627710104,  accuracy: 0.314
[2025-09-12 18:32:08,350][flp2p.graph_runner][INFO] - Train, Round 132 : loss => 1.385998885780573,  accuracy: 0.50858, gradient_norm : 5.393610250596769
[2025-09-12 18:32:19,737][flp2p.graph_runner][INFO] - Test, Round 132 : loss => 1.9690440501511097,  accuracy: 0.3195
[2025-09-12 18:32:30,684][flp2p.graph_runner][INFO] - Train, Round 133 : loss => 1.3707179237902165,  accuracy: 0.51264, gradient_norm : 5.348490240924591
[2025-09-12 18:32:54,565][flp2p.graph_runner][INFO] - Test, Round 133 : loss => 1.9328052525579928,  accuracy: 0.3352
[2025-09-12 18:33:05,480][flp2p.graph_runner][INFO] - Train, Round 134 : loss => 1.350654390603304,  accuracy: 0.52164, gradient_norm : 5.741153978008249
[2025-09-12 18:33:34,285][flp2p.graph_runner][INFO] - Test, Round 134 : loss => 1.9414256958186626,  accuracy: 0.3376
[2025-09-12 18:33:45,414][flp2p.graph_runner][INFO] - Train, Round 135 : loss => 1.3362127009034157,  accuracy: 0.52636, gradient_norm : 5.68005260207442
[2025-09-12 18:34:03,724][flp2p.graph_runner][INFO] - Test, Round 135 : loss => 1.9717261391818524,  accuracy: 0.3298
[2025-09-12 18:34:14,773][flp2p.graph_runner][INFO] - Train, Round 136 : loss => 1.3257006561011075,  accuracy: 0.52948, gradient_norm : 5.709020482020353
[2025-09-12 18:34:34,482][flp2p.graph_runner][INFO] - Test, Round 136 : loss => 1.9724908269584178,  accuracy: 0.331
[2025-09-12 18:34:45,127][flp2p.graph_runner][INFO] - Train, Round 137 : loss => 1.320298668667674,  accuracy: 0.53142, gradient_norm : 5.719735200348924
[2025-09-12 18:35:06,614][flp2p.graph_runner][INFO] - Test, Round 137 : loss => 2.393049597299099,  accuracy: 0.2809
[2025-09-12 18:35:17,200][flp2p.graph_runner][INFO] - Train, Round 138 : loss => 1.3829370756447315,  accuracy: 0.51518, gradient_norm : 5.4966292942913135
[2025-09-12 18:35:42,148][flp2p.graph_runner][INFO] - Test, Round 138 : loss => 1.984548491668701,  accuracy: 0.3277
[2025-09-12 18:35:52,701][flp2p.graph_runner][INFO] - Train, Round 139 : loss => 1.2949647565186024,  accuracy: 0.54188, gradient_norm : 5.81702294819949
[2025-09-12 18:36:04,646][flp2p.graph_runner][INFO] - Test, Round 139 : loss => 1.9952736488938332,  accuracy: 0.3297
[2025-09-12 18:36:15,282][flp2p.graph_runner][INFO] - Train, Round 140 : loss => 1.2787791063636542,  accuracy: 0.5482, gradient_norm : 5.756920716115376
[2025-09-12 18:36:27,002][flp2p.graph_runner][INFO] - Test, Round 140 : loss => 1.9625310784339904,  accuracy: 0.3324
[2025-09-12 18:36:38,136][flp2p.graph_runner][INFO] - Train, Round 141 : loss => 1.2681911359727382,  accuracy: 0.55206, gradient_norm : 5.844102975573939
[2025-09-12 18:36:53,013][flp2p.graph_runner][INFO] - Test, Round 141 : loss => 1.9337284778118133,  accuracy: 0.3457
[2025-09-12 18:37:03,780][flp2p.graph_runner][INFO] - Train, Round 142 : loss => 1.248160232976079,  accuracy: 0.55692, gradient_norm : 6.035949225809959
[2025-09-12 18:37:34,154][flp2p.graph_runner][INFO] - Test, Round 142 : loss => 2.042846746516228,  accuracy: 0.3197
[2025-09-12 18:37:44,836][flp2p.graph_runner][INFO] - Train, Round 143 : loss => 1.2485532200336455,  accuracy: 0.56086, gradient_norm : 5.972912796785567
[2025-09-12 18:38:12,691][flp2p.graph_runner][INFO] - Test, Round 143 : loss => 2.0494952757656573,  accuracy: 0.3295
[2025-09-12 18:38:23,439][flp2p.graph_runner][INFO] - Train, Round 144 : loss => 1.2396846982836722,  accuracy: 0.56056, gradient_norm : 6.0384279457369745
[2025-09-12 18:38:48,371][flp2p.graph_runner][INFO] - Test, Round 144 : loss => 2.0193870615124703,  accuracy: 0.3224
[2025-09-12 18:38:59,334][flp2p.graph_runner][INFO] - Train, Round 145 : loss => 1.2294788416475058,  accuracy: 0.56664, gradient_norm : 6.192762331600615
[2025-09-12 18:39:17,556][flp2p.graph_runner][INFO] - Test, Round 145 : loss => 2.0021818407416343,  accuracy: 0.3315
[2025-09-12 18:39:28,523][flp2p.graph_runner][INFO] - Train, Round 146 : loss => 1.209762434437871,  accuracy: 0.57156, gradient_norm : 6.346305520736281
[2025-09-12 18:39:42,553][flp2p.graph_runner][INFO] - Test, Round 146 : loss => 2.4352837089419364,  accuracy: 0.2795
[2025-09-12 18:39:53,184][flp2p.graph_runner][INFO] - Train, Round 147 : loss => 1.2577783838659524,  accuracy: 0.56072, gradient_norm : 6.155021103680739
[2025-09-12 18:40:07,207][flp2p.graph_runner][INFO] - Test, Round 147 : loss => 2.001227025818825,  accuracy: 0.3419
[2025-09-12 18:40:17,799][flp2p.graph_runner][INFO] - Train, Round 148 : loss => 1.17375182621181,  accuracy: 0.58414, gradient_norm : 6.340714726647796
[2025-09-12 18:40:31,398][flp2p.graph_runner][INFO] - Test, Round 148 : loss => 2.0844582749426364,  accuracy: 0.3292
[2025-09-12 18:40:41,952][flp2p.graph_runner][INFO] - Train, Round 149 : loss => 1.1803674817830325,  accuracy: 0.58146, gradient_norm : 6.199899883266709
[2025-09-12 18:40:56,230][flp2p.graph_runner][INFO] - Test, Round 149 : loss => 2.064983289080858,  accuracy: 0.3264
[2025-09-12 18:41:06,860][flp2p.graph_runner][INFO] - Train, Round 150 : loss => 1.1613325386494397,  accuracy: 0.59222, gradient_norm : 6.400441678549397
[2025-09-12 18:41:19,138][flp2p.graph_runner][INFO] - Test, Round 150 : loss => 2.0470090796649454,  accuracy: 0.3334
[2025-09-12 18:41:29,726][flp2p.graph_runner][INFO] - Train, Round 151 : loss => 1.1566109345853328,  accuracy: 0.59342, gradient_norm : 7.163475321078545
[2025-09-12 18:41:41,710][flp2p.graph_runner][INFO] - Test, Round 151 : loss => 2.0433132691025735,  accuracy: 0.3357
[2025-09-12 18:41:52,431][flp2p.graph_runner][INFO] - Train, Round 152 : loss => 1.1244917213916779,  accuracy: 0.60616, gradient_norm : 6.568594964752136
[2025-09-12 18:42:04,396][flp2p.graph_runner][INFO] - Test, Round 152 : loss => 2.182447874194384,  accuracy: 0.3218
[2025-09-12 18:42:15,224][flp2p.graph_runner][INFO] - Train, Round 153 : loss => 1.134963884651661,  accuracy: 0.6009, gradient_norm : 6.754723973441187
[2025-09-12 18:42:27,171][flp2p.graph_runner][INFO] - Test, Round 153 : loss => 2.179794382715225,  accuracy: 0.3214
[2025-09-12 18:42:38,009][flp2p.graph_runner][INFO] - Train, Round 154 : loss => 1.1331238017231227,  accuracy: 0.60042, gradient_norm : 6.429313913344187
[2025-09-12 18:42:50,088][flp2p.graph_runner][INFO] - Test, Round 154 : loss => 2.204813979268074,  accuracy: 0.3235
[2025-09-12 18:43:01,046][flp2p.graph_runner][INFO] - Train, Round 155 : loss => 1.1153455214202403,  accuracy: 0.61286, gradient_norm : 6.625717017551306
[2025-09-12 18:43:12,853][flp2p.graph_runner][INFO] - Test, Round 155 : loss => 2.0408657877743246,  accuracy: 0.3444
[2025-09-12 18:43:23,745][flp2p.graph_runner][INFO] - Train, Round 156 : loss => 1.066271638646722,  accuracy: 0.6262, gradient_norm : 6.648356455997438
[2025-09-12 18:43:35,505][flp2p.graph_runner][INFO] - Test, Round 156 : loss => 2.1148989457547667,  accuracy: 0.3334
[2025-09-12 18:43:46,574][flp2p.graph_runner][INFO] - Train, Round 157 : loss => 1.0691327982395888,  accuracy: 0.62224, gradient_norm : 6.713240801409301
[2025-09-12 18:43:58,305][flp2p.graph_runner][INFO] - Test, Round 157 : loss => 2.1559800778985023,  accuracy: 0.3293
[2025-09-12 18:44:09,354][flp2p.graph_runner][INFO] - Train, Round 158 : loss => 1.056258222013712,  accuracy: 0.63012, gradient_norm : 6.88890692247778
[2025-09-12 18:44:20,924][flp2p.graph_runner][INFO] - Test, Round 158 : loss => 2.1889069558143617,  accuracy: 0.3181
[2025-09-12 18:44:31,824][flp2p.graph_runner][INFO] - Train, Round 159 : loss => 1.055287859067321,  accuracy: 0.62664, gradient_norm : 7.212761518050811
[2025-09-12 18:44:43,558][flp2p.graph_runner][INFO] - Test, Round 159 : loss => 2.1698183338344097,  accuracy: 0.3351
[2025-09-12 18:44:54,504][flp2p.graph_runner][INFO] - Train, Round 160 : loss => 1.0246713001281023,  accuracy: 0.6423, gradient_norm : 6.809296214346944
[2025-09-12 18:45:06,290][flp2p.graph_runner][INFO] - Test, Round 160 : loss => 2.271298691695929,  accuracy: 0.3204
[2025-09-12 18:45:17,302][flp2p.graph_runner][INFO] - Train, Round 161 : loss => 1.0316174356639385,  accuracy: 0.63926, gradient_norm : 6.944298389716988
[2025-09-12 18:45:28,853][flp2p.graph_runner][INFO] - Test, Round 161 : loss => 2.402123581320047,  accuracy: 0.2981
[2025-09-12 18:45:39,866][flp2p.graph_runner][INFO] - Train, Round 162 : loss => 1.056611212193966,  accuracy: 0.6319, gradient_norm : 6.68241664022655
[2025-09-12 18:45:51,447][flp2p.graph_runner][INFO] - Test, Round 162 : loss => 2.1613304534554483,  accuracy: 0.3345
[2025-09-12 18:46:02,491][flp2p.graph_runner][INFO] - Train, Round 163 : loss => 0.9765640859305859,  accuracy: 0.66112, gradient_norm : 6.882424764796773
[2025-09-12 18:46:14,169][flp2p.graph_runner][INFO] - Test, Round 163 : loss => 2.5080720500469207,  accuracy: 0.2932
[2025-09-12 18:46:25,050][flp2p.graph_runner][INFO] - Train, Round 164 : loss => 1.0313585849106313,  accuracy: 0.64336, gradient_norm : 6.6936082422068175
[2025-09-12 18:46:37,126][flp2p.graph_runner][INFO] - Test, Round 164 : loss => 2.181466547214985,  accuracy: 0.3417
[2025-09-12 18:46:47,930][flp2p.graph_runner][INFO] - Train, Round 165 : loss => 0.9554248039424419,  accuracy: 0.67124, gradient_norm : 7.186972544471447
[2025-09-12 18:47:03,375][flp2p.graph_runner][INFO] - Test, Round 165 : loss => 2.392983939266205,  accuracy: 0.3204
[2025-09-12 18:47:14,214][flp2p.graph_runner][INFO] - Train, Round 166 : loss => 0.9722625732421875,  accuracy: 0.66182, gradient_norm : 7.342409509227815
[2025-09-12 18:47:36,818][flp2p.graph_runner][INFO] - Test, Round 166 : loss => 2.3638434665322303,  accuracy: 0.3091
[2025-09-12 18:47:47,866][flp2p.graph_runner][INFO] - Train, Round 167 : loss => 0.9521685414761305,  accuracy: 0.66792, gradient_norm : 6.999795279882378
[2025-09-12 18:48:03,705][flp2p.graph_runner][INFO] - Test, Round 167 : loss => 2.3103656761467457,  accuracy: 0.326
[2025-09-12 18:48:14,611][flp2p.graph_runner][INFO] - Train, Round 168 : loss => 0.920654718875885,  accuracy: 0.68214, gradient_norm : 6.939581608402027
[2025-09-12 18:48:31,100][flp2p.graph_runner][INFO] - Test, Round 168 : loss => 2.3027171525478365,  accuracy: 0.3263
[2025-09-12 18:48:41,893][flp2p.graph_runner][INFO] - Train, Round 169 : loss => 0.9097144530713558,  accuracy: 0.6834, gradient_norm : 7.79234159787181
[2025-09-12 18:48:59,232][flp2p.graph_runner][INFO] - Test, Round 169 : loss => 2.4369378748714925,  accuracy: 0.3214
[2025-09-12 18:49:09,854][flp2p.graph_runner][INFO] - Train, Round 170 : loss => 0.9055161532759667,  accuracy: 0.69114, gradient_norm : 7.052460980836416
[2025-09-12 18:49:29,705][flp2p.graph_runner][INFO] - Test, Round 170 : loss => 2.1890959563970567,  accuracy: 0.35
[2025-09-12 18:49:40,263][flp2p.graph_runner][INFO] - Train, Round 171 : loss => 0.8424310699477792,  accuracy: 0.71256, gradient_norm : 7.448999994919268
[2025-09-12 18:49:52,211][flp2p.graph_runner][INFO] - Test, Round 171 : loss => 2.273880256921053,  accuracy: 0.3343
[2025-09-12 18:50:02,978][flp2p.graph_runner][INFO] - Train, Round 172 : loss => 0.8502973455935716,  accuracy: 0.70768, gradient_norm : 7.436005889333378
[2025-09-12 18:50:14,921][flp2p.graph_runner][INFO] - Test, Round 172 : loss => 2.4871213009536266,  accuracy: 0.3073
[2025-09-12 18:50:25,905][flp2p.graph_runner][INFO] - Train, Round 173 : loss => 0.8836439989134669,  accuracy: 0.69756, gradient_norm : 7.318974517374438
[2025-09-12 18:50:37,358][flp2p.graph_runner][INFO] - Test, Round 173 : loss => 2.8512445088744163,  accuracy: 0.298
[2025-09-12 18:50:48,316][flp2p.graph_runner][INFO] - Train, Round 174 : loss => 0.9229825332015753,  accuracy: 0.68708, gradient_norm : 7.266693900049967
[2025-09-12 18:51:03,980][flp2p.graph_runner][INFO] - Test, Round 174 : loss => 2.4544248736441134,  accuracy: 0.325
[2025-09-12 18:51:14,725][flp2p.graph_runner][INFO] - Train, Round 175 : loss => 0.8399711778759956,  accuracy: 0.7157, gradient_norm : 7.0375612146712925
[2025-09-12 18:51:32,473][flp2p.graph_runner][INFO] - Test, Round 175 : loss => 2.410853243547678,  accuracy: 0.3171
[2025-09-12 18:51:42,955][flp2p.graph_runner][INFO] - Train, Round 176 : loss => 0.8059886422008276,  accuracy: 0.7231, gradient_norm : 7.264277564388756
[2025-09-12 18:52:05,521][flp2p.graph_runner][INFO] - Test, Round 176 : loss => 2.4486537440001963,  accuracy: 0.326
[2025-09-12 18:52:16,324][flp2p.graph_runner][INFO] - Train, Round 177 : loss => 0.805884734839201,  accuracy: 0.72576, gradient_norm : 7.776864234122741
[2025-09-12 18:52:40,484][flp2p.graph_runner][INFO] - Test, Round 177 : loss => 2.535488091266155,  accuracy: 0.3189
[2025-09-12 18:52:51,477][flp2p.graph_runner][INFO] - Train, Round 178 : loss => 0.7845544865354895,  accuracy: 0.73422, gradient_norm : 7.688766528935786
[2025-09-12 18:53:08,253][flp2p.graph_runner][INFO] - Test, Round 178 : loss => 2.4176806012630463,  accuracy: 0.3402
[2025-09-12 18:53:19,233][flp2p.graph_runner][INFO] - Train, Round 179 : loss => 0.7532546385750174,  accuracy: 0.74422, gradient_norm : 7.332876345687488
[2025-09-12 18:53:36,108][flp2p.graph_runner][INFO] - Test, Round 179 : loss => 2.573159200590849,  accuracy: 0.3086
[2025-09-12 18:53:46,783][flp2p.graph_runner][INFO] - Train, Round 180 : loss => 0.772858099155128,  accuracy: 0.7367, gradient_norm : 7.801444153327362
[2025-09-12 18:54:03,876][flp2p.graph_runner][INFO] - Test, Round 180 : loss => 2.4070019434869288,  accuracy: 0.3311
[2025-09-12 18:54:14,544][flp2p.graph_runner][INFO] - Train, Round 181 : loss => 0.7050532865524292,  accuracy: 0.76774, gradient_norm : 7.413257218046317
[2025-09-12 18:54:37,945][flp2p.graph_runner][INFO] - Test, Round 181 : loss => 2.5151344394743442,  accuracy: 0.3299
[2025-09-12 18:54:48,548][flp2p.graph_runner][INFO] - Train, Round 182 : loss => 0.7161875468119979,  accuracy: 0.75924, gradient_norm : 7.927893038496243
[2025-09-12 18:55:00,529][flp2p.graph_runner][INFO] - Test, Round 182 : loss => 3.0108824016809463,  accuracy: 0.2876
[2025-09-12 18:55:11,279][flp2p.graph_runner][INFO] - Train, Round 183 : loss => 0.8023155552521348,  accuracy: 0.7338, gradient_norm : 6.739196143646325
[2025-09-12 18:55:23,041][flp2p.graph_runner][INFO] - Test, Round 183 : loss => 2.456787014758587,  accuracy: 0.3367
[2025-09-12 18:55:34,251][flp2p.graph_runner][INFO] - Train, Round 184 : loss => 0.6653165294975042,  accuracy: 0.78186, gradient_norm : 7.793421161141582
[2025-09-12 18:55:46,035][flp2p.graph_runner][INFO] - Test, Round 184 : loss => 2.558310067385435,  accuracy: 0.326
[2025-09-12 18:55:56,679][flp2p.graph_runner][INFO] - Train, Round 185 : loss => 0.6640470193326473,  accuracy: 0.78276, gradient_norm : 7.473621735791003
[2025-09-12 18:56:21,261][flp2p.graph_runner][INFO] - Test, Round 185 : loss => 2.841294687640667,  accuracy: 0.3018
[2025-09-12 18:56:31,866][flp2p.graph_runner][INFO] - Train, Round 186 : loss => 0.7264927850663662,  accuracy: 0.7585, gradient_norm : 6.8382653936539874
[2025-09-12 18:56:52,225][flp2p.graph_runner][INFO] - Test, Round 186 : loss => 2.6356470566272736,  accuracy: 0.3148
[2025-09-12 18:57:02,932][flp2p.graph_runner][INFO] - Train, Round 187 : loss => 0.6548317945748567,  accuracy: 0.78518, gradient_norm : 7.322428101917108
[2025-09-12 18:57:27,729][flp2p.graph_runner][INFO] - Test, Round 187 : loss => 3.4454032682180404,  accuracy: 0.2543
[2025-09-12 18:57:38,472][flp2p.graph_runner][INFO] - Train, Round 188 : loss => 0.8497878379002214,  accuracy: 0.72846, gradient_norm : 6.52380738054485
[2025-09-12 18:58:06,091][flp2p.graph_runner][INFO] - Test, Round 188 : loss => 2.6766163626849653,  accuracy: 0.3253
[2025-09-12 18:58:17,056][flp2p.graph_runner][INFO] - Train, Round 189 : loss => 0.6151808200404048,  accuracy: 0.8005, gradient_norm : 7.172242556069357
[2025-09-12 18:58:35,661][flp2p.graph_runner][INFO] - Test, Round 189 : loss => 2.620797480893135,  accuracy: 0.3313
[2025-09-12 18:58:46,660][flp2p.graph_runner][INFO] - Train, Round 190 : loss => 0.5756458816304802,  accuracy: 0.81734, gradient_norm : 7.439557771327035
[2025-09-12 18:59:04,905][flp2p.graph_runner][INFO] - Test, Round 190 : loss => 2.7244012556791306,  accuracy: 0.3217
[2025-09-12 18:59:15,561][flp2p.graph_runner][INFO] - Train, Round 191 : loss => 0.5924944450333715,  accuracy: 0.81222, gradient_norm : 6.943499809290542
[2025-09-12 18:59:34,525][flp2p.graph_runner][INFO] - Test, Round 191 : loss => 2.704419624567032,  accuracy: 0.3227
[2025-09-12 18:59:45,165][flp2p.graph_runner][INFO] - Train, Round 192 : loss => 0.5608154212683439,  accuracy: 0.8231, gradient_norm : 7.15843845713165
[2025-09-12 19:00:00,231][flp2p.graph_runner][INFO] - Test, Round 192 : loss => 2.8621056798577307,  accuracy: 0.3107
[2025-09-12 19:00:10,808][flp2p.graph_runner][INFO] - Train, Round 193 : loss => 0.5840009533241391,  accuracy: 0.81388, gradient_norm : 7.2678048147265235
[2025-09-12 19:00:22,768][flp2p.graph_runner][INFO] - Test, Round 193 : loss => 3.1726310350298883,  accuracy: 0.3165
[2025-09-12 19:00:33,806][flp2p.graph_runner][INFO] - Train, Round 194 : loss => 0.6117072730138898,  accuracy: 0.80642, gradient_norm : 6.369565690803205
[2025-09-12 19:00:45,234][flp2p.graph_runner][INFO] - Test, Round 194 : loss => 2.8392076198577882,  accuracy: 0.3161
[2025-09-12 19:00:56,055][flp2p.graph_runner][INFO] - Train, Round 195 : loss => 0.5360586826317012,  accuracy: 0.82996, gradient_norm : 6.784678252014758
[2025-09-12 19:01:17,872][flp2p.graph_runner][INFO] - Test, Round 195 : loss => 2.9688658639490604,  accuracy: 0.3202
[2025-09-12 19:01:28,488][flp2p.graph_runner][INFO] - Train, Round 196 : loss => 0.5633829934895038,  accuracy: 0.82442, gradient_norm : 7.313337868175828
[2025-09-12 19:01:49,930][flp2p.graph_runner][INFO] - Test, Round 196 : loss => 2.896878086388111,  accuracy: 0.3213
[2025-09-12 19:02:00,410][flp2p.graph_runner][INFO] - Train, Round 197 : loss => 0.5012145583890378,  accuracy: 0.84618, gradient_norm : 6.368206264171781
[2025-09-12 19:02:21,265][flp2p.graph_runner][INFO] - Test, Round 197 : loss => 3.025349916934967,  accuracy: 0.3167
[2025-09-12 19:02:31,849][flp2p.graph_runner][INFO] - Train, Round 198 : loss => 0.5273086559958756,  accuracy: 0.83418, gradient_norm : 6.366832104033622
[2025-09-12 19:02:59,675][flp2p.graph_runner][INFO] - Test, Round 198 : loss => 2.8770769002854824,  accuracy: 0.3331
[2025-09-12 19:03:10,299][flp2p.graph_runner][INFO] - Train, Round 199 : loss => 0.44199283365160225,  accuracy: 0.86788, gradient_norm : 7.012865106576968
[2025-09-12 19:03:42,370][flp2p.graph_runner][INFO] - Test, Round 199 : loss => 2.8774036774635317,  accuracy: 0.3334
[2025-09-12 19:03:42,374][__main__][INFO] - Train, Round 001: loss=2.3053, accuracy=0.1024, gradient_norm=0.1797, 
[2025-09-12 19:03:42,374][__main__][INFO] - Train, Round 002: loss=2.3048, accuracy=0.1048, gradient_norm=0.1742, 
[2025-09-12 19:03:42,374][__main__][INFO] - Train, Round 003: loss=2.3042, accuracy=0.1069, gradient_norm=0.1732, 
[2025-09-12 19:03:42,374][__main__][INFO] - Train, Round 004: loss=2.3037, accuracy=0.1095, gradient_norm=0.1733, 
[2025-09-12 19:03:42,374][__main__][INFO] - Train, Round 005: loss=2.3032, accuracy=0.1118, gradient_norm=0.1808, 
[2025-09-12 19:03:42,374][__main__][INFO] - Train, Round 006: loss=2.3027, accuracy=0.1149, gradient_norm=0.1754, 
[2025-09-12 19:03:42,374][__main__][INFO] - Train, Round 007: loss=2.3022, accuracy=0.1171, gradient_norm=0.1752, 
[2025-09-12 19:03:42,374][__main__][INFO] - Train, Round 008: loss=2.3017, accuracy=0.1190, gradient_norm=0.1810, 
[2025-09-12 19:03:42,374][__main__][INFO] - Train, Round 009: loss=2.3012, accuracy=0.1212, gradient_norm=0.1721, 
[2025-09-12 19:03:42,374][__main__][INFO] - Train, Round 010: loss=2.3008, accuracy=0.1217, gradient_norm=0.1804, 
[2025-09-12 19:03:42,374][__main__][INFO] - Train, Round 011: loss=2.3003, accuracy=0.1234, gradient_norm=0.1766, 
[2025-09-12 19:03:42,374][__main__][INFO] - Train, Round 012: loss=2.2998, accuracy=0.1250, gradient_norm=0.1790, 
[2025-09-12 19:03:42,374][__main__][INFO] - Train, Round 013: loss=2.2993, accuracy=0.1258, gradient_norm=0.1815, 
[2025-09-12 19:03:42,374][__main__][INFO] - Train, Round 014: loss=2.2988, accuracy=0.1272, gradient_norm=0.1770, 
[2025-09-12 19:03:42,375][__main__][INFO] - Train, Round 015: loss=2.2983, accuracy=0.1281, gradient_norm=0.1790, 
[2025-09-12 19:03:42,375][__main__][INFO] - Train, Round 016: loss=2.2978, accuracy=0.1296, gradient_norm=0.1855, 
[2025-09-12 19:03:42,375][__main__][INFO] - Train, Round 017: loss=2.2973, accuracy=0.1314, gradient_norm=0.1832, 
[2025-09-12 19:03:42,375][__main__][INFO] - Train, Round 018: loss=2.2967, accuracy=0.1317, gradient_norm=0.1875, 
[2025-09-12 19:03:42,375][__main__][INFO] - Train, Round 019: loss=2.2961, accuracy=0.1327, gradient_norm=0.1881, 
[2025-09-12 19:03:42,375][__main__][INFO] - Train, Round 020: loss=2.2955, accuracy=0.1346, gradient_norm=0.1899, 
[2025-09-12 19:03:42,375][__main__][INFO] - Train, Round 021: loss=2.2949, accuracy=0.1360, gradient_norm=0.1881, 
[2025-09-12 19:03:42,375][__main__][INFO] - Train, Round 022: loss=2.2942, accuracy=0.1380, gradient_norm=0.1959, 
[2025-09-12 19:03:42,375][__main__][INFO] - Train, Round 023: loss=2.2935, accuracy=0.1399, gradient_norm=0.1911, 
[2025-09-12 19:03:42,375][__main__][INFO] - Train, Round 024: loss=2.2928, accuracy=0.1407, gradient_norm=0.2002, 
[2025-09-12 19:03:42,375][__main__][INFO] - Train, Round 025: loss=2.2920, accuracy=0.1422, gradient_norm=0.2045, 
[2025-09-12 19:03:42,375][__main__][INFO] - Train, Round 026: loss=2.2911, accuracy=0.1432, gradient_norm=0.2128, 
[2025-09-12 19:03:42,375][__main__][INFO] - Train, Round 027: loss=2.2902, accuracy=0.1457, gradient_norm=0.2120, 
[2025-09-12 19:03:42,375][__main__][INFO] - Train, Round 028: loss=2.2892, accuracy=0.1478, gradient_norm=0.2218, 
[2025-09-12 19:03:42,375][__main__][INFO] - Train, Round 029: loss=2.2881, accuracy=0.1502, gradient_norm=0.2254, 
[2025-09-12 19:03:42,375][__main__][INFO] - Train, Round 030: loss=2.2869, accuracy=0.1517, gradient_norm=0.2308, 
[2025-09-12 19:03:42,375][__main__][INFO] - Train, Round 031: loss=2.2856, accuracy=0.1541, gradient_norm=0.2330, 
[2025-09-12 19:03:42,375][__main__][INFO] - Train, Round 032: loss=2.2842, accuracy=0.1561, gradient_norm=0.2411, 
[2025-09-12 19:03:42,375][__main__][INFO] - Train, Round 033: loss=2.2826, accuracy=0.1575, gradient_norm=0.2530, 
[2025-09-12 19:03:42,375][__main__][INFO] - Train, Round 034: loss=2.2809, accuracy=0.1591, gradient_norm=0.2685, 
[2025-09-12 19:03:42,375][__main__][INFO] - Train, Round 035: loss=2.2790, accuracy=0.1611, gradient_norm=0.2761, 
[2025-09-12 19:03:42,375][__main__][INFO] - Train, Round 036: loss=2.2768, accuracy=0.1628, gradient_norm=0.2830, 
[2025-09-12 19:03:42,375][__main__][INFO] - Train, Round 037: loss=2.2743, accuracy=0.1643, gradient_norm=0.3037, 
[2025-09-12 19:03:42,375][__main__][INFO] - Train, Round 038: loss=2.2715, accuracy=0.1657, gradient_norm=0.3227, 
[2025-09-12 19:03:42,375][__main__][INFO] - Train, Round 039: loss=2.2684, accuracy=0.1678, gradient_norm=0.3295, 
[2025-09-12 19:03:42,375][__main__][INFO] - Train, Round 040: loss=2.2649, accuracy=0.1695, gradient_norm=0.3664, 
[2025-09-12 19:03:42,375][__main__][INFO] - Train, Round 041: loss=2.2610, accuracy=0.1722, gradient_norm=0.3913, 
[2025-09-12 19:03:42,375][__main__][INFO] - Train, Round 042: loss=2.2565, accuracy=0.1741, gradient_norm=0.4021, 
[2025-09-12 19:03:42,375][__main__][INFO] - Train, Round 043: loss=2.2514, accuracy=0.1756, gradient_norm=0.4416, 
[2025-09-12 19:03:42,375][__main__][INFO] - Train, Round 044: loss=2.2455, accuracy=0.1785, gradient_norm=0.4724, 
[2025-09-12 19:03:42,375][__main__][INFO] - Train, Round 045: loss=2.2390, accuracy=0.1805, gradient_norm=0.5001, 
[2025-09-12 19:03:42,375][__main__][INFO] - Train, Round 046: loss=2.2317, accuracy=0.1843, gradient_norm=0.5409, 
[2025-09-12 19:03:42,375][__main__][INFO] - Train, Round 047: loss=2.2234, accuracy=0.1870, gradient_norm=0.5923, 
[2025-09-12 19:03:42,375][__main__][INFO] - Train, Round 048: loss=2.2140, accuracy=0.1894, gradient_norm=0.6263, 
[2025-09-12 19:03:42,375][__main__][INFO] - Train, Round 049: loss=2.2037, accuracy=0.1933, gradient_norm=0.6881, 
[2025-09-12 19:03:42,375][__main__][INFO] - Train, Round 050: loss=2.1927, accuracy=0.1947, gradient_norm=0.7432, 
[2025-09-12 19:03:42,375][__main__][INFO] - Train, Round 051: loss=2.1811, accuracy=0.1989, gradient_norm=0.7908, 
[2025-09-12 19:03:42,375][__main__][INFO] - Train, Round 052: loss=2.1687, accuracy=0.2014, gradient_norm=0.9055, 
[2025-09-12 19:03:42,375][__main__][INFO] - Train, Round 053: loss=2.1562, accuracy=0.2044, gradient_norm=0.9502, 
[2025-09-12 19:03:42,375][__main__][INFO] - Train, Round 054: loss=2.1436, accuracy=0.2105, gradient_norm=1.0325, 
[2025-09-12 19:03:42,375][__main__][INFO] - Train, Round 055: loss=2.1313, accuracy=0.2179, gradient_norm=1.0571, 
[2025-09-12 19:03:42,375][__main__][INFO] - Train, Round 056: loss=2.1197, accuracy=0.2193, gradient_norm=1.1794, 
[2025-09-12 19:03:42,375][__main__][INFO] - Train, Round 057: loss=2.1076, accuracy=0.2293, gradient_norm=1.2815, 
[2025-09-12 19:03:42,375][__main__][INFO] - Train, Round 058: loss=2.0963, accuracy=0.2361, gradient_norm=1.3063, 
[2025-09-12 19:03:42,375][__main__][INFO] - Train, Round 059: loss=2.0854, accuracy=0.2425, gradient_norm=1.4590, 
[2025-09-12 19:03:42,375][__main__][INFO] - Train, Round 060: loss=2.0742, accuracy=0.2504, gradient_norm=1.4733, 
[2025-09-12 19:03:42,375][__main__][INFO] - Train, Round 061: loss=2.0627, accuracy=0.2585, gradient_norm=1.5364, 
[2025-09-12 19:03:42,375][__main__][INFO] - Train, Round 062: loss=2.0507, accuracy=0.2672, gradient_norm=1.5907, 
[2025-09-12 19:03:42,375][__main__][INFO] - Train, Round 063: loss=2.0441, accuracy=0.2725, gradient_norm=1.6985, 
[2025-09-12 19:03:42,375][__main__][INFO] - Train, Round 064: loss=2.0271, accuracy=0.2809, gradient_norm=1.7868, 
[2025-09-12 19:03:42,375][__main__][INFO] - Train, Round 065: loss=2.0150, accuracy=0.2900, gradient_norm=1.8483, 
[2025-09-12 19:03:42,375][__main__][INFO] - Train, Round 066: loss=2.0039, accuracy=0.2905, gradient_norm=1.9567, 
[2025-09-12 19:03:42,375][__main__][INFO] - Train, Round 067: loss=1.9910, accuracy=0.2962, gradient_norm=1.9772, 
[2025-09-12 19:03:42,375][__main__][INFO] - Train, Round 068: loss=1.9801, accuracy=0.3035, gradient_norm=2.1665, 
[2025-09-12 19:03:42,375][__main__][INFO] - Train, Round 069: loss=1.9710, accuracy=0.3048, gradient_norm=2.1779, 
[2025-09-12 19:03:42,375][__main__][INFO] - Train, Round 070: loss=1.9569, accuracy=0.3102, gradient_norm=2.3164, 
[2025-09-12 19:03:42,375][__main__][INFO] - Train, Round 071: loss=1.9456, accuracy=0.3138, gradient_norm=2.4214, 
[2025-09-12 19:03:42,375][__main__][INFO] - Train, Round 072: loss=1.9334, accuracy=0.3182, gradient_norm=2.4391, 
[2025-09-12 19:03:42,375][__main__][INFO] - Train, Round 073: loss=1.9267, accuracy=0.3191, gradient_norm=2.4479, 
[2025-09-12 19:03:42,375][__main__][INFO] - Train, Round 074: loss=1.9126, accuracy=0.3217, gradient_norm=2.5795, 
[2025-09-12 19:03:42,375][__main__][INFO] - Train, Round 075: loss=1.9068, accuracy=0.3226, gradient_norm=2.5457, 
[2025-09-12 19:03:42,375][__main__][INFO] - Train, Round 076: loss=1.8965, accuracy=0.3244, gradient_norm=2.8994, 
[2025-09-12 19:03:42,375][__main__][INFO] - Train, Round 077: loss=1.8892, accuracy=0.3277, gradient_norm=2.8711, 
[2025-09-12 19:03:42,375][__main__][INFO] - Train, Round 078: loss=1.8840, accuracy=0.3287, gradient_norm=3.1349, 
[2025-09-12 19:03:42,375][__main__][INFO] - Train, Round 079: loss=1.8739, accuracy=0.3323, gradient_norm=3.1286, 
[2025-09-12 19:03:42,376][__main__][INFO] - Train, Round 080: loss=1.8776, accuracy=0.3312, gradient_norm=2.9396, 
[2025-09-12 19:03:42,376][__main__][INFO] - Train, Round 081: loss=1.8545, accuracy=0.3385, gradient_norm=3.1659, 
[2025-09-12 19:03:42,376][__main__][INFO] - Train, Round 082: loss=1.8551, accuracy=0.3388, gradient_norm=3.0370, 
[2025-09-12 19:03:42,376][__main__][INFO] - Train, Round 083: loss=1.8403, accuracy=0.3423, gradient_norm=3.1594, 
[2025-09-12 19:03:42,376][__main__][INFO] - Train, Round 084: loss=1.8344, accuracy=0.3453, gradient_norm=3.2184, 
[2025-09-12 19:03:42,376][__main__][INFO] - Train, Round 085: loss=1.8306, accuracy=0.3460, gradient_norm=3.3460, 
[2025-09-12 19:03:42,376][__main__][INFO] - Train, Round 086: loss=1.8418, accuracy=0.3442, gradient_norm=3.0926, 
[2025-09-12 19:03:42,376][__main__][INFO] - Train, Round 087: loss=1.8192, accuracy=0.3501, gradient_norm=3.2358, 
[2025-09-12 19:03:42,376][__main__][INFO] - Train, Round 088: loss=1.8059, accuracy=0.3561, gradient_norm=3.5066, 
[2025-09-12 19:03:42,376][__main__][INFO] - Train, Round 089: loss=1.7996, accuracy=0.3572, gradient_norm=3.4863, 
[2025-09-12 19:03:42,376][__main__][INFO] - Train, Round 090: loss=1.8131, accuracy=0.3551, gradient_norm=3.4217, 
[2025-09-12 19:03:42,376][__main__][INFO] - Train, Round 091: loss=1.7836, accuracy=0.3618, gradient_norm=3.6025, 
[2025-09-12 19:03:42,376][__main__][INFO] - Train, Round 092: loss=1.7745, accuracy=0.3661, gradient_norm=3.6477, 
[2025-09-12 19:03:42,376][__main__][INFO] - Train, Round 093: loss=1.7651, accuracy=0.3683, gradient_norm=3.5993, 
[2025-09-12 19:03:42,376][__main__][INFO] - Train, Round 094: loss=1.7658, accuracy=0.3711, gradient_norm=3.6968, 
[2025-09-12 19:03:42,376][__main__][INFO] - Train, Round 095: loss=1.7605, accuracy=0.3745, gradient_norm=3.9484, 
[2025-09-12 19:03:42,376][__main__][INFO] - Train, Round 096: loss=1.7496, accuracy=0.3751, gradient_norm=3.7264, 
[2025-09-12 19:03:42,376][__main__][INFO] - Train, Round 097: loss=1.7363, accuracy=0.3811, gradient_norm=3.5798, 
[2025-09-12 19:03:42,376][__main__][INFO] - Train, Round 098: loss=1.7255, accuracy=0.3839, gradient_norm=3.7423, 
[2025-09-12 19:03:42,376][__main__][INFO] - Train, Round 099: loss=1.7217, accuracy=0.3837, gradient_norm=3.9222, 
[2025-09-12 19:03:42,376][__main__][INFO] - Train, Round 100: loss=1.7159, accuracy=0.3903, gradient_norm=3.9240, 
[2025-09-12 19:03:42,376][__main__][INFO] - Train, Round 101: loss=1.7099, accuracy=0.3921, gradient_norm=3.9761, 
[2025-09-12 19:03:42,376][__main__][INFO] - Train, Round 102: loss=1.6929, accuracy=0.3965, gradient_norm=3.9790, 
[2025-09-12 19:03:42,376][__main__][INFO] - Train, Round 103: loss=1.6873, accuracy=0.4005, gradient_norm=4.0194, 
[2025-09-12 19:03:42,376][__main__][INFO] - Train, Round 104: loss=1.6762, accuracy=0.4048, gradient_norm=4.2338, 
[2025-09-12 19:03:42,376][__main__][INFO] - Train, Round 105: loss=1.6789, accuracy=0.4027, gradient_norm=4.2590, 
[2025-09-12 19:03:42,376][__main__][INFO] - Train, Round 106: loss=1.6614, accuracy=0.4110, gradient_norm=4.1605, 
[2025-09-12 19:03:42,376][__main__][INFO] - Train, Round 107: loss=1.6577, accuracy=0.4102, gradient_norm=4.1774, 
[2025-09-12 19:03:42,376][__main__][INFO] - Train, Round 108: loss=1.6414, accuracy=0.4172, gradient_norm=4.4048, 
[2025-09-12 19:03:42,376][__main__][INFO] - Train, Round 109: loss=1.6315, accuracy=0.4205, gradient_norm=4.1561, 
[2025-09-12 19:03:42,376][__main__][INFO] - Train, Round 110: loss=1.6299, accuracy=0.4225, gradient_norm=4.4772, 
[2025-09-12 19:03:42,376][__main__][INFO] - Train, Round 111: loss=1.6121, accuracy=0.4288, gradient_norm=4.3481, 
[2025-09-12 19:03:42,376][__main__][INFO] - Train, Round 112: loss=1.6374, accuracy=0.4191, gradient_norm=4.4715, 
[2025-09-12 19:03:42,376][__main__][INFO] - Train, Round 113: loss=1.5947, accuracy=0.4337, gradient_norm=4.2262, 
[2025-09-12 19:03:42,376][__main__][INFO] - Train, Round 114: loss=1.5857, accuracy=0.4370, gradient_norm=4.5081, 
[2025-09-12 19:03:42,376][__main__][INFO] - Train, Round 115: loss=1.5809, accuracy=0.4421, gradient_norm=4.6026, 
[2025-09-12 19:03:42,376][__main__][INFO] - Train, Round 116: loss=1.5616, accuracy=0.4437, gradient_norm=4.6385, 
[2025-09-12 19:03:42,376][__main__][INFO] - Train, Round 117: loss=1.5609, accuracy=0.4469, gradient_norm=4.8591, 
[2025-09-12 19:03:42,376][__main__][INFO] - Train, Round 118: loss=1.5329, accuracy=0.4549, gradient_norm=4.6822, 
[2025-09-12 19:03:42,376][__main__][INFO] - Train, Round 119: loss=1.5428, accuracy=0.4535, gradient_norm=4.8581, 
[2025-09-12 19:03:42,376][__main__][INFO] - Train, Round 120: loss=1.5295, accuracy=0.4585, gradient_norm=4.9283, 
[2025-09-12 19:03:42,376][__main__][INFO] - Train, Round 121: loss=1.5173, accuracy=0.4631, gradient_norm=4.9321, 
[2025-09-12 19:03:42,376][__main__][INFO] - Train, Round 122: loss=1.5019, accuracy=0.4673, gradient_norm=4.9552, 
[2025-09-12 19:03:42,376][__main__][INFO] - Train, Round 123: loss=1.4929, accuracy=0.4714, gradient_norm=5.0089, 
[2025-09-12 19:03:42,376][__main__][INFO] - Train, Round 124: loss=1.5397, accuracy=0.4574, gradient_norm=4.9090, 
[2025-09-12 19:03:42,376][__main__][INFO] - Train, Round 125: loss=1.4806, accuracy=0.4752, gradient_norm=4.9484, 
[2025-09-12 19:03:42,376][__main__][INFO] - Train, Round 126: loss=1.4690, accuracy=0.4783, gradient_norm=5.1453, 
[2025-09-12 19:03:42,376][__main__][INFO] - Train, Round 127: loss=1.4420, accuracy=0.4904, gradient_norm=5.3133, 
[2025-09-12 19:03:42,376][__main__][INFO] - Train, Round 128: loss=1.4309, accuracy=0.4904, gradient_norm=5.0663, 
[2025-09-12 19:03:42,376][__main__][INFO] - Train, Round 129: loss=1.4266, accuracy=0.4941, gradient_norm=5.2731, 
[2025-09-12 19:03:42,376][__main__][INFO] - Train, Round 130: loss=1.4511, accuracy=0.4841, gradient_norm=5.2291, 
[2025-09-12 19:03:42,376][__main__][INFO] - Train, Round 131: loss=1.4217, accuracy=0.4959, gradient_norm=5.2560, 
[2025-09-12 19:03:42,376][__main__][INFO] - Train, Round 132: loss=1.3995, accuracy=0.4996, gradient_norm=5.5625, 
[2025-09-12 19:03:42,376][__main__][INFO] - Train, Round 133: loss=1.3860, accuracy=0.5086, gradient_norm=5.3936, 
[2025-09-12 19:03:42,376][__main__][INFO] - Train, Round 134: loss=1.3707, accuracy=0.5126, gradient_norm=5.3485, 
[2025-09-12 19:03:42,376][__main__][INFO] - Train, Round 135: loss=1.3507, accuracy=0.5216, gradient_norm=5.7412, 
[2025-09-12 19:03:42,376][__main__][INFO] - Train, Round 136: loss=1.3362, accuracy=0.5264, gradient_norm=5.6801, 
[2025-09-12 19:03:42,376][__main__][INFO] - Train, Round 137: loss=1.3257, accuracy=0.5295, gradient_norm=5.7090, 
[2025-09-12 19:03:42,376][__main__][INFO] - Train, Round 138: loss=1.3203, accuracy=0.5314, gradient_norm=5.7197, 
[2025-09-12 19:03:42,376][__main__][INFO] - Train, Round 139: loss=1.3829, accuracy=0.5152, gradient_norm=5.4966, 
[2025-09-12 19:03:42,376][__main__][INFO] - Train, Round 140: loss=1.2950, accuracy=0.5419, gradient_norm=5.8170, 
[2025-09-12 19:03:42,376][__main__][INFO] - Train, Round 141: loss=1.2788, accuracy=0.5482, gradient_norm=5.7569, 
[2025-09-12 19:03:42,376][__main__][INFO] - Train, Round 142: loss=1.2682, accuracy=0.5521, gradient_norm=5.8441, 
[2025-09-12 19:03:42,376][__main__][INFO] - Train, Round 143: loss=1.2482, accuracy=0.5569, gradient_norm=6.0359, 
[2025-09-12 19:03:42,376][__main__][INFO] - Train, Round 144: loss=1.2486, accuracy=0.5609, gradient_norm=5.9729, 
[2025-09-12 19:03:42,376][__main__][INFO] - Train, Round 145: loss=1.2397, accuracy=0.5606, gradient_norm=6.0384, 
[2025-09-12 19:03:42,376][__main__][INFO] - Train, Round 146: loss=1.2295, accuracy=0.5666, gradient_norm=6.1928, 
[2025-09-12 19:03:42,376][__main__][INFO] - Train, Round 147: loss=1.2098, accuracy=0.5716, gradient_norm=6.3463, 
[2025-09-12 19:03:42,376][__main__][INFO] - Train, Round 148: loss=1.2578, accuracy=0.5607, gradient_norm=6.1550, 
[2025-09-12 19:03:42,377][__main__][INFO] - Train, Round 149: loss=1.1738, accuracy=0.5841, gradient_norm=6.3407, 
[2025-09-12 19:03:42,377][__main__][INFO] - Train, Round 150: loss=1.1804, accuracy=0.5815, gradient_norm=6.1999, 
[2025-09-12 19:03:42,377][__main__][INFO] - Train, Round 151: loss=1.1613, accuracy=0.5922, gradient_norm=6.4004, 
[2025-09-12 19:03:42,377][__main__][INFO] - Train, Round 152: loss=1.1566, accuracy=0.5934, gradient_norm=7.1635, 
[2025-09-12 19:03:42,377][__main__][INFO] - Train, Round 153: loss=1.1245, accuracy=0.6062, gradient_norm=6.5686, 
[2025-09-12 19:03:42,377][__main__][INFO] - Train, Round 154: loss=1.1350, accuracy=0.6009, gradient_norm=6.7547, 
[2025-09-12 19:03:42,377][__main__][INFO] - Train, Round 155: loss=1.1331, accuracy=0.6004, gradient_norm=6.4293, 
[2025-09-12 19:03:42,377][__main__][INFO] - Train, Round 156: loss=1.1153, accuracy=0.6129, gradient_norm=6.6257, 
[2025-09-12 19:03:42,377][__main__][INFO] - Train, Round 157: loss=1.0663, accuracy=0.6262, gradient_norm=6.6484, 
[2025-09-12 19:03:42,377][__main__][INFO] - Train, Round 158: loss=1.0691, accuracy=0.6222, gradient_norm=6.7132, 
[2025-09-12 19:03:42,377][__main__][INFO] - Train, Round 159: loss=1.0563, accuracy=0.6301, gradient_norm=6.8889, 
[2025-09-12 19:03:42,377][__main__][INFO] - Train, Round 160: loss=1.0553, accuracy=0.6266, gradient_norm=7.2128, 
[2025-09-12 19:03:42,377][__main__][INFO] - Train, Round 161: loss=1.0247, accuracy=0.6423, gradient_norm=6.8093, 
[2025-09-12 19:03:42,377][__main__][INFO] - Train, Round 162: loss=1.0316, accuracy=0.6393, gradient_norm=6.9443, 
[2025-09-12 19:03:42,377][__main__][INFO] - Train, Round 163: loss=1.0566, accuracy=0.6319, gradient_norm=6.6824, 
[2025-09-12 19:03:42,377][__main__][INFO] - Train, Round 164: loss=0.9766, accuracy=0.6611, gradient_norm=6.8824, 
[2025-09-12 19:03:42,377][__main__][INFO] - Train, Round 165: loss=1.0314, accuracy=0.6434, gradient_norm=6.6936, 
[2025-09-12 19:03:42,377][__main__][INFO] - Train, Round 166: loss=0.9554, accuracy=0.6712, gradient_norm=7.1870, 
[2025-09-12 19:03:42,377][__main__][INFO] - Train, Round 167: loss=0.9723, accuracy=0.6618, gradient_norm=7.3424, 
[2025-09-12 19:03:42,377][__main__][INFO] - Train, Round 168: loss=0.9522, accuracy=0.6679, gradient_norm=6.9998, 
[2025-09-12 19:03:42,377][__main__][INFO] - Train, Round 169: loss=0.9207, accuracy=0.6821, gradient_norm=6.9396, 
[2025-09-12 19:03:42,377][__main__][INFO] - Train, Round 170: loss=0.9097, accuracy=0.6834, gradient_norm=7.7923, 
[2025-09-12 19:03:42,377][__main__][INFO] - Train, Round 171: loss=0.9055, accuracy=0.6911, gradient_norm=7.0525, 
[2025-09-12 19:03:42,377][__main__][INFO] - Train, Round 172: loss=0.8424, accuracy=0.7126, gradient_norm=7.4490, 
[2025-09-12 19:03:42,377][__main__][INFO] - Train, Round 173: loss=0.8503, accuracy=0.7077, gradient_norm=7.4360, 
[2025-09-12 19:03:42,377][__main__][INFO] - Train, Round 174: loss=0.8836, accuracy=0.6976, gradient_norm=7.3190, 
[2025-09-12 19:03:42,377][__main__][INFO] - Train, Round 175: loss=0.9230, accuracy=0.6871, gradient_norm=7.2667, 
[2025-09-12 19:03:42,377][__main__][INFO] - Train, Round 176: loss=0.8400, accuracy=0.7157, gradient_norm=7.0376, 
[2025-09-12 19:03:42,377][__main__][INFO] - Train, Round 177: loss=0.8060, accuracy=0.7231, gradient_norm=7.2643, 
[2025-09-12 19:03:42,377][__main__][INFO] - Train, Round 178: loss=0.8059, accuracy=0.7258, gradient_norm=7.7769, 
[2025-09-12 19:03:42,377][__main__][INFO] - Train, Round 179: loss=0.7846, accuracy=0.7342, gradient_norm=7.6888, 
[2025-09-12 19:03:42,377][__main__][INFO] - Train, Round 180: loss=0.7533, accuracy=0.7442, gradient_norm=7.3329, 
[2025-09-12 19:03:42,377][__main__][INFO] - Train, Round 181: loss=0.7729, accuracy=0.7367, gradient_norm=7.8014, 
[2025-09-12 19:03:42,377][__main__][INFO] - Train, Round 182: loss=0.7051, accuracy=0.7677, gradient_norm=7.4133, 
[2025-09-12 19:03:42,377][__main__][INFO] - Train, Round 183: loss=0.7162, accuracy=0.7592, gradient_norm=7.9279, 
[2025-09-12 19:03:42,377][__main__][INFO] - Train, Round 184: loss=0.8023, accuracy=0.7338, gradient_norm=6.7392, 
[2025-09-12 19:03:42,377][__main__][INFO] - Train, Round 185: loss=0.6653, accuracy=0.7819, gradient_norm=7.7934, 
[2025-09-12 19:03:42,377][__main__][INFO] - Train, Round 186: loss=0.6640, accuracy=0.7828, gradient_norm=7.4736, 
[2025-09-12 19:03:42,377][__main__][INFO] - Train, Round 187: loss=0.7265, accuracy=0.7585, gradient_norm=6.8383, 
[2025-09-12 19:03:42,377][__main__][INFO] - Train, Round 188: loss=0.6548, accuracy=0.7852, gradient_norm=7.3224, 
[2025-09-12 19:03:42,377][__main__][INFO] - Train, Round 189: loss=0.8498, accuracy=0.7285, gradient_norm=6.5238, 
[2025-09-12 19:03:42,377][__main__][INFO] - Train, Round 190: loss=0.6152, accuracy=0.8005, gradient_norm=7.1722, 
[2025-09-12 19:03:42,377][__main__][INFO] - Train, Round 191: loss=0.5756, accuracy=0.8173, gradient_norm=7.4396, 
[2025-09-12 19:03:42,377][__main__][INFO] - Train, Round 192: loss=0.5925, accuracy=0.8122, gradient_norm=6.9435, 
[2025-09-12 19:03:42,377][__main__][INFO] - Train, Round 193: loss=0.5608, accuracy=0.8231, gradient_norm=7.1584, 
[2025-09-12 19:03:42,377][__main__][INFO] - Train, Round 194: loss=0.5840, accuracy=0.8139, gradient_norm=7.2678, 
[2025-09-12 19:03:42,377][__main__][INFO] - Train, Round 195: loss=0.6117, accuracy=0.8064, gradient_norm=6.3696, 
[2025-09-12 19:03:42,377][__main__][INFO] - Train, Round 196: loss=0.5361, accuracy=0.8300, gradient_norm=6.7847, 
[2025-09-12 19:03:42,377][__main__][INFO] - Train, Round 197: loss=0.5634, accuracy=0.8244, gradient_norm=7.3133, 
[2025-09-12 19:03:42,377][__main__][INFO] - Train, Round 198: loss=0.5012, accuracy=0.8462, gradient_norm=6.3682, 
[2025-09-12 19:03:42,377][__main__][INFO] - Train, Round 199: loss=0.5273, accuracy=0.8342, gradient_norm=6.3668, 
[2025-09-12 19:03:42,377][__main__][INFO] - Train, Round 200: loss=0.4420, accuracy=0.8679, gradient_norm=7.0129, 
[2025-09-12 19:03:42,377][__main__][INFO] - Test, Round 001: loss=2.3053, accuracy=0.1011, 
[2025-09-12 19:03:42,377][__main__][INFO] - Test, Round 002: loss=2.3047, accuracy=0.1036, 
[2025-09-12 19:03:42,377][__main__][INFO] - Test, Round 003: loss=2.3042, accuracy=0.1100, 
[2025-09-12 19:03:42,377][__main__][INFO] - Test, Round 004: loss=2.3038, accuracy=0.1131, 
[2025-09-12 19:03:42,377][__main__][INFO] - Test, Round 005: loss=2.3033, accuracy=0.1155, 
[2025-09-12 19:03:42,377][__main__][INFO] - Test, Round 006: loss=2.3028, accuracy=0.1155, 
[2025-09-12 19:03:42,377][__main__][INFO] - Test, Round 007: loss=2.3024, accuracy=0.1167, 
[2025-09-12 19:03:42,377][__main__][INFO] - Test, Round 008: loss=2.3019, accuracy=0.1192, 
[2025-09-12 19:03:42,377][__main__][INFO] - Test, Round 009: loss=2.3015, accuracy=0.1228, 
[2025-09-12 19:03:42,377][__main__][INFO] - Test, Round 010: loss=2.3010, accuracy=0.1248, 
[2025-09-12 19:03:42,377][__main__][INFO] - Test, Round 011: loss=2.3006, accuracy=0.1259, 
[2025-09-12 19:03:42,377][__main__][INFO] - Test, Round 012: loss=2.3001, accuracy=0.1265, 
[2025-09-12 19:03:42,377][__main__][INFO] - Test, Round 013: loss=2.2996, accuracy=0.1284, 
[2025-09-12 19:03:42,377][__main__][INFO] - Test, Round 014: loss=2.2992, accuracy=0.1303, 
[2025-09-12 19:03:42,377][__main__][INFO] - Test, Round 015: loss=2.2987, accuracy=0.1309, 
[2025-09-12 19:03:42,377][__main__][INFO] - Test, Round 016: loss=2.2981, accuracy=0.1339, 
[2025-09-12 19:03:42,377][__main__][INFO] - Test, Round 017: loss=2.2976, accuracy=0.1323, 
[2025-09-12 19:03:42,378][__main__][INFO] - Test, Round 018: loss=2.2971, accuracy=0.1325, 
[2025-09-12 19:03:42,378][__main__][INFO] - Test, Round 019: loss=2.2965, accuracy=0.1329, 
[2025-09-12 19:03:42,378][__main__][INFO] - Test, Round 020: loss=2.2960, accuracy=0.1335, 
[2025-09-12 19:03:42,378][__main__][INFO] - Test, Round 021: loss=2.2954, accuracy=0.1343, 
[2025-09-12 19:03:42,378][__main__][INFO] - Test, Round 022: loss=2.2947, accuracy=0.1339, 
[2025-09-12 19:03:42,378][__main__][INFO] - Test, Round 023: loss=2.2941, accuracy=0.1353, 
[2025-09-12 19:03:42,378][__main__][INFO] - Test, Round 024: loss=2.2933, accuracy=0.1373, 
[2025-09-12 19:03:42,378][__main__][INFO] - Test, Round 025: loss=2.2925, accuracy=0.1389, 
[2025-09-12 19:03:42,378][__main__][INFO] - Test, Round 026: loss=2.2917, accuracy=0.1411, 
[2025-09-12 19:03:42,378][__main__][INFO] - Test, Round 027: loss=2.2908, accuracy=0.1419, 
[2025-09-12 19:03:42,378][__main__][INFO] - Test, Round 028: loss=2.2898, accuracy=0.1461, 
[2025-09-12 19:03:42,378][__main__][INFO] - Test, Round 029: loss=2.2887, accuracy=0.1486, 
[2025-09-12 19:03:42,378][__main__][INFO] - Test, Round 030: loss=2.2875, accuracy=0.1485, 
[2025-09-12 19:03:42,378][__main__][INFO] - Test, Round 031: loss=2.2863, accuracy=0.1507, 
[2025-09-12 19:03:42,378][__main__][INFO] - Test, Round 032: loss=2.2849, accuracy=0.1508, 
[2025-09-12 19:03:42,378][__main__][INFO] - Test, Round 033: loss=2.2833, accuracy=0.1519, 
[2025-09-12 19:03:42,378][__main__][INFO] - Test, Round 034: loss=2.2816, accuracy=0.1538, 
[2025-09-12 19:03:42,378][__main__][INFO] - Test, Round 035: loss=2.2796, accuracy=0.1565, 
[2025-09-12 19:03:42,378][__main__][INFO] - Test, Round 036: loss=2.2775, accuracy=0.1570, 
[2025-09-12 19:03:42,378][__main__][INFO] - Test, Round 037: loss=2.2750, accuracy=0.1588, 
[2025-09-12 19:03:42,378][__main__][INFO] - Test, Round 038: loss=2.2722, accuracy=0.1590, 
[2025-09-12 19:03:42,378][__main__][INFO] - Test, Round 039: loss=2.2691, accuracy=0.1602, 
[2025-09-12 19:03:42,378][__main__][INFO] - Test, Round 040: loss=2.2656, accuracy=0.1618, 
[2025-09-12 19:03:42,378][__main__][INFO] - Test, Round 041: loss=2.2617, accuracy=0.1639, 
[2025-09-12 19:03:42,378][__main__][INFO] - Test, Round 042: loss=2.2572, accuracy=0.1670, 
[2025-09-12 19:03:42,378][__main__][INFO] - Test, Round 043: loss=2.2520, accuracy=0.1685, 
[2025-09-12 19:03:42,378][__main__][INFO] - Test, Round 044: loss=2.2462, accuracy=0.1705, 
[2025-09-12 19:03:42,378][__main__][INFO] - Test, Round 045: loss=2.2398, accuracy=0.1734, 
[2025-09-12 19:03:42,378][__main__][INFO] - Test, Round 046: loss=2.2326, accuracy=0.1750, 
[2025-09-12 19:03:42,378][__main__][INFO] - Test, Round 047: loss=2.2244, accuracy=0.1790, 
[2025-09-12 19:03:42,378][__main__][INFO] - Test, Round 048: loss=2.2155, accuracy=0.1806, 
[2025-09-12 19:03:42,378][__main__][INFO] - Test, Round 049: loss=2.2057, accuracy=0.1841, 
[2025-09-12 19:03:42,378][__main__][INFO] - Test, Round 050: loss=2.1957, accuracy=0.1873, 
[2025-09-12 19:03:42,378][__main__][INFO] - Test, Round 051: loss=2.1851, accuracy=0.1879, 
[2025-09-12 19:03:42,378][__main__][INFO] - Test, Round 052: loss=2.1742, accuracy=0.1906, 
[2025-09-12 19:03:42,378][__main__][INFO] - Test, Round 053: loss=2.1631, accuracy=0.1964, 
[2025-09-12 19:03:42,378][__main__][INFO] - Test, Round 054: loss=2.1532, accuracy=0.2029, 
[2025-09-12 19:03:42,378][__main__][INFO] - Test, Round 055: loss=2.1434, accuracy=0.2024, 
[2025-09-12 19:03:42,378][__main__][INFO] - Test, Round 056: loss=2.1339, accuracy=0.2145, 
[2025-09-12 19:03:42,378][__main__][INFO] - Test, Round 057: loss=2.1248, accuracy=0.2191, 
[2025-09-12 19:03:42,378][__main__][INFO] - Test, Round 058: loss=2.1155, accuracy=0.2233, 
[2025-09-12 19:03:42,378][__main__][INFO] - Test, Round 059: loss=2.1070, accuracy=0.2300, 
[2025-09-12 19:03:42,378][__main__][INFO] - Test, Round 060: loss=2.0976, accuracy=0.2358, 
[2025-09-12 19:03:42,378][__main__][INFO] - Test, Round 061: loss=2.0899, accuracy=0.2444, 
[2025-09-12 19:03:42,378][__main__][INFO] - Test, Round 062: loss=2.1044, accuracy=0.2403, 
[2025-09-12 19:03:42,378][__main__][INFO] - Test, Round 063: loss=2.0707, accuracy=0.2571, 
[2025-09-12 19:03:42,378][__main__][INFO] - Test, Round 064: loss=2.0640, accuracy=0.2598, 
[2025-09-12 19:03:42,378][__main__][INFO] - Test, Round 065: loss=2.0625, accuracy=0.2593, 
[2025-09-12 19:03:42,378][__main__][INFO] - Test, Round 066: loss=2.0446, accuracy=0.2675, 
[2025-09-12 19:03:42,378][__main__][INFO] - Test, Round 067: loss=2.0370, accuracy=0.2752, 
[2025-09-12 19:03:42,378][__main__][INFO] - Test, Round 068: loss=2.0445, accuracy=0.2759, 
[2025-09-12 19:03:42,378][__main__][INFO] - Test, Round 069: loss=2.0238, accuracy=0.2768, 
[2025-09-12 19:03:42,378][__main__][INFO] - Test, Round 070: loss=2.0120, accuracy=0.2847, 
[2025-09-12 19:03:42,378][__main__][INFO] - Test, Round 071: loss=1.9990, accuracy=0.2853, 
[2025-09-12 19:03:42,378][__main__][INFO] - Test, Round 072: loss=2.0051, accuracy=0.2871, 
[2025-09-12 19:03:42,378][__main__][INFO] - Test, Round 073: loss=1.9885, accuracy=0.2952, 
[2025-09-12 19:03:42,378][__main__][INFO] - Test, Round 074: loss=1.9985, accuracy=0.2840, 
[2025-09-12 19:03:42,378][__main__][INFO] - Test, Round 075: loss=1.9826, accuracy=0.2841, 
[2025-09-12 19:03:42,378][__main__][INFO] - Test, Round 076: loss=1.9869, accuracy=0.2906, 
[2025-09-12 19:03:42,378][__main__][INFO] - Test, Round 077: loss=1.9993, accuracy=0.2806, 
[2025-09-12 19:03:42,378][__main__][INFO] - Test, Round 078: loss=1.9960, accuracy=0.2924, 
[2025-09-12 19:03:42,378][__main__][INFO] - Test, Round 079: loss=2.0787, accuracy=0.2713, 
[2025-09-12 19:03:42,378][__main__][INFO] - Test, Round 080: loss=1.9727, accuracy=0.3012, 
[2025-09-12 19:03:42,378][__main__][INFO] - Test, Round 081: loss=2.0177, accuracy=0.2820, 
[2025-09-12 19:03:42,378][__main__][INFO] - Test, Round 082: loss=1.9476, accuracy=0.2996, 
[2025-09-12 19:03:42,378][__main__][INFO] - Test, Round 083: loss=1.9700, accuracy=0.2971, 
[2025-09-12 19:03:42,378][__main__][INFO] - Test, Round 084: loss=1.9824, accuracy=0.2938, 
[2025-09-12 19:03:42,378][__main__][INFO] - Test, Round 085: loss=2.1243, accuracy=0.2626, 
[2025-09-12 19:03:42,378][__main__][INFO] - Test, Round 086: loss=2.0212, accuracy=0.2864, 
[2025-09-12 19:03:42,378][__main__][INFO] - Test, Round 087: loss=1.9615, accuracy=0.3003, 
[2025-09-12 19:03:42,379][__main__][INFO] - Test, Round 088: loss=1.9742, accuracy=0.3037, 
[2025-09-12 19:03:42,379][__main__][INFO] - Test, Round 089: loss=2.0956, accuracy=0.2698, 
[2025-09-12 19:03:42,379][__main__][INFO] - Test, Round 090: loss=1.9589, accuracy=0.3052, 
[2025-09-12 19:03:42,379][__main__][INFO] - Test, Round 091: loss=1.9405, accuracy=0.3093, 
[2025-09-12 19:03:42,379][__main__][INFO] - Test, Round 092: loss=1.9380, accuracy=0.3072, 
[2025-09-12 19:03:42,379][__main__][INFO] - Test, Round 093: loss=1.9792, accuracy=0.2990, 
[2025-09-12 19:03:42,379][__main__][INFO] - Test, Round 094: loss=1.9901, accuracy=0.2957, 
[2025-09-12 19:03:42,379][__main__][INFO] - Test, Round 095: loss=1.9743, accuracy=0.3000, 
[2025-09-12 19:03:42,379][__main__][INFO] - Test, Round 096: loss=1.9466, accuracy=0.3029, 
[2025-09-12 19:03:42,379][__main__][INFO] - Test, Round 097: loss=1.9211, accuracy=0.3098, 
[2025-09-12 19:03:42,379][__main__][INFO] - Test, Round 098: loss=1.9238, accuracy=0.3078, 
[2025-09-12 19:03:42,379][__main__][INFO] - Test, Round 099: loss=1.9568, accuracy=0.3068, 
[2025-09-12 19:03:42,379][__main__][INFO] - Test, Round 100: loss=1.9761, accuracy=0.3043, 
[2025-09-12 19:03:42,379][__main__][INFO] - Test, Round 101: loss=1.9301, accuracy=0.3070, 
[2025-09-12 19:03:42,379][__main__][INFO] - Test, Round 102: loss=1.9299, accuracy=0.3156, 
[2025-09-12 19:03:42,379][__main__][INFO] - Test, Round 103: loss=1.9111, accuracy=0.3109, 
[2025-09-12 19:03:42,379][__main__][INFO] - Test, Round 104: loss=1.9706, accuracy=0.3028, 
[2025-09-12 19:03:42,379][__main__][INFO] - Test, Round 105: loss=1.9095, accuracy=0.3223, 
[2025-09-12 19:03:42,379][__main__][INFO] - Test, Round 106: loss=1.9532, accuracy=0.3094, 
[2025-09-12 19:03:42,379][__main__][INFO] - Test, Round 107: loss=1.9089, accuracy=0.3201, 
[2025-09-12 19:03:42,379][__main__][INFO] - Test, Round 108: loss=1.9097, accuracy=0.3203, 
[2025-09-12 19:03:42,379][__main__][INFO] - Test, Round 109: loss=1.9620, accuracy=0.3113, 
[2025-09-12 19:03:42,379][__main__][INFO] - Test, Round 110: loss=1.9202, accuracy=0.3177, 
[2025-09-12 19:03:42,379][__main__][INFO] - Test, Round 111: loss=2.1238, accuracy=0.2760, 
[2025-09-12 19:03:42,379][__main__][INFO] - Test, Round 112: loss=1.9116, accuracy=0.3222, 
[2025-09-12 19:03:42,379][__main__][INFO] - Test, Round 113: loss=1.9262, accuracy=0.3156, 
[2025-09-12 19:03:42,379][__main__][INFO] - Test, Round 114: loss=1.9616, accuracy=0.3106, 
[2025-09-12 19:03:42,379][__main__][INFO] - Test, Round 115: loss=1.9078, accuracy=0.3172, 
[2025-09-12 19:03:42,379][__main__][INFO] - Test, Round 116: loss=1.9578, accuracy=0.3083, 
[2025-09-12 19:03:42,379][__main__][INFO] - Test, Round 117: loss=1.8891, accuracy=0.3304, 
[2025-09-12 19:03:42,379][__main__][INFO] - Test, Round 118: loss=1.9552, accuracy=0.3189, 
[2025-09-12 19:03:42,379][__main__][INFO] - Test, Round 119: loss=1.9591, accuracy=0.3134, 
[2025-09-12 19:03:42,379][__main__][INFO] - Test, Round 120: loss=1.9489, accuracy=0.3164, 
[2025-09-12 19:03:42,379][__main__][INFO] - Test, Round 121: loss=1.9242, accuracy=0.3205, 
[2025-09-12 19:03:42,379][__main__][INFO] - Test, Round 122: loss=1.9278, accuracy=0.3279, 
[2025-09-12 19:03:42,379][__main__][INFO] - Test, Round 123: loss=2.3015, accuracy=0.2777, 
[2025-09-12 19:03:42,379][__main__][INFO] - Test, Round 124: loss=2.0155, accuracy=0.3165, 
[2025-09-12 19:03:42,379][__main__][INFO] - Test, Round 125: loss=2.0022, accuracy=0.3053, 
[2025-09-12 19:03:42,379][__main__][INFO] - Test, Round 126: loss=1.9207, accuracy=0.3232, 
[2025-09-12 19:03:42,379][__main__][INFO] - Test, Round 127: loss=1.8871, accuracy=0.3353, 
[2025-09-12 19:03:42,379][__main__][INFO] - Test, Round 128: loss=1.9846, accuracy=0.3227, 
[2025-09-12 19:03:42,379][__main__][INFO] - Test, Round 129: loss=2.1625, accuracy=0.2873, 
[2025-09-12 19:03:42,379][__main__][INFO] - Test, Round 130: loss=2.0959, accuracy=0.2993, 
[2025-09-12 19:03:42,379][__main__][INFO] - Test, Round 131: loss=1.9944, accuracy=0.3210, 
[2025-09-12 19:03:42,379][__main__][INFO] - Test, Round 132: loss=2.0267, accuracy=0.3140, 
[2025-09-12 19:03:42,379][__main__][INFO] - Test, Round 133: loss=1.9690, accuracy=0.3195, 
[2025-09-12 19:03:42,379][__main__][INFO] - Test, Round 134: loss=1.9328, accuracy=0.3352, 
[2025-09-12 19:03:42,379][__main__][INFO] - Test, Round 135: loss=1.9414, accuracy=0.3376, 
[2025-09-12 19:03:42,379][__main__][INFO] - Test, Round 136: loss=1.9717, accuracy=0.3298, 
[2025-09-12 19:03:42,379][__main__][INFO] - Test, Round 137: loss=1.9725, accuracy=0.3310, 
[2025-09-12 19:03:42,379][__main__][INFO] - Test, Round 138: loss=2.3930, accuracy=0.2809, 
[2025-09-12 19:03:42,379][__main__][INFO] - Test, Round 139: loss=1.9845, accuracy=0.3277, 
[2025-09-12 19:03:42,379][__main__][INFO] - Test, Round 140: loss=1.9953, accuracy=0.3297, 
[2025-09-12 19:03:42,379][__main__][INFO] - Test, Round 141: loss=1.9625, accuracy=0.3324, 
[2025-09-12 19:03:42,379][__main__][INFO] - Test, Round 142: loss=1.9337, accuracy=0.3457, 
[2025-09-12 19:03:42,379][__main__][INFO] - Test, Round 143: loss=2.0428, accuracy=0.3197, 
[2025-09-12 19:03:42,379][__main__][INFO] - Test, Round 144: loss=2.0495, accuracy=0.3295, 
[2025-09-12 19:03:42,379][__main__][INFO] - Test, Round 145: loss=2.0194, accuracy=0.3224, 
[2025-09-12 19:03:42,379][__main__][INFO] - Test, Round 146: loss=2.0022, accuracy=0.3315, 
[2025-09-12 19:03:42,379][__main__][INFO] - Test, Round 147: loss=2.4353, accuracy=0.2795, 
[2025-09-12 19:03:42,379][__main__][INFO] - Test, Round 148: loss=2.0012, accuracy=0.3419, 
[2025-09-12 19:03:42,379][__main__][INFO] - Test, Round 149: loss=2.0845, accuracy=0.3292, 
[2025-09-12 19:03:42,379][__main__][INFO] - Test, Round 150: loss=2.0650, accuracy=0.3264, 
[2025-09-12 19:03:42,379][__main__][INFO] - Test, Round 151: loss=2.0470, accuracy=0.3334, 
[2025-09-12 19:03:42,379][__main__][INFO] - Test, Round 152: loss=2.0433, accuracy=0.3357, 
[2025-09-12 19:03:42,379][__main__][INFO] - Test, Round 153: loss=2.1824, accuracy=0.3218, 
[2025-09-12 19:03:42,379][__main__][INFO] - Test, Round 154: loss=2.1798, accuracy=0.3214, 
[2025-09-12 19:03:42,379][__main__][INFO] - Test, Round 155: loss=2.2048, accuracy=0.3235, 
[2025-09-12 19:03:42,379][__main__][INFO] - Test, Round 156: loss=2.0409, accuracy=0.3444, 
[2025-09-12 19:03:42,379][__main__][INFO] - Test, Round 157: loss=2.1149, accuracy=0.3334, 
[2025-09-12 19:03:42,379][__main__][INFO] - Test, Round 158: loss=2.1560, accuracy=0.3293, 
[2025-09-12 19:03:42,380][__main__][INFO] - Test, Round 159: loss=2.1889, accuracy=0.3181, 
[2025-09-12 19:03:42,380][__main__][INFO] - Test, Round 160: loss=2.1698, accuracy=0.3351, 
[2025-09-12 19:03:42,380][__main__][INFO] - Test, Round 161: loss=2.2713, accuracy=0.3204, 
[2025-09-12 19:03:42,380][__main__][INFO] - Test, Round 162: loss=2.4021, accuracy=0.2981, 
[2025-09-12 19:03:42,380][__main__][INFO] - Test, Round 163: loss=2.1613, accuracy=0.3345, 
[2025-09-12 19:03:42,380][__main__][INFO] - Test, Round 164: loss=2.5081, accuracy=0.2932, 
[2025-09-12 19:03:42,380][__main__][INFO] - Test, Round 165: loss=2.1815, accuracy=0.3417, 
[2025-09-12 19:03:42,380][__main__][INFO] - Test, Round 166: loss=2.3930, accuracy=0.3204, 
[2025-09-12 19:03:42,380][__main__][INFO] - Test, Round 167: loss=2.3638, accuracy=0.3091, 
[2025-09-12 19:03:42,380][__main__][INFO] - Test, Round 168: loss=2.3104, accuracy=0.3260, 
[2025-09-12 19:03:42,380][__main__][INFO] - Test, Round 169: loss=2.3027, accuracy=0.3263, 
[2025-09-12 19:03:42,380][__main__][INFO] - Test, Round 170: loss=2.4369, accuracy=0.3214, 
[2025-09-12 19:03:42,380][__main__][INFO] - Test, Round 171: loss=2.1891, accuracy=0.3500, 
[2025-09-12 19:03:42,380][__main__][INFO] - Test, Round 172: loss=2.2739, accuracy=0.3343, 
[2025-09-12 19:03:42,380][__main__][INFO] - Test, Round 173: loss=2.4871, accuracy=0.3073, 
[2025-09-12 19:03:42,380][__main__][INFO] - Test, Round 174: loss=2.8512, accuracy=0.2980, 
[2025-09-12 19:03:42,380][__main__][INFO] - Test, Round 175: loss=2.4544, accuracy=0.3250, 
[2025-09-12 19:03:42,380][__main__][INFO] - Test, Round 176: loss=2.4109, accuracy=0.3171, 
[2025-09-12 19:03:42,380][__main__][INFO] - Test, Round 177: loss=2.4487, accuracy=0.3260, 
[2025-09-12 19:03:42,380][__main__][INFO] - Test, Round 178: loss=2.5355, accuracy=0.3189, 
[2025-09-12 19:03:42,380][__main__][INFO] - Test, Round 179: loss=2.4177, accuracy=0.3402, 
[2025-09-12 19:03:42,380][__main__][INFO] - Test, Round 180: loss=2.5732, accuracy=0.3086, 
[2025-09-12 19:03:42,380][__main__][INFO] - Test, Round 181: loss=2.4070, accuracy=0.3311, 
[2025-09-12 19:03:42,380][__main__][INFO] - Test, Round 182: loss=2.5151, accuracy=0.3299, 
[2025-09-12 19:03:42,380][__main__][INFO] - Test, Round 183: loss=3.0109, accuracy=0.2876, 
[2025-09-12 19:03:42,380][__main__][INFO] - Test, Round 184: loss=2.4568, accuracy=0.3367, 
[2025-09-12 19:03:42,380][__main__][INFO] - Test, Round 185: loss=2.5583, accuracy=0.3260, 
[2025-09-12 19:03:42,380][__main__][INFO] - Test, Round 186: loss=2.8413, accuracy=0.3018, 
[2025-09-12 19:03:42,380][__main__][INFO] - Test, Round 187: loss=2.6356, accuracy=0.3148, 
[2025-09-12 19:03:42,380][__main__][INFO] - Test, Round 188: loss=3.4454, accuracy=0.2543, 
[2025-09-12 19:03:42,380][__main__][INFO] - Test, Round 189: loss=2.6766, accuracy=0.3253, 
[2025-09-12 19:03:42,380][__main__][INFO] - Test, Round 190: loss=2.6208, accuracy=0.3313, 
[2025-09-12 19:03:42,380][__main__][INFO] - Test, Round 191: loss=2.7244, accuracy=0.3217, 
[2025-09-12 19:03:42,380][__main__][INFO] - Test, Round 192: loss=2.7044, accuracy=0.3227, 
[2025-09-12 19:03:42,380][__main__][INFO] - Test, Round 193: loss=2.8621, accuracy=0.3107, 
[2025-09-12 19:03:42,380][__main__][INFO] - Test, Round 194: loss=3.1726, accuracy=0.3165, 
[2025-09-12 19:03:42,380][__main__][INFO] - Test, Round 195: loss=2.8392, accuracy=0.3161, 
[2025-09-12 19:03:42,380][__main__][INFO] - Test, Round 196: loss=2.9689, accuracy=0.3202, 
[2025-09-12 19:03:42,380][__main__][INFO] - Test, Round 197: loss=2.8969, accuracy=0.3213, 
[2025-09-12 19:03:42,380][__main__][INFO] - Test, Round 198: loss=3.0253, accuracy=0.3167, 
[2025-09-12 19:03:42,380][__main__][INFO] - Test, Round 199: loss=2.8771, accuracy=0.3331, 
[2025-09-12 19:03:42,380][__main__][INFO] - Test, Round 200: loss=2.8774, accuracy=0.3334, 
