[2025-09-11 16:53:14,438][flp2p.graph_runner][INFO] - Train, Round 0 : loss => 2.304904338816801,  accuracy: 0.10404666666666668, gradient_norm : 0.1727394645447522
[2025-09-11 16:53:26,229][flp2p.graph_runner][INFO] - Test, Round 0 : loss => 2.30461284263134,  accuracy: 0.1062
[2025-09-11 16:53:58,577][flp2p.graph_runner][INFO] - Train, Round 1 : loss => 2.303723912139733,  accuracy: 0.10942666666666667, gradient_norm : 0.17258212574563733
[2025-09-11 16:54:10,475][flp2p.graph_runner][INFO] - Test, Round 1 : loss => 2.303489652109146,  accuracy: 0.1115
[2025-09-11 16:54:42,935][flp2p.graph_runner][INFO] - Train, Round 2 : loss => 2.3025153793891273,  accuracy: 0.11395333333333332, gradient_norm : 0.17517326723658372
[2025-09-11 16:54:54,837][flp2p.graph_runner][INFO] - Test, Round 2 : loss => 2.302413396513462,  accuracy: 0.1168
[2025-09-11 16:55:27,553][flp2p.graph_runner][INFO] - Train, Round 3 : loss => 2.3013643072048824,  accuracy: 0.11803333333333331, gradient_norm : 0.17417835543071758
[2025-09-11 16:55:39,522][flp2p.graph_runner][INFO] - Test, Round 3 : loss => 2.3014042202711105,  accuracy: 0.1224
[2025-09-11 16:56:12,143][flp2p.graph_runner][INFO] - Train, Round 4 : loss => 2.300285902122657,  accuracy: 0.1220666666666667, gradient_norm : 0.17338689196056314
[2025-09-11 16:56:23,962][flp2p.graph_runner][INFO] - Test, Round 4 : loss => 2.300410962307453,  accuracy: 0.1248
[2025-09-11 16:56:56,697][flp2p.graph_runner][INFO] - Train, Round 5 : loss => 2.299208966493606,  accuracy: 0.12477999999999999, gradient_norm : 0.17627292640208003
[2025-09-11 16:57:08,518][flp2p.graph_runner][INFO] - Test, Round 5 : loss => 2.2994075806617738,  accuracy: 0.1282
[2025-09-11 16:57:41,236][flp2p.graph_runner][INFO] - Train, Round 6 : loss => 2.2981060696641595,  accuracy: 0.12605333333333335, gradient_norm : 0.182736222745826
[2025-09-11 16:57:53,284][flp2p.graph_runner][INFO] - Test, Round 6 : loss => 2.2984080507874487,  accuracy: 0.1295
[2025-09-11 16:58:25,983][flp2p.graph_runner][INFO] - Train, Round 7 : loss => 2.2969779616594317,  accuracy: 0.1285133333333333, gradient_norm : 0.18059951768834032
[2025-09-11 16:58:37,804][flp2p.graph_runner][INFO] - Test, Round 7 : loss => 2.297390205204487,  accuracy: 0.1304
[2025-09-11 16:59:10,540][flp2p.graph_runner][INFO] - Train, Round 8 : loss => 2.29581071605285,  accuracy: 0.13070000000000004, gradient_norm : 0.18818815021002006
[2025-09-11 16:59:22,462][flp2p.graph_runner][INFO] - Test, Round 8 : loss => 2.2963206198453903,  accuracy: 0.1301
[2025-09-11 16:59:55,299][flp2p.graph_runner][INFO] - Train, Round 9 : loss => 2.2946031985680264,  accuracy: 0.13281333333333337, gradient_norm : 0.19189928836305462
[2025-09-11 17:00:07,193][flp2p.graph_runner][INFO] - Test, Round 9 : loss => 2.2951706707954407,  accuracy: 0.1321
[2025-09-11 17:00:40,126][flp2p.graph_runner][INFO] - Train, Round 10 : loss => 2.2932847273349752,  accuracy: 0.13518, gradient_norm : 0.1954989630587856
[2025-09-11 17:00:52,070][flp2p.graph_runner][INFO] - Test, Round 10 : loss => 2.2939317319869996,  accuracy: 0.1325
[2025-09-11 17:01:24,717][flp2p.graph_runner][INFO] - Train, Round 11 : loss => 2.2918427081902824,  accuracy: 0.13793999999999995, gradient_norm : 0.19868485087500054
[2025-09-11 17:01:36,551][flp2p.graph_runner][INFO] - Test, Round 11 : loss => 2.292546479535103,  accuracy: 0.1366
[2025-09-11 17:02:09,161][flp2p.graph_runner][INFO] - Train, Round 12 : loss => 2.290190312067667,  accuracy: 0.14130666666666664, gradient_norm : 0.20713956953632928
[2025-09-11 17:02:20,940][flp2p.graph_runner][INFO] - Test, Round 12 : loss => 2.291007291686535,  accuracy: 0.1387
[2025-09-11 17:02:53,897][flp2p.graph_runner][INFO] - Train, Round 13 : loss => 2.2883690704902007,  accuracy: 0.14514666666666667, gradient_norm : 0.2139415625770411
[2025-09-11 17:03:05,766][flp2p.graph_runner][INFO] - Test, Round 13 : loss => 2.2892461638808252,  accuracy: 0.1405
[2025-09-11 17:03:38,267][flp2p.graph_runner][INFO] - Train, Round 14 : loss => 2.2862429600954055,  accuracy: 0.14837333333333327, gradient_norm : 0.2354851666203938
[2025-09-11 17:03:50,156][flp2p.graph_runner][INFO] - Test, Round 14 : loss => 2.287177444398403,  accuracy: 0.1423
[2025-09-11 17:04:22,786][flp2p.graph_runner][INFO] - Train, Round 15 : loss => 2.2837467713157333,  accuracy: 0.15151999999999993, gradient_norm : 0.23429099207599335
[2025-09-11 17:04:34,520][flp2p.graph_runner][INFO] - Test, Round 15 : loss => 2.2847578543663025,  accuracy: 0.145
[2025-09-11 17:05:07,509][flp2p.graph_runner][INFO] - Train, Round 16 : loss => 2.2807421433925628,  accuracy: 0.15519333333333332, gradient_norm : 0.25531424860970253
[2025-09-11 17:05:19,552][flp2p.graph_runner][INFO] - Test, Round 16 : loss => 2.2818388416171076,  accuracy: 0.1483
[2025-09-11 17:05:52,316][flp2p.graph_runner][INFO] - Train, Round 17 : loss => 2.277134537299473,  accuracy: 0.15944000000000003, gradient_norm : 0.2852410290397694
[2025-09-11 17:06:05,091][flp2p.graph_runner][INFO] - Test, Round 17 : loss => 2.2782689933657645,  accuracy: 0.1514
[2025-09-11 17:06:40,960][flp2p.graph_runner][INFO] - Train, Round 18 : loss => 2.2726938850680996,  accuracy: 0.16382666666666662, gradient_norm : 0.30095543455397566
[2025-09-11 17:06:54,920][flp2p.graph_runner][INFO] - Test, Round 18 : loss => 2.273919505405426,  accuracy: 0.1553
[2025-09-11 17:07:30,384][flp2p.graph_runner][INFO] - Train, Round 19 : loss => 2.2671759315331776,  accuracy: 0.16747333333333328, gradient_norm : 0.34122282299154355
[2025-09-11 17:07:44,525][flp2p.graph_runner][INFO] - Test, Round 19 : loss => 2.2684611949920654,  accuracy: 0.1602
[2025-09-11 17:08:20,257][flp2p.graph_runner][INFO] - Train, Round 20 : loss => 2.260256189405918,  accuracy: 0.1719733333333333, gradient_norm : 0.3790415373805843
[2025-09-11 17:08:34,166][flp2p.graph_runner][INFO] - Test, Round 20 : loss => 2.2617432891488076,  accuracy: 0.1636
[2025-09-11 17:09:09,908][flp2p.graph_runner][INFO] - Train, Round 21 : loss => 2.2516245245933537,  accuracy: 0.17715333333333336, gradient_norm : 0.4144125685413209
[2025-09-11 17:09:23,734][flp2p.graph_runner][INFO] - Test, Round 21 : loss => 2.2532166296601295,  accuracy: 0.1677
[2025-09-11 17:09:59,200][flp2p.graph_runner][INFO] - Train, Round 22 : loss => 2.2407095361749327,  accuracy: 0.1808466666666667, gradient_norm : 0.490003497544463
[2025-09-11 17:10:13,038][flp2p.graph_runner][INFO] - Test, Round 22 : loss => 2.2424939631462095,  accuracy: 0.1709
[2025-09-11 17:10:48,894][flp2p.graph_runner][INFO] - Train, Round 23 : loss => 2.2269277300437293,  accuracy: 0.18663333333333335, gradient_norm : 0.5696791929330776
[2025-09-11 17:11:02,781][flp2p.graph_runner][INFO] - Test, Round 23 : loss => 2.229263661456108,  accuracy: 0.1757
[2025-09-11 17:11:38,799][flp2p.graph_runner][INFO] - Train, Round 24 : loss => 2.2099814072251323,  accuracy: 0.19416666666666665, gradient_norm : 0.6404606503875115
[2025-09-11 17:11:52,970][flp2p.graph_runner][INFO] - Test, Round 24 : loss => 2.2135478498458863,  accuracy: 0.1828
[2025-09-11 17:12:29,100][flp2p.graph_runner][INFO] - Train, Round 25 : loss => 2.1899496035774555,  accuracy: 0.20141333333333333, gradient_norm : 0.7678094385715898
[2025-09-11 17:12:43,292][flp2p.graph_runner][INFO] - Test, Round 25 : loss => 2.195705528485775,  accuracy: 0.1902
[2025-09-11 17:13:19,110][flp2p.graph_runner][INFO] - Train, Round 26 : loss => 2.167637447764477,  accuracy: 0.20902666666666672, gradient_norm : 0.9411282692527707
[2025-09-11 17:13:33,422][flp2p.graph_runner][INFO] - Test, Round 26 : loss => 2.1767687649488447,  accuracy: 0.1961
[2025-09-11 17:14:09,976][flp2p.graph_runner][INFO] - Train, Round 27 : loss => 2.143592595209679,  accuracy: 0.2178133333333334, gradient_norm : 1.0243239892004454
[2025-09-11 17:14:24,071][flp2p.graph_runner][INFO] - Test, Round 27 : loss => 2.157964543068409,  accuracy: 0.2024
[2025-09-11 17:14:59,943][flp2p.graph_runner][INFO] - Train, Round 28 : loss => 2.1186370128393177,  accuracy: 0.22843999999999992, gradient_norm : 1.1708667443394896
[2025-09-11 17:15:14,130][flp2p.graph_runner][INFO] - Test, Round 28 : loss => 2.1391861151754856,  accuracy: 0.2141
[2025-09-11 17:15:50,295][flp2p.graph_runner][INFO] - Train, Round 29 : loss => 2.0940779370566207,  accuracy: 0.2431266666666666, gradient_norm : 1.358037548751043
[2025-09-11 17:16:04,461][flp2p.graph_runner][INFO] - Test, Round 29 : loss => 2.120911568212509,  accuracy: 0.2279
[2025-09-11 17:16:40,571][flp2p.graph_runner][INFO] - Train, Round 30 : loss => 2.0697082352141543,  accuracy: 0.25795999999999997, gradient_norm : 1.5687150758175077
[2025-09-11 17:16:54,945][flp2p.graph_runner][INFO] - Test, Round 30 : loss => 2.1033308185219766,  accuracy: 0.2351
[2025-09-11 17:17:31,314][flp2p.graph_runner][INFO] - Train, Round 31 : loss => 2.044633806198835,  accuracy: 0.27243333333333336, gradient_norm : 1.7512640821516154
[2025-09-11 17:17:45,351][flp2p.graph_runner][INFO] - Test, Round 31 : loss => 2.085324844264984,  accuracy: 0.2502
[2025-09-11 17:18:21,313][flp2p.graph_runner][INFO] - Train, Round 32 : loss => 2.0190403064588707,  accuracy: 0.28523333333333334, gradient_norm : 1.7457430919977512
[2025-09-11 17:18:35,512][flp2p.graph_runner][INFO] - Test, Round 32 : loss => 2.0698611641287803,  accuracy: 0.259
[2025-09-11 17:19:11,505][flp2p.graph_runner][INFO] - Train, Round 33 : loss => 1.9939436978101737,  accuracy: 0.2954, gradient_norm : 1.9958318202150922
[2025-09-11 17:19:25,719][flp2p.graph_runner][INFO] - Test, Round 33 : loss => 2.050664652532339,  accuracy: 0.2714
[2025-09-11 17:20:01,762][flp2p.graph_runner][INFO] - Train, Round 34 : loss => 1.9676735992729673,  accuracy: 0.3066000000000001, gradient_norm : 2.1846690747401794
[2025-09-11 17:20:15,996][flp2p.graph_runner][INFO] - Test, Round 34 : loss => 2.0414366798818113,  accuracy: 0.2753
[2025-09-11 17:20:52,321][flp2p.graph_runner][INFO] - Train, Round 35 : loss => 1.9430682740608847,  accuracy: 0.3149866666666667, gradient_norm : 2.247704978479588
[2025-09-11 17:21:06,753][flp2p.graph_runner][INFO] - Test, Round 35 : loss => 2.0222538338065146,  accuracy: 0.2739
[2025-09-11 17:21:42,593][flp2p.graph_runner][INFO] - Train, Round 36 : loss => 1.91883569975694,  accuracy: 0.32199999999999995, gradient_norm : 2.504204469946513
[2025-09-11 17:21:56,897][flp2p.graph_runner][INFO] - Test, Round 36 : loss => 2.016629643255472,  accuracy: 0.2784
[2025-09-11 17:22:33,072][flp2p.graph_runner][INFO] - Train, Round 37 : loss => 1.897365404913823,  accuracy: 0.32723999999999986, gradient_norm : 2.6989290621493223
[2025-09-11 17:22:47,214][flp2p.graph_runner][INFO] - Test, Round 37 : loss => 2.0023887412667274,  accuracy: 0.2829
[2025-09-11 17:23:23,434][flp2p.graph_runner][INFO] - Train, Round 38 : loss => 1.876655897547801,  accuracy: 0.33331999999999995, gradient_norm : 2.803439651098809
[2025-09-11 17:23:37,681][flp2p.graph_runner][INFO] - Test, Round 38 : loss => 2.007861785519123,  accuracy: 0.2849
[2025-09-11 17:24:13,836][flp2p.graph_runner][INFO] - Train, Round 39 : loss => 1.8591824577748768,  accuracy: 0.3403866666666667, gradient_norm : 2.9725152480571007
[2025-09-11 17:24:28,250][flp2p.graph_runner][INFO] - Test, Round 39 : loss => 1.9814101914823055,  accuracy: 0.2928
[2025-09-11 17:25:04,051][flp2p.graph_runner][INFO] - Train, Round 40 : loss => 1.8394732970992722,  accuracy: 0.3470933333333332, gradient_norm : 3.23653939233119
[2025-09-11 17:25:18,703][flp2p.graph_runner][INFO] - Test, Round 40 : loss => 1.976978765708208,  accuracy: 0.297
[2025-09-11 17:25:54,292][flp2p.graph_runner][INFO] - Train, Round 41 : loss => 1.8227409616112709,  accuracy: 0.35184, gradient_norm : 3.227322451264064
[2025-09-11 17:26:08,780][flp2p.graph_runner][INFO] - Test, Round 41 : loss => 1.9994230700731277,  accuracy: 0.2908
[2025-09-11 17:26:44,436][flp2p.graph_runner][INFO] - Train, Round 42 : loss => 1.8057896444698178,  accuracy: 0.3580066666666667, gradient_norm : 3.4938326780886295
[2025-09-11 17:26:58,901][flp2p.graph_runner][INFO] - Test, Round 42 : loss => 1.981020317775011,  accuracy: 0.2896
[2025-09-11 17:27:34,608][flp2p.graph_runner][INFO] - Train, Round 43 : loss => 1.7868056572973723,  accuracy: 0.3650066666666667, gradient_norm : 3.732022372468468
[2025-09-11 17:27:50,473][flp2p.graph_runner][INFO] - Test, Round 43 : loss => 1.964670028990507,  accuracy: 0.2971
[2025-09-11 17:28:26,514][flp2p.graph_runner][INFO] - Train, Round 44 : loss => 1.7703991927703218,  accuracy: 0.37155333333333324, gradient_norm : 3.689981534522723
[2025-09-11 17:28:46,195][flp2p.graph_runner][INFO] - Test, Round 44 : loss => 1.9994513887763024,  accuracy: 0.2799
[2025-09-11 17:29:21,949][flp2p.graph_runner][INFO] - Train, Round 45 : loss => 1.753253803302845,  accuracy: 0.3777399999999999, gradient_norm : 3.7516899785067137
[2025-09-11 17:29:43,012][flp2p.graph_runner][INFO] - Test, Round 45 : loss => 1.9715374608755112,  accuracy: 0.2955
[2025-09-11 17:30:18,968][flp2p.graph_runner][INFO] - Train, Round 46 : loss => 1.7346759674449768,  accuracy: 0.38256000000000023, gradient_norm : 3.9589316790636224
[2025-09-11 17:30:36,909][flp2p.graph_runner][INFO] - Test, Round 46 : loss => 2.00837301774621,  accuracy: 0.2961
[2025-09-11 17:31:12,673][flp2p.graph_runner][INFO] - Train, Round 47 : loss => 1.7190290829042596,  accuracy: 0.3905200000000001, gradient_norm : 4.162238369645354
[2025-09-11 17:31:28,518][flp2p.graph_runner][INFO] - Test, Round 47 : loss => 2.005640829026699,  accuracy: 0.2926
[2025-09-11 17:32:04,140][flp2p.graph_runner][INFO] - Train, Round 48 : loss => 1.6999371512234207,  accuracy: 0.39882000000000006, gradient_norm : 4.153727044545214
[2025-09-11 17:32:18,019][flp2p.graph_runner][INFO] - Test, Round 48 : loss => 1.979677546519041,  accuracy: 0.2985
[2025-09-11 17:32:53,749][flp2p.graph_runner][INFO] - Train, Round 49 : loss => 1.6789778875311223,  accuracy: 0.4047666666666667, gradient_norm : 4.276944006622744
[2025-09-11 17:33:07,538][flp2p.graph_runner][INFO] - Test, Round 49 : loss => 1.9794650697886944,  accuracy: 0.2966
[2025-09-11 17:33:43,642][flp2p.graph_runner][INFO] - Train, Round 50 : loss => 1.6595642077922823,  accuracy: 0.4124999999999999, gradient_norm : 4.319688991645673
[2025-09-11 17:33:57,372][flp2p.graph_runner][INFO] - Test, Round 50 : loss => 2.036040101158619,  accuracy: 0.2964
[2025-09-11 17:34:33,135][flp2p.graph_runner][INFO] - Train, Round 51 : loss => 1.6399797872205581,  accuracy: 0.4203666666666668, gradient_norm : 4.535292949279368
[2025-09-11 17:34:47,167][flp2p.graph_runner][INFO] - Test, Round 51 : loss => 1.9511776001632213,  accuracy: 0.307
[2025-09-11 17:35:22,956][flp2p.graph_runner][INFO] - Train, Round 52 : loss => 1.6147718184689677,  accuracy: 0.4300000000000002, gradient_norm : 4.6722312864157
[2025-09-11 17:35:36,734][flp2p.graph_runner][INFO] - Test, Round 52 : loss => 2.0243554273068907,  accuracy: 0.2875
[2025-09-11 17:36:12,471][flp2p.graph_runner][INFO] - Train, Round 53 : loss => 1.5953951086103917,  accuracy: 0.43703999999999993, gradient_norm : 4.579137945591791
[2025-09-11 17:36:26,290][flp2p.graph_runner][INFO] - Test, Round 53 : loss => 2.0447523483633994,  accuracy: 0.2903
[2025-09-11 17:37:02,071][flp2p.graph_runner][INFO] - Train, Round 54 : loss => 1.5749765183528248,  accuracy: 0.4448066666666666, gradient_norm : 4.8699706305164465
[2025-09-11 17:37:15,930][flp2p.graph_runner][INFO] - Test, Round 54 : loss => 2.019215946263075,  accuracy: 0.2996
[2025-09-11 17:37:51,569][flp2p.graph_runner][INFO] - Train, Round 55 : loss => 1.5468427178760364,  accuracy: 0.4528666666666667, gradient_norm : 5.039560452410103
[2025-09-11 17:38:05,415][flp2p.graph_runner][INFO] - Test, Round 55 : loss => 1.9843749973952771,  accuracy: 0.3026
[2025-09-11 17:38:41,703][flp2p.graph_runner][INFO] - Train, Round 56 : loss => 1.518596969097852,  accuracy: 0.4651600000000001, gradient_norm : 4.987637371136895
[2025-09-11 17:38:55,795][flp2p.graph_runner][INFO] - Test, Round 56 : loss => 1.9929702901542188,  accuracy: 0.3025
[2025-09-11 17:39:31,517][flp2p.graph_runner][INFO] - Train, Round 57 : loss => 1.4990375265479097,  accuracy: 0.47273999999999994, gradient_norm : 5.5002733666345875
[2025-09-11 17:39:45,841][flp2p.graph_runner][INFO] - Test, Round 57 : loss => 2.058015250337124,  accuracy: 0.2915
[2025-09-11 17:40:21,804][flp2p.graph_runner][INFO] - Train, Round 58 : loss => 1.47249217847983,  accuracy: 0.48342000000000007, gradient_norm : 5.205803590207449
[2025-09-11 17:40:36,204][flp2p.graph_runner][INFO] - Test, Round 58 : loss => 2.007152943634987,  accuracy: 0.3105
[2025-09-11 17:41:12,192][flp2p.graph_runner][INFO] - Train, Round 59 : loss => 1.445965301593145,  accuracy: 0.4911466666666666, gradient_norm : 5.587138765196134
[2025-09-11 17:41:26,311][flp2p.graph_runner][INFO] - Test, Round 59 : loss => 2.164276627665758,  accuracy: 0.2844
[2025-09-11 17:42:02,265][flp2p.graph_runner][INFO] - Train, Round 60 : loss => 1.4200214555362853,  accuracy: 0.5016466666666666, gradient_norm : 5.962898214748338
[2025-09-11 17:42:16,434][flp2p.graph_runner][INFO] - Test, Round 60 : loss => 2.0542879120111466,  accuracy: 0.2993
[2025-09-11 17:42:52,503][flp2p.graph_runner][INFO] - Train, Round 61 : loss => 1.3892206088453538,  accuracy: 0.5117466666666668, gradient_norm : 5.7637206142214135
[2025-09-11 17:43:06,636][flp2p.graph_runner][INFO] - Test, Round 61 : loss => 2.0586779420614243,  accuracy: 0.309
[2025-09-11 17:43:42,683][flp2p.graph_runner][INFO] - Train, Round 62 : loss => 1.360833432575067,  accuracy: 0.5225666666666668, gradient_norm : 5.970960584331042
[2025-09-11 17:43:57,016][flp2p.graph_runner][INFO] - Test, Round 62 : loss => 2.0101789501428606,  accuracy: 0.3255
[2025-09-11 17:44:32,900][flp2p.graph_runner][INFO] - Train, Round 63 : loss => 1.3304573645691082,  accuracy: 0.5325733333333331, gradient_norm : 5.913482812754751
[2025-09-11 17:44:47,001][flp2p.graph_runner][INFO] - Test, Round 63 : loss => 2.1217381651341913,  accuracy: 0.292
[2025-09-11 17:45:23,168][flp2p.graph_runner][INFO] - Train, Round 64 : loss => 1.3037151119112966,  accuracy: 0.5440466666666667, gradient_norm : 5.8612925901533774
[2025-09-11 17:45:37,369][flp2p.graph_runner][INFO] - Test, Round 64 : loss => 2.0701086950182916,  accuracy: 0.3061
[2025-09-11 17:46:13,205][flp2p.graph_runner][INFO] - Train, Round 65 : loss => 1.2673232020934426,  accuracy: 0.5562866666666666, gradient_norm : 6.443577780786914
[2025-09-11 17:46:27,324][flp2p.graph_runner][INFO] - Test, Round 65 : loss => 2.154476794338226,  accuracy: 0.3074
[2025-09-11 17:47:03,192][flp2p.graph_runner][INFO] - Train, Round 66 : loss => 1.2426973826686538,  accuracy: 0.5654333333333333, gradient_norm : 6.361626192661223
[2025-09-11 17:47:17,370][flp2p.graph_runner][INFO] - Test, Round 66 : loss => 2.1999323737382888,  accuracy: 0.2963
[2025-09-11 17:47:53,364][flp2p.graph_runner][INFO] - Train, Round 67 : loss => 1.2179486472407979,  accuracy: 0.5765466666666664, gradient_norm : 7.063084602807895
[2025-09-11 17:48:07,841][flp2p.graph_runner][INFO] - Test, Round 67 : loss => 2.1393662112534044,  accuracy: 0.3216
[2025-09-11 17:48:43,929][flp2p.graph_runner][INFO] - Train, Round 68 : loss => 1.180735491514206,  accuracy: 0.5881466666666666, gradient_norm : 6.818285986889683
[2025-09-11 17:48:58,087][flp2p.graph_runner][INFO] - Test, Round 68 : loss => 2.2428919085264205,  accuracy: 0.3068
[2025-09-11 17:49:34,101][flp2p.graph_runner][INFO] - Train, Round 69 : loss => 1.149375134234628,  accuracy: 0.6023533333333334, gradient_norm : 6.966510301557521
[2025-09-11 17:49:48,285][flp2p.graph_runner][INFO] - Test, Round 69 : loss => 2.3133661282479765,  accuracy: 0.3
[2025-09-11 17:50:24,272][flp2p.graph_runner][INFO] - Train, Round 70 : loss => 1.1194647695372506,  accuracy: 0.6139600000000002, gradient_norm : 7.1446193328397865
[2025-09-11 17:50:38,451][flp2p.graph_runner][INFO] - Test, Round 70 : loss => 2.6280467586815357,  accuracy: 0.2588
[2025-09-11 17:51:14,567][flp2p.graph_runner][INFO] - Train, Round 71 : loss => 1.096095726316174,  accuracy: 0.6242733333333332, gradient_norm : 7.241583991138997
[2025-09-11 17:51:28,478][flp2p.graph_runner][INFO] - Test, Round 71 : loss => 2.3311195950984955,  accuracy: 0.305
[2025-09-11 17:52:04,508][flp2p.graph_runner][INFO] - Train, Round 72 : loss => 1.0485171020279331,  accuracy: 0.6400733333333335, gradient_norm : 7.034069347378192
[2025-09-11 17:52:18,759][flp2p.graph_runner][INFO] - Test, Round 72 : loss => 2.5186261526405813,  accuracy: 0.2781
[2025-09-11 17:52:54,624][flp2p.graph_runner][INFO] - Train, Round 73 : loss => 1.0243993564695124,  accuracy: 0.6497066666666665, gradient_norm : 6.995648795052888
[2025-09-11 17:53:09,493][flp2p.graph_runner][INFO] - Test, Round 73 : loss => 2.331704395848513,  accuracy: 0.2951
[2025-09-11 17:53:45,467][flp2p.graph_runner][INFO] - Train, Round 74 : loss => 0.97015195886294,  accuracy: 0.6685733333333335, gradient_norm : 7.388901260326488
[2025-09-11 17:54:00,135][flp2p.graph_runner][INFO] - Test, Round 74 : loss => 2.3116276354193688,  accuracy: 0.3045
[2025-09-11 17:54:36,085][flp2p.graph_runner][INFO] - Train, Round 75 : loss => 0.9393523216744265,  accuracy: 0.6804133333333333, gradient_norm : 7.50831697802158
[2025-09-11 17:54:50,600][flp2p.graph_runner][INFO] - Test, Round 75 : loss => 2.358888038420677,  accuracy: 0.3065
[2025-09-11 17:55:26,493][flp2p.graph_runner][INFO] - Train, Round 76 : loss => 0.9045874384293954,  accuracy: 0.6937399999999998, gradient_norm : 7.479436139384403
[2025-09-11 17:55:42,076][flp2p.graph_runner][INFO] - Test, Round 76 : loss => 2.352213174813986,  accuracy: 0.3084
[2025-09-11 17:56:18,051][flp2p.graph_runner][INFO] - Train, Round 77 : loss => 0.8670711492622888,  accuracy: 0.7075333333333335, gradient_norm : 7.629153807839738
[2025-09-11 17:56:37,550][flp2p.graph_runner][INFO] - Test, Round 77 : loss => 2.682492056131363,  accuracy: 0.2679
[2025-09-11 17:57:13,182][flp2p.graph_runner][INFO] - Train, Round 78 : loss => 0.8481506858890249,  accuracy: 0.7174933333333331, gradient_norm : 7.488830996772938
[2025-09-11 17:57:34,329][flp2p.graph_runner][INFO] - Test, Round 78 : loss => 2.428125701016188,  accuracy: 0.3116
[2025-09-11 17:58:10,043][flp2p.graph_runner][INFO] - Train, Round 79 : loss => 0.7919867385178806,  accuracy: 0.737346666666667, gradient_norm : 8.125333403842715
[2025-09-11 17:58:29,165][flp2p.graph_runner][INFO] - Test, Round 79 : loss => 2.632172696721554,  accuracy: 0.303
[2025-09-11 17:59:04,897][flp2p.graph_runner][INFO] - Train, Round 80 : loss => 0.7625783802196385,  accuracy: 0.7496266666666663, gradient_norm : 7.774949151167065
[2025-09-11 17:59:21,591][flp2p.graph_runner][INFO] - Test, Round 80 : loss => 2.583475574761629,  accuracy: 0.3128
[2025-09-11 17:59:56,914][flp2p.graph_runner][INFO] - Train, Round 81 : loss => 0.7206408851345382,  accuracy: 0.7640933333333336, gradient_norm : 8.093981926161865
[2025-09-11 18:00:12,131][flp2p.graph_runner][INFO] - Test, Round 81 : loss => 2.7941034019589424,  accuracy: 0.2922
[2025-09-11 18:00:47,623][flp2p.graph_runner][INFO] - Train, Round 82 : loss => 0.693561094502608,  accuracy: 0.7765266666666667, gradient_norm : 7.907580943370089
[2025-09-11 18:01:01,999][flp2p.graph_runner][INFO] - Test, Round 82 : loss => 2.8885561926007273,  accuracy: 0.276
[2025-09-11 18:01:37,751][flp2p.graph_runner][INFO] - Train, Round 83 : loss => 0.6560781467954319,  accuracy: 0.7930066666666669, gradient_norm : 8.017608888813418
[2025-09-11 18:01:51,653][flp2p.graph_runner][INFO] - Test, Round 83 : loss => 2.6909456099152567,  accuracy: 0.3057
[2025-09-11 18:02:27,441][flp2p.graph_runner][INFO] - Train, Round 84 : loss => 0.6014895952368777,  accuracy: 0.8125133333333334, gradient_norm : 7.893756884903532
[2025-09-11 18:02:41,436][flp2p.graph_runner][INFO] - Test, Round 84 : loss => 3.4406326895833015,  accuracy: 0.2738
[2025-09-11 18:03:17,509][flp2p.graph_runner][INFO] - Train, Round 85 : loss => 0.6211071601261697,  accuracy: 0.813653333333333, gradient_norm : 7.185619549742505
[2025-09-11 18:03:31,351][flp2p.graph_runner][INFO] - Test, Round 85 : loss => 2.7185205172657967,  accuracy: 0.3189
[2025-09-11 18:04:07,399][flp2p.graph_runner][INFO] - Train, Round 86 : loss => 0.5252655198921761,  accuracy: 0.841526666666667, gradient_norm : 7.869086650127661
[2025-09-11 18:04:21,353][flp2p.graph_runner][INFO] - Test, Round 86 : loss => 2.913506470274925,  accuracy: 0.3056
[2025-09-11 18:04:57,682][flp2p.graph_runner][INFO] - Train, Round 87 : loss => 0.5010587438071767,  accuracy: 0.8520000000000001, gradient_norm : 7.0434900116219294
[2025-09-11 18:05:11,523][flp2p.graph_runner][INFO] - Test, Round 87 : loss => 3.122152232992649,  accuracy: 0.2952
[2025-09-11 18:05:47,477][flp2p.graph_runner][INFO] - Train, Round 88 : loss => 0.47943470449497305,  accuracy: 0.8601533333333333, gradient_norm : 7.120604236164296
[2025-09-11 18:06:01,491][flp2p.graph_runner][INFO] - Test, Round 88 : loss => 3.0119588227033613,  accuracy: 0.3064
[2025-09-11 18:06:37,316][flp2p.graph_runner][INFO] - Train, Round 89 : loss => 0.4292791393958032,  accuracy: 0.8796399999999999, gradient_norm : 6.863138142674475
[2025-09-11 18:06:51,398][flp2p.graph_runner][INFO] - Test, Round 89 : loss => 3.100797354924679,  accuracy: 0.3145
[2025-09-11 18:07:27,256][flp2p.graph_runner][INFO] - Train, Round 90 : loss => 0.4042084252989542,  accuracy: 0.8898333333333336, gradient_norm : 6.217494475986291
[2025-09-11 18:07:41,302][flp2p.graph_runner][INFO] - Test, Round 90 : loss => 3.0791069934606554,  accuracy: 0.317
[2025-09-11 18:08:17,203][flp2p.graph_runner][INFO] - Train, Round 91 : loss => 0.3651418249774725,  accuracy: 0.9045000000000003, gradient_norm : 6.593252182213437
[2025-09-11 18:08:31,459][flp2p.graph_runner][INFO] - Test, Round 91 : loss => 3.3703891127228736,  accuracy: 0.3043
[2025-09-11 18:09:07,636][flp2p.graph_runner][INFO] - Train, Round 92 : loss => 0.357194338394329,  accuracy: 0.907966666666667, gradient_norm : 5.36717395797696
[2025-09-11 18:09:21,896][flp2p.graph_runner][INFO] - Test, Round 92 : loss => 3.2558517089009285,  accuracy: 0.3123
[2025-09-11 18:09:57,778][flp2p.graph_runner][INFO] - Train, Round 93 : loss => 0.29818430830724546,  accuracy: 0.9301800000000001, gradient_norm : 5.797568873531849
[2025-09-11 18:10:12,223][flp2p.graph_runner][INFO] - Test, Round 93 : loss => 3.3701069879889487,  accuracy: 0.315
[2025-09-11 18:10:48,400][flp2p.graph_runner][INFO] - Train, Round 94 : loss => 0.29896761426391694,  accuracy: 0.9299266666666665, gradient_norm : 4.85432881833696
[2025-09-11 18:11:02,798][flp2p.graph_runner][INFO] - Test, Round 94 : loss => 3.32301826941967,  accuracy: 0.3261
[2025-09-11 18:11:38,442][flp2p.graph_runner][INFO] - Train, Round 95 : loss => 0.2549838131961104,  accuracy: 0.9453733333333334, gradient_norm : 5.174531815091441
[2025-09-11 18:11:52,634][flp2p.graph_runner][INFO] - Test, Round 95 : loss => 3.694029079687595,  accuracy: 0.3093
[2025-09-11 18:12:28,871][flp2p.graph_runner][INFO] - Train, Round 96 : loss => 0.33361150085615615,  accuracy: 0.9255999999999995, gradient_norm : 4.270620605650852
[2025-09-11 18:12:43,043][flp2p.graph_runner][INFO] - Test, Round 96 : loss => 3.449563462638855,  accuracy: 0.3111
[2025-09-11 18:13:18,969][flp2p.graph_runner][INFO] - Train, Round 97 : loss => 0.19298525951181844,  accuracy: 0.9657933333333332, gradient_norm : 3.895947742371256
[2025-09-11 18:13:33,380][flp2p.graph_runner][INFO] - Test, Round 97 : loss => 3.6171938857913015,  accuracy: 0.3063
[2025-09-11 18:14:09,456][flp2p.graph_runner][INFO] - Train, Round 98 : loss => 0.2108395361558844,  accuracy: 0.9592666666666662, gradient_norm : 3.4496603336082385
[2025-09-11 18:14:23,652][flp2p.graph_runner][INFO] - Test, Round 98 : loss => 3.644416638314724,  accuracy: 0.3218
[2025-09-11 18:14:59,532][flp2p.graph_runner][INFO] - Train, Round 99 : loss => 0.19370049821678553,  accuracy: 0.9639533333333334, gradient_norm : 3.042774462723632
[2025-09-11 18:15:13,776][flp2p.graph_runner][INFO] - Test, Round 99 : loss => 3.532367245543003,  accuracy: 0.3252
[2025-09-11 18:15:49,729][flp2p.graph_runner][INFO] - Train, Round 100 : loss => 0.12132517960698654,  accuracy: 0.9861599999999997, gradient_norm : 3.13206895165582
[2025-09-11 18:16:04,128][flp2p.graph_runner][INFO] - Test, Round 100 : loss => 3.657410078215599,  accuracy: 0.3245
[2025-09-11 18:16:40,307][flp2p.graph_runner][INFO] - Train, Round 101 : loss => 0.11716037826923033,  accuracy: 0.9858600000000002, gradient_norm : 3.1466514459699795
[2025-09-11 18:16:54,337][flp2p.graph_runner][INFO] - Test, Round 101 : loss => 3.9020250644683836,  accuracy: 0.315
[2025-09-11 18:17:30,560][flp2p.graph_runner][INFO] - Train, Round 102 : loss => 0.2135223907876449,  accuracy: 0.9610600000000002, gradient_norm : 2.3110054230866552
[2025-09-11 18:17:44,834][flp2p.graph_runner][INFO] - Test, Round 102 : loss => 3.669297526586056,  accuracy: 0.3271
[2025-09-11 18:18:21,057][flp2p.graph_runner][INFO] - Train, Round 103 : loss => 0.08687824357921878,  accuracy: 0.9931999999999999, gradient_norm : 2.0961463556458173
[2025-09-11 18:18:35,112][flp2p.graph_runner][INFO] - Test, Round 103 : loss => 3.8148708146810533,  accuracy: 0.3248
[2025-09-11 18:19:11,502][flp2p.graph_runner][INFO] - Train, Round 104 : loss => 0.06712629371400303,  accuracy: 0.9969866666666667, gradient_norm : 2.114533740606141
[2025-09-11 18:19:25,918][flp2p.graph_runner][INFO] - Test, Round 104 : loss => 3.927297751712799,  accuracy: 0.3266
[2025-09-11 18:20:01,556][flp2p.graph_runner][INFO] - Train, Round 105 : loss => 0.08147294926845158,  accuracy: 0.992433333333333, gradient_norm : 1.7219918065247575
[2025-09-11 18:20:16,422][flp2p.graph_runner][INFO] - Test, Round 105 : loss => 3.9426260952234267,  accuracy: 0.3279
[2025-09-11 18:20:52,274][flp2p.graph_runner][INFO] - Train, Round 106 : loss => 0.058505522837707165,  accuracy: 0.9970466666666665, gradient_norm : 1.6963901766867622
[2025-09-11 18:21:06,743][flp2p.graph_runner][INFO] - Test, Round 106 : loss => 4.0367786130309105,  accuracy: 0.3287
[2025-09-11 18:21:42,393][flp2p.graph_runner][INFO] - Train, Round 107 : loss => 0.05111259612798071,  accuracy: 0.997773333333333, gradient_norm : 1.4680610230245557
[2025-09-11 18:21:57,076][flp2p.graph_runner][INFO] - Test, Round 107 : loss => 4.0811721045970915,  accuracy: 0.3281
[2025-09-11 18:22:32,879][flp2p.graph_runner][INFO] - Train, Round 108 : loss => 0.04191040149152589,  accuracy: 0.9992333333333334, gradient_norm : 1.417082595709044
[2025-09-11 18:22:47,320][flp2p.graph_runner][INFO] - Test, Round 108 : loss => 4.170393913650512,  accuracy: 0.322
[2025-09-11 18:23:22,723][flp2p.graph_runner][INFO] - Train, Round 109 : loss => 0.03850800412357786,  accuracy: 0.9991600000000002, gradient_norm : 1.2528787794873615
[2025-09-11 18:23:39,837][flp2p.graph_runner][INFO] - Test, Round 109 : loss => 4.2105681355595586,  accuracy: 0.3259
[2025-09-11 18:24:15,579][flp2p.graph_runner][INFO] - Train, Round 110 : loss => 0.034330224940010035,  accuracy: 0.9996, gradient_norm : 1.1333820965665569
[2025-09-11 18:24:35,606][flp2p.graph_runner][INFO] - Test, Round 110 : loss => 4.270564495944977,  accuracy: 0.3258
[2025-09-11 18:25:11,247][flp2p.graph_runner][INFO] - Train, Round 111 : loss => 0.031394993568149704,  accuracy: 0.9997266666666668, gradient_norm : 1.1135989271749087
[2025-09-11 18:25:32,014][flp2p.graph_runner][INFO] - Test, Round 111 : loss => 4.341258678424358,  accuracy: 0.3241
[2025-09-11 18:26:08,153][flp2p.graph_runner][INFO] - Train, Round 112 : loss => 0.029385612539093303,  accuracy: 0.9996733333333333, gradient_norm : 1.0537598019636811
[2025-09-11 18:26:24,842][flp2p.graph_runner][INFO] - Test, Round 112 : loss => 4.379546705138683,  accuracy: 0.3229
[2025-09-11 18:27:00,870][flp2p.graph_runner][INFO] - Train, Round 113 : loss => 0.026770905997448913,  accuracy: 0.9997733333333334, gradient_norm : 0.9198066381958632
[2025-09-11 18:27:15,045][flp2p.graph_runner][INFO] - Test, Round 113 : loss => 4.414737641334534,  accuracy: 0.3248
[2025-09-11 18:27:50,827][flp2p.graph_runner][INFO] - Train, Round 114 : loss => 0.02600425111789565,  accuracy: 0.9995133333333335, gradient_norm : 0.9717822399766041
[2025-09-11 18:28:04,785][flp2p.graph_runner][INFO] - Test, Round 114 : loss => 4.467333003926277,  accuracy: 0.3248
[2025-09-11 18:28:41,163][flp2p.graph_runner][INFO] - Train, Round 115 : loss => 0.023210065605817357,  accuracy: 0.9998533333333333, gradient_norm : 0.8508608203214161
[2025-09-11 18:28:54,993][flp2p.graph_runner][INFO] - Test, Round 115 : loss => 4.50832503323555,  accuracy: 0.3252
[2025-09-11 18:29:30,893][flp2p.graph_runner][INFO] - Train, Round 116 : loss => 0.029072571904398496,  accuracy: 0.99826, gradient_norm : 0.814657418298137
[2025-09-11 18:29:44,893][flp2p.graph_runner][INFO] - Test, Round 116 : loss => 4.56801885111332,  accuracy: 0.324
[2025-09-11 18:30:20,965][flp2p.graph_runner][INFO] - Train, Round 117 : loss => 0.02077187692164444,  accuracy: 0.9999066666666667, gradient_norm : 0.7259981649111086
[2025-09-11 18:30:34,821][flp2p.graph_runner][INFO] - Test, Round 117 : loss => 4.595669180428982,  accuracy: 0.3254
[2025-09-11 18:31:10,908][flp2p.graph_runner][INFO] - Train, Round 118 : loss => 0.019326589828512316,  accuracy: 0.9998866666666667, gradient_norm : 0.7833966114454712
[2025-09-11 18:31:24,784][flp2p.graph_runner][INFO] - Test, Round 118 : loss => 4.629107914733886,  accuracy: 0.325
[2025-09-11 18:32:00,792][flp2p.graph_runner][INFO] - Train, Round 119 : loss => 0.018141175577184185,  accuracy: 0.9999133333333333, gradient_norm : 0.6867909671428448
[2025-09-11 18:32:14,709][flp2p.graph_runner][INFO] - Test, Round 119 : loss => 4.673524045240879,  accuracy: 0.3233
[2025-09-11 18:32:50,869][flp2p.graph_runner][INFO] - Train, Round 120 : loss => 0.017073974418259842,  accuracy: 0.99994, gradient_norm : 0.6831751045319868
[2025-09-11 18:33:04,924][flp2p.graph_runner][INFO] - Test, Round 120 : loss => 4.719641548991203,  accuracy: 0.3241
[2025-09-11 18:33:40,887][flp2p.graph_runner][INFO] - Train, Round 121 : loss => 0.016281818171264607,  accuracy: 0.99994, gradient_norm : 0.6130986378781514
[2025-09-11 18:33:55,158][flp2p.graph_runner][INFO] - Test, Round 121 : loss => 4.744466381919384,  accuracy: 0.3253
[2025-09-11 18:34:31,159][flp2p.graph_runner][INFO] - Train, Round 122 : loss => 0.015315798455849287,  accuracy: 0.9999666666666667, gradient_norm : 0.603972496889114
[2025-09-11 18:34:45,638][flp2p.graph_runner][INFO] - Test, Round 122 : loss => 4.77847269718647,  accuracy: 0.3233
[2025-09-11 18:35:21,541][flp2p.graph_runner][INFO] - Train, Round 123 : loss => 0.014528046833584085,  accuracy: 1.0, gradient_norm : 0.5740470757691013
[2025-09-11 18:35:35,622][flp2p.graph_runner][INFO] - Test, Round 123 : loss => 4.81253396795988,  accuracy: 0.3234
[2025-09-11 18:36:11,874][flp2p.graph_runner][INFO] - Train, Round 124 : loss => 0.014006190915242769,  accuracy: 0.9999333333333332, gradient_norm : 0.5532413444980336
[2025-09-11 18:36:26,056][flp2p.graph_runner][INFO] - Test, Round 124 : loss => 4.844258699178695,  accuracy: 0.3247
[2025-09-11 18:37:02,332][flp2p.graph_runner][INFO] - Train, Round 125 : loss => 0.013237633833196007,  accuracy: 0.9999933333333333, gradient_norm : 0.5477537124238321
[2025-09-11 18:37:16,657][flp2p.graph_runner][INFO] - Test, Round 125 : loss => 4.8731686074733735,  accuracy: 0.3228
[2025-09-11 18:37:52,514][flp2p.graph_runner][INFO] - Train, Round 126 : loss => 0.01263175517281828,  accuracy: 0.9999733333333334, gradient_norm : 0.49952436284526386
[2025-09-11 18:38:06,836][flp2p.graph_runner][INFO] - Test, Round 126 : loss => 4.908809846830368,  accuracy: 0.3238
[2025-09-11 18:38:43,283][flp2p.graph_runner][INFO] - Train, Round 127 : loss => 0.012082405793480577,  accuracy: 0.9999933333333333, gradient_norm : 0.5152070187145258
[2025-09-11 18:38:57,404][flp2p.graph_runner][INFO] - Test, Round 127 : loss => 4.941542412221431,  accuracy: 0.3231
[2025-09-11 18:39:33,651][flp2p.graph_runner][INFO] - Train, Round 128 : loss => 0.011565421976459522,  accuracy: 1.0, gradient_norm : 0.4882154911436998
[2025-09-11 18:39:47,924][flp2p.graph_runner][INFO] - Test, Round 128 : loss => 4.961222317862511,  accuracy: 0.3222
[2025-09-11 18:40:23,726][flp2p.graph_runner][INFO] - Train, Round 129 : loss => 0.011124226146882087,  accuracy: 0.9999933333333333, gradient_norm : 0.47034497885891163
[2025-09-11 18:40:37,915][flp2p.graph_runner][INFO] - Test, Round 129 : loss => 4.991485065245628,  accuracy: 0.3238
[2025-09-11 18:41:14,210][flp2p.graph_runner][INFO] - Train, Round 130 : loss => 0.010674485094108,  accuracy: 1.0, gradient_norm : 0.43053033982172684
[2025-09-11 18:41:28,332][flp2p.graph_runner][INFO] - Test, Round 130 : loss => 5.0201302297949795,  accuracy: 0.3233
[2025-09-11 18:42:04,333][flp2p.graph_runner][INFO] - Train, Round 131 : loss => 0.010261755830918746,  accuracy: 1.0, gradient_norm : 0.41379438542602026
[2025-09-11 18:42:18,506][flp2p.graph_runner][INFO] - Test, Round 131 : loss => 5.041258845961094,  accuracy: 0.3222
[2025-09-11 18:42:54,674][flp2p.graph_runner][INFO] - Train, Round 132 : loss => 0.009891181741065035,  accuracy: 1.0, gradient_norm : 0.41338947340331494
[2025-09-11 18:43:09,006][flp2p.graph_runner][INFO] - Test, Round 132 : loss => 5.066925946438313,  accuracy: 0.3233
[2025-09-11 18:43:45,066][flp2p.graph_runner][INFO] - Train, Round 133 : loss => 0.009528818503216222,  accuracy: 1.0, gradient_norm : 0.380785374691266
[2025-09-11 18:43:59,304][flp2p.graph_runner][INFO] - Test, Round 133 : loss => 5.095529325044155,  accuracy: 0.3242
[2025-09-11 18:44:35,337][flp2p.graph_runner][INFO] - Train, Round 134 : loss => 0.009208258469977106,  accuracy: 1.0, gradient_norm : 0.3981189919260937
[2025-09-11 18:44:49,717][flp2p.graph_runner][INFO] - Test, Round 134 : loss => 5.11752416588068,  accuracy: 0.3246
[2025-09-11 18:45:25,707][flp2p.graph_runner][INFO] - Train, Round 135 : loss => 0.008915668242261745,  accuracy: 1.0, gradient_norm : 0.3820643763653273
[2025-09-11 18:45:40,113][flp2p.graph_runner][INFO] - Test, Round 135 : loss => 5.144854719281197,  accuracy: 0.3231
[2025-09-11 18:46:16,144][flp2p.graph_runner][INFO] - Train, Round 136 : loss => 0.008609367439639751,  accuracy: 1.0, gradient_norm : 0.37222187998570966
[2025-09-11 18:46:30,797][flp2p.graph_runner][INFO] - Test, Round 136 : loss => 5.16157697930336,  accuracy: 0.3239
[2025-09-11 18:47:06,540][flp2p.graph_runner][INFO] - Train, Round 137 : loss => 0.008325837884331121,  accuracy: 1.0, gradient_norm : 0.3564643664251887
[2025-09-11 18:47:20,841][flp2p.graph_runner][INFO] - Test, Round 137 : loss => 5.183625758934021,  accuracy: 0.3246
[2025-09-11 18:47:56,733][flp2p.graph_runner][INFO] - Train, Round 138 : loss => 0.008062014588504095,  accuracy: 1.0, gradient_norm : 0.32921769639158005
[2025-09-11 18:48:11,314][flp2p.graph_runner][INFO] - Test, Round 138 : loss => 5.20568650265932,  accuracy: 0.3225
[2025-09-11 18:48:47,286][flp2p.graph_runner][INFO] - Train, Round 139 : loss => 0.007807496155922613,  accuracy: 1.0, gradient_norm : 0.32612502519613656
[2025-09-11 18:49:03,829][flp2p.graph_runner][INFO] - Test, Round 139 : loss => 5.228613475930691,  accuracy: 0.3239
[2025-09-11 18:49:39,624][flp2p.graph_runner][INFO] - Train, Round 140 : loss => 0.007583505096748317,  accuracy: 1.0, gradient_norm : 0.3225471428406799
[2025-09-11 18:50:00,005][flp2p.graph_runner][INFO] - Test, Round 140 : loss => 5.2487645068883895,  accuracy: 0.324
[2025-09-11 18:50:35,845][flp2p.graph_runner][INFO] - Train, Round 141 : loss => 0.007365038745920173,  accuracy: 1.0, gradient_norm : 0.31702141548235524
[2025-09-11 18:50:57,100][flp2p.graph_runner][INFO] - Test, Round 141 : loss => 5.268195836126805,  accuracy: 0.3235
[2025-09-11 18:51:33,125][flp2p.graph_runner][INFO] - Train, Round 142 : loss => 0.007161624852742534,  accuracy: 1.0, gradient_norm : 0.31030597957434464
[2025-09-11 18:51:51,120][flp2p.graph_runner][INFO] - Test, Round 142 : loss => 5.282801571500301,  accuracy: 0.3235
[2025-09-11 18:52:27,104][flp2p.graph_runner][INFO] - Train, Round 143 : loss => 0.0069659313104542256,  accuracy: 1.0, gradient_norm : 0.2917899227623347
[2025-09-11 18:52:43,119][flp2p.graph_runner][INFO] - Test, Round 143 : loss => 5.304573349130154,  accuracy: 0.3232
[2025-09-11 18:53:19,130][flp2p.graph_runner][INFO] - Train, Round 144 : loss => 0.0067723678733455014,  accuracy: 1.0, gradient_norm : 0.2995986198737262
[2025-09-11 18:53:32,856][flp2p.graph_runner][INFO] - Test, Round 144 : loss => 5.320930126547814,  accuracy: 0.3237
[2025-09-11 18:54:08,924][flp2p.graph_runner][INFO] - Train, Round 145 : loss => 0.006591867024059561,  accuracy: 1.0, gradient_norm : 0.2745713817734112
[2025-09-11 18:54:22,847][flp2p.graph_runner][INFO] - Test, Round 145 : loss => 5.341953625786305,  accuracy: 0.3235
[2025-09-11 18:54:58,847][flp2p.graph_runner][INFO] - Train, Round 146 : loss => 0.006418749415315684,  accuracy: 1.0, gradient_norm : 0.27866903225077216
[2025-09-11 18:55:12,691][flp2p.graph_runner][INFO] - Test, Round 146 : loss => 5.361939945328236,  accuracy: 0.3229
[2025-09-11 18:55:48,516][flp2p.graph_runner][INFO] - Train, Round 147 : loss => 0.00625923936982872,  accuracy: 1.0, gradient_norm : 0.2780465005209603
[2025-09-11 18:56:02,333][flp2p.graph_runner][INFO] - Test, Round 147 : loss => 5.376378436815739,  accuracy: 0.3231
[2025-09-11 18:56:38,245][flp2p.graph_runner][INFO] - Train, Round 148 : loss => 0.006108505480903356,  accuracy: 1.0, gradient_norm : 0.27387479319831665
[2025-09-11 18:56:52,136][flp2p.graph_runner][INFO] - Test, Round 148 : loss => 5.398282871830464,  accuracy: 0.3227
[2025-09-11 18:57:28,176][flp2p.graph_runner][INFO] - Train, Round 149 : loss => 0.005956075450521894,  accuracy: 1.0, gradient_norm : 0.25613573025393127
[2025-09-11 18:57:42,320][flp2p.graph_runner][INFO] - Test, Round 149 : loss => 5.41171125355959,  accuracy: 0.3229
[2025-09-11 18:58:18,221][flp2p.graph_runner][INFO] - Train, Round 150 : loss => 0.005815045317188679,  accuracy: 1.0, gradient_norm : 0.2518595897103793
[2025-09-11 18:58:32,323][flp2p.graph_runner][INFO] - Test, Round 150 : loss => 5.4283396046280865,  accuracy: 0.3232
[2025-09-11 18:59:08,873][flp2p.graph_runner][INFO] - Train, Round 151 : loss => 0.005681745678496858,  accuracy: 1.0, gradient_norm : 0.25154467395359315
[2025-09-11 18:59:22,997][flp2p.graph_runner][INFO] - Test, Round 151 : loss => 5.443890830433369,  accuracy: 0.3228
[2025-09-11 18:59:59,123][flp2p.graph_runner][INFO] - Train, Round 152 : loss => 0.0055570202529391585,  accuracy: 1.0, gradient_norm : 0.2623372324987942
[2025-09-11 19:00:13,439][flp2p.graph_runner][INFO] - Test, Round 152 : loss => 5.458887776887416,  accuracy: 0.3239
[2025-09-11 19:00:49,601][flp2p.graph_runner][INFO] - Train, Round 153 : loss => 0.005433232143986971,  accuracy: 1.0, gradient_norm : 0.24389044718087569
[2025-09-11 19:01:04,037][flp2p.graph_runner][INFO] - Test, Round 153 : loss => 5.475514323890209,  accuracy: 0.3234
[2025-09-11 19:01:40,319][flp2p.graph_runner][INFO] - Train, Round 154 : loss => 0.005310033080750146,  accuracy: 1.0, gradient_norm : 0.24004556019810872
[2025-09-11 19:01:54,903][flp2p.graph_runner][INFO] - Test, Round 154 : loss => 5.492412731194496,  accuracy: 0.3244
[2025-09-11 19:02:31,276][flp2p.graph_runner][INFO] - Train, Round 155 : loss => 0.005195717622118539,  accuracy: 1.0, gradient_norm : 0.23191672224265333
[2025-09-11 19:02:45,419][flp2p.graph_runner][INFO] - Test, Round 155 : loss => 5.506270169508457,  accuracy: 0.3219
[2025-09-11 19:03:21,610][flp2p.graph_runner][INFO] - Train, Round 156 : loss => 0.00508906234599029,  accuracy: 1.0, gradient_norm : 0.22899413782744182
[2025-09-11 19:03:35,812][flp2p.graph_runner][INFO] - Test, Round 156 : loss => 5.522866334140301,  accuracy: 0.323
[2025-09-11 19:04:12,224][flp2p.graph_runner][INFO] - Train, Round 157 : loss => 0.004989208773768041,  accuracy: 1.0, gradient_norm : 0.21698530118604686
[2025-09-11 19:04:26,561][flp2p.graph_runner][INFO] - Test, Round 157 : loss => 5.533466747891903,  accuracy: 0.3226
[2025-09-11 19:05:03,010][flp2p.graph_runner][INFO] - Train, Round 158 : loss => 0.0048776246761553935,  accuracy: 1.0, gradient_norm : 0.22066572147702398
[2025-09-11 19:05:17,260][flp2p.graph_runner][INFO] - Test, Round 158 : loss => 5.547128697645665,  accuracy: 0.3241
[2025-09-11 19:05:53,440][flp2p.graph_runner][INFO] - Train, Round 159 : loss => 0.004784076751481433,  accuracy: 1.0, gradient_norm : 0.20638257658144904
[2025-09-11 19:06:07,871][flp2p.graph_runner][INFO] - Test, Round 159 : loss => 5.562242309713364,  accuracy: 0.3232
[2025-09-11 19:06:44,207][flp2p.graph_runner][INFO] - Train, Round 160 : loss => 0.0046881429690984085,  accuracy: 1.0, gradient_norm : 0.2071314214802075
[2025-09-11 19:06:58,463][flp2p.graph_runner][INFO] - Test, Round 160 : loss => 5.572543844842911,  accuracy: 0.3235
[2025-09-11 19:07:34,733][flp2p.graph_runner][INFO] - Train, Round 161 : loss => 0.004606514123249022,  accuracy: 1.0, gradient_norm : 0.21018235589109058
[2025-09-11 19:07:48,844][flp2p.graph_runner][INFO] - Test, Round 161 : loss => 5.590058331286907,  accuracy: 0.3234
[2025-09-11 19:08:24,778][flp2p.graph_runner][INFO] - Train, Round 162 : loss => 0.004520036230969709,  accuracy: 1.0, gradient_norm : 0.20492681377152916
[2025-09-11 19:08:39,160][flp2p.graph_runner][INFO] - Test, Round 162 : loss => 5.600819061267376,  accuracy: 0.3233
[2025-09-11 19:09:15,398][flp2p.graph_runner][INFO] - Train, Round 163 : loss => 0.004432638996707587,  accuracy: 1.0, gradient_norm : 0.20070103044442913
[2025-09-11 19:09:29,817][flp2p.graph_runner][INFO] - Test, Round 163 : loss => 5.61255414942503,  accuracy: 0.3233
[2025-09-11 19:10:05,532][flp2p.graph_runner][INFO] - Train, Round 164 : loss => 0.004350691322276058,  accuracy: 1.0, gradient_norm : 0.19247159600391497
[2025-09-11 19:10:20,095][flp2p.graph_runner][INFO] - Test, Round 164 : loss => 5.626056426823139,  accuracy: 0.3225
[2025-09-11 19:10:56,067][flp2p.graph_runner][INFO] - Train, Round 165 : loss => 0.004275746808173911,  accuracy: 1.0, gradient_norm : 0.19493192490336816
[2025-09-11 19:11:10,847][flp2p.graph_runner][INFO] - Test, Round 165 : loss => 5.6362612340092655,  accuracy: 0.3235
[2025-09-11 19:11:46,413][flp2p.graph_runner][INFO] - Train, Round 166 : loss => 0.00419935409639341,  accuracy: 1.0, gradient_norm : 0.19887264799965665
[2025-09-11 19:12:01,063][flp2p.graph_runner][INFO] - Test, Round 166 : loss => 5.64980921498537,  accuracy: 0.3224
[2025-09-11 19:12:37,281][flp2p.graph_runner][INFO] - Train, Round 167 : loss => 0.004121644755650775,  accuracy: 1.0, gradient_norm : 0.18305560287727554
[2025-09-11 19:12:54,660][flp2p.graph_runner][INFO] - Test, Round 167 : loss => 5.661974083304405,  accuracy: 0.325
[2025-09-11 19:13:30,269][flp2p.graph_runner][INFO] - Train, Round 168 : loss => 0.004054860125276415,  accuracy: 1.0, gradient_norm : 0.17731327818918868
[2025-09-11 19:13:50,456][flp2p.graph_runner][INFO] - Test, Round 168 : loss => 5.675428097438812,  accuracy: 0.3235
[2025-09-11 19:14:25,971][flp2p.graph_runner][INFO] - Train, Round 169 : loss => 0.003989522352640051,  accuracy: 1.0, gradient_norm : 0.1841934395657966
[2025-09-11 19:14:47,096][flp2p.graph_runner][INFO] - Test, Round 169 : loss => 5.686711126697063,  accuracy: 0.3221
[2025-09-11 19:15:22,832][flp2p.graph_runner][INFO] - Train, Round 170 : loss => 0.003919289475064339,  accuracy: 1.0, gradient_norm : 0.1732756107462957
[2025-09-11 19:15:42,536][flp2p.graph_runner][INFO] - Test, Round 170 : loss => 5.697914372336864,  accuracy: 0.3238
[2025-09-11 19:16:18,424][flp2p.graph_runner][INFO] - Train, Round 171 : loss => 0.003857397423271322,  accuracy: 1.0, gradient_norm : 0.17512379360746405
[2025-09-11 19:16:34,210][flp2p.graph_runner][INFO] - Test, Round 171 : loss => 5.706885533595085,  accuracy: 0.3234
[2025-09-11 19:17:10,091][flp2p.graph_runner][INFO] - Train, Round 172 : loss => 0.0037957362495944836,  accuracy: 1.0, gradient_norm : 0.17667018241442223
[2025-09-11 19:17:24,002][flp2p.graph_runner][INFO] - Test, Round 172 : loss => 5.717879388487339,  accuracy: 0.3228
[2025-09-11 19:17:59,815][flp2p.graph_runner][INFO] - Train, Round 173 : loss => 0.003735594362321231,  accuracy: 1.0, gradient_norm : 0.16594357681348504
[2025-09-11 19:18:13,687][flp2p.graph_runner][INFO] - Test, Round 173 : loss => 5.73020078009367,  accuracy: 0.3239
[2025-09-11 19:18:49,576][flp2p.graph_runner][INFO] - Train, Round 174 : loss => 0.003675610624874633,  accuracy: 1.0, gradient_norm : 0.16287647798660757
[2025-09-11 19:19:03,409][flp2p.graph_runner][INFO] - Test, Round 174 : loss => 5.740631493198872,  accuracy: 0.3229
[2025-09-11 19:19:39,008][flp2p.graph_runner][INFO] - Train, Round 175 : loss => 0.0036208078323397793,  accuracy: 1.0, gradient_norm : 0.1651690596930276
[2025-09-11 19:19:52,748][flp2p.graph_runner][INFO] - Test, Round 175 : loss => 5.751832778918743,  accuracy: 0.3225
[2025-09-11 19:20:28,472][flp2p.graph_runner][INFO] - Train, Round 176 : loss => 0.0035657927669429535,  accuracy: 1.0, gradient_norm : 0.16439652938854163
[2025-09-11 19:20:42,645][flp2p.graph_runner][INFO] - Test, Round 176 : loss => 5.762289458310604,  accuracy: 0.3228
[2025-09-11 19:21:18,479][flp2p.graph_runner][INFO] - Train, Round 177 : loss => 0.0035134298178309113,  accuracy: 1.0, gradient_norm : 0.16577995053172226
[2025-09-11 19:21:32,361][flp2p.graph_runner][INFO] - Test, Round 177 : loss => 5.771241927683353,  accuracy: 0.323
[2025-09-11 19:22:08,605][flp2p.graph_runner][INFO] - Train, Round 178 : loss => 0.0034629309089117062,  accuracy: 1.0, gradient_norm : 0.16330756967725774
[2025-09-11 19:22:22,452][flp2p.graph_runner][INFO] - Test, Round 178 : loss => 5.7817225870132445,  accuracy: 0.323
[2025-09-11 19:22:58,512][flp2p.graph_runner][INFO] - Train, Round 179 : loss => 0.0034135672114401433,  accuracy: 1.0, gradient_norm : 0.16172630892146567
[2025-09-11 19:23:12,434][flp2p.graph_runner][INFO] - Test, Round 179 : loss => 5.792450107872486,  accuracy: 0.3224
[2025-09-11 19:23:48,379][flp2p.graph_runner][INFO] - Train, Round 180 : loss => 0.003360322687998027,  accuracy: 1.0, gradient_norm : 0.15787721625717752
[2025-09-11 19:24:02,298][flp2p.graph_runner][INFO] - Test, Round 180 : loss => 5.80291225233078,  accuracy: 0.3232
[2025-09-11 19:24:38,625][flp2p.graph_runner][INFO] - Train, Round 181 : loss => 0.003317013929966681,  accuracy: 1.0, gradient_norm : 0.15330367985884624
[2025-09-11 19:24:52,959][flp2p.graph_runner][INFO] - Test, Round 181 : loss => 5.8112902699947355,  accuracy: 0.3231
[2025-09-11 19:25:29,133][flp2p.graph_runner][INFO] - Train, Round 182 : loss => 0.003270045414246852,  accuracy: 1.0, gradient_norm : 0.15426405797973694
[2025-09-11 19:25:43,423][flp2p.graph_runner][INFO] - Test, Round 182 : loss => 5.819695028150082,  accuracy: 0.3234
[2025-09-11 19:26:19,557][flp2p.graph_runner][INFO] - Train, Round 183 : loss => 0.0032272755945450633,  accuracy: 1.0, gradient_norm : 0.15667752527285445
[2025-09-11 19:26:33,754][flp2p.graph_runner][INFO] - Test, Round 183 : loss => 5.830377497363091,  accuracy: 0.3235
[2025-09-11 19:27:09,936][flp2p.graph_runner][INFO] - Train, Round 184 : loss => 0.003181637476421505,  accuracy: 1.0, gradient_norm : 0.1510811503572222
[2025-09-11 19:27:23,943][flp2p.graph_runner][INFO] - Test, Round 184 : loss => 5.840168447399139,  accuracy: 0.3229
[2025-09-11 19:28:00,101][flp2p.graph_runner][INFO] - Train, Round 185 : loss => 0.003139633306273027,  accuracy: 1.0, gradient_norm : 0.14803879162794512
[2025-09-11 19:28:14,469][flp2p.graph_runner][INFO] - Test, Round 185 : loss => 5.849466091990471,  accuracy: 0.3235
[2025-09-11 19:28:50,710][flp2p.graph_runner][INFO] - Train, Round 186 : loss => 0.0030988502535910815,  accuracy: 1.0, gradient_norm : 0.14535770864937161
[2025-09-11 19:29:05,059][flp2p.graph_runner][INFO] - Test, Round 186 : loss => 5.858979650092125,  accuracy: 0.3223
[2025-09-11 19:29:41,021][flp2p.graph_runner][INFO] - Train, Round 187 : loss => 0.003059171737064996,  accuracy: 1.0, gradient_norm : 0.14453890440838224
[2025-09-11 19:29:55,396][flp2p.graph_runner][INFO] - Test, Round 187 : loss => 5.86562437595129,  accuracy: 0.3226
[2025-09-11 19:30:31,279][flp2p.graph_runner][INFO] - Train, Round 188 : loss => 0.003019544199293403,  accuracy: 1.0, gradient_norm : 0.14048941353381164
[2025-09-11 19:30:45,552][flp2p.graph_runner][INFO] - Test, Round 188 : loss => 5.875616184318066,  accuracy: 0.3232
[2025-09-11 19:31:21,634][flp2p.graph_runner][INFO] - Train, Round 189 : loss => 0.0029814675180629526,  accuracy: 1.0, gradient_norm : 0.14515071603715995
[2025-09-11 19:31:35,982][flp2p.graph_runner][INFO] - Test, Round 189 : loss => 5.882715558445454,  accuracy: 0.3231
[2025-09-11 19:32:12,071][flp2p.graph_runner][INFO] - Train, Round 190 : loss => 0.0029449662750024187,  accuracy: 1.0, gradient_norm : 0.1395450111426017
[2025-09-11 19:32:26,193][flp2p.graph_runner][INFO] - Test, Round 190 : loss => 5.890326685070992,  accuracy: 0.3227
[2025-09-11 19:33:02,458][flp2p.graph_runner][INFO] - Train, Round 191 : loss => 0.0029090919575052495,  accuracy: 1.0, gradient_norm : 0.1421275608561405
[2025-09-11 19:33:16,728][flp2p.graph_runner][INFO] - Test, Round 191 : loss => 5.900237342739105,  accuracy: 0.3229
[2025-09-11 19:33:52,765][flp2p.graph_runner][INFO] - Train, Round 192 : loss => 0.0028728799070328647,  accuracy: 1.0, gradient_norm : 0.13754167577297197
[2025-09-11 19:34:07,297][flp2p.graph_runner][INFO] - Test, Round 192 : loss => 5.907017987310886,  accuracy: 0.3226
[2025-09-11 19:34:43,539][flp2p.graph_runner][INFO] - Train, Round 193 : loss => 0.00283842540695332,  accuracy: 1.0, gradient_norm : 0.13346666614542013
[2025-09-11 19:34:57,742][flp2p.graph_runner][INFO] - Test, Round 193 : loss => 5.9163400216341016,  accuracy: 0.3231
[2025-09-11 19:35:34,084][flp2p.graph_runner][INFO] - Train, Round 194 : loss => 0.0028052501009854804,  accuracy: 1.0, gradient_norm : 0.1373463998950894
[2025-09-11 19:35:48,673][flp2p.graph_runner][INFO] - Test, Round 194 : loss => 5.923256739807129,  accuracy: 0.3229
[2025-09-11 19:36:24,798][flp2p.graph_runner][INFO] - Train, Round 195 : loss => 0.0027723066557276367,  accuracy: 1.0, gradient_norm : 0.13338273491514407
[2025-09-11 19:36:39,303][flp2p.graph_runner][INFO] - Test, Round 195 : loss => 5.928993956792355,  accuracy: 0.3233
[2025-09-11 19:37:15,534][flp2p.graph_runner][INFO] - Train, Round 196 : loss => 0.0027418454895572113,  accuracy: 1.0, gradient_norm : 0.13377042182725643
[2025-09-11 19:37:29,839][flp2p.graph_runner][INFO] - Test, Round 196 : loss => 5.939724207019806,  accuracy: 0.3225
[2025-09-11 19:38:05,645][flp2p.graph_runner][INFO] - Train, Round 197 : loss => 0.0027100480087392494,  accuracy: 1.0, gradient_norm : 0.12848781316326058
[2025-09-11 19:38:20,380][flp2p.graph_runner][INFO] - Test, Round 197 : loss => 5.94558469645977,  accuracy: 0.3227
[2025-09-11 19:38:55,898][flp2p.graph_runner][INFO] - Train, Round 198 : loss => 0.002676408219413133,  accuracy: 1.0, gradient_norm : 0.12496431342767922
[2025-09-11 19:39:13,544][flp2p.graph_runner][INFO] - Test, Round 198 : loss => 5.956279728531838,  accuracy: 0.323
[2025-09-11 19:39:49,334][flp2p.graph_runner][INFO] - Train, Round 199 : loss => 0.002649869771169809,  accuracy: 1.0, gradient_norm : 0.13755584202400392
[2025-09-11 19:40:10,348][flp2p.graph_runner][INFO] - Test, Round 199 : loss => 5.962061416983604,  accuracy: 0.3232
[2025-09-11 19:40:45,933][flp2p.graph_runner][INFO] - Train, Round 200 : loss => 0.002620803601215205,  accuracy: 1.0, gradient_norm : 0.12670994023426846
[2025-09-11 19:41:06,895][flp2p.graph_runner][INFO] - Test, Round 200 : loss => 5.969913982820511,  accuracy: 0.3219
[2025-09-11 19:41:42,851][flp2p.graph_runner][INFO] - Train, Round 201 : loss => 0.0025906665304501085,  accuracy: 1.0, gradient_norm : 0.1272460381545673
[2025-09-11 19:42:00,604][flp2p.graph_runner][INFO] - Test, Round 201 : loss => 5.9759067160606385,  accuracy: 0.3228
[2025-09-11 19:42:36,953][flp2p.graph_runner][INFO] - Train, Round 202 : loss => 0.002562813452192737,  accuracy: 1.0, gradient_norm : 0.12276636537510419
[2025-09-11 19:42:50,807][flp2p.graph_runner][INFO] - Test, Round 202 : loss => 5.9835091105461125,  accuracy: 0.3224
[2025-09-11 19:43:26,834][flp2p.graph_runner][INFO] - Train, Round 203 : loss => 0.0025362562601124712,  accuracy: 1.0, gradient_norm : 0.12166461182432245
[2025-09-11 19:43:40,823][flp2p.graph_runner][INFO] - Test, Round 203 : loss => 5.992278311014175,  accuracy: 0.3219
[2025-09-11 19:44:16,864][flp2p.graph_runner][INFO] - Train, Round 204 : loss => 0.0025092260684080737,  accuracy: 1.0, gradient_norm : 0.12346940089839288
[2025-09-11 19:44:30,815][flp2p.graph_runner][INFO] - Test, Round 204 : loss => 5.997720101118087,  accuracy: 0.3227
[2025-09-11 19:45:06,687][flp2p.graph_runner][INFO] - Train, Round 205 : loss => 0.0024854557567838727,  accuracy: 1.0, gradient_norm : 0.11995239154201005
[2025-09-11 19:45:20,672][flp2p.graph_runner][INFO] - Test, Round 205 : loss => 6.005975595462322,  accuracy: 0.3237
[2025-09-11 19:45:56,704][flp2p.graph_runner][INFO] - Train, Round 206 : loss => 0.0024572473824567473,  accuracy: 1.0, gradient_norm : 0.11324396651812464
[2025-09-11 19:46:10,525][flp2p.graph_runner][INFO] - Test, Round 206 : loss => 6.012776751542091,  accuracy: 0.3232
[2025-09-11 19:46:46,534][flp2p.graph_runner][INFO] - Train, Round 207 : loss => 0.002433214141443993,  accuracy: 1.0, gradient_norm : 0.12023248106517323
[2025-09-11 19:47:00,404][flp2p.graph_runner][INFO] - Test, Round 207 : loss => 6.0176449323654175,  accuracy: 0.3223
[2025-09-11 19:47:36,516][flp2p.graph_runner][INFO] - Train, Round 208 : loss => 0.002408946719903421,  accuracy: 1.0, gradient_norm : 0.11438769346655075
[2025-09-11 19:47:50,747][flp2p.graph_runner][INFO] - Test, Round 208 : loss => 6.024515590846539,  accuracy: 0.3229
[2025-09-11 19:48:26,704][flp2p.graph_runner][INFO] - Train, Round 209 : loss => 0.002385118683353843,  accuracy: 1.0, gradient_norm : 0.12195621530834928
[2025-09-11 19:48:40,835][flp2p.graph_runner][INFO] - Test, Round 209 : loss => 6.031804073536396,  accuracy: 0.3228
[2025-09-11 19:49:17,326][flp2p.graph_runner][INFO] - Train, Round 210 : loss => 0.002361761965827707,  accuracy: 1.0, gradient_norm : 0.10716147110586281
[2025-09-11 19:49:31,556][flp2p.graph_runner][INFO] - Test, Round 210 : loss => 6.038400921928883,  accuracy: 0.3223
[2025-09-11 19:50:07,566][flp2p.graph_runner][INFO] - Train, Round 211 : loss => 0.002338663489839139,  accuracy: 1.0, gradient_norm : 0.11301738457413352
[2025-09-11 19:50:21,866][flp2p.graph_runner][INFO] - Test, Round 211 : loss => 6.044023452532292,  accuracy: 0.3227
[2025-09-11 19:50:58,625][flp2p.graph_runner][INFO] - Train, Round 212 : loss => 0.00231699225900229,  accuracy: 1.0, gradient_norm : 0.11472096704937812
[2025-09-11 19:51:12,940][flp2p.graph_runner][INFO] - Test, Round 212 : loss => 6.051021396327019,  accuracy: 0.3226
[2025-09-11 19:51:49,238][flp2p.graph_runner][INFO] - Train, Round 213 : loss => 0.0022955519052629838,  accuracy: 1.0, gradient_norm : 0.11684055383902052
[2025-09-11 19:52:03,381][flp2p.graph_runner][INFO] - Test, Round 213 : loss => 6.057863994717598,  accuracy: 0.3227
[2025-09-11 19:52:39,264][flp2p.graph_runner][INFO] - Train, Round 214 : loss => 0.002273721642656407,  accuracy: 1.0, gradient_norm : 0.10912528349025971
[2025-09-11 19:52:53,731][flp2p.graph_runner][INFO] - Test, Round 214 : loss => 6.063636707031727,  accuracy: 0.3226
[2025-09-11 19:53:29,717][flp2p.graph_runner][INFO] - Train, Round 215 : loss => 0.0022508144126428906,  accuracy: 1.0, gradient_norm : 0.1099218803942048
[2025-09-11 19:53:44,011][flp2p.graph_runner][INFO] - Test, Round 215 : loss => 6.069349554598332,  accuracy: 0.3228
[2025-09-11 19:54:20,483][flp2p.graph_runner][INFO] - Train, Round 216 : loss => 0.0022317276781056226,  accuracy: 1.0, gradient_norm : 0.1133169091515657
[2025-09-11 19:54:34,689][flp2p.graph_runner][INFO] - Test, Round 216 : loss => 6.077436061632633,  accuracy: 0.3228
[2025-09-11 19:55:10,789][flp2p.graph_runner][INFO] - Train, Round 217 : loss => 0.002211455682408996,  accuracy: 1.0, gradient_norm : 0.1077460917889518
[2025-09-11 19:55:24,913][flp2p.graph_runner][INFO] - Test, Round 217 : loss => 6.0805914457559584,  accuracy: 0.3233
[2025-09-11 19:56:01,422][flp2p.graph_runner][INFO] - Train, Round 218 : loss => 0.002192041916374971,  accuracy: 1.0, gradient_norm : 0.1071371249567004
[2025-09-11 19:56:15,774][flp2p.graph_runner][INFO] - Test, Round 218 : loss => 6.087360805869102,  accuracy: 0.3222
[2025-09-11 19:56:52,007][flp2p.graph_runner][INFO] - Train, Round 219 : loss => 0.002172416017856448,  accuracy: 1.0, gradient_norm : 0.10794200759533928
[2025-09-11 19:57:06,440][flp2p.graph_runner][INFO] - Test, Round 219 : loss => 6.094299907577038,  accuracy: 0.3223
[2025-09-11 19:57:42,633][flp2p.graph_runner][INFO] - Train, Round 220 : loss => 0.0021526415049447686,  accuracy: 1.0, gradient_norm : 0.10661891126035213
[2025-09-11 19:57:56,769][flp2p.graph_runner][INFO] - Test, Round 220 : loss => 6.09885125772953,  accuracy: 0.3237
[2025-09-11 19:58:33,050][flp2p.graph_runner][INFO] - Train, Round 221 : loss => 0.0021354089131151947,  accuracy: 1.0, gradient_norm : 0.10665758680455091
[2025-09-11 19:58:47,424][flp2p.graph_runner][INFO] - Test, Round 221 : loss => 6.104517638897896,  accuracy: 0.3229
[2025-09-11 19:59:23,403][flp2p.graph_runner][INFO] - Train, Round 222 : loss => 0.002115856940799859,  accuracy: 1.0, gradient_norm : 0.10454415730619632
[2025-09-11 19:59:37,708][flp2p.graph_runner][INFO] - Test, Round 222 : loss => 6.110553845286369,  accuracy: 0.323
[2025-09-11 20:00:13,918][flp2p.graph_runner][INFO] - Train, Round 223 : loss => 0.002098786212494208,  accuracy: 1.0, gradient_norm : 0.10621131088651962
[2025-09-11 20:00:28,359][flp2p.graph_runner][INFO] - Test, Round 223 : loss => 6.116272845351696,  accuracy: 0.3219
[2025-09-11 20:01:04,309][flp2p.graph_runner][INFO] - Train, Round 224 : loss => 0.002078475791737825,  accuracy: 1.0, gradient_norm : 0.09896526299162899
[2025-09-11 20:01:19,068][flp2p.graph_runner][INFO] - Test, Round 224 : loss => 6.1204949571609495,  accuracy: 0.3224
[2025-09-11 20:01:54,931][flp2p.graph_runner][INFO] - Train, Round 225 : loss => 0.0020629576505840908,  accuracy: 1.0, gradient_norm : 0.09864086266644613
[2025-09-11 20:02:09,341][flp2p.graph_runner][INFO] - Test, Round 225 : loss => 6.127591981530189,  accuracy: 0.322
[2025-09-11 20:02:45,197][flp2p.graph_runner][INFO] - Train, Round 226 : loss => 0.0020456296875878857,  accuracy: 1.0, gradient_norm : 0.10306223725022694
[2025-09-11 20:02:59,788][flp2p.graph_runner][INFO] - Test, Round 226 : loss => 6.132853290295601,  accuracy: 0.3227
[2025-09-11 20:03:35,709][flp2p.graph_runner][INFO] - Train, Round 227 : loss => 0.002028801469229317,  accuracy: 1.0, gradient_norm : 0.09774615091181732
[2025-09-11 20:03:53,757][flp2p.graph_runner][INFO] - Test, Round 227 : loss => 6.1384992678165435,  accuracy: 0.3226
[2025-09-11 20:04:29,515][flp2p.graph_runner][INFO] - Train, Round 228 : loss => 0.002011313801631331,  accuracy: 1.0, gradient_norm : 0.10179984378206756
[2025-09-11 20:04:49,990][flp2p.graph_runner][INFO] - Test, Round 228 : loss => 6.145043228793144,  accuracy: 0.3228
[2025-09-11 20:05:25,757][flp2p.graph_runner][INFO] - Train, Round 229 : loss => 0.0019961045752764526,  accuracy: 1.0, gradient_norm : 0.09762418241337795
[2025-09-11 20:05:46,633][flp2p.graph_runner][INFO] - Test, Round 229 : loss => 6.148586438417435,  accuracy: 0.3227
[2025-09-11 20:06:22,493][flp2p.graph_runner][INFO] - Train, Round 230 : loss => 0.001981031553974996,  accuracy: 1.0, gradient_norm : 0.10216085629300058
[2025-09-11 20:06:40,133][flp2p.graph_runner][INFO] - Test, Round 230 : loss => 6.155101268601418,  accuracy: 0.3229
[2025-09-11 20:07:16,007][flp2p.graph_runner][INFO] - Train, Round 231 : loss => 0.001964094954989074,  accuracy: 1.0, gradient_norm : 0.095757720347
[2025-09-11 20:07:31,111][flp2p.graph_runner][INFO] - Test, Round 231 : loss => 6.158174686908722,  accuracy: 0.3229
[2025-09-11 20:08:07,211][flp2p.graph_runner][INFO] - Train, Round 232 : loss => 0.0019489149829799623,  accuracy: 1.0, gradient_norm : 0.09471858210471513
[2025-09-11 20:08:20,979][flp2p.graph_runner][INFO] - Test, Round 232 : loss => 6.162835985207558,  accuracy: 0.3231
[2025-09-11 20:08:56,995][flp2p.graph_runner][INFO] - Train, Round 233 : loss => 0.0019339614936325221,  accuracy: 1.0, gradient_norm : 0.09530176810548253
[2025-09-11 20:09:10,751][flp2p.graph_runner][INFO] - Test, Round 233 : loss => 6.168917892169953,  accuracy: 0.3229
[2025-09-11 20:09:46,840][flp2p.graph_runner][INFO] - Train, Round 234 : loss => 0.0019196602869487845,  accuracy: 1.0, gradient_norm : 0.10020860135394838
[2025-09-11 20:10:00,857][flp2p.graph_runner][INFO] - Test, Round 234 : loss => 6.175226117992401,  accuracy: 0.323
[2025-09-11 20:10:36,803][flp2p.graph_runner][INFO] - Train, Round 235 : loss => 0.0019051682622133136,  accuracy: 1.0, gradient_norm : 0.09784430095867495
[2025-09-11 20:10:50,895][flp2p.graph_runner][INFO] - Test, Round 235 : loss => 6.179794206142425,  accuracy: 0.3226
[2025-09-11 20:11:26,886][flp2p.graph_runner][INFO] - Train, Round 236 : loss => 0.0018906401130273792,  accuracy: 1.0, gradient_norm : 0.09513811265787332
[2025-09-11 20:11:40,903][flp2p.graph_runner][INFO] - Test, Round 236 : loss => 6.184614840149879,  accuracy: 0.3226
[2025-09-11 20:12:16,622][flp2p.graph_runner][INFO] - Train, Round 237 : loss => 0.0018769424628893225,  accuracy: 1.0, gradient_norm : 0.09257477855902349
[2025-09-11 20:12:30,475][flp2p.graph_runner][INFO] - Test, Round 237 : loss => 6.188913474345207,  accuracy: 0.3228
[2025-09-11 20:13:06,752][flp2p.graph_runner][INFO] - Train, Round 238 : loss => 0.001863214593516507,  accuracy: 1.0, gradient_norm : 0.09343349105847741
[2025-09-11 20:13:20,877][flp2p.graph_runner][INFO] - Test, Round 238 : loss => 6.193083671927452,  accuracy: 0.3226
[2025-09-11 20:13:57,489][flp2p.graph_runner][INFO] - Train, Round 239 : loss => 0.0018492427319870327,  accuracy: 1.0, gradient_norm : 0.09043224237947575
[2025-09-11 20:14:11,874][flp2p.graph_runner][INFO] - Test, Round 239 : loss => 6.197869196391106,  accuracy: 0.3226
[2025-09-11 20:14:48,074][flp2p.graph_runner][INFO] - Train, Round 240 : loss => 0.0018362865318340478,  accuracy: 1.0, gradient_norm : 0.09399993572450274
[2025-09-11 20:15:02,584][flp2p.graph_runner][INFO] - Test, Round 240 : loss => 6.2025465652227405,  accuracy: 0.3226
[2025-09-11 20:15:38,693][flp2p.graph_runner][INFO] - Train, Round 241 : loss => 0.0018230007044136676,  accuracy: 1.0, gradient_norm : 0.09231496383971972
[2025-09-11 20:15:53,063][flp2p.graph_runner][INFO] - Test, Round 241 : loss => 6.20712771692276,  accuracy: 0.3225
[2025-09-11 20:16:29,474][flp2p.graph_runner][INFO] - Train, Round 242 : loss => 0.0018106876578046166,  accuracy: 1.0, gradient_norm : 0.09382391949611488
[2025-09-11 20:16:43,845][flp2p.graph_runner][INFO] - Test, Round 242 : loss => 6.214326349639893,  accuracy: 0.3229
[2025-09-11 20:17:20,233][flp2p.graph_runner][INFO] - Train, Round 243 : loss => 0.0017970813261733083,  accuracy: 1.0, gradient_norm : 0.08808851717629242
[2025-09-11 20:17:34,421][flp2p.graph_runner][INFO] - Test, Round 243 : loss => 6.217768481588363,  accuracy: 0.3232
[2025-09-11 20:18:10,657][flp2p.graph_runner][INFO] - Train, Round 244 : loss => 0.0017843122660512248,  accuracy: 1.0, gradient_norm : 0.08999749195369006
[2025-09-11 20:18:24,877][flp2p.graph_runner][INFO] - Test, Round 244 : loss => 6.222247801733017,  accuracy: 0.322
[2025-09-11 20:19:01,106][flp2p.graph_runner][INFO] - Train, Round 245 : loss => 0.0017725200600640773,  accuracy: 1.0, gradient_norm : 0.08863707778112334
[2025-09-11 20:19:15,165][flp2p.graph_runner][INFO] - Test, Round 245 : loss => 6.227048821520805,  accuracy: 0.3227
[2025-09-11 20:19:51,401][flp2p.graph_runner][INFO] - Train, Round 246 : loss => 0.0017603241286997218,  accuracy: 1.0, gradient_norm : 0.09162267394174395
[2025-09-11 20:20:05,639][flp2p.graph_runner][INFO] - Test, Round 246 : loss => 6.230187689614296,  accuracy: 0.3222
[2025-09-11 20:20:42,087][flp2p.graph_runner][INFO] - Train, Round 247 : loss => 0.001748065782012419,  accuracy: 1.0, gradient_norm : 0.08992891467523811
[2025-09-11 20:20:56,328][flp2p.graph_runner][INFO] - Test, Round 247 : loss => 6.2361552141428,  accuracy: 0.3219
[2025-09-11 20:21:32,824][flp2p.graph_runner][INFO] - Train, Round 248 : loss => 0.0017364180737058632,  accuracy: 1.0, gradient_norm : 0.08732946075535816
[2025-09-11 20:21:47,184][flp2p.graph_runner][INFO] - Test, Round 248 : loss => 6.238557571196556,  accuracy: 0.3227
[2025-09-11 20:22:23,696][flp2p.graph_runner][INFO] - Train, Round 249 : loss => 0.0017243304654645422,  accuracy: 1.0, gradient_norm : 0.08618900692016211
[2025-09-11 20:22:37,852][flp2p.graph_runner][INFO] - Test, Round 249 : loss => 6.242408995699883,  accuracy: 0.3234
[2025-09-11 20:23:14,157][flp2p.graph_runner][INFO] - Train, Round 250 : loss => 0.0017135606948189281,  accuracy: 1.0, gradient_norm : 0.08677685872028075
[2025-09-11 20:23:28,438][flp2p.graph_runner][INFO] - Test, Round 250 : loss => 6.24827463889122,  accuracy: 0.3221
[2025-09-11 20:24:04,553][flp2p.graph_runner][INFO] - Train, Round 251 : loss => 0.0017020627689392615,  accuracy: 1.0, gradient_norm : 0.08401050661511182
[2025-09-11 20:24:19,025][flp2p.graph_runner][INFO] - Test, Round 251 : loss => 6.251775337266922,  accuracy: 0.3226
[2025-09-11 20:24:55,092][flp2p.graph_runner][INFO] - Train, Round 252 : loss => 0.0016908801441362197,  accuracy: 1.0, gradient_norm : 0.08530244335369078
[2025-09-11 20:25:09,761][flp2p.graph_runner][INFO] - Test, Round 252 : loss => 6.256083059501648,  accuracy: 0.3221
[2025-09-11 20:25:45,638][flp2p.graph_runner][INFO] - Train, Round 253 : loss => 0.0016798990999086524,  accuracy: 1.0, gradient_norm : 0.08238402715008818
[2025-09-11 20:26:00,201][flp2p.graph_runner][INFO] - Test, Round 253 : loss => 6.25999888458252,  accuracy: 0.3223
[2025-09-11 20:26:36,212][flp2p.graph_runner][INFO] - Train, Round 254 : loss => 0.0016701251163855583,  accuracy: 1.0, gradient_norm : 0.08481474897566339
[2025-09-11 20:26:50,680][flp2p.graph_runner][INFO] - Test, Round 254 : loss => 6.263976303815841,  accuracy: 0.3226
[2025-09-11 20:27:26,734][flp2p.graph_runner][INFO] - Train, Round 255 : loss => 0.0016585943503499346,  accuracy: 1.0, gradient_norm : 0.08651336554163212
[2025-09-11 20:27:44,608][flp2p.graph_runner][INFO] - Test, Round 255 : loss => 6.268066849970817,  accuracy: 0.3226
[2025-09-11 20:28:20,531][flp2p.graph_runner][INFO] - Train, Round 256 : loss => 0.00164836749221043,  accuracy: 1.0, gradient_norm : 0.08295409069597375
[2025-09-11 20:28:41,852][flp2p.graph_runner][INFO] - Test, Round 256 : loss => 6.271972968435287,  accuracy: 0.3221
[2025-09-11 20:29:17,733][flp2p.graph_runner][INFO] - Train, Round 257 : loss => 0.0016382319488911896,  accuracy: 1.0, gradient_norm : 0.08322954745647035
[2025-09-11 20:29:37,117][flp2p.graph_runner][INFO] - Test, Round 257 : loss => 6.27556079826355,  accuracy: 0.3225
[2025-09-11 20:30:12,391][flp2p.graph_runner][INFO] - Train, Round 258 : loss => 0.0016282584568883371,  accuracy: 1.0, gradient_norm : 0.08329157459818011
[2025-09-11 20:30:30,241][flp2p.graph_runner][INFO] - Test, Round 258 : loss => 6.279486933398247,  accuracy: 0.3231
[2025-09-11 20:31:06,558][flp2p.graph_runner][INFO] - Train, Round 259 : loss => 0.0016181175520857017,  accuracy: 1.0, gradient_norm : 0.08374954120345184
[2025-09-11 20:31:21,125][flp2p.graph_runner][INFO] - Test, Round 259 : loss => 6.28452545516491,  accuracy: 0.3225
[2025-09-11 20:31:57,356][flp2p.graph_runner][INFO] - Train, Round 260 : loss => 0.001608986518452487,  accuracy: 1.0, gradient_norm : 0.07992367032063168
[2025-09-11 20:32:11,273][flp2p.graph_runner][INFO] - Test, Round 260 : loss => 6.2879953341007235,  accuracy: 0.3225
[2025-09-11 20:32:47,161][flp2p.graph_runner][INFO] - Train, Round 261 : loss => 0.0015989282081379013,  accuracy: 1.0, gradient_norm : 0.08188116324904172
[2025-09-11 20:33:00,906][flp2p.graph_runner][INFO] - Test, Round 261 : loss => 6.291238573837281,  accuracy: 0.3227
[2025-09-11 20:33:36,974][flp2p.graph_runner][INFO] - Train, Round 262 : loss => 0.0015886478659861796,  accuracy: 1.0, gradient_norm : 0.07924456445581593
[2025-09-11 20:33:51,006][flp2p.graph_runner][INFO] - Test, Round 262 : loss => 6.293927999401093,  accuracy: 0.322
[2025-09-11 20:34:26,742][flp2p.graph_runner][INFO] - Train, Round 263 : loss => 0.0015805545924619461,  accuracy: 1.0, gradient_norm : 0.07786517142315816
[2025-09-11 20:34:40,508][flp2p.graph_runner][INFO] - Test, Round 263 : loss => 6.299079975056649,  accuracy: 0.3222
[2025-09-11 20:35:16,806][flp2p.graph_runner][INFO] - Train, Round 264 : loss => 0.0015711093930197728,  accuracy: 1.0, gradient_norm : 0.0801838886208333
[2025-09-11 20:35:30,576][flp2p.graph_runner][INFO] - Test, Round 264 : loss => 6.302089849090576,  accuracy: 0.3217
[2025-09-11 20:36:06,670][flp2p.graph_runner][INFO] - Train, Round 265 : loss => 0.0015623661645804531,  accuracy: 1.0, gradient_norm : 0.07962114569003759
[2025-09-11 20:36:20,683][flp2p.graph_runner][INFO] - Test, Round 265 : loss => 6.305967054009438,  accuracy: 0.3225
[2025-09-11 20:36:56,618][flp2p.graph_runner][INFO] - Train, Round 266 : loss => 0.0015524461440509188,  accuracy: 1.0, gradient_norm : 0.07761668347414644
[2025-09-11 20:37:10,562][flp2p.graph_runner][INFO] - Test, Round 266 : loss => 6.309648806548118,  accuracy: 0.322
[2025-09-11 20:37:46,889][flp2p.graph_runner][INFO] - Train, Round 267 : loss => 0.0015439781809254787,  accuracy: 1.0, gradient_norm : 0.0793041983509219
[2025-09-11 20:38:01,158][flp2p.graph_runner][INFO] - Test, Round 267 : loss => 6.314943824577331,  accuracy: 0.3222
[2025-09-11 20:38:37,295][flp2p.graph_runner][INFO] - Train, Round 268 : loss => 0.0015352955578176383,  accuracy: 1.0, gradient_norm : 0.07837500837226308
[2025-09-11 20:38:51,836][flp2p.graph_runner][INFO] - Test, Round 268 : loss => 6.3177254257678985,  accuracy: 0.3228
[2025-09-11 20:39:28,056][flp2p.graph_runner][INFO] - Train, Round 269 : loss => 0.0015264049877926783,  accuracy: 1.0, gradient_norm : 0.07765080805256958
[2025-09-11 20:39:42,326][flp2p.graph_runner][INFO] - Test, Round 269 : loss => 6.321033823823929,  accuracy: 0.3217
[2025-09-11 20:40:18,430][flp2p.graph_runner][INFO] - Train, Round 270 : loss => 0.0015173557738550395,  accuracy: 1.0, gradient_norm : 0.078085499883457
[2025-09-11 20:40:32,761][flp2p.graph_runner][INFO] - Test, Round 270 : loss => 6.324847167444229,  accuracy: 0.3225
[2025-09-11 20:41:08,970][flp2p.graph_runner][INFO] - Train, Round 271 : loss => 0.0015099869194818893,  accuracy: 1.0, gradient_norm : 0.08062243677671792
[2025-09-11 20:41:22,977][flp2p.graph_runner][INFO] - Test, Round 271 : loss => 6.327817830300331,  accuracy: 0.3227
[2025-09-11 20:41:59,169][flp2p.graph_runner][INFO] - Train, Round 272 : loss => 0.0015016792001551946,  accuracy: 1.0, gradient_norm : 0.07574722596867826
[2025-09-11 20:42:13,530][flp2p.graph_runner][INFO] - Test, Round 272 : loss => 6.332291840648651,  accuracy: 0.3222
[2025-09-11 20:42:49,619][flp2p.graph_runner][INFO] - Train, Round 273 : loss => 0.001493299583453336,  accuracy: 1.0, gradient_norm : 0.07647144449730008
[2025-09-11 20:43:03,948][flp2p.graph_runner][INFO] - Test, Round 273 : loss => 6.33379161772728,  accuracy: 0.3221
[2025-09-11 20:43:40,287][flp2p.graph_runner][INFO] - Train, Round 274 : loss => 0.0014850251236930487,  accuracy: 1.0, gradient_norm : 0.0763052125595173
[2025-09-11 20:43:54,586][flp2p.graph_runner][INFO] - Test, Round 274 : loss => 6.337080046844482,  accuracy: 0.3217
[2025-09-11 20:44:30,971][flp2p.graph_runner][INFO] - Train, Round 275 : loss => 0.0014782254524470769,  accuracy: 1.0, gradient_norm : 0.07586748447267311
[2025-09-11 20:44:45,383][flp2p.graph_runner][INFO] - Test, Round 275 : loss => 6.341045007872581,  accuracy: 0.3221
[2025-09-11 20:45:21,608][flp2p.graph_runner][INFO] - Train, Round 276 : loss => 0.0014690617310164575,  accuracy: 1.0, gradient_norm : 0.07340515254257293
[2025-09-11 20:45:35,930][flp2p.graph_runner][INFO] - Test, Round 276 : loss => 6.343964529132843,  accuracy: 0.3222
[2025-09-11 20:46:12,472][flp2p.graph_runner][INFO] - Train, Round 277 : loss => 0.0014613745207558775,  accuracy: 1.0, gradient_norm : 0.07603713746619883
[2025-09-11 20:46:26,546][flp2p.graph_runner][INFO] - Test, Round 277 : loss => 6.347549811720848,  accuracy: 0.3222
[2025-09-11 20:47:02,961][flp2p.graph_runner][INFO] - Train, Round 278 : loss => 0.001453973745519761,  accuracy: 1.0, gradient_norm : 0.07286061571499305
[2025-09-11 20:47:17,192][flp2p.graph_runner][INFO] - Test, Round 278 : loss => 6.349830943036079,  accuracy: 0.322
[2025-09-11 20:47:53,607][flp2p.graph_runner][INFO] - Train, Round 279 : loss => 0.001446104723678824,  accuracy: 1.0, gradient_norm : 0.0777312464113632
[2025-09-11 20:48:08,327][flp2p.graph_runner][INFO] - Test, Round 279 : loss => 6.352942884397507,  accuracy: 0.322
[2025-09-11 20:48:44,770][flp2p.graph_runner][INFO] - Train, Round 280 : loss => 0.0014391959629332035,  accuracy: 1.0, gradient_norm : 0.07329643458920314
[2025-09-11 20:48:59,307][flp2p.graph_runner][INFO] - Test, Round 280 : loss => 6.357410416841507,  accuracy: 0.322
[2025-09-11 20:49:35,332][flp2p.graph_runner][INFO] - Train, Round 281 : loss => 0.001431879698623864,  accuracy: 1.0, gradient_norm : 0.07475396969542378
[2025-09-11 20:49:49,723][flp2p.graph_runner][INFO] - Test, Round 281 : loss => 6.35975721476078,  accuracy: 0.3222
[2025-09-11 20:50:25,416][flp2p.graph_runner][INFO] - Train, Round 282 : loss => 0.0014241083288410055,  accuracy: 1.0, gradient_norm : 0.07324080346677331
[2025-09-11 20:50:40,010][flp2p.graph_runner][INFO] - Test, Round 282 : loss => 6.363958353185653,  accuracy: 0.3225
[2025-09-11 20:51:15,989][flp2p.graph_runner][INFO] - Train, Round 283 : loss => 0.0014168049136787883,  accuracy: 1.0, gradient_norm : 0.07385324233113137
[2025-09-11 20:51:32,243][flp2p.graph_runner][INFO] - Test, Round 283 : loss => 6.3667415313243865,  accuracy: 0.3221
[2025-09-11 20:52:08,291][flp2p.graph_runner][INFO] - Train, Round 284 : loss => 0.0014098234938380006,  accuracy: 1.0, gradient_norm : 0.06916279816349231
[2025-09-11 20:52:29,213][flp2p.graph_runner][INFO] - Test, Round 284 : loss => 6.369769705915451,  accuracy: 0.3217
[2025-09-11 20:53:05,120][flp2p.graph_runner][INFO] - Train, Round 285 : loss => 0.0014034430971514673,  accuracy: 1.0, gradient_norm : 0.07067428292047225
[2025-09-11 20:53:25,318][flp2p.graph_runner][INFO] - Test, Round 285 : loss => 6.371998605370521,  accuracy: 0.3222
[2025-09-11 20:54:01,211][flp2p.graph_runner][INFO] - Train, Round 286 : loss => 0.0013961538826940043,  accuracy: 1.0, gradient_norm : 0.07094751700239081
[2025-09-11 20:54:19,209][flp2p.graph_runner][INFO] - Test, Round 286 : loss => 6.376016457772255,  accuracy: 0.3218
[2025-09-11 20:54:55,228][flp2p.graph_runner][INFO] - Train, Round 287 : loss => 0.0013892628479758667,  accuracy: 1.0, gradient_norm : 0.07065154037408665
[2025-09-11 20:55:10,453][flp2p.graph_runner][INFO] - Test, Round 287 : loss => 6.378401230120659,  accuracy: 0.3221
[2025-09-11 20:55:46,416][flp2p.graph_runner][INFO] - Train, Round 288 : loss => 0.001382489830987956,  accuracy: 1.0, gradient_norm : 0.07213343450231467
[2025-09-11 20:56:00,198][flp2p.graph_runner][INFO] - Test, Round 288 : loss => 6.381936491107941,  accuracy: 0.3221
[2025-09-11 20:56:36,179][flp2p.graph_runner][INFO] - Train, Round 289 : loss => 0.0013764913365109039,  accuracy: 1.0, gradient_norm : 0.07403752756278108
[2025-09-11 20:56:49,935][flp2p.graph_runner][INFO] - Test, Round 289 : loss => 6.385259249067307,  accuracy: 0.3219
[2025-09-11 20:57:26,023][flp2p.graph_runner][INFO] - Train, Round 290 : loss => 0.001369905761142339,  accuracy: 1.0, gradient_norm : 0.07365379696497774
[2025-09-11 20:57:40,257][flp2p.graph_runner][INFO] - Test, Round 290 : loss => 6.38773164486885,  accuracy: 0.3217
[2025-09-11 20:58:16,699][flp2p.graph_runner][INFO] - Train, Round 291 : loss => 0.001362752467175596,  accuracy: 1.0, gradient_norm : 0.06899366860664558
[2025-09-11 20:58:30,660][flp2p.graph_runner][INFO] - Test, Round 291 : loss => 6.38996161403656,  accuracy: 0.3217
[2025-09-11 20:59:07,080][flp2p.graph_runner][INFO] - Train, Round 292 : loss => 0.0013560294882093635,  accuracy: 1.0, gradient_norm : 0.06812539918134654
[2025-09-11 20:59:21,135][flp2p.graph_runner][INFO] - Test, Round 292 : loss => 6.393290301179886,  accuracy: 0.3224
[2025-09-11 20:59:57,450][flp2p.graph_runner][INFO] - Train, Round 293 : loss => 0.0013501956180698468,  accuracy: 1.0, gradient_norm : 0.06603406417050833
[2025-09-11 21:00:11,621][flp2p.graph_runner][INFO] - Test, Round 293 : loss => 6.396481374740601,  accuracy: 0.3218
[2025-09-11 21:00:47,855][flp2p.graph_runner][INFO] - Train, Round 294 : loss => 0.0013437110852585954,  accuracy: 1.0, gradient_norm : 0.06841205747852404
[2025-09-11 21:01:01,885][flp2p.graph_runner][INFO] - Test, Round 294 : loss => 6.3992493079900745,  accuracy: 0.3218
[2025-09-11 21:01:38,134][flp2p.graph_runner][INFO] - Train, Round 295 : loss => 0.0013378937969294692,  accuracy: 1.0, gradient_norm : 0.06927121723132569
[2025-09-11 21:01:52,368][flp2p.graph_runner][INFO] - Test, Round 295 : loss => 6.4014559892654415,  accuracy: 0.3212
[2025-09-11 21:02:28,285][flp2p.graph_runner][INFO] - Train, Round 296 : loss => 0.0013319010771374448,  accuracy: 1.0, gradient_norm : 0.0693037346447787
[2025-09-11 21:02:42,396][flp2p.graph_runner][INFO] - Test, Round 296 : loss => 6.404937900424003,  accuracy: 0.3216
[2025-09-11 21:03:18,633][flp2p.graph_runner][INFO] - Train, Round 297 : loss => 0.0013256341955760343,  accuracy: 1.0, gradient_norm : 0.06600703565333026
[2025-09-11 21:03:32,769][flp2p.graph_runner][INFO] - Test, Round 297 : loss => 6.4069489336967465,  accuracy: 0.322
[2025-09-11 21:04:09,161][flp2p.graph_runner][INFO] - Train, Round 298 : loss => 0.0013196725236048223,  accuracy: 1.0, gradient_norm : 0.06668599623332634
[2025-09-11 21:04:23,528][flp2p.graph_runner][INFO] - Test, Round 298 : loss => 6.410522408795357,  accuracy: 0.3216
[2025-09-11 21:04:59,960][flp2p.graph_runner][INFO] - Train, Round 299 : loss => 0.0013142794472757186,  accuracy: 1.0, gradient_norm : 0.06822490353755346
[2025-09-11 21:05:14,105][flp2p.graph_runner][INFO] - Test, Round 299 : loss => 6.412795336461067,  accuracy: 0.3217
[2025-09-11 21:05:50,231][flp2p.graph_runner][INFO] - Train, Round 300 : loss => 0.00130853160272333,  accuracy: 1.0, gradient_norm : 0.06764181486457276
[2025-09-11 21:06:04,752][flp2p.graph_runner][INFO] - Test, Round 300 : loss => 6.4146776900529865,  accuracy: 0.3222
[2025-09-11 21:06:41,366][flp2p.graph_runner][INFO] - Train, Round 301 : loss => 0.0013024249481046956,  accuracy: 1.0, gradient_norm : 0.06569734304803537
[2025-09-11 21:06:55,509][flp2p.graph_runner][INFO] - Test, Round 301 : loss => 6.417540451574325,  accuracy: 0.3219
[2025-09-11 21:07:31,688][flp2p.graph_runner][INFO] - Train, Round 302 : loss => 0.0012967011483609287,  accuracy: 1.0, gradient_norm : 0.06459480621071574
[2025-09-11 21:07:45,834][flp2p.graph_runner][INFO] - Test, Round 302 : loss => 6.420865242362022,  accuracy: 0.3219
[2025-09-11 21:08:22,046][flp2p.graph_runner][INFO] - Train, Round 303 : loss => 0.0012910012262727832,  accuracy: 1.0, gradient_norm : 0.06594581491381675
[2025-09-11 21:08:36,376][flp2p.graph_runner][INFO] - Test, Round 303 : loss => 6.422999515509606,  accuracy: 0.3221
[2025-09-11 21:09:12,330][flp2p.graph_runner][INFO] - Train, Round 304 : loss => 0.001286020697176961,  accuracy: 1.0, gradient_norm : 0.06726844280388236
[2025-09-11 21:09:26,545][flp2p.graph_runner][INFO] - Test, Round 304 : loss => 6.425720655417442,  accuracy: 0.3222
[2025-09-11 21:10:02,995][flp2p.graph_runner][INFO] - Train, Round 305 : loss => 0.001280553160055812,  accuracy: 1.0, gradient_norm : 0.06738976289816768
[2025-09-11 21:10:17,405][flp2p.graph_runner][INFO] - Test, Round 305 : loss => 6.428847621607781,  accuracy: 0.3217
[2025-09-11 21:10:53,833][flp2p.graph_runner][INFO] - Train, Round 306 : loss => 0.0012750508972627968,  accuracy: 1.0, gradient_norm : 0.06660082426483316
[2025-09-11 21:11:08,682][flp2p.graph_runner][INFO] - Test, Round 306 : loss => 6.430828554081917,  accuracy: 0.322
[2025-09-11 21:11:44,620][flp2p.graph_runner][INFO] - Train, Round 307 : loss => 0.0012690665653159767,  accuracy: 1.0, gradient_norm : 0.06516960424058674
[2025-09-11 21:11:59,102][flp2p.graph_runner][INFO] - Test, Round 307 : loss => 6.431848883247375,  accuracy: 0.322
[2025-09-11 21:12:35,002][flp2p.graph_runner][INFO] - Train, Round 308 : loss => 0.0012642048932563436,  accuracy: 1.0, gradient_norm : 0.0641447914258727
[2025-09-11 21:12:49,381][flp2p.graph_runner][INFO] - Test, Round 308 : loss => 6.436506442546844,  accuracy: 0.3221
[2025-09-11 21:13:24,922][flp2p.graph_runner][INFO] - Train, Round 309 : loss => 0.0012588109757780326,  accuracy: 1.0, gradient_norm : 0.06332573529451692
[2025-09-11 21:13:40,655][flp2p.graph_runner][INFO] - Test, Round 309 : loss => 6.438046468901634,  accuracy: 0.3219
[2025-09-11 21:14:16,446][flp2p.graph_runner][INFO] - Train, Round 310 : loss => 0.001253686332371823,  accuracy: 1.0, gradient_norm : 0.06621873102911428
[2025-09-11 21:14:35,646][flp2p.graph_runner][INFO] - Test, Round 310 : loss => 6.440975244665146,  accuracy: 0.322
[2025-09-11 21:15:11,668][flp2p.graph_runner][INFO] - Train, Round 311 : loss => 0.0012485944504927226,  accuracy: 1.0, gradient_norm : 0.0647662390536278
[2025-09-11 21:15:34,075][flp2p.graph_runner][INFO] - Test, Round 311 : loss => 6.442858564639091,  accuracy: 0.3216
[2025-09-11 21:16:09,828][flp2p.graph_runner][INFO] - Train, Round 312 : loss => 0.0012432385883342548,  accuracy: 1.0, gradient_norm : 0.06284007663458675
[2025-09-11 21:16:29,069][flp2p.graph_runner][INFO] - Test, Round 312 : loss => 6.444848601031303,  accuracy: 0.3214
[2025-09-11 21:17:04,880][flp2p.graph_runner][INFO] - Train, Round 313 : loss => 0.001238684243920337,  accuracy: 1.0, gradient_norm : 0.0642260258441299
[2025-09-11 21:17:21,315][flp2p.graph_runner][INFO] - Test, Round 313 : loss => 6.447059407258034,  accuracy: 0.3219
[2025-09-11 21:17:57,452][flp2p.graph_runner][INFO] - Train, Round 314 : loss => 0.0012337839876515016,  accuracy: 1.0, gradient_norm : 0.06341730263551233
[2025-09-11 21:18:11,865][flp2p.graph_runner][INFO] - Test, Round 314 : loss => 6.449505786395073,  accuracy: 0.3219
[2025-09-11 21:18:48,188][flp2p.graph_runner][INFO] - Train, Round 315 : loss => 0.001228892725727443,  accuracy: 1.0, gradient_norm : 0.06280404449564946
[2025-09-11 21:19:02,118][flp2p.graph_runner][INFO] - Test, Round 315 : loss => 6.4526841897964475,  accuracy: 0.3215
[2025-09-11 21:19:38,280][flp2p.graph_runner][INFO] - Train, Round 316 : loss => 0.0012245311464842723,  accuracy: 1.0, gradient_norm : 0.06132323589623358
[2025-09-11 21:19:52,109][flp2p.graph_runner][INFO] - Test, Round 316 : loss => 6.454010743117332,  accuracy: 0.3218
[2025-09-11 21:20:28,401][flp2p.graph_runner][INFO] - Train, Round 317 : loss => 0.0012194988281650392,  accuracy: 1.0, gradient_norm : 0.06492332978036383
[2025-09-11 21:20:42,332][flp2p.graph_runner][INFO] - Test, Round 317 : loss => 6.4575575384616855,  accuracy: 0.3219
[2025-09-11 21:21:18,271][flp2p.graph_runner][INFO] - Train, Round 318 : loss => 0.0012149756193199812,  accuracy: 1.0, gradient_norm : 0.06459071817553341
[2025-09-11 21:21:32,156][flp2p.graph_runner][INFO] - Test, Round 318 : loss => 6.459147489452362,  accuracy: 0.322
[2025-09-11 21:22:07,925][flp2p.graph_runner][INFO] - Train, Round 319 : loss => 0.0012097575540004374,  accuracy: 1.0, gradient_norm : 0.06371817327946898
[2025-09-11 21:22:21,826][flp2p.graph_runner][INFO] - Test, Round 319 : loss => 6.461870109772682,  accuracy: 0.3216
[2025-09-11 21:22:58,089][flp2p.graph_runner][INFO] - Train, Round 320 : loss => 0.0012054180059446178,  accuracy: 1.0, gradient_norm : 0.06275779644724724
[2025-09-11 21:23:12,014][flp2p.graph_runner][INFO] - Test, Round 320 : loss => 6.463516413211822,  accuracy: 0.3224
[2025-09-11 21:23:48,123][flp2p.graph_runner][INFO] - Train, Round 321 : loss => 0.0012007597351960913,  accuracy: 1.0, gradient_norm : 0.061113186323886556
[2025-09-11 21:24:02,218][flp2p.graph_runner][INFO] - Test, Round 321 : loss => 6.465974822568893,  accuracy: 0.3221
[2025-09-11 21:24:38,426][flp2p.graph_runner][INFO] - Train, Round 322 : loss => 0.0011957057292844788,  accuracy: 1.0, gradient_norm : 0.06171032450739384
[2025-09-11 21:24:52,653][flp2p.graph_runner][INFO] - Test, Round 322 : loss => 6.46793584792614,  accuracy: 0.3211
[2025-09-11 21:25:28,909][flp2p.graph_runner][INFO] - Train, Round 323 : loss => 0.001191749888190922,  accuracy: 1.0, gradient_norm : 0.0617783423057453
[2025-09-11 21:25:43,098][flp2p.graph_runner][INFO] - Test, Round 323 : loss => 6.470358934450149,  accuracy: 0.3216
[2025-09-11 21:26:19,369][flp2p.graph_runner][INFO] - Train, Round 324 : loss => 0.0011869530235223164,  accuracy: 1.0, gradient_norm : 0.06128858487094746
[2025-09-11 21:26:33,834][flp2p.graph_runner][INFO] - Test, Round 324 : loss => 6.471875587368012,  accuracy: 0.3219
[2025-09-11 21:27:10,132][flp2p.graph_runner][INFO] - Train, Round 325 : loss => 0.0011828706697754874,  accuracy: 1.0, gradient_norm : 0.06137025452287014
[2025-09-11 21:27:24,218][flp2p.graph_runner][INFO] - Test, Round 325 : loss => 6.474444805526733,  accuracy: 0.3218
[2025-09-11 21:28:00,141][flp2p.graph_runner][INFO] - Train, Round 326 : loss => 0.0011784298051255367,  accuracy: 1.0, gradient_norm : 0.06271069464574648
[2025-09-11 21:28:14,607][flp2p.graph_runner][INFO] - Test, Round 326 : loss => 6.477103172922134,  accuracy: 0.3216
[2025-09-11 21:28:50,871][flp2p.graph_runner][INFO] - Train, Round 327 : loss => 0.00117412206234197,  accuracy: 1.0, gradient_norm : 0.06042191185679099
[2025-09-11 21:29:05,441][flp2p.graph_runner][INFO] - Test, Round 327 : loss => 6.478390363907814,  accuracy: 0.3215
[2025-09-11 21:29:41,721][flp2p.graph_runner][INFO] - Train, Round 328 : loss => 0.0011695813790235368,  accuracy: 1.0, gradient_norm : 0.06341291738784852
[2025-09-11 21:29:55,981][flp2p.graph_runner][INFO] - Test, Round 328 : loss => 6.481448476576805,  accuracy: 0.3222
[2025-09-11 21:30:32,120][flp2p.graph_runner][INFO] - Train, Round 329 : loss => 0.0011651973810027508,  accuracy: 1.0, gradient_norm : 0.06120825636599132
[2025-09-11 21:30:46,460][flp2p.graph_runner][INFO] - Test, Round 329 : loss => 6.483213439416885,  accuracy: 0.3219
[2025-09-11 21:31:22,470][flp2p.graph_runner][INFO] - Train, Round 330 : loss => 0.0011612632446000738,  accuracy: 1.0, gradient_norm : 0.06059153330951288
[2025-09-11 21:31:36,815][flp2p.graph_runner][INFO] - Test, Round 330 : loss => 6.485037476325035,  accuracy: 0.3215
[2025-09-11 21:32:13,287][flp2p.graph_runner][INFO] - Train, Round 331 : loss => 0.001157787285022399,  accuracy: 1.0, gradient_norm : 0.0624473971040348
[2025-09-11 21:32:27,420][flp2p.graph_runner][INFO] - Test, Round 331 : loss => 6.486897898626328,  accuracy: 0.3221
[2025-09-11 21:33:03,639][flp2p.graph_runner][INFO] - Train, Round 332 : loss => 0.001153450312655574,  accuracy: 1.0, gradient_norm : 0.06213704793289113
[2025-09-11 21:33:17,879][flp2p.graph_runner][INFO] - Test, Round 332 : loss => 6.488824041223526,  accuracy: 0.3217
[2025-09-11 21:33:53,903][flp2p.graph_runner][INFO] - Train, Round 333 : loss => 0.0011492167035855045,  accuracy: 1.0, gradient_norm : 0.06151402529867177
[2025-09-11 21:34:08,269][flp2p.graph_runner][INFO] - Test, Round 333 : loss => 6.491252286624908,  accuracy: 0.3216
[2025-09-11 21:34:44,592][flp2p.graph_runner][INFO] - Train, Round 334 : loss => 0.0011454136535515623,  accuracy: 1.0, gradient_norm : 0.05887377996846294
[2025-09-11 21:34:59,167][flp2p.graph_runner][INFO] - Test, Round 334 : loss => 6.493139507889747,  accuracy: 0.322
[2025-09-11 21:35:35,538][flp2p.graph_runner][INFO] - Train, Round 335 : loss => 0.0011412447959204047,  accuracy: 1.0, gradient_norm : 0.061084381577725534
[2025-09-11 21:35:50,195][flp2p.graph_runner][INFO] - Test, Round 335 : loss => 6.495550915908813,  accuracy: 0.3218
[2025-09-11 21:36:26,058][flp2p.graph_runner][INFO] - Train, Round 336 : loss => 0.0011363976754364558,  accuracy: 1.0, gradient_norm : 0.05956814145206126
[2025-09-11 21:36:40,608][flp2p.graph_runner][INFO] - Test, Round 336 : loss => 6.498113527154922,  accuracy: 0.3213
[2025-09-11 21:37:16,811][flp2p.graph_runner][INFO] - Train, Round 337 : loss => 0.0011342298930200436,  accuracy: 1.0, gradient_norm : 0.059585751136921584
[2025-09-11 21:37:31,297][flp2p.graph_runner][INFO] - Test, Round 337 : loss => 6.499363781309127,  accuracy: 0.3218
[2025-09-11 21:38:07,106][flp2p.graph_runner][INFO] - Train, Round 338 : loss => 0.0011294327119442944,  accuracy: 1.0, gradient_norm : 0.05751725866458452
[2025-09-11 21:38:24,635][flp2p.graph_runner][INFO] - Test, Round 338 : loss => 6.501154441690445,  accuracy: 0.322
[2025-09-11 21:39:00,537][flp2p.graph_runner][INFO] - Train, Round 339 : loss => 0.0011256369752906423,  accuracy: 1.0, gradient_norm : 0.05739839963249569
[2025-09-11 21:39:21,274][flp2p.graph_runner][INFO] - Test, Round 339 : loss => 6.502382184505462,  accuracy: 0.3217
[2025-09-11 21:39:57,044][flp2p.graph_runner][INFO] - Train, Round 340 : loss => 0.001122095946096427,  accuracy: 1.0, gradient_norm : 0.05935660842140155
[2025-09-11 21:40:17,540][flp2p.graph_runner][INFO] - Test, Round 340 : loss => 6.504771585249901,  accuracy: 0.322
[2025-09-11 21:40:53,597][flp2p.graph_runner][INFO] - Train, Round 341 : loss => 0.0011185256906173888,  accuracy: 1.0, gradient_norm : 0.057779021217629234
[2025-09-11 21:41:11,697][flp2p.graph_runner][INFO] - Test, Round 341 : loss => 6.505937260842323,  accuracy: 0.3217
[2025-09-11 21:41:47,433][flp2p.graph_runner][INFO] - Train, Round 342 : loss => 0.0011148995268983218,  accuracy: 1.0, gradient_norm : 0.059889087223170705
[2025-09-11 21:42:02,895][flp2p.graph_runner][INFO] - Test, Round 342 : loss => 6.5082236073255535,  accuracy: 0.3217
[2025-09-11 21:42:38,805][flp2p.graph_runner][INFO] - Train, Round 343 : loss => 0.001111049494550874,  accuracy: 1.0, gradient_norm : 0.05718838687300327
[2025-09-11 21:42:52,909][flp2p.graph_runner][INFO] - Test, Round 343 : loss => 6.509505033016205,  accuracy: 0.3219
[2025-09-11 21:43:29,032][flp2p.graph_runner][INFO] - Train, Round 344 : loss => 0.001106807687077283,  accuracy: 1.0, gradient_norm : 0.05660294910476602
[2025-09-11 21:43:42,886][flp2p.graph_runner][INFO] - Test, Round 344 : loss => 6.512114595460892,  accuracy: 0.3218
[2025-09-11 21:44:18,949][flp2p.graph_runner][INFO] - Train, Round 345 : loss => 0.0011042251004740444,  accuracy: 1.0, gradient_norm : 0.0598917785208054
[2025-09-11 21:44:32,817][flp2p.graph_runner][INFO] - Test, Round 345 : loss => 6.513491263175011,  accuracy: 0.3219
[2025-09-11 21:45:08,852][flp2p.graph_runner][INFO] - Train, Round 346 : loss => 0.0010999049640183028,  accuracy: 1.0, gradient_norm : 0.057203865528487344
[2025-09-11 21:45:22,608][flp2p.graph_runner][INFO] - Test, Round 346 : loss => 6.515173870372772,  accuracy: 0.3215
[2025-09-11 21:45:58,759][flp2p.graph_runner][INFO] - Train, Round 347 : loss => 0.0010967911123831678,  accuracy: 1.0, gradient_norm : 0.059259583200403455
[2025-09-11 21:46:12,542][flp2p.graph_runner][INFO] - Test, Round 347 : loss => 6.518120227909088,  accuracy: 0.3216
[2025-09-11 21:46:48,465][flp2p.graph_runner][INFO] - Train, Round 348 : loss => 0.001092884661702556,  accuracy: 1.0, gradient_norm : 0.05664915882693916
[2025-09-11 21:47:02,462][flp2p.graph_runner][INFO] - Test, Round 348 : loss => 6.519951988244057,  accuracy: 0.3217
[2025-09-11 21:47:38,316][flp2p.graph_runner][INFO] - Train, Round 349 : loss => 0.001089281820459291,  accuracy: 1.0, gradient_norm : 0.05840454687152667
[2025-09-11 21:47:52,437][flp2p.graph_runner][INFO] - Test, Round 349 : loss => 6.521575032019615,  accuracy: 0.3218
[2025-09-11 21:48:28,621][flp2p.graph_runner][INFO] - Train, Round 350 : loss => 0.0010867101095694428,  accuracy: 1.0, gradient_norm : 0.05531205052366604
[2025-09-11 21:48:42,821][flp2p.graph_runner][INFO] - Test, Round 350 : loss => 6.522275897312165,  accuracy: 0.3219
[2025-09-11 21:49:19,169][flp2p.graph_runner][INFO] - Train, Round 351 : loss => 0.0010829437532811424,  accuracy: 1.0, gradient_norm : 0.05752943512810343
[2025-09-11 21:49:33,360][flp2p.graph_runner][INFO] - Test, Round 351 : loss => 6.5245620688676835,  accuracy: 0.3215
[2025-09-11 21:50:09,315][flp2p.graph_runner][INFO] - Train, Round 352 : loss => 0.0010799244416314952,  accuracy: 1.0, gradient_norm : 0.05762438412025131
[2025-09-11 21:50:23,625][flp2p.graph_runner][INFO] - Test, Round 352 : loss => 6.525812285637856,  accuracy: 0.3224
[2025-09-11 21:50:59,928][flp2p.graph_runner][INFO] - Train, Round 353 : loss => 0.0010759449959247532,  accuracy: 1.0, gradient_norm : 0.05520180658500512
[2025-09-11 21:51:14,231][flp2p.graph_runner][INFO] - Test, Round 353 : loss => 6.5286516751050945,  accuracy: 0.3217
[2025-09-11 21:51:50,439][flp2p.graph_runner][INFO] - Train, Round 354 : loss => 0.0010727841815726905,  accuracy: 1.0, gradient_norm : 0.05684884281008517
[2025-09-11 21:52:04,830][flp2p.graph_runner][INFO] - Test, Round 354 : loss => 6.529492401599884,  accuracy: 0.3214
[2025-09-11 21:52:41,181][flp2p.graph_runner][INFO] - Train, Round 355 : loss => 0.0010692848454103413,  accuracy: 1.0, gradient_norm : 0.05499339954554576
[2025-09-11 21:52:55,431][flp2p.graph_runner][INFO] - Test, Round 355 : loss => 6.531608872270584,  accuracy: 0.3222
[2025-09-11 21:53:31,478][flp2p.graph_runner][INFO] - Train, Round 356 : loss => 0.0010668397073216815,  accuracy: 1.0, gradient_norm : 0.056984454814423095
[2025-09-11 21:53:45,683][flp2p.graph_runner][INFO] - Test, Round 356 : loss => 6.532589283227921,  accuracy: 0.3221
[2025-09-11 21:54:21,870][flp2p.graph_runner][INFO] - Train, Round 357 : loss => 0.0010637221776414664,  accuracy: 1.0, gradient_norm : 0.05561799069909359
[2025-09-11 21:54:36,053][flp2p.graph_runner][INFO] - Test, Round 357 : loss => 6.53451857392788,  accuracy: 0.3218
[2025-09-11 21:55:11,994][flp2p.graph_runner][INFO] - Train, Round 358 : loss => 0.0010605825753737012,  accuracy: 1.0, gradient_norm : 0.056139666262616766
[2025-09-11 21:55:26,106][flp2p.graph_runner][INFO] - Test, Round 358 : loss => 6.535451058053971,  accuracy: 0.3218
[2025-09-11 21:56:02,164][flp2p.graph_runner][INFO] - Train, Round 359 : loss => 0.001056967122470572,  accuracy: 1.0, gradient_norm : 0.05467973992008688
[2025-09-11 21:56:16,358][flp2p.graph_runner][INFO] - Test, Round 359 : loss => 6.537091538357735,  accuracy: 0.3219
[2025-09-11 21:56:52,352][flp2p.graph_runner][INFO] - Train, Round 360 : loss => 0.0010541429980366957,  accuracy: 1.0, gradient_norm : 0.05644353857727857
[2025-09-11 21:57:06,720][flp2p.graph_runner][INFO] - Test, Round 360 : loss => 6.539134786653519,  accuracy: 0.3216
[2025-09-11 21:57:43,134][flp2p.graph_runner][INFO] - Train, Round 361 : loss => 0.0010509039530006703,  accuracy: 1.0, gradient_norm : 0.05419066227329191
[2025-09-11 21:57:57,394][flp2p.graph_runner][INFO] - Test, Round 361 : loss => 6.540665041279793,  accuracy: 0.3217
[2025-09-11 21:58:33,801][flp2p.graph_runner][INFO] - Train, Round 362 : loss => 0.0010479874815064248,  accuracy: 1.0, gradient_norm : 0.05590488662674795
[2025-09-11 21:58:47,897][flp2p.graph_runner][INFO] - Test, Round 362 : loss => 6.5417289700031285,  accuracy: 0.3215
[2025-09-11 21:59:24,047][flp2p.graph_runner][INFO] - Train, Round 363 : loss => 0.001044905504289394,  accuracy: 1.0, gradient_norm : 0.05506401705995068
[2025-09-11 21:59:38,466][flp2p.graph_runner][INFO] - Test, Round 363 : loss => 6.543833686828613,  accuracy: 0.3215
[2025-09-11 22:00:14,766][flp2p.graph_runner][INFO] - Train, Round 364 : loss => 0.001041576234735354,  accuracy: 1.0, gradient_norm : 0.05317319049242166
[2025-09-11 22:00:29,307][flp2p.graph_runner][INFO] - Test, Round 364 : loss => 6.54526903629303,  accuracy: 0.322
[2025-09-11 22:01:05,541][flp2p.graph_runner][INFO] - Train, Round 365 : loss => 0.0010388553815088627,  accuracy: 1.0, gradient_norm : 0.054995226239042586
[2025-09-11 22:01:20,322][flp2p.graph_runner][INFO] - Test, Round 365 : loss => 6.547159643483162,  accuracy: 0.3216
[2025-09-11 22:01:56,249][flp2p.graph_runner][INFO] - Train, Round 366 : loss => 0.0010354305795772232,  accuracy: 1.0, gradient_norm : 0.05423082331080311
[2025-09-11 22:02:10,697][flp2p.graph_runner][INFO] - Test, Round 366 : loss => 6.548296694612503,  accuracy: 0.3209
[2025-09-11 22:02:46,554][flp2p.graph_runner][INFO] - Train, Round 367 : loss => 0.0010333165860599066,  accuracy: 1.0, gradient_norm : 0.05449998338468769
[2025-09-11 22:03:01,364][flp2p.graph_runner][INFO] - Test, Round 367 : loss => 6.5495374747276305,  accuracy: 0.3214
[2025-09-11 22:03:37,335][flp2p.graph_runner][INFO] - Train, Round 368 : loss => 0.0010304849968815687,  accuracy: 1.0, gradient_norm : 0.05585762059969815
[2025-09-11 22:03:53,120][flp2p.graph_runner][INFO] - Test, Round 368 : loss => 6.551911494255066,  accuracy: 0.3215
[2025-09-11 22:04:29,096][flp2p.graph_runner][INFO] - Train, Round 369 : loss => 0.0010271749948151407,  accuracy: 1.0, gradient_norm : 0.052490659457259205
[2025-09-11 22:04:48,964][flp2p.graph_runner][INFO] - Test, Round 369 : loss => 6.552669239115715,  accuracy: 0.3214
[2025-09-11 22:05:24,573][flp2p.graph_runner][INFO] - Train, Round 370 : loss => 0.00102423573291162,  accuracy: 1.0, gradient_norm : 0.05249177334143479
[2025-09-11 22:05:45,575][flp2p.graph_runner][INFO] - Test, Round 370 : loss => 6.553919288611412,  accuracy: 0.3214
[2025-09-11 22:06:21,464][flp2p.graph_runner][INFO] - Train, Round 371 : loss => 0.001021569215420944,  accuracy: 1.0, gradient_norm : 0.05256048694122256
[2025-09-11 22:06:39,343][flp2p.graph_runner][INFO] - Test, Round 371 : loss => 6.554990256023407,  accuracy: 0.3218
[2025-09-11 22:07:15,160][flp2p.graph_runner][INFO] - Train, Round 372 : loss => 0.0010185988949039408,  accuracy: 1.0, gradient_norm : 0.05384038046412568
[2025-09-11 22:07:31,567][flp2p.graph_runner][INFO] - Test, Round 372 : loss => 6.556548742985726,  accuracy: 0.3216
[2025-09-11 22:08:07,386][flp2p.graph_runner][INFO] - Train, Round 373 : loss => 0.001015978042875455,  accuracy: 1.0, gradient_norm : 0.05312951583165739
[2025-09-11 22:08:21,300][flp2p.graph_runner][INFO] - Test, Round 373 : loss => 6.558022955846787,  accuracy: 0.321
[2025-09-11 22:08:57,448][flp2p.graph_runner][INFO] - Train, Round 374 : loss => 0.0010132210584318574,  accuracy: 1.0, gradient_norm : 0.053556835305618354
[2025-09-11 22:09:11,441][flp2p.graph_runner][INFO] - Test, Round 374 : loss => 6.559910972690583,  accuracy: 0.3222
[2025-09-11 22:09:47,638][flp2p.graph_runner][INFO] - Train, Round 375 : loss => 0.0010108341568896627,  accuracy: 1.0, gradient_norm : 0.05470100294118736
[2025-09-11 22:10:01,486][flp2p.graph_runner][INFO] - Test, Round 375 : loss => 6.56137505338192,  accuracy: 0.321
[2025-09-11 22:10:37,745][flp2p.graph_runner][INFO] - Train, Round 376 : loss => 0.0010082928580959559,  accuracy: 1.0, gradient_norm : 0.05451913178709334
[2025-09-11 22:10:51,761][flp2p.graph_runner][INFO] - Test, Round 376 : loss => 6.562043531703949,  accuracy: 0.3217
[2025-09-11 22:11:27,901][flp2p.graph_runner][INFO] - Train, Round 377 : loss => 0.0010056979253810518,  accuracy: 1.0, gradient_norm : 0.05272074991706601
[2025-09-11 22:11:41,801][flp2p.graph_runner][INFO] - Test, Round 377 : loss => 6.564158129358292,  accuracy: 0.3215
[2025-09-11 22:12:18,081][flp2p.graph_runner][INFO] - Train, Round 378 : loss => 0.0010027947020716968,  accuracy: 1.0, gradient_norm : 0.052986083197689327
[2025-09-11 22:12:32,160][flp2p.graph_runner][INFO] - Test, Round 378 : loss => 6.565981017518044,  accuracy: 0.3213
[2025-09-11 22:13:08,060][flp2p.graph_runner][INFO] - Train, Round 379 : loss => 0.0009999834008582792,  accuracy: 1.0, gradient_norm : 0.05118070918980097
[2025-09-11 22:13:22,073][flp2p.graph_runner][INFO] - Test, Round 379 : loss => 6.566194243144989,  accuracy: 0.3217
[2025-09-11 22:13:58,372][flp2p.graph_runner][INFO] - Train, Round 380 : loss => 0.0009974596558216342,  accuracy: 1.0, gradient_norm : 0.05276582129930394
[2025-09-11 22:14:12,655][flp2p.graph_runner][INFO] - Test, Round 380 : loss => 6.568240733361244,  accuracy: 0.3214
[2025-09-11 22:14:48,942][flp2p.graph_runner][INFO] - Train, Round 381 : loss => 0.000995003940576377,  accuracy: 1.0, gradient_norm : 0.05379770921840458
[2025-09-11 22:15:03,316][flp2p.graph_runner][INFO] - Test, Round 381 : loss => 6.569720716905594,  accuracy: 0.3212
[2025-09-11 22:15:39,279][flp2p.graph_runner][INFO] - Train, Round 382 : loss => 0.0009925168086677629,  accuracy: 1.0, gradient_norm : 0.05173430481104356
[2025-09-11 22:15:53,627][flp2p.graph_runner][INFO] - Test, Round 382 : loss => 6.570934502506256,  accuracy: 0.3211
[2025-09-11 22:16:29,922][flp2p.graph_runner][INFO] - Train, Round 383 : loss => 0.0009899025859582858,  accuracy: 1.0, gradient_norm : 0.053636334704628336
[2025-09-11 22:16:44,202][flp2p.graph_runner][INFO] - Test, Round 383 : loss => 6.572378900051117,  accuracy: 0.3208
[2025-09-11 22:17:20,625][flp2p.graph_runner][INFO] - Train, Round 384 : loss => 0.0009870903842723543,  accuracy: 1.0, gradient_norm : 0.049175069332526966
[2025-09-11 22:17:34,899][flp2p.graph_runner][INFO] - Test, Round 384 : loss => 6.573606860613823,  accuracy: 0.3211
[2025-09-11 22:18:11,206][flp2p.graph_runner][INFO] - Train, Round 385 : loss => 0.0009849722317691583,  accuracy: 1.0, gradient_norm : 0.053794238899645234
[2025-09-11 22:18:25,420][flp2p.graph_runner][INFO] - Test, Round 385 : loss => 6.575422714567185,  accuracy: 0.3212
[2025-09-11 22:19:01,688][flp2p.graph_runner][INFO] - Train, Round 386 : loss => 0.000982393546655658,  accuracy: 1.0, gradient_norm : 0.0521075676314121
[2025-09-11 22:19:16,066][flp2p.graph_runner][INFO] - Test, Round 386 : loss => 6.575511533379554,  accuracy: 0.3216
[2025-09-11 22:19:52,132][flp2p.graph_runner][INFO] - Train, Round 387 : loss => 0.000979660379816778,  accuracy: 1.0, gradient_norm : 0.05101763035734583
[2025-09-11 22:20:06,687][flp2p.graph_runner][INFO] - Test, Round 387 : loss => 6.577514899873734,  accuracy: 0.3212
[2025-09-11 22:20:42,925][flp2p.graph_runner][INFO] - Train, Round 388 : loss => 0.0009777963555340338,  accuracy: 1.0, gradient_norm : 0.049588878913413466
[2025-09-11 22:20:57,121][flp2p.graph_runner][INFO] - Test, Round 388 : loss => 6.5788236024618145,  accuracy: 0.321
[2025-09-11 22:21:33,405][flp2p.graph_runner][INFO] - Train, Round 389 : loss => 0.000975456525872384,  accuracy: 1.0, gradient_norm : 0.051088434779246746
[2025-09-11 22:21:47,697][flp2p.graph_runner][INFO] - Test, Round 389 : loss => 6.580265550804138,  accuracy: 0.3212
[2025-09-11 22:22:23,956][flp2p.graph_runner][INFO] - Train, Round 390 : loss => 0.000973083828842694,  accuracy: 1.0, gradient_norm : 0.050056514995592376
[2025-09-11 22:22:38,124][flp2p.graph_runner][INFO] - Test, Round 390 : loss => 6.580870901942253,  accuracy: 0.3211
[2025-09-11 22:23:14,582][flp2p.graph_runner][INFO] - Train, Round 391 : loss => 0.0009710441708496848,  accuracy: 1.0, gradient_norm : 0.05303376793835475
[2025-09-11 22:23:28,737][flp2p.graph_runner][INFO] - Test, Round 391 : loss => 6.58298045463562,  accuracy: 0.321
[2025-09-11 22:24:04,641][flp2p.graph_runner][INFO] - Train, Round 392 : loss => 0.0009684482743856881,  accuracy: 1.0, gradient_norm : 0.051687405816142
[2025-09-11 22:24:18,862][flp2p.graph_runner][INFO] - Test, Round 392 : loss => 6.583950949525833,  accuracy: 0.3212
[2025-09-11 22:24:54,953][flp2p.graph_runner][INFO] - Train, Round 393 : loss => 0.0009658688475125622,  accuracy: 1.0, gradient_norm : 0.051715549287541435
[2025-09-11 22:25:09,473][flp2p.graph_runner][INFO] - Test, Round 393 : loss => 6.584099263548851,  accuracy: 0.3208
[2025-09-11 22:25:45,660][flp2p.graph_runner][INFO] - Train, Round 394 : loss => 0.0009633878864406138,  accuracy: 1.0, gradient_norm : 0.050258383590092805
[2025-09-11 22:26:00,175][flp2p.graph_runner][INFO] - Test, Round 394 : loss => 6.585940639090538,  accuracy: 0.3215
[2025-09-11 22:26:36,476][flp2p.graph_runner][INFO] - Train, Round 395 : loss => 0.0009607609727148278,  accuracy: 1.0, gradient_norm : 0.04969296425796224
[2025-09-11 22:26:50,897][flp2p.graph_runner][INFO] - Test, Round 395 : loss => 6.586836514186859,  accuracy: 0.3211
[2025-09-11 22:27:26,837][flp2p.graph_runner][INFO] - Train, Round 396 : loss => 0.0009586444011802087,  accuracy: 1.0, gradient_norm : 0.049555631438513145
[2025-09-11 22:27:41,734][flp2p.graph_runner][INFO] - Test, Round 396 : loss => 6.588089601755142,  accuracy: 0.3215
[2025-09-11 22:28:17,529][flp2p.graph_runner][INFO] - Train, Round 397 : loss => 0.0009567927167275533,  accuracy: 1.0, gradient_norm : 0.04881829222201137
[2025-09-11 22:28:35,989][flp2p.graph_runner][INFO] - Test, Round 397 : loss => 6.589752787065506,  accuracy: 0.3212
[2025-09-11 22:29:11,743][flp2p.graph_runner][INFO] - Train, Round 398 : loss => 0.0009545276502952525,  accuracy: 1.0, gradient_norm : 0.04783151677059893
[2025-09-11 22:29:34,319][flp2p.graph_runner][INFO] - Test, Round 398 : loss => 6.590866444134712,  accuracy: 0.321
[2025-09-11 22:30:10,341][flp2p.graph_runner][INFO] - Train, Round 399 : loss => 0.0009522117561573391,  accuracy: 1.0, gradient_norm : 0.051132203690717096
[2025-09-11 22:30:28,942][flp2p.graph_runner][INFO] - Test, Round 399 : loss => 6.592716108179093,  accuracy: 0.321
[2025-09-11 22:31:04,204][flp2p.graph_runner][INFO] - Train, Round 400 : loss => 0.0009501123502559497,  accuracy: 1.0, gradient_norm : 0.05112034263469153
[2025-09-11 22:31:21,288][flp2p.graph_runner][INFO] - Test, Round 400 : loss => 6.593732169556618,  accuracy: 0.3215
[2025-09-11 22:31:56,954][flp2p.graph_runner][INFO] - Train, Round 401 : loss => 0.0009478761299154337,  accuracy: 1.0, gradient_norm : 0.04813124317901735
[2025-09-11 22:32:12,587][flp2p.graph_runner][INFO] - Test, Round 401 : loss => 6.594433899474144,  accuracy: 0.3208
[2025-09-11 22:32:48,541][flp2p.graph_runner][INFO] - Train, Round 402 : loss => 0.0009458392440622756,  accuracy: 1.0, gradient_norm : 0.04876356173033265
[2025-09-11 22:33:02,442][flp2p.graph_runner][INFO] - Test, Round 402 : loss => 6.5950043887138365,  accuracy: 0.3208
[2025-09-11 22:33:38,325][flp2p.graph_runner][INFO] - Train, Round 403 : loss => 0.0009435140543306869,  accuracy: 1.0, gradient_norm : 0.049625094224746785
[2025-09-11 22:33:52,431][flp2p.graph_runner][INFO] - Test, Round 403 : loss => 6.5968070911407475,  accuracy: 0.3215
[2025-09-11 22:34:28,295][flp2p.graph_runner][INFO] - Train, Round 404 : loss => 0.0009416650732843356,  accuracy: 1.0, gradient_norm : 0.0488397859420886
[2025-09-11 22:34:42,176][flp2p.graph_runner][INFO] - Test, Round 404 : loss => 6.598129859685898,  accuracy: 0.3214
[2025-09-11 22:35:18,292][flp2p.graph_runner][INFO] - Train, Round 405 : loss => 0.0009394951981084885,  accuracy: 1.0, gradient_norm : 0.04989107727072483
[2025-09-11 22:35:32,052][flp2p.graph_runner][INFO] - Test, Round 405 : loss => 6.598389199376106,  accuracy: 0.3216
[2025-09-11 22:36:08,189][flp2p.graph_runner][INFO] - Train, Round 406 : loss => 0.0009374486691134128,  accuracy: 1.0, gradient_norm : 0.04926219710934422
[2025-09-11 22:36:22,380][flp2p.graph_runner][INFO] - Test, Round 406 : loss => 6.60023394920826,  accuracy: 0.321
[2025-09-11 22:36:58,507][flp2p.graph_runner][INFO] - Train, Round 407 : loss => 0.0009351451914941812,  accuracy: 1.0, gradient_norm : 0.04985332041134312
[2025-09-11 22:37:12,339][flp2p.graph_runner][INFO] - Test, Round 407 : loss => 6.601355398774147,  accuracy: 0.321
[2025-09-11 22:37:48,455][flp2p.graph_runner][INFO] - Train, Round 408 : loss => 0.0009332489563288011,  accuracy: 1.0, gradient_norm : 0.05018824525287192
[2025-09-11 22:38:02,439][flp2p.graph_runner][INFO] - Test, Round 408 : loss => 6.602130375814438,  accuracy: 0.321
[2025-09-11 22:38:38,789][flp2p.graph_runner][INFO] - Train, Round 409 : loss => 0.0009310355124762283,  accuracy: 1.0, gradient_norm : 0.046640139559891255
[2025-09-11 22:38:53,107][flp2p.graph_runner][INFO] - Test, Round 409 : loss => 6.602820885944366,  accuracy: 0.3215
[2025-09-11 22:39:29,515][flp2p.graph_runner][INFO] - Train, Round 410 : loss => 0.000929162650742607,  accuracy: 1.0, gradient_norm : 0.049841810711263486
[2025-09-11 22:39:43,542][flp2p.graph_runner][INFO] - Test, Round 410 : loss => 6.604425324010849,  accuracy: 0.321
[2025-09-11 22:40:19,740][flp2p.graph_runner][INFO] - Train, Round 411 : loss => 0.0009270238521033508,  accuracy: 1.0, gradient_norm : 0.04798134155192149
[2025-09-11 22:40:33,852][flp2p.graph_runner][INFO] - Test, Round 411 : loss => 6.605780954051018,  accuracy: 0.3211
[2025-09-11 22:41:09,856][flp2p.graph_runner][INFO] - Train, Round 412 : loss => 0.0009249135572463275,  accuracy: 1.0, gradient_norm : 0.048586474962631965
[2025-09-11 22:41:24,032][flp2p.graph_runner][INFO] - Test, Round 412 : loss => 6.605846461462974,  accuracy: 0.321
[2025-09-11 22:42:00,175][flp2p.graph_runner][INFO] - Train, Round 413 : loss => 0.000923153917198457,  accuracy: 1.0, gradient_norm : 0.05012144531930687
[2025-09-11 22:42:14,300][flp2p.graph_runner][INFO] - Test, Round 413 : loss => 6.607387671780586,  accuracy: 0.3208
[2025-09-11 22:42:50,168][flp2p.graph_runner][INFO] - Train, Round 414 : loss => 0.0009213854071034193,  accuracy: 1.0, gradient_norm : 0.04976657882487391
[2025-09-11 22:43:04,699][flp2p.graph_runner][INFO] - Test, Round 414 : loss => 6.607819848442078,  accuracy: 0.3216
[2025-09-11 22:43:41,112][flp2p.graph_runner][INFO] - Train, Round 415 : loss => 0.0009196549160697031,  accuracy: 1.0, gradient_norm : 0.04866918705164051
[2025-09-11 22:43:55,288][flp2p.graph_runner][INFO] - Test, Round 415 : loss => 6.608891621494293,  accuracy: 0.3214
[2025-09-11 22:44:31,687][flp2p.graph_runner][INFO] - Train, Round 416 : loss => 0.0009177695023390695,  accuracy: 1.0, gradient_norm : 0.04960037213214157
[2025-09-11 22:44:45,770][flp2p.graph_runner][INFO] - Test, Round 416 : loss => 6.610888046526909,  accuracy: 0.3207
[2025-09-11 22:45:22,169][flp2p.graph_runner][INFO] - Train, Round 417 : loss => 0.0009154477678142334,  accuracy: 1.0, gradient_norm : 0.048629803741538774
[2025-09-11 22:45:36,540][flp2p.graph_runner][INFO] - Test, Round 417 : loss => 6.610472479486465,  accuracy: 0.3211
[2025-09-11 22:46:12,705][flp2p.graph_runner][INFO] - Train, Round 418 : loss => 0.0009136727668859146,  accuracy: 1.0, gradient_norm : 0.04893147913365998
[2025-09-11 22:46:26,914][flp2p.graph_runner][INFO] - Test, Round 418 : loss => 6.6128773236989975,  accuracy: 0.3208
[2025-09-11 22:47:03,010][flp2p.graph_runner][INFO] - Train, Round 419 : loss => 0.0009116996554803332,  accuracy: 1.0, gradient_norm : 0.04854880254508929
[2025-09-11 22:47:17,326][flp2p.graph_runner][INFO] - Test, Round 419 : loss => 6.613470512843132,  accuracy: 0.3207
[2025-09-11 22:47:53,601][flp2p.graph_runner][INFO] - Train, Round 420 : loss => 0.0009095948407533192,  accuracy: 1.0, gradient_norm : 0.04829742752980873
[2025-09-11 22:48:07,894][flp2p.graph_runner][INFO] - Test, Round 420 : loss => 6.613910713648796,  accuracy: 0.3205
[2025-09-11 22:48:44,195][flp2p.graph_runner][INFO] - Train, Round 421 : loss => 0.0009076758320103788,  accuracy: 1.0, gradient_norm : 0.04683861305398685
[2025-09-11 22:48:58,426][flp2p.graph_runner][INFO] - Test, Round 421 : loss => 6.615542066216469,  accuracy: 0.3209
[2025-09-11 22:49:34,685][flp2p.graph_runner][INFO] - Train, Round 422 : loss => 0.0009061894754631795,  accuracy: 1.0, gradient_norm : 0.048332174447390484
[2025-09-11 22:49:49,025][flp2p.graph_runner][INFO] - Test, Round 422 : loss => 6.615993076753616,  accuracy: 0.3207
[2025-09-11 22:50:25,184][flp2p.graph_runner][INFO] - Train, Round 423 : loss => 0.0009037866539438253,  accuracy: 1.0, gradient_norm : 0.04885837829300588
[2025-09-11 22:50:39,646][flp2p.graph_runner][INFO] - Test, Round 423 : loss => 6.616661632514,  accuracy: 0.321
[2025-09-11 22:51:15,659][flp2p.graph_runner][INFO] - Train, Round 424 : loss => 0.0009026099963860662,  accuracy: 1.0, gradient_norm : 0.04732574157142637
[2025-09-11 22:51:30,228][flp2p.graph_runner][INFO] - Test, Round 424 : loss => 6.617315970277787,  accuracy: 0.321
[2025-09-11 22:52:06,427][flp2p.graph_runner][INFO] - Train, Round 425 : loss => 0.0009010201227890017,  accuracy: 1.0, gradient_norm : 0.04743302273754393
[2025-09-11 22:52:21,152][flp2p.graph_runner][INFO] - Test, Round 425 : loss => 6.617995876145363,  accuracy: 0.3212
[2025-09-11 22:52:57,015][flp2p.graph_runner][INFO] - Train, Round 426 : loss => 0.000899058080249233,  accuracy: 1.0, gradient_norm : 0.04660100099419532
[2025-09-11 22:53:11,669][flp2p.graph_runner][INFO] - Test, Round 426 : loss => 6.619105546236038,  accuracy: 0.3208
[2025-09-11 22:53:47,498][flp2p.graph_runner][INFO] - Train, Round 427 : loss => 0.0008971832971656113,  accuracy: 1.0, gradient_norm : 0.04719486057448997
[2025-09-11 22:54:04,811][flp2p.graph_runner][INFO] - Test, Round 427 : loss => 6.62077431344986,  accuracy: 0.3206
[2025-09-11 22:54:40,677][flp2p.graph_runner][INFO] - Train, Round 428 : loss => 0.0008955697704125975,  accuracy: 1.0, gradient_norm : 0.04818836246526816
[2025-09-11 22:55:02,116][flp2p.graph_runner][INFO] - Test, Round 428 : loss => 6.621374399447441,  accuracy: 0.321
[2025-09-11 22:55:37,907][flp2p.graph_runner][INFO] - Train, Round 429 : loss => 0.0008939634725053713,  accuracy: 1.0, gradient_norm : 0.04831074815783257
[2025-09-11 22:55:57,873][flp2p.graph_runner][INFO] - Test, Round 429 : loss => 6.622849816799164,  accuracy: 0.321
[2025-09-11 22:56:33,668][flp2p.graph_runner][INFO] - Train, Round 430 : loss => 0.0008918098737922263,  accuracy: 1.0, gradient_norm : 0.046227500920603924
[2025-09-11 22:56:51,262][flp2p.graph_runner][INFO] - Test, Round 430 : loss => 6.623076255249977,  accuracy: 0.3212
[2025-09-11 22:57:27,056][flp2p.graph_runner][INFO] - Train, Round 431 : loss => 0.0008904649364315752,  accuracy: 1.0, gradient_norm : 0.047720745643000884
[2025-09-11 22:57:41,813][flp2p.graph_runner][INFO] - Test, Round 431 : loss => 6.624297511219979,  accuracy: 0.321
[2025-09-11 22:58:17,905][flp2p.graph_runner][INFO] - Train, Round 432 : loss => 0.000888653198805211,  accuracy: 1.0, gradient_norm : 0.04704088290152018
[2025-09-11 22:58:31,949][flp2p.graph_runner][INFO] - Test, Round 432 : loss => 6.625290808010101,  accuracy: 0.321
[2025-09-11 22:59:08,014][flp2p.graph_runner][INFO] - Train, Round 433 : loss => 0.0008868138767623649,  accuracy: 1.0, gradient_norm : 0.046896559924496393
[2025-09-11 22:59:21,717][flp2p.graph_runner][INFO] - Test, Round 433 : loss => 6.626046630167961,  accuracy: 0.3209
[2025-09-11 22:59:58,063][flp2p.graph_runner][INFO] - Train, Round 434 : loss => 0.0008849849078008752,  accuracy: 1.0, gradient_norm : 0.047472015198687864
[2025-09-11 23:00:11,916][flp2p.graph_runner][INFO] - Test, Round 434 : loss => 6.626677582383156,  accuracy: 0.3209
[2025-09-11 23:00:47,815][flp2p.graph_runner][INFO] - Train, Round 435 : loss => 0.0008834662604931509,  accuracy: 1.0, gradient_norm : 0.04743138348565608
[2025-09-11 23:01:01,759][flp2p.graph_runner][INFO] - Test, Round 435 : loss => 6.6270525159358975,  accuracy: 0.3208
[2025-09-11 23:01:37,833][flp2p.graph_runner][INFO] - Train, Round 436 : loss => 0.0008819174438637373,  accuracy: 1.0, gradient_norm : 0.04645408956453207
[2025-09-11 23:01:52,005][flp2p.graph_runner][INFO] - Test, Round 436 : loss => 6.6277195859909055,  accuracy: 0.3213
[2025-09-11 23:02:27,765][flp2p.graph_runner][INFO] - Train, Round 437 : loss => 0.0008802032069797864,  accuracy: 1.0, gradient_norm : 0.047381511549405134
[2025-09-11 23:02:41,803][flp2p.graph_runner][INFO] - Test, Round 437 : loss => 6.628942963314056,  accuracy: 0.3212
[2025-09-11 23:03:17,908][flp2p.graph_runner][INFO] - Train, Round 438 : loss => 0.0008786548952290709,  accuracy: 1.0, gradient_norm : 0.04509705983447519
[2025-09-11 23:03:31,725][flp2p.graph_runner][INFO] - Test, Round 438 : loss => 6.629911795020104,  accuracy: 0.3213
[2025-09-11 23:04:08,210][flp2p.graph_runner][INFO] - Train, Round 439 : loss => 0.0008770309919297383,  accuracy: 1.0, gradient_norm : 0.047164217047781884
[2025-09-11 23:04:22,259][flp2p.graph_runner][INFO] - Test, Round 439 : loss => 6.630900553584099,  accuracy: 0.3208
[2025-09-11 23:04:58,724][flp2p.graph_runner][INFO] - Train, Round 440 : loss => 0.0008756063683055501,  accuracy: 1.0, gradient_norm : 0.046138750485264876
[2025-09-11 23:05:12,985][flp2p.graph_runner][INFO] - Test, Round 440 : loss => 6.632114695048332,  accuracy: 0.3202
[2025-09-11 23:05:48,949][flp2p.graph_runner][INFO] - Train, Round 441 : loss => 0.0008737503943363358,  accuracy: 1.0, gradient_norm : 0.04534799020359128
[2025-09-11 23:06:03,532][flp2p.graph_runner][INFO] - Test, Round 441 : loss => 6.632499628424645,  accuracy: 0.3207
[2025-09-11 23:06:39,836][flp2p.graph_runner][INFO] - Train, Round 442 : loss => 0.0008725219034871164,  accuracy: 1.0, gradient_norm : 0.045352759844453375
[2025-09-11 23:06:54,051][flp2p.graph_runner][INFO] - Test, Round 442 : loss => 6.633265110254288,  accuracy: 0.321
[2025-09-11 23:07:30,058][flp2p.graph_runner][INFO] - Train, Round 443 : loss => 0.0008705073546298087,  accuracy: 1.0, gradient_norm : 0.04695757362620252
[2025-09-11 23:07:44,156][flp2p.graph_runner][INFO] - Test, Round 443 : loss => 6.634387587547303,  accuracy: 0.3208
[2025-09-11 23:08:20,211][flp2p.graph_runner][INFO] - Train, Round 444 : loss => 0.0008693456882610919,  accuracy: 1.0, gradient_norm : 0.04556229232989687
[2025-09-11 23:08:34,412][flp2p.graph_runner][INFO] - Test, Round 444 : loss => 6.635239469909668,  accuracy: 0.3213
[2025-09-11 23:09:10,513][flp2p.graph_runner][INFO] - Train, Round 445 : loss => 0.000867703882443796,  accuracy: 1.0, gradient_norm : 0.04556069612549436
[2025-09-11 23:09:24,842][flp2p.graph_runner][INFO] - Test, Round 445 : loss => 6.636148446345329,  accuracy: 0.3212
[2025-09-11 23:10:00,801][flp2p.graph_runner][INFO] - Train, Round 446 : loss => 0.0008662230731715681,  accuracy: 1.0, gradient_norm : 0.048583278453762535
[2025-09-11 23:10:15,001][flp2p.graph_runner][INFO] - Test, Round 446 : loss => 6.635928443622589,  accuracy: 0.3215
[2025-09-11 23:10:51,364][flp2p.graph_runner][INFO] - Train, Round 447 : loss => 0.000864856357196307,  accuracy: 1.0, gradient_norm : 0.045169486377080945
[2025-09-11 23:11:05,819][flp2p.graph_runner][INFO] - Test, Round 447 : loss => 6.635994936156273,  accuracy: 0.3208
[2025-09-11 23:11:41,936][flp2p.graph_runner][INFO] - Train, Round 448 : loss => 0.0008634236209278849,  accuracy: 1.0, gradient_norm : 0.04535255835301391
[2025-09-11 23:11:56,254][flp2p.graph_runner][INFO] - Test, Round 448 : loss => 6.637973776245117,  accuracy: 0.3205
[2025-09-11 23:12:32,191][flp2p.graph_runner][INFO] - Train, Round 449 : loss => 0.0008615328442586667,  accuracy: 1.0, gradient_norm : 0.044969061592265366
[2025-09-11 23:12:46,294][flp2p.graph_runner][INFO] - Test, Round 449 : loss => 6.638951233935356,  accuracy: 0.3212
[2025-09-11 23:13:22,353][flp2p.graph_runner][INFO] - Train, Round 450 : loss => 0.0008599065739448026,  accuracy: 1.0, gradient_norm : 0.043401998230444994
[2025-09-11 23:13:36,733][flp2p.graph_runner][INFO] - Test, Round 450 : loss => 6.6394828743934635,  accuracy: 0.3208
[2025-09-11 23:14:12,799][flp2p.graph_runner][INFO] - Train, Round 451 : loss => 0.0008586273907106564,  accuracy: 1.0, gradient_norm : 0.04591226662181938
[2025-09-11 23:14:26,868][flp2p.graph_runner][INFO] - Test, Round 451 : loss => 6.639989903235436,  accuracy: 0.3207
[2025-09-11 23:15:03,177][flp2p.graph_runner][INFO] - Train, Round 452 : loss => 0.0008572523754992291,  accuracy: 1.0, gradient_norm : 0.04527997515481364
[2025-09-11 23:15:17,654][flp2p.graph_runner][INFO] - Test, Round 452 : loss => 6.640321708536148,  accuracy: 0.3209
[2025-09-11 23:15:53,705][flp2p.graph_runner][INFO] - Train, Round 453 : loss => 0.0008555654913167623,  accuracy: 1.0, gradient_norm : 0.04438660222741201
[2025-09-11 23:16:08,111][flp2p.graph_runner][INFO] - Test, Round 453 : loss => 6.6413676949501035,  accuracy: 0.3209
[2025-09-11 23:16:44,334][flp2p.graph_runner][INFO] - Train, Round 454 : loss => 0.000854008529058774,  accuracy: 1.0, gradient_norm : 0.04325852239422949
[2025-09-11 23:16:58,912][flp2p.graph_runner][INFO] - Test, Round 454 : loss => 6.642339031171799,  accuracy: 0.3212
[2025-09-11 23:17:34,819][flp2p.graph_runner][INFO] - Train, Round 455 : loss => 0.0008527857335623895,  accuracy: 1.0, gradient_norm : 0.04513191895354183
[2025-09-11 23:17:49,275][flp2p.graph_runner][INFO] - Test, Round 455 : loss => 6.642375683903694,  accuracy: 0.3213
[2025-09-11 23:18:25,280][flp2p.graph_runner][INFO] - Train, Round 456 : loss => 0.0008514916901670705,  accuracy: 1.0, gradient_norm : 0.04574468533674061
[2025-09-11 23:18:40,017][flp2p.graph_runner][INFO] - Test, Round 456 : loss => 6.6429680368185045,  accuracy: 0.3208
[2025-09-11 23:19:15,608][flp2p.graph_runner][INFO] - Train, Round 457 : loss => 0.0008503624525716684,  accuracy: 1.0, gradient_norm : 0.045639026998360206
[2025-09-11 23:19:30,640][flp2p.graph_runner][INFO] - Test, Round 457 : loss => 6.643864755582809,  accuracy: 0.3212
[2025-09-11 23:20:06,528][flp2p.graph_runner][INFO] - Train, Round 458 : loss => 0.000848424956348026,  accuracy: 1.0, gradient_norm : 0.045023248560106845
[2025-09-11 23:20:25,819][flp2p.graph_runner][INFO] - Test, Round 458 : loss => 6.644907985806465,  accuracy: 0.3207
[2025-09-11 23:21:01,460][flp2p.graph_runner][INFO] - Train, Round 459 : loss => 0.0008473201645635222,  accuracy: 1.0, gradient_norm : 0.045215210947974954
[2025-09-11 23:21:23,594][flp2p.graph_runner][INFO] - Test, Round 459 : loss => 6.646202465081215,  accuracy: 0.3213
[2025-09-11 23:21:59,385][flp2p.graph_runner][INFO] - Train, Round 460 : loss => 0.0008454865063928687,  accuracy: 1.0, gradient_norm : 0.04398882023135492
[2025-09-11 23:22:19,324][flp2p.graph_runner][INFO] - Test, Round 460 : loss => 6.646063411784172,  accuracy: 0.321
[2025-09-11 23:22:54,785][flp2p.graph_runner][INFO] - Train, Round 461 : loss => 0.0008444703063772366,  accuracy: 1.0, gradient_norm : 0.044374590465483685
[2025-09-11 23:23:12,075][flp2p.graph_runner][INFO] - Test, Round 461 : loss => 6.645912657809258,  accuracy: 0.3213
[2025-09-11 23:23:48,187][flp2p.graph_runner][INFO] - Train, Round 462 : loss => 0.0008429897042515225,  accuracy: 1.0, gradient_norm : 0.044659828689060106
[2025-09-11 23:24:02,409][flp2p.graph_runner][INFO] - Test, Round 462 : loss => 6.64699621064663,  accuracy: 0.3209
[2025-09-11 23:24:38,197][flp2p.graph_runner][INFO] - Train, Round 463 : loss => 0.0008417779537073028,  accuracy: 1.0, gradient_norm : 0.04545710502128071
[2025-09-11 23:24:52,210][flp2p.graph_runner][INFO] - Test, Round 463 : loss => 6.6481682250499725,  accuracy: 0.3212
[2025-09-11 23:25:28,371][flp2p.graph_runner][INFO] - Train, Round 464 : loss => 0.0008402255498125064,  accuracy: 1.0, gradient_norm : 0.04367955429795172
[2025-09-11 23:25:42,201][flp2p.graph_runner][INFO] - Test, Round 464 : loss => 6.64947603456974,  accuracy: 0.3209
[2025-09-11 23:26:18,170][flp2p.graph_runner][INFO] - Train, Round 465 : loss => 0.0008393800706350397,  accuracy: 1.0, gradient_norm : 0.044186212183539936
[2025-09-11 23:26:32,020][flp2p.graph_runner][INFO] - Test, Round 465 : loss => 6.649638635873795,  accuracy: 0.3207
[2025-09-11 23:27:08,248][flp2p.graph_runner][INFO] - Train, Round 466 : loss => 0.0008378540129342582,  accuracy: 1.0, gradient_norm : 0.04562187077134828
[2025-09-11 23:27:22,019][flp2p.graph_runner][INFO] - Test, Round 466 : loss => 6.6498152148246765,  accuracy: 0.3208
[2025-09-11 23:27:58,238][flp2p.graph_runner][INFO] - Train, Round 467 : loss => 0.0008365288679973066,  accuracy: 1.0, gradient_norm : 0.04416135023439629
[2025-09-11 23:28:12,169][flp2p.graph_runner][INFO] - Test, Round 467 : loss => 6.651038316655159,  accuracy: 0.3207
[2025-09-11 23:28:48,115][flp2p.graph_runner][INFO] - Train, Round 468 : loss => 0.0008351353508624017,  accuracy: 1.0, gradient_norm : 0.043431579736619055
[2025-09-11 23:29:02,394][flp2p.graph_runner][INFO] - Test, Round 468 : loss => 6.651558501553535,  accuracy: 0.3213
[2025-09-11 23:29:38,780][flp2p.graph_runner][INFO] - Train, Round 469 : loss => 0.0008338812662380708,  accuracy: 1.0, gradient_norm : 0.04356028213783673
[2025-09-11 23:29:53,126][flp2p.graph_runner][INFO] - Test, Round 469 : loss => 6.652460095643997,  accuracy: 0.3205
[2025-09-11 23:30:29,464][flp2p.graph_runner][INFO] - Train, Round 470 : loss => 0.0008327576004861235,  accuracy: 1.0, gradient_norm : 0.0432393331985505
[2025-09-11 23:30:43,687][flp2p.graph_runner][INFO] - Test, Round 470 : loss => 6.652842629051208,  accuracy: 0.3206
[2025-09-11 23:31:19,987][flp2p.graph_runner][INFO] - Train, Round 471 : loss => 0.0008314879198587736,  accuracy: 1.0, gradient_norm : 0.04382818521436954
[2025-09-11 23:31:34,439][flp2p.graph_runner][INFO] - Test, Round 471 : loss => 6.653209919857979,  accuracy: 0.3212
[2025-09-11 23:32:10,876][flp2p.graph_runner][INFO] - Train, Round 472 : loss => 0.0008299904305871075,  accuracy: 1.0, gradient_norm : 0.043790868351754886
[2025-09-11 23:32:25,102][flp2p.graph_runner][INFO] - Test, Round 472 : loss => 6.6535956841468815,  accuracy: 0.321
[2025-09-11 23:33:01,329][flp2p.graph_runner][INFO] - Train, Round 473 : loss => 0.0008287932816892866,  accuracy: 1.0, gradient_norm : 0.04446170166261765
[2025-09-11 23:33:15,496][flp2p.graph_runner][INFO] - Test, Round 473 : loss => 6.654257625484466,  accuracy: 0.3207
[2025-09-11 23:33:51,705][flp2p.graph_runner][INFO] - Train, Round 474 : loss => 0.0008273142241038537,  accuracy: 1.0, gradient_norm : 0.0430454281181765
[2025-09-11 23:34:05,966][flp2p.graph_runner][INFO] - Test, Round 474 : loss => 6.654703548192978,  accuracy: 0.321
[2025-09-11 23:34:42,265][flp2p.graph_runner][INFO] - Train, Round 475 : loss => 0.0008261744409173845,  accuracy: 1.0, gradient_norm : 0.042607613185589074
[2025-09-11 23:34:56,532][flp2p.graph_runner][INFO] - Test, Round 475 : loss => 6.655900995278358,  accuracy: 0.3209
[2025-09-11 23:35:32,919][flp2p.graph_runner][INFO] - Train, Round 476 : loss => 0.0008248842006408571,  accuracy: 1.0, gradient_norm : 0.044181512819603704
[2025-09-11 23:35:47,083][flp2p.graph_runner][INFO] - Test, Round 476 : loss => 6.655470317482949,  accuracy: 0.3209
[2025-09-11 23:36:23,595][flp2p.graph_runner][INFO] - Train, Round 477 : loss => 0.0008237514968277539,  accuracy: 1.0, gradient_norm : 0.04342507165670821
[2025-09-11 23:36:37,963][flp2p.graph_runner][INFO] - Test, Round 477 : loss => 6.65675946598053,  accuracy: 0.321
[2025-09-11 23:37:14,012][flp2p.graph_runner][INFO] - Train, Round 478 : loss => 0.0008226299246841031,  accuracy: 1.0, gradient_norm : 0.045624062336847133
[2025-09-11 23:37:28,521][flp2p.graph_runner][INFO] - Test, Round 478 : loss => 6.657482411813736,  accuracy: 0.3208
[2025-09-11 23:38:04,858][flp2p.graph_runner][INFO] - Train, Round 479 : loss => 0.0008211941538320388,  accuracy: 1.0, gradient_norm : 0.04368778114860818
[2025-09-11 23:38:19,189][flp2p.graph_runner][INFO] - Test, Round 479 : loss => 6.656873848843574,  accuracy: 0.3208
[2025-09-11 23:38:55,612][flp2p.graph_runner][INFO] - Train, Round 480 : loss => 0.0008198416166730262,  accuracy: 1.0, gradient_norm : 0.04245578666698415
[2025-09-11 23:39:09,956][flp2p.graph_runner][INFO] - Test, Round 480 : loss => 6.658698569202423,  accuracy: 0.3208
[2025-09-11 23:39:46,308][flp2p.graph_runner][INFO] - Train, Round 481 : loss => 0.0008191739481359644,  accuracy: 1.0, gradient_norm : 0.04377779983033271
[2025-09-11 23:40:00,936][flp2p.graph_runner][INFO] - Test, Round 481 : loss => 6.659194108319283,  accuracy: 0.3212
[2025-09-11 23:40:36,921][flp2p.graph_runner][INFO] - Train, Round 482 : loss => 0.0008179676823541135,  accuracy: 1.0, gradient_norm : 0.04481512212185049
[2025-09-11 23:40:51,456][flp2p.graph_runner][INFO] - Test, Round 482 : loss => 6.659677063179016,  accuracy: 0.3209
[2025-09-11 23:41:26,970][flp2p.graph_runner][INFO] - Train, Round 483 : loss => 0.0008166174528741976,  accuracy: 1.0, gradient_norm : 0.04507739928363434
[2025-09-11 23:41:41,336][flp2p.graph_runner][INFO] - Test, Round 483 : loss => 6.6598709346532825,  accuracy: 0.3208
[2025-09-11 23:42:17,358][flp2p.graph_runner][INFO] - Train, Round 484 : loss => 0.0008153604132045679,  accuracy: 1.0, gradient_norm : 0.04366100539524718
[2025-09-11 23:42:31,863][flp2p.graph_runner][INFO] - Test, Round 484 : loss => 6.66033472044468,  accuracy: 0.321
[2025-09-11 23:43:07,858][flp2p.graph_runner][INFO] - Train, Round 485 : loss => 0.0008144409492645842,  accuracy: 1.0, gradient_norm : 0.04599474252074285
[2025-09-11 23:43:26,644][flp2p.graph_runner][INFO] - Test, Round 485 : loss => 6.661414376163482,  accuracy: 0.3211
[2025-09-11 23:44:02,494][flp2p.graph_runner][INFO] - Train, Round 486 : loss => 0.0008133413249985702,  accuracy: 1.0, gradient_norm : 0.04320903383332925
[2025-09-11 23:44:25,135][flp2p.graph_runner][INFO] - Test, Round 486 : loss => 6.661942813563347,  accuracy: 0.321
[2025-09-11 23:45:00,944][flp2p.graph_runner][INFO] - Train, Round 487 : loss => 0.0008119508908566782,  accuracy: 1.0, gradient_norm : 0.04224240112429165
[2025-09-11 23:45:19,861][flp2p.graph_runner][INFO] - Test, Round 487 : loss => 6.662868974900245,  accuracy: 0.3209
[2025-09-11 23:45:55,898][flp2p.graph_runner][INFO] - Train, Round 488 : loss => 0.0008108031538116243,  accuracy: 1.0, gradient_norm : 0.04286991444569498
[2025-09-11 23:46:12,566][flp2p.graph_runner][INFO] - Test, Round 488 : loss => 6.662684048008919,  accuracy: 0.3208
[2025-09-11 23:46:48,595][flp2p.graph_runner][INFO] - Train, Round 489 : loss => 0.0008094062410721862,  accuracy: 1.0, gradient_norm : 0.04337130415140979
[2025-09-11 23:47:02,660][flp2p.graph_runner][INFO] - Test, Round 489 : loss => 6.663791692233086,  accuracy: 0.3213
[2025-09-11 23:47:38,747][flp2p.graph_runner][INFO] - Train, Round 490 : loss => 0.0008087247451840084,  accuracy: 1.0, gradient_norm : 0.04361654631753571
[2025-09-11 23:47:52,698][flp2p.graph_runner][INFO] - Test, Round 490 : loss => 6.663440080809593,  accuracy: 0.3208
[2025-09-11 23:48:28,587][flp2p.graph_runner][INFO] - Train, Round 491 : loss => 0.0008074159346627614,  accuracy: 1.0, gradient_norm : 0.04311272001551094
[2025-09-11 23:48:42,583][flp2p.graph_runner][INFO] - Test, Round 491 : loss => 6.664796829891205,  accuracy: 0.3211
[2025-09-11 23:49:18,440][flp2p.graph_runner][INFO] - Train, Round 492 : loss => 0.0008065251198907692,  accuracy: 1.0, gradient_norm : 0.04402041975755702
[2025-09-11 23:49:32,360][flp2p.graph_runner][INFO] - Test, Round 492 : loss => 6.6649758345842365,  accuracy: 0.3211
[2025-09-11 23:50:08,332][flp2p.graph_runner][INFO] - Train, Round 493 : loss => 0.000805478617391297,  accuracy: 1.0, gradient_norm : 0.04253264117693806
[2025-09-11 23:50:22,367][flp2p.graph_runner][INFO] - Test, Round 493 : loss => 6.665982766985893,  accuracy: 0.3212
[2025-09-11 23:50:58,039][flp2p.graph_runner][INFO] - Train, Round 494 : loss => 0.0008041396661549995,  accuracy: 1.0, gradient_norm : 0.04260800723276329
[2025-09-11 23:51:11,818][flp2p.graph_runner][INFO] - Test, Round 494 : loss => 6.6655706325531,  accuracy: 0.3206
[2025-09-11 23:51:47,721][flp2p.graph_runner][INFO] - Train, Round 495 : loss => 0.000803142517785697,  accuracy: 1.0, gradient_norm : 0.04181836658302982
[2025-09-11 23:52:01,638][flp2p.graph_runner][INFO] - Test, Round 495 : loss => 6.666773776745797,  accuracy: 0.3207
[2025-09-11 23:52:37,974][flp2p.graph_runner][INFO] - Train, Round 496 : loss => 0.0008016932995815297,  accuracy: 1.0, gradient_norm : 0.04251800792481501
[2025-09-11 23:52:52,084][flp2p.graph_runner][INFO] - Test, Round 496 : loss => 6.6671712527751925,  accuracy: 0.3212
[2025-09-11 23:53:28,370][flp2p.graph_runner][INFO] - Train, Round 497 : loss => 0.0008010291468841991,  accuracy: 1.0, gradient_norm : 0.04376427227143651
[2025-09-11 23:53:42,575][flp2p.graph_runner][INFO] - Test, Round 497 : loss => 6.667534064531326,  accuracy: 0.321
[2025-09-11 23:54:18,660][flp2p.graph_runner][INFO] - Train, Round 498 : loss => 0.0007997687544775546,  accuracy: 1.0, gradient_norm : 0.04174352056737467
[2025-09-11 23:54:32,856][flp2p.graph_runner][INFO] - Test, Round 498 : loss => 6.667739844441414,  accuracy: 0.3208
[2025-09-11 23:55:09,117][flp2p.graph_runner][INFO] - Train, Round 499 : loss => 0.0007986604929707636,  accuracy: 1.0, gradient_norm : 0.043393846154964465
[2025-09-11 23:55:23,334][flp2p.graph_runner][INFO] - Test, Round 499 : loss => 6.668182191252709,  accuracy: 0.3209
[2025-09-11 23:55:23,338][__main__][INFO] - Train, Round 001: loss=2.3049, accuracy=0.1040, gradient_norm=0.1727, 
[2025-09-11 23:55:23,339][__main__][INFO] - Train, Round 002: loss=2.3037, accuracy=0.1094, gradient_norm=0.1726, 
[2025-09-11 23:55:23,339][__main__][INFO] - Train, Round 003: loss=2.3025, accuracy=0.1140, gradient_norm=0.1752, 
[2025-09-11 23:55:23,339][__main__][INFO] - Train, Round 004: loss=2.3014, accuracy=0.1180, gradient_norm=0.1742, 
[2025-09-11 23:55:23,339][__main__][INFO] - Train, Round 005: loss=2.3003, accuracy=0.1221, gradient_norm=0.1734, 
[2025-09-11 23:55:23,339][__main__][INFO] - Train, Round 006: loss=2.2992, accuracy=0.1248, gradient_norm=0.1763, 
[2025-09-11 23:55:23,339][__main__][INFO] - Train, Round 007: loss=2.2981, accuracy=0.1261, gradient_norm=0.1827, 
[2025-09-11 23:55:23,339][__main__][INFO] - Train, Round 008: loss=2.2970, accuracy=0.1285, gradient_norm=0.1806, 
[2025-09-11 23:55:23,339][__main__][INFO] - Train, Round 009: loss=2.2958, accuracy=0.1307, gradient_norm=0.1882, 
[2025-09-11 23:55:23,339][__main__][INFO] - Train, Round 010: loss=2.2946, accuracy=0.1328, gradient_norm=0.1919, 
[2025-09-11 23:55:23,339][__main__][INFO] - Train, Round 011: loss=2.2933, accuracy=0.1352, gradient_norm=0.1955, 
[2025-09-11 23:55:23,339][__main__][INFO] - Train, Round 012: loss=2.2918, accuracy=0.1379, gradient_norm=0.1987, 
[2025-09-11 23:55:23,339][__main__][INFO] - Train, Round 013: loss=2.2902, accuracy=0.1413, gradient_norm=0.2071, 
[2025-09-11 23:55:23,339][__main__][INFO] - Train, Round 014: loss=2.2884, accuracy=0.1451, gradient_norm=0.2139, 
[2025-09-11 23:55:23,339][__main__][INFO] - Train, Round 015: loss=2.2862, accuracy=0.1484, gradient_norm=0.2355, 
[2025-09-11 23:55:23,339][__main__][INFO] - Train, Round 016: loss=2.2837, accuracy=0.1515, gradient_norm=0.2343, 
[2025-09-11 23:55:23,339][__main__][INFO] - Train, Round 017: loss=2.2807, accuracy=0.1552, gradient_norm=0.2553, 
[2025-09-11 23:55:23,339][__main__][INFO] - Train, Round 018: loss=2.2771, accuracy=0.1594, gradient_norm=0.2852, 
[2025-09-11 23:55:23,339][__main__][INFO] - Train, Round 019: loss=2.2727, accuracy=0.1638, gradient_norm=0.3010, 
[2025-09-11 23:55:23,339][__main__][INFO] - Train, Round 020: loss=2.2672, accuracy=0.1675, gradient_norm=0.3412, 
[2025-09-11 23:55:23,339][__main__][INFO] - Train, Round 021: loss=2.2603, accuracy=0.1720, gradient_norm=0.3790, 
[2025-09-11 23:55:23,339][__main__][INFO] - Train, Round 022: loss=2.2516, accuracy=0.1772, gradient_norm=0.4144, 
[2025-09-11 23:55:23,339][__main__][INFO] - Train, Round 023: loss=2.2407, accuracy=0.1808, gradient_norm=0.4900, 
[2025-09-11 23:55:23,339][__main__][INFO] - Train, Round 024: loss=2.2269, accuracy=0.1866, gradient_norm=0.5697, 
[2025-09-11 23:55:23,339][__main__][INFO] - Train, Round 025: loss=2.2100, accuracy=0.1942, gradient_norm=0.6405, 
[2025-09-11 23:55:23,339][__main__][INFO] - Train, Round 026: loss=2.1899, accuracy=0.2014, gradient_norm=0.7678, 
[2025-09-11 23:55:23,339][__main__][INFO] - Train, Round 027: loss=2.1676, accuracy=0.2090, gradient_norm=0.9411, 
[2025-09-11 23:55:23,339][__main__][INFO] - Train, Round 028: loss=2.1436, accuracy=0.2178, gradient_norm=1.0243, 
[2025-09-11 23:55:23,339][__main__][INFO] - Train, Round 029: loss=2.1186, accuracy=0.2284, gradient_norm=1.1709, 
[2025-09-11 23:55:23,339][__main__][INFO] - Train, Round 030: loss=2.0941, accuracy=0.2431, gradient_norm=1.3580, 
[2025-09-11 23:55:23,339][__main__][INFO] - Train, Round 031: loss=2.0697, accuracy=0.2580, gradient_norm=1.5687, 
[2025-09-11 23:55:23,339][__main__][INFO] - Train, Round 032: loss=2.0446, accuracy=0.2724, gradient_norm=1.7513, 
[2025-09-11 23:55:23,339][__main__][INFO] - Train, Round 033: loss=2.0190, accuracy=0.2852, gradient_norm=1.7457, 
[2025-09-11 23:55:23,339][__main__][INFO] - Train, Round 034: loss=1.9939, accuracy=0.2954, gradient_norm=1.9958, 
[2025-09-11 23:55:23,339][__main__][INFO] - Train, Round 035: loss=1.9677, accuracy=0.3066, gradient_norm=2.1847, 
[2025-09-11 23:55:23,339][__main__][INFO] - Train, Round 036: loss=1.9431, accuracy=0.3150, gradient_norm=2.2477, 
[2025-09-11 23:55:23,339][__main__][INFO] - Train, Round 037: loss=1.9188, accuracy=0.3220, gradient_norm=2.5042, 
[2025-09-11 23:55:23,339][__main__][INFO] - Train, Round 038: loss=1.8974, accuracy=0.3272, gradient_norm=2.6989, 
[2025-09-11 23:55:23,339][__main__][INFO] - Train, Round 039: loss=1.8767, accuracy=0.3333, gradient_norm=2.8034, 
[2025-09-11 23:55:23,339][__main__][INFO] - Train, Round 040: loss=1.8592, accuracy=0.3404, gradient_norm=2.9725, 
[2025-09-11 23:55:23,339][__main__][INFO] - Train, Round 041: loss=1.8395, accuracy=0.3471, gradient_norm=3.2365, 
[2025-09-11 23:55:23,339][__main__][INFO] - Train, Round 042: loss=1.8227, accuracy=0.3518, gradient_norm=3.2273, 
[2025-09-11 23:55:23,339][__main__][INFO] - Train, Round 043: loss=1.8058, accuracy=0.3580, gradient_norm=3.4938, 
[2025-09-11 23:55:23,339][__main__][INFO] - Train, Round 044: loss=1.7868, accuracy=0.3650, gradient_norm=3.7320, 
[2025-09-11 23:55:23,339][__main__][INFO] - Train, Round 045: loss=1.7704, accuracy=0.3716, gradient_norm=3.6900, 
[2025-09-11 23:55:23,339][__main__][INFO] - Train, Round 046: loss=1.7533, accuracy=0.3777, gradient_norm=3.7517, 
[2025-09-11 23:55:23,339][__main__][INFO] - Train, Round 047: loss=1.7347, accuracy=0.3826, gradient_norm=3.9589, 
[2025-09-11 23:55:23,339][__main__][INFO] - Train, Round 048: loss=1.7190, accuracy=0.3905, gradient_norm=4.1622, 
[2025-09-11 23:55:23,339][__main__][INFO] - Train, Round 049: loss=1.6999, accuracy=0.3988, gradient_norm=4.1537, 
[2025-09-11 23:55:23,339][__main__][INFO] - Train, Round 050: loss=1.6790, accuracy=0.4048, gradient_norm=4.2769, 
[2025-09-11 23:55:23,339][__main__][INFO] - Train, Round 051: loss=1.6596, accuracy=0.4125, gradient_norm=4.3197, 
[2025-09-11 23:55:23,339][__main__][INFO] - Train, Round 052: loss=1.6400, accuracy=0.4204, gradient_norm=4.5353, 
[2025-09-11 23:55:23,339][__main__][INFO] - Train, Round 053: loss=1.6148, accuracy=0.4300, gradient_norm=4.6722, 
[2025-09-11 23:55:23,339][__main__][INFO] - Train, Round 054: loss=1.5954, accuracy=0.4370, gradient_norm=4.5791, 
[2025-09-11 23:55:23,339][__main__][INFO] - Train, Round 055: loss=1.5750, accuracy=0.4448, gradient_norm=4.8700, 
[2025-09-11 23:55:23,339][__main__][INFO] - Train, Round 056: loss=1.5468, accuracy=0.4529, gradient_norm=5.0396, 
[2025-09-11 23:55:23,340][__main__][INFO] - Train, Round 057: loss=1.5186, accuracy=0.4652, gradient_norm=4.9876, 
[2025-09-11 23:55:23,340][__main__][INFO] - Train, Round 058: loss=1.4990, accuracy=0.4727, gradient_norm=5.5003, 
[2025-09-11 23:55:23,340][__main__][INFO] - Train, Round 059: loss=1.4725, accuracy=0.4834, gradient_norm=5.2058, 
[2025-09-11 23:55:23,340][__main__][INFO] - Train, Round 060: loss=1.4460, accuracy=0.4911, gradient_norm=5.5871, 
[2025-09-11 23:55:23,340][__main__][INFO] - Train, Round 061: loss=1.4200, accuracy=0.5016, gradient_norm=5.9629, 
[2025-09-11 23:55:23,340][__main__][INFO] - Train, Round 062: loss=1.3892, accuracy=0.5117, gradient_norm=5.7637, 
[2025-09-11 23:55:23,340][__main__][INFO] - Train, Round 063: loss=1.3608, accuracy=0.5226, gradient_norm=5.9710, 
[2025-09-11 23:55:23,340][__main__][INFO] - Train, Round 064: loss=1.3305, accuracy=0.5326, gradient_norm=5.9135, 
[2025-09-11 23:55:23,340][__main__][INFO] - Train, Round 065: loss=1.3037, accuracy=0.5440, gradient_norm=5.8613, 
[2025-09-11 23:55:23,340][__main__][INFO] - Train, Round 066: loss=1.2673, accuracy=0.5563, gradient_norm=6.4436, 
[2025-09-11 23:55:23,340][__main__][INFO] - Train, Round 067: loss=1.2427, accuracy=0.5654, gradient_norm=6.3616, 
[2025-09-11 23:55:23,340][__main__][INFO] - Train, Round 068: loss=1.2179, accuracy=0.5765, gradient_norm=7.0631, 
[2025-09-11 23:55:23,340][__main__][INFO] - Train, Round 069: loss=1.1807, accuracy=0.5881, gradient_norm=6.8183, 
[2025-09-11 23:55:23,340][__main__][INFO] - Train, Round 070: loss=1.1494, accuracy=0.6024, gradient_norm=6.9665, 
[2025-09-11 23:55:23,340][__main__][INFO] - Train, Round 071: loss=1.1195, accuracy=0.6140, gradient_norm=7.1446, 
[2025-09-11 23:55:23,340][__main__][INFO] - Train, Round 072: loss=1.0961, accuracy=0.6243, gradient_norm=7.2416, 
[2025-09-11 23:55:23,340][__main__][INFO] - Train, Round 073: loss=1.0485, accuracy=0.6401, gradient_norm=7.0341, 
[2025-09-11 23:55:23,340][__main__][INFO] - Train, Round 074: loss=1.0244, accuracy=0.6497, gradient_norm=6.9956, 
[2025-09-11 23:55:23,340][__main__][INFO] - Train, Round 075: loss=0.9702, accuracy=0.6686, gradient_norm=7.3889, 
[2025-09-11 23:55:23,340][__main__][INFO] - Train, Round 076: loss=0.9394, accuracy=0.6804, gradient_norm=7.5083, 
[2025-09-11 23:55:23,340][__main__][INFO] - Train, Round 077: loss=0.9046, accuracy=0.6937, gradient_norm=7.4794, 
[2025-09-11 23:55:23,340][__main__][INFO] - Train, Round 078: loss=0.8671, accuracy=0.7075, gradient_norm=7.6292, 
[2025-09-11 23:55:23,340][__main__][INFO] - Train, Round 079: loss=0.8482, accuracy=0.7175, gradient_norm=7.4888, 
[2025-09-11 23:55:23,340][__main__][INFO] - Train, Round 080: loss=0.7920, accuracy=0.7373, gradient_norm=8.1253, 
[2025-09-11 23:55:23,340][__main__][INFO] - Train, Round 081: loss=0.7626, accuracy=0.7496, gradient_norm=7.7749, 
[2025-09-11 23:55:23,340][__main__][INFO] - Train, Round 082: loss=0.7206, accuracy=0.7641, gradient_norm=8.0940, 
[2025-09-11 23:55:23,340][__main__][INFO] - Train, Round 083: loss=0.6936, accuracy=0.7765, gradient_norm=7.9076, 
[2025-09-11 23:55:23,340][__main__][INFO] - Train, Round 084: loss=0.6561, accuracy=0.7930, gradient_norm=8.0176, 
[2025-09-11 23:55:23,340][__main__][INFO] - Train, Round 085: loss=0.6015, accuracy=0.8125, gradient_norm=7.8938, 
[2025-09-11 23:55:23,340][__main__][INFO] - Train, Round 086: loss=0.6211, accuracy=0.8137, gradient_norm=7.1856, 
[2025-09-11 23:55:23,340][__main__][INFO] - Train, Round 087: loss=0.5253, accuracy=0.8415, gradient_norm=7.8691, 
[2025-09-11 23:55:23,340][__main__][INFO] - Train, Round 088: loss=0.5011, accuracy=0.8520, gradient_norm=7.0435, 
[2025-09-11 23:55:23,340][__main__][INFO] - Train, Round 089: loss=0.4794, accuracy=0.8602, gradient_norm=7.1206, 
[2025-09-11 23:55:23,340][__main__][INFO] - Train, Round 090: loss=0.4293, accuracy=0.8796, gradient_norm=6.8631, 
[2025-09-11 23:55:23,340][__main__][INFO] - Train, Round 091: loss=0.4042, accuracy=0.8898, gradient_norm=6.2175, 
[2025-09-11 23:55:23,340][__main__][INFO] - Train, Round 092: loss=0.3651, accuracy=0.9045, gradient_norm=6.5933, 
[2025-09-11 23:55:23,340][__main__][INFO] - Train, Round 093: loss=0.3572, accuracy=0.9080, gradient_norm=5.3672, 
[2025-09-11 23:55:23,340][__main__][INFO] - Train, Round 094: loss=0.2982, accuracy=0.9302, gradient_norm=5.7976, 
[2025-09-11 23:55:23,340][__main__][INFO] - Train, Round 095: loss=0.2990, accuracy=0.9299, gradient_norm=4.8543, 
[2025-09-11 23:55:23,340][__main__][INFO] - Train, Round 096: loss=0.2550, accuracy=0.9454, gradient_norm=5.1745, 
[2025-09-11 23:55:23,340][__main__][INFO] - Train, Round 097: loss=0.3336, accuracy=0.9256, gradient_norm=4.2706, 
[2025-09-11 23:55:23,340][__main__][INFO] - Train, Round 098: loss=0.1930, accuracy=0.9658, gradient_norm=3.8959, 
[2025-09-11 23:55:23,340][__main__][INFO] - Train, Round 099: loss=0.2108, accuracy=0.9593, gradient_norm=3.4497, 
[2025-09-11 23:55:23,340][__main__][INFO] - Train, Round 100: loss=0.1937, accuracy=0.9640, gradient_norm=3.0428, 
[2025-09-11 23:55:23,340][__main__][INFO] - Train, Round 101: loss=0.1213, accuracy=0.9862, gradient_norm=3.1321, 
[2025-09-11 23:55:23,340][__main__][INFO] - Train, Round 102: loss=0.1172, accuracy=0.9859, gradient_norm=3.1467, 
[2025-09-11 23:55:23,340][__main__][INFO] - Train, Round 103: loss=0.2135, accuracy=0.9611, gradient_norm=2.3110, 
[2025-09-11 23:55:23,340][__main__][INFO] - Train, Round 104: loss=0.0869, accuracy=0.9932, gradient_norm=2.0961, 
[2025-09-11 23:55:23,340][__main__][INFO] - Train, Round 105: loss=0.0671, accuracy=0.9970, gradient_norm=2.1145, 
[2025-09-11 23:55:23,340][__main__][INFO] - Train, Round 106: loss=0.0815, accuracy=0.9924, gradient_norm=1.7220, 
[2025-09-11 23:55:23,340][__main__][INFO] - Train, Round 107: loss=0.0585, accuracy=0.9970, gradient_norm=1.6964, 
[2025-09-11 23:55:23,340][__main__][INFO] - Train, Round 108: loss=0.0511, accuracy=0.9978, gradient_norm=1.4681, 
[2025-09-11 23:55:23,340][__main__][INFO] - Train, Round 109: loss=0.0419, accuracy=0.9992, gradient_norm=1.4171, 
[2025-09-11 23:55:23,340][__main__][INFO] - Train, Round 110: loss=0.0385, accuracy=0.9992, gradient_norm=1.2529, 
[2025-09-11 23:55:23,340][__main__][INFO] - Train, Round 111: loss=0.0343, accuracy=0.9996, gradient_norm=1.1334, 
[2025-09-11 23:55:23,340][__main__][INFO] - Train, Round 112: loss=0.0314, accuracy=0.9997, gradient_norm=1.1136, 
[2025-09-11 23:55:23,340][__main__][INFO] - Train, Round 113: loss=0.0294, accuracy=0.9997, gradient_norm=1.0538, 
[2025-09-11 23:55:23,340][__main__][INFO] - Train, Round 114: loss=0.0268, accuracy=0.9998, gradient_norm=0.9198, 
[2025-09-11 23:55:23,340][__main__][INFO] - Train, Round 115: loss=0.0260, accuracy=0.9995, gradient_norm=0.9718, 
[2025-09-11 23:55:23,340][__main__][INFO] - Train, Round 116: loss=0.0232, accuracy=0.9999, gradient_norm=0.8509, 
[2025-09-11 23:55:23,340][__main__][INFO] - Train, Round 117: loss=0.0291, accuracy=0.9983, gradient_norm=0.8147, 
[2025-09-11 23:55:23,340][__main__][INFO] - Train, Round 118: loss=0.0208, accuracy=0.9999, gradient_norm=0.7260, 
[2025-09-11 23:55:23,340][__main__][INFO] - Train, Round 119: loss=0.0193, accuracy=0.9999, gradient_norm=0.7834, 
[2025-09-11 23:55:23,340][__main__][INFO] - Train, Round 120: loss=0.0181, accuracy=0.9999, gradient_norm=0.6868, 
[2025-09-11 23:55:23,340][__main__][INFO] - Train, Round 121: loss=0.0171, accuracy=0.9999, gradient_norm=0.6832, 
[2025-09-11 23:55:23,340][__main__][INFO] - Train, Round 122: loss=0.0163, accuracy=0.9999, gradient_norm=0.6131, 
[2025-09-11 23:55:23,340][__main__][INFO] - Train, Round 123: loss=0.0153, accuracy=1.0000, gradient_norm=0.6040, 
[2025-09-11 23:55:23,340][__main__][INFO] - Train, Round 124: loss=0.0145, accuracy=1.0000, gradient_norm=0.5740, 
[2025-09-11 23:55:23,340][__main__][INFO] - Train, Round 125: loss=0.0140, accuracy=0.9999, gradient_norm=0.5532, 
[2025-09-11 23:55:23,340][__main__][INFO] - Train, Round 126: loss=0.0132, accuracy=1.0000, gradient_norm=0.5478, 
[2025-09-11 23:55:23,341][__main__][INFO] - Train, Round 127: loss=0.0126, accuracy=1.0000, gradient_norm=0.4995, 
[2025-09-11 23:55:23,341][__main__][INFO] - Train, Round 128: loss=0.0121, accuracy=1.0000, gradient_norm=0.5152, 
[2025-09-11 23:55:23,341][__main__][INFO] - Train, Round 129: loss=0.0116, accuracy=1.0000, gradient_norm=0.4882, 
[2025-09-11 23:55:23,341][__main__][INFO] - Train, Round 130: loss=0.0111, accuracy=1.0000, gradient_norm=0.4703, 
[2025-09-11 23:55:23,341][__main__][INFO] - Train, Round 131: loss=0.0107, accuracy=1.0000, gradient_norm=0.4305, 
[2025-09-11 23:55:23,341][__main__][INFO] - Train, Round 132: loss=0.0103, accuracy=1.0000, gradient_norm=0.4138, 
[2025-09-11 23:55:23,341][__main__][INFO] - Train, Round 133: loss=0.0099, accuracy=1.0000, gradient_norm=0.4134, 
[2025-09-11 23:55:23,341][__main__][INFO] - Train, Round 134: loss=0.0095, accuracy=1.0000, gradient_norm=0.3808, 
[2025-09-11 23:55:23,341][__main__][INFO] - Train, Round 135: loss=0.0092, accuracy=1.0000, gradient_norm=0.3981, 
[2025-09-11 23:55:23,341][__main__][INFO] - Train, Round 136: loss=0.0089, accuracy=1.0000, gradient_norm=0.3821, 
[2025-09-11 23:55:23,341][__main__][INFO] - Train, Round 137: loss=0.0086, accuracy=1.0000, gradient_norm=0.3722, 
[2025-09-11 23:55:23,341][__main__][INFO] - Train, Round 138: loss=0.0083, accuracy=1.0000, gradient_norm=0.3565, 
[2025-09-11 23:55:23,341][__main__][INFO] - Train, Round 139: loss=0.0081, accuracy=1.0000, gradient_norm=0.3292, 
[2025-09-11 23:55:23,341][__main__][INFO] - Train, Round 140: loss=0.0078, accuracy=1.0000, gradient_norm=0.3261, 
[2025-09-11 23:55:23,341][__main__][INFO] - Train, Round 141: loss=0.0076, accuracy=1.0000, gradient_norm=0.3225, 
[2025-09-11 23:55:23,341][__main__][INFO] - Train, Round 142: loss=0.0074, accuracy=1.0000, gradient_norm=0.3170, 
[2025-09-11 23:55:23,341][__main__][INFO] - Train, Round 143: loss=0.0072, accuracy=1.0000, gradient_norm=0.3103, 
[2025-09-11 23:55:23,341][__main__][INFO] - Train, Round 144: loss=0.0070, accuracy=1.0000, gradient_norm=0.2918, 
[2025-09-11 23:55:23,341][__main__][INFO] - Train, Round 145: loss=0.0068, accuracy=1.0000, gradient_norm=0.2996, 
[2025-09-11 23:55:23,341][__main__][INFO] - Train, Round 146: loss=0.0066, accuracy=1.0000, gradient_norm=0.2746, 
[2025-09-11 23:55:23,341][__main__][INFO] - Train, Round 147: loss=0.0064, accuracy=1.0000, gradient_norm=0.2787, 
[2025-09-11 23:55:23,341][__main__][INFO] - Train, Round 148: loss=0.0063, accuracy=1.0000, gradient_norm=0.2780, 
[2025-09-11 23:55:23,341][__main__][INFO] - Train, Round 149: loss=0.0061, accuracy=1.0000, gradient_norm=0.2739, 
[2025-09-11 23:55:23,341][__main__][INFO] - Train, Round 150: loss=0.0060, accuracy=1.0000, gradient_norm=0.2561, 
[2025-09-11 23:55:23,341][__main__][INFO] - Train, Round 151: loss=0.0058, accuracy=1.0000, gradient_norm=0.2519, 
[2025-09-11 23:55:23,341][__main__][INFO] - Train, Round 152: loss=0.0057, accuracy=1.0000, gradient_norm=0.2515, 
[2025-09-11 23:55:23,341][__main__][INFO] - Train, Round 153: loss=0.0056, accuracy=1.0000, gradient_norm=0.2623, 
[2025-09-11 23:55:23,341][__main__][INFO] - Train, Round 154: loss=0.0054, accuracy=1.0000, gradient_norm=0.2439, 
[2025-09-11 23:55:23,341][__main__][INFO] - Train, Round 155: loss=0.0053, accuracy=1.0000, gradient_norm=0.2400, 
[2025-09-11 23:55:23,341][__main__][INFO] - Train, Round 156: loss=0.0052, accuracy=1.0000, gradient_norm=0.2319, 
[2025-09-11 23:55:23,341][__main__][INFO] - Train, Round 157: loss=0.0051, accuracy=1.0000, gradient_norm=0.2290, 
[2025-09-11 23:55:23,341][__main__][INFO] - Train, Round 158: loss=0.0050, accuracy=1.0000, gradient_norm=0.2170, 
[2025-09-11 23:55:23,341][__main__][INFO] - Train, Round 159: loss=0.0049, accuracy=1.0000, gradient_norm=0.2207, 
[2025-09-11 23:55:23,341][__main__][INFO] - Train, Round 160: loss=0.0048, accuracy=1.0000, gradient_norm=0.2064, 
[2025-09-11 23:55:23,341][__main__][INFO] - Train, Round 161: loss=0.0047, accuracy=1.0000, gradient_norm=0.2071, 
[2025-09-11 23:55:23,341][__main__][INFO] - Train, Round 162: loss=0.0046, accuracy=1.0000, gradient_norm=0.2102, 
[2025-09-11 23:55:23,341][__main__][INFO] - Train, Round 163: loss=0.0045, accuracy=1.0000, gradient_norm=0.2049, 
[2025-09-11 23:55:23,341][__main__][INFO] - Train, Round 164: loss=0.0044, accuracy=1.0000, gradient_norm=0.2007, 
[2025-09-11 23:55:23,341][__main__][INFO] - Train, Round 165: loss=0.0044, accuracy=1.0000, gradient_norm=0.1925, 
[2025-09-11 23:55:23,341][__main__][INFO] - Train, Round 166: loss=0.0043, accuracy=1.0000, gradient_norm=0.1949, 
[2025-09-11 23:55:23,341][__main__][INFO] - Train, Round 167: loss=0.0042, accuracy=1.0000, gradient_norm=0.1989, 
[2025-09-11 23:55:23,341][__main__][INFO] - Train, Round 168: loss=0.0041, accuracy=1.0000, gradient_norm=0.1831, 
[2025-09-11 23:55:23,341][__main__][INFO] - Train, Round 169: loss=0.0041, accuracy=1.0000, gradient_norm=0.1773, 
[2025-09-11 23:55:23,341][__main__][INFO] - Train, Round 170: loss=0.0040, accuracy=1.0000, gradient_norm=0.1842, 
[2025-09-11 23:55:23,341][__main__][INFO] - Train, Round 171: loss=0.0039, accuracy=1.0000, gradient_norm=0.1733, 
[2025-09-11 23:55:23,341][__main__][INFO] - Train, Round 172: loss=0.0039, accuracy=1.0000, gradient_norm=0.1751, 
[2025-09-11 23:55:23,341][__main__][INFO] - Train, Round 173: loss=0.0038, accuracy=1.0000, gradient_norm=0.1767, 
[2025-09-11 23:55:23,341][__main__][INFO] - Train, Round 174: loss=0.0037, accuracy=1.0000, gradient_norm=0.1659, 
[2025-09-11 23:55:23,341][__main__][INFO] - Train, Round 175: loss=0.0037, accuracy=1.0000, gradient_norm=0.1629, 
[2025-09-11 23:55:23,341][__main__][INFO] - Train, Round 176: loss=0.0036, accuracy=1.0000, gradient_norm=0.1652, 
[2025-09-11 23:55:23,341][__main__][INFO] - Train, Round 177: loss=0.0036, accuracy=1.0000, gradient_norm=0.1644, 
[2025-09-11 23:55:23,341][__main__][INFO] - Train, Round 178: loss=0.0035, accuracy=1.0000, gradient_norm=0.1658, 
[2025-09-11 23:55:23,341][__main__][INFO] - Train, Round 179: loss=0.0035, accuracy=1.0000, gradient_norm=0.1633, 
[2025-09-11 23:55:23,341][__main__][INFO] - Train, Round 180: loss=0.0034, accuracy=1.0000, gradient_norm=0.1617, 
[2025-09-11 23:55:23,341][__main__][INFO] - Train, Round 181: loss=0.0034, accuracy=1.0000, gradient_norm=0.1579, 
[2025-09-11 23:55:23,341][__main__][INFO] - Train, Round 182: loss=0.0033, accuracy=1.0000, gradient_norm=0.1533, 
[2025-09-11 23:55:23,341][__main__][INFO] - Train, Round 183: loss=0.0033, accuracy=1.0000, gradient_norm=0.1543, 
[2025-09-11 23:55:23,341][__main__][INFO] - Train, Round 184: loss=0.0032, accuracy=1.0000, gradient_norm=0.1567, 
[2025-09-11 23:55:23,341][__main__][INFO] - Train, Round 185: loss=0.0032, accuracy=1.0000, gradient_norm=0.1511, 
[2025-09-11 23:55:23,341][__main__][INFO] - Train, Round 186: loss=0.0031, accuracy=1.0000, gradient_norm=0.1480, 
[2025-09-11 23:55:23,341][__main__][INFO] - Train, Round 187: loss=0.0031, accuracy=1.0000, gradient_norm=0.1454, 
[2025-09-11 23:55:23,341][__main__][INFO] - Train, Round 188: loss=0.0031, accuracy=1.0000, gradient_norm=0.1445, 
[2025-09-11 23:55:23,341][__main__][INFO] - Train, Round 189: loss=0.0030, accuracy=1.0000, gradient_norm=0.1405, 
[2025-09-11 23:55:23,341][__main__][INFO] - Train, Round 190: loss=0.0030, accuracy=1.0000, gradient_norm=0.1452, 
[2025-09-11 23:55:23,341][__main__][INFO] - Train, Round 191: loss=0.0029, accuracy=1.0000, gradient_norm=0.1395, 
[2025-09-11 23:55:23,341][__main__][INFO] - Train, Round 192: loss=0.0029, accuracy=1.0000, gradient_norm=0.1421, 
[2025-09-11 23:55:23,341][__main__][INFO] - Train, Round 193: loss=0.0029, accuracy=1.0000, gradient_norm=0.1375, 
[2025-09-11 23:55:23,341][__main__][INFO] - Train, Round 194: loss=0.0028, accuracy=1.0000, gradient_norm=0.1335, 
[2025-09-11 23:55:23,341][__main__][INFO] - Train, Round 195: loss=0.0028, accuracy=1.0000, gradient_norm=0.1373, 
[2025-09-11 23:55:23,341][__main__][INFO] - Train, Round 196: loss=0.0028, accuracy=1.0000, gradient_norm=0.1334, 
[2025-09-11 23:55:23,341][__main__][INFO] - Train, Round 197: loss=0.0027, accuracy=1.0000, gradient_norm=0.1338, 
[2025-09-11 23:55:23,342][__main__][INFO] - Train, Round 198: loss=0.0027, accuracy=1.0000, gradient_norm=0.1285, 
[2025-09-11 23:55:23,342][__main__][INFO] - Train, Round 199: loss=0.0027, accuracy=1.0000, gradient_norm=0.1250, 
[2025-09-11 23:55:23,342][__main__][INFO] - Train, Round 200: loss=0.0026, accuracy=1.0000, gradient_norm=0.1376, 
[2025-09-11 23:55:23,342][__main__][INFO] - Train, Round 201: loss=0.0026, accuracy=1.0000, gradient_norm=0.1267, 
[2025-09-11 23:55:23,342][__main__][INFO] - Train, Round 202: loss=0.0026, accuracy=1.0000, gradient_norm=0.1272, 
[2025-09-11 23:55:23,342][__main__][INFO] - Train, Round 203: loss=0.0026, accuracy=1.0000, gradient_norm=0.1228, 
[2025-09-11 23:55:23,342][__main__][INFO] - Train, Round 204: loss=0.0025, accuracy=1.0000, gradient_norm=0.1217, 
[2025-09-11 23:55:23,342][__main__][INFO] - Train, Round 205: loss=0.0025, accuracy=1.0000, gradient_norm=0.1235, 
[2025-09-11 23:55:23,342][__main__][INFO] - Train, Round 206: loss=0.0025, accuracy=1.0000, gradient_norm=0.1200, 
[2025-09-11 23:55:23,342][__main__][INFO] - Train, Round 207: loss=0.0025, accuracy=1.0000, gradient_norm=0.1132, 
[2025-09-11 23:55:23,342][__main__][INFO] - Train, Round 208: loss=0.0024, accuracy=1.0000, gradient_norm=0.1202, 
[2025-09-11 23:55:23,342][__main__][INFO] - Train, Round 209: loss=0.0024, accuracy=1.0000, gradient_norm=0.1144, 
[2025-09-11 23:55:23,342][__main__][INFO] - Train, Round 210: loss=0.0024, accuracy=1.0000, gradient_norm=0.1220, 
[2025-09-11 23:55:23,342][__main__][INFO] - Train, Round 211: loss=0.0024, accuracy=1.0000, gradient_norm=0.1072, 
[2025-09-11 23:55:23,342][__main__][INFO] - Train, Round 212: loss=0.0023, accuracy=1.0000, gradient_norm=0.1130, 
[2025-09-11 23:55:23,342][__main__][INFO] - Train, Round 213: loss=0.0023, accuracy=1.0000, gradient_norm=0.1147, 
[2025-09-11 23:55:23,342][__main__][INFO] - Train, Round 214: loss=0.0023, accuracy=1.0000, gradient_norm=0.1168, 
[2025-09-11 23:55:23,342][__main__][INFO] - Train, Round 215: loss=0.0023, accuracy=1.0000, gradient_norm=0.1091, 
[2025-09-11 23:55:23,342][__main__][INFO] - Train, Round 216: loss=0.0023, accuracy=1.0000, gradient_norm=0.1099, 
[2025-09-11 23:55:23,342][__main__][INFO] - Train, Round 217: loss=0.0022, accuracy=1.0000, gradient_norm=0.1133, 
[2025-09-11 23:55:23,342][__main__][INFO] - Train, Round 218: loss=0.0022, accuracy=1.0000, gradient_norm=0.1077, 
[2025-09-11 23:55:23,342][__main__][INFO] - Train, Round 219: loss=0.0022, accuracy=1.0000, gradient_norm=0.1071, 
[2025-09-11 23:55:23,342][__main__][INFO] - Train, Round 220: loss=0.0022, accuracy=1.0000, gradient_norm=0.1079, 
[2025-09-11 23:55:23,342][__main__][INFO] - Train, Round 221: loss=0.0022, accuracy=1.0000, gradient_norm=0.1066, 
[2025-09-11 23:55:23,342][__main__][INFO] - Train, Round 222: loss=0.0021, accuracy=1.0000, gradient_norm=0.1067, 
[2025-09-11 23:55:23,342][__main__][INFO] - Train, Round 223: loss=0.0021, accuracy=1.0000, gradient_norm=0.1045, 
[2025-09-11 23:55:23,342][__main__][INFO] - Train, Round 224: loss=0.0021, accuracy=1.0000, gradient_norm=0.1062, 
[2025-09-11 23:55:23,342][__main__][INFO] - Train, Round 225: loss=0.0021, accuracy=1.0000, gradient_norm=0.0990, 
[2025-09-11 23:55:23,342][__main__][INFO] - Train, Round 226: loss=0.0021, accuracy=1.0000, gradient_norm=0.0986, 
[2025-09-11 23:55:23,342][__main__][INFO] - Train, Round 227: loss=0.0020, accuracy=1.0000, gradient_norm=0.1031, 
[2025-09-11 23:55:23,342][__main__][INFO] - Train, Round 228: loss=0.0020, accuracy=1.0000, gradient_norm=0.0977, 
[2025-09-11 23:55:23,342][__main__][INFO] - Train, Round 229: loss=0.0020, accuracy=1.0000, gradient_norm=0.1018, 
[2025-09-11 23:55:23,342][__main__][INFO] - Train, Round 230: loss=0.0020, accuracy=1.0000, gradient_norm=0.0976, 
[2025-09-11 23:55:23,342][__main__][INFO] - Train, Round 231: loss=0.0020, accuracy=1.0000, gradient_norm=0.1022, 
[2025-09-11 23:55:23,342][__main__][INFO] - Train, Round 232: loss=0.0020, accuracy=1.0000, gradient_norm=0.0958, 
[2025-09-11 23:55:23,342][__main__][INFO] - Train, Round 233: loss=0.0019, accuracy=1.0000, gradient_norm=0.0947, 
[2025-09-11 23:55:23,342][__main__][INFO] - Train, Round 234: loss=0.0019, accuracy=1.0000, gradient_norm=0.0953, 
[2025-09-11 23:55:23,342][__main__][INFO] - Train, Round 235: loss=0.0019, accuracy=1.0000, gradient_norm=0.1002, 
[2025-09-11 23:55:23,342][__main__][INFO] - Train, Round 236: loss=0.0019, accuracy=1.0000, gradient_norm=0.0978, 
[2025-09-11 23:55:23,342][__main__][INFO] - Train, Round 237: loss=0.0019, accuracy=1.0000, gradient_norm=0.0951, 
[2025-09-11 23:55:23,342][__main__][INFO] - Train, Round 238: loss=0.0019, accuracy=1.0000, gradient_norm=0.0926, 
[2025-09-11 23:55:23,342][__main__][INFO] - Train, Round 239: loss=0.0019, accuracy=1.0000, gradient_norm=0.0934, 
[2025-09-11 23:55:23,342][__main__][INFO] - Train, Round 240: loss=0.0018, accuracy=1.0000, gradient_norm=0.0904, 
[2025-09-11 23:55:23,342][__main__][INFO] - Train, Round 241: loss=0.0018, accuracy=1.0000, gradient_norm=0.0940, 
[2025-09-11 23:55:23,342][__main__][INFO] - Train, Round 242: loss=0.0018, accuracy=1.0000, gradient_norm=0.0923, 
[2025-09-11 23:55:23,342][__main__][INFO] - Train, Round 243: loss=0.0018, accuracy=1.0000, gradient_norm=0.0938, 
[2025-09-11 23:55:23,342][__main__][INFO] - Train, Round 244: loss=0.0018, accuracy=1.0000, gradient_norm=0.0881, 
[2025-09-11 23:55:23,342][__main__][INFO] - Train, Round 245: loss=0.0018, accuracy=1.0000, gradient_norm=0.0900, 
[2025-09-11 23:55:23,342][__main__][INFO] - Train, Round 246: loss=0.0018, accuracy=1.0000, gradient_norm=0.0886, 
[2025-09-11 23:55:23,342][__main__][INFO] - Train, Round 247: loss=0.0018, accuracy=1.0000, gradient_norm=0.0916, 
[2025-09-11 23:55:23,342][__main__][INFO] - Train, Round 248: loss=0.0017, accuracy=1.0000, gradient_norm=0.0899, 
[2025-09-11 23:55:23,342][__main__][INFO] - Train, Round 249: loss=0.0017, accuracy=1.0000, gradient_norm=0.0873, 
[2025-09-11 23:55:23,342][__main__][INFO] - Train, Round 250: loss=0.0017, accuracy=1.0000, gradient_norm=0.0862, 
[2025-09-11 23:55:23,342][__main__][INFO] - Train, Round 251: loss=0.0017, accuracy=1.0000, gradient_norm=0.0868, 
[2025-09-11 23:55:23,342][__main__][INFO] - Train, Round 252: loss=0.0017, accuracy=1.0000, gradient_norm=0.0840, 
[2025-09-11 23:55:23,342][__main__][INFO] - Train, Round 253: loss=0.0017, accuracy=1.0000, gradient_norm=0.0853, 
[2025-09-11 23:55:23,342][__main__][INFO] - Train, Round 254: loss=0.0017, accuracy=1.0000, gradient_norm=0.0824, 
[2025-09-11 23:55:23,342][__main__][INFO] - Train, Round 255: loss=0.0017, accuracy=1.0000, gradient_norm=0.0848, 
[2025-09-11 23:55:23,342][__main__][INFO] - Train, Round 256: loss=0.0017, accuracy=1.0000, gradient_norm=0.0865, 
[2025-09-11 23:55:23,342][__main__][INFO] - Train, Round 257: loss=0.0016, accuracy=1.0000, gradient_norm=0.0830, 
[2025-09-11 23:55:23,342][__main__][INFO] - Train, Round 258: loss=0.0016, accuracy=1.0000, gradient_norm=0.0832, 
[2025-09-11 23:55:23,342][__main__][INFO] - Train, Round 259: loss=0.0016, accuracy=1.0000, gradient_norm=0.0833, 
[2025-09-11 23:55:23,342][__main__][INFO] - Train, Round 260: loss=0.0016, accuracy=1.0000, gradient_norm=0.0837, 
[2025-09-11 23:55:23,342][__main__][INFO] - Train, Round 261: loss=0.0016, accuracy=1.0000, gradient_norm=0.0799, 
[2025-09-11 23:55:23,342][__main__][INFO] - Train, Round 262: loss=0.0016, accuracy=1.0000, gradient_norm=0.0819, 
[2025-09-11 23:55:23,342][__main__][INFO] - Train, Round 263: loss=0.0016, accuracy=1.0000, gradient_norm=0.0792, 
[2025-09-11 23:55:23,342][__main__][INFO] - Train, Round 264: loss=0.0016, accuracy=1.0000, gradient_norm=0.0779, 
[2025-09-11 23:55:23,342][__main__][INFO] - Train, Round 265: loss=0.0016, accuracy=1.0000, gradient_norm=0.0802, 
[2025-09-11 23:55:23,342][__main__][INFO] - Train, Round 266: loss=0.0016, accuracy=1.0000, gradient_norm=0.0796, 
[2025-09-11 23:55:23,342][__main__][INFO] - Train, Round 267: loss=0.0016, accuracy=1.0000, gradient_norm=0.0776, 
[2025-09-11 23:55:23,343][__main__][INFO] - Train, Round 268: loss=0.0015, accuracy=1.0000, gradient_norm=0.0793, 
[2025-09-11 23:55:23,343][__main__][INFO] - Train, Round 269: loss=0.0015, accuracy=1.0000, gradient_norm=0.0784, 
[2025-09-11 23:55:23,343][__main__][INFO] - Train, Round 270: loss=0.0015, accuracy=1.0000, gradient_norm=0.0777, 
[2025-09-11 23:55:23,343][__main__][INFO] - Train, Round 271: loss=0.0015, accuracy=1.0000, gradient_norm=0.0781, 
[2025-09-11 23:55:23,343][__main__][INFO] - Train, Round 272: loss=0.0015, accuracy=1.0000, gradient_norm=0.0806, 
[2025-09-11 23:55:23,343][__main__][INFO] - Train, Round 273: loss=0.0015, accuracy=1.0000, gradient_norm=0.0757, 
[2025-09-11 23:55:23,343][__main__][INFO] - Train, Round 274: loss=0.0015, accuracy=1.0000, gradient_norm=0.0765, 
[2025-09-11 23:55:23,343][__main__][INFO] - Train, Round 275: loss=0.0015, accuracy=1.0000, gradient_norm=0.0763, 
[2025-09-11 23:55:23,343][__main__][INFO] - Train, Round 276: loss=0.0015, accuracy=1.0000, gradient_norm=0.0759, 
[2025-09-11 23:55:23,343][__main__][INFO] - Train, Round 277: loss=0.0015, accuracy=1.0000, gradient_norm=0.0734, 
[2025-09-11 23:55:23,343][__main__][INFO] - Train, Round 278: loss=0.0015, accuracy=1.0000, gradient_norm=0.0760, 
[2025-09-11 23:55:23,343][__main__][INFO] - Train, Round 279: loss=0.0015, accuracy=1.0000, gradient_norm=0.0729, 
[2025-09-11 23:55:23,343][__main__][INFO] - Train, Round 280: loss=0.0014, accuracy=1.0000, gradient_norm=0.0777, 
[2025-09-11 23:55:23,343][__main__][INFO] - Train, Round 281: loss=0.0014, accuracy=1.0000, gradient_norm=0.0733, 
[2025-09-11 23:55:23,343][__main__][INFO] - Train, Round 282: loss=0.0014, accuracy=1.0000, gradient_norm=0.0748, 
[2025-09-11 23:55:23,343][__main__][INFO] - Train, Round 283: loss=0.0014, accuracy=1.0000, gradient_norm=0.0732, 
[2025-09-11 23:55:23,343][__main__][INFO] - Train, Round 284: loss=0.0014, accuracy=1.0000, gradient_norm=0.0739, 
[2025-09-11 23:55:23,343][__main__][INFO] - Train, Round 285: loss=0.0014, accuracy=1.0000, gradient_norm=0.0692, 
[2025-09-11 23:55:23,343][__main__][INFO] - Train, Round 286: loss=0.0014, accuracy=1.0000, gradient_norm=0.0707, 
[2025-09-11 23:55:23,343][__main__][INFO] - Train, Round 287: loss=0.0014, accuracy=1.0000, gradient_norm=0.0709, 
[2025-09-11 23:55:23,343][__main__][INFO] - Train, Round 288: loss=0.0014, accuracy=1.0000, gradient_norm=0.0707, 
[2025-09-11 23:55:23,343][__main__][INFO] - Train, Round 289: loss=0.0014, accuracy=1.0000, gradient_norm=0.0721, 
[2025-09-11 23:55:23,343][__main__][INFO] - Train, Round 290: loss=0.0014, accuracy=1.0000, gradient_norm=0.0740, 
[2025-09-11 23:55:23,343][__main__][INFO] - Train, Round 291: loss=0.0014, accuracy=1.0000, gradient_norm=0.0737, 
[2025-09-11 23:55:23,343][__main__][INFO] - Train, Round 292: loss=0.0014, accuracy=1.0000, gradient_norm=0.0690, 
[2025-09-11 23:55:23,343][__main__][INFO] - Train, Round 293: loss=0.0014, accuracy=1.0000, gradient_norm=0.0681, 
[2025-09-11 23:55:23,343][__main__][INFO] - Train, Round 294: loss=0.0014, accuracy=1.0000, gradient_norm=0.0660, 
[2025-09-11 23:55:23,343][__main__][INFO] - Train, Round 295: loss=0.0013, accuracy=1.0000, gradient_norm=0.0684, 
[2025-09-11 23:55:23,343][__main__][INFO] - Train, Round 296: loss=0.0013, accuracy=1.0000, gradient_norm=0.0693, 
[2025-09-11 23:55:23,343][__main__][INFO] - Train, Round 297: loss=0.0013, accuracy=1.0000, gradient_norm=0.0693, 
[2025-09-11 23:55:23,343][__main__][INFO] - Train, Round 298: loss=0.0013, accuracy=1.0000, gradient_norm=0.0660, 
[2025-09-11 23:55:23,343][__main__][INFO] - Train, Round 299: loss=0.0013, accuracy=1.0000, gradient_norm=0.0667, 
[2025-09-11 23:55:23,343][__main__][INFO] - Train, Round 300: loss=0.0013, accuracy=1.0000, gradient_norm=0.0682, 
[2025-09-11 23:55:23,343][__main__][INFO] - Train, Round 301: loss=0.0013, accuracy=1.0000, gradient_norm=0.0676, 
[2025-09-11 23:55:23,343][__main__][INFO] - Train, Round 302: loss=0.0013, accuracy=1.0000, gradient_norm=0.0657, 
[2025-09-11 23:55:23,343][__main__][INFO] - Train, Round 303: loss=0.0013, accuracy=1.0000, gradient_norm=0.0646, 
[2025-09-11 23:55:23,343][__main__][INFO] - Train, Round 304: loss=0.0013, accuracy=1.0000, gradient_norm=0.0659, 
[2025-09-11 23:55:23,343][__main__][INFO] - Train, Round 305: loss=0.0013, accuracy=1.0000, gradient_norm=0.0673, 
[2025-09-11 23:55:23,343][__main__][INFO] - Train, Round 306: loss=0.0013, accuracy=1.0000, gradient_norm=0.0674, 
[2025-09-11 23:55:23,343][__main__][INFO] - Train, Round 307: loss=0.0013, accuracy=1.0000, gradient_norm=0.0666, 
[2025-09-11 23:55:23,343][__main__][INFO] - Train, Round 308: loss=0.0013, accuracy=1.0000, gradient_norm=0.0652, 
[2025-09-11 23:55:23,343][__main__][INFO] - Train, Round 309: loss=0.0013, accuracy=1.0000, gradient_norm=0.0641, 
[2025-09-11 23:55:23,343][__main__][INFO] - Train, Round 310: loss=0.0013, accuracy=1.0000, gradient_norm=0.0633, 
[2025-09-11 23:55:23,343][__main__][INFO] - Train, Round 311: loss=0.0013, accuracy=1.0000, gradient_norm=0.0662, 
[2025-09-11 23:55:23,343][__main__][INFO] - Train, Round 312: loss=0.0012, accuracy=1.0000, gradient_norm=0.0648, 
[2025-09-11 23:55:23,343][__main__][INFO] - Train, Round 313: loss=0.0012, accuracy=1.0000, gradient_norm=0.0628, 
[2025-09-11 23:55:23,343][__main__][INFO] - Train, Round 314: loss=0.0012, accuracy=1.0000, gradient_norm=0.0642, 
[2025-09-11 23:55:23,343][__main__][INFO] - Train, Round 315: loss=0.0012, accuracy=1.0000, gradient_norm=0.0634, 
[2025-09-11 23:55:23,343][__main__][INFO] - Train, Round 316: loss=0.0012, accuracy=1.0000, gradient_norm=0.0628, 
[2025-09-11 23:55:23,343][__main__][INFO] - Train, Round 317: loss=0.0012, accuracy=1.0000, gradient_norm=0.0613, 
[2025-09-11 23:55:23,343][__main__][INFO] - Train, Round 318: loss=0.0012, accuracy=1.0000, gradient_norm=0.0649, 
[2025-09-11 23:55:23,343][__main__][INFO] - Train, Round 319: loss=0.0012, accuracy=1.0000, gradient_norm=0.0646, 
[2025-09-11 23:55:23,343][__main__][INFO] - Train, Round 320: loss=0.0012, accuracy=1.0000, gradient_norm=0.0637, 
[2025-09-11 23:55:23,343][__main__][INFO] - Train, Round 321: loss=0.0012, accuracy=1.0000, gradient_norm=0.0628, 
[2025-09-11 23:55:23,343][__main__][INFO] - Train, Round 322: loss=0.0012, accuracy=1.0000, gradient_norm=0.0611, 
[2025-09-11 23:55:23,343][__main__][INFO] - Train, Round 323: loss=0.0012, accuracy=1.0000, gradient_norm=0.0617, 
[2025-09-11 23:55:23,343][__main__][INFO] - Train, Round 324: loss=0.0012, accuracy=1.0000, gradient_norm=0.0618, 
[2025-09-11 23:55:23,343][__main__][INFO] - Train, Round 325: loss=0.0012, accuracy=1.0000, gradient_norm=0.0613, 
[2025-09-11 23:55:23,343][__main__][INFO] - Train, Round 326: loss=0.0012, accuracy=1.0000, gradient_norm=0.0614, 
[2025-09-11 23:55:23,343][__main__][INFO] - Train, Round 327: loss=0.0012, accuracy=1.0000, gradient_norm=0.0627, 
[2025-09-11 23:55:23,343][__main__][INFO] - Train, Round 328: loss=0.0012, accuracy=1.0000, gradient_norm=0.0604, 
[2025-09-11 23:55:23,343][__main__][INFO] - Train, Round 329: loss=0.0012, accuracy=1.0000, gradient_norm=0.0634, 
[2025-09-11 23:55:23,343][__main__][INFO] - Train, Round 330: loss=0.0012, accuracy=1.0000, gradient_norm=0.0612, 
[2025-09-11 23:55:23,343][__main__][INFO] - Train, Round 331: loss=0.0012, accuracy=1.0000, gradient_norm=0.0606, 
[2025-09-11 23:55:23,343][__main__][INFO] - Train, Round 332: loss=0.0012, accuracy=1.0000, gradient_norm=0.0624, 
[2025-09-11 23:55:23,343][__main__][INFO] - Train, Round 333: loss=0.0012, accuracy=1.0000, gradient_norm=0.0621, 
[2025-09-11 23:55:23,343][__main__][INFO] - Train, Round 334: loss=0.0011, accuracy=1.0000, gradient_norm=0.0615, 
[2025-09-11 23:55:23,343][__main__][INFO] - Train, Round 335: loss=0.0011, accuracy=1.0000, gradient_norm=0.0589, 
[2025-09-11 23:55:23,343][__main__][INFO] - Train, Round 336: loss=0.0011, accuracy=1.0000, gradient_norm=0.0611, 
[2025-09-11 23:55:23,343][__main__][INFO] - Train, Round 337: loss=0.0011, accuracy=1.0000, gradient_norm=0.0596, 
[2025-09-11 23:55:23,343][__main__][INFO] - Train, Round 338: loss=0.0011, accuracy=1.0000, gradient_norm=0.0596, 
[2025-09-11 23:55:23,344][__main__][INFO] - Train, Round 339: loss=0.0011, accuracy=1.0000, gradient_norm=0.0575, 
[2025-09-11 23:55:23,344][__main__][INFO] - Train, Round 340: loss=0.0011, accuracy=1.0000, gradient_norm=0.0574, 
[2025-09-11 23:55:23,344][__main__][INFO] - Train, Round 341: loss=0.0011, accuracy=1.0000, gradient_norm=0.0594, 
[2025-09-11 23:55:23,344][__main__][INFO] - Train, Round 342: loss=0.0011, accuracy=1.0000, gradient_norm=0.0578, 
[2025-09-11 23:55:23,344][__main__][INFO] - Train, Round 343: loss=0.0011, accuracy=1.0000, gradient_norm=0.0599, 
[2025-09-11 23:55:23,344][__main__][INFO] - Train, Round 344: loss=0.0011, accuracy=1.0000, gradient_norm=0.0572, 
[2025-09-11 23:55:23,344][__main__][INFO] - Train, Round 345: loss=0.0011, accuracy=1.0000, gradient_norm=0.0566, 
[2025-09-11 23:55:23,344][__main__][INFO] - Train, Round 346: loss=0.0011, accuracy=1.0000, gradient_norm=0.0599, 
[2025-09-11 23:55:23,344][__main__][INFO] - Train, Round 347: loss=0.0011, accuracy=1.0000, gradient_norm=0.0572, 
[2025-09-11 23:55:23,344][__main__][INFO] - Train, Round 348: loss=0.0011, accuracy=1.0000, gradient_norm=0.0593, 
[2025-09-11 23:55:23,344][__main__][INFO] - Train, Round 349: loss=0.0011, accuracy=1.0000, gradient_norm=0.0566, 
[2025-09-11 23:55:23,344][__main__][INFO] - Train, Round 350: loss=0.0011, accuracy=1.0000, gradient_norm=0.0584, 
[2025-09-11 23:55:23,344][__main__][INFO] - Train, Round 351: loss=0.0011, accuracy=1.0000, gradient_norm=0.0553, 
[2025-09-11 23:55:23,344][__main__][INFO] - Train, Round 352: loss=0.0011, accuracy=1.0000, gradient_norm=0.0575, 
[2025-09-11 23:55:23,344][__main__][INFO] - Train, Round 353: loss=0.0011, accuracy=1.0000, gradient_norm=0.0576, 
[2025-09-11 23:55:23,344][__main__][INFO] - Train, Round 354: loss=0.0011, accuracy=1.0000, gradient_norm=0.0552, 
[2025-09-11 23:55:23,344][__main__][INFO] - Train, Round 355: loss=0.0011, accuracy=1.0000, gradient_norm=0.0568, 
[2025-09-11 23:55:23,344][__main__][INFO] - Train, Round 356: loss=0.0011, accuracy=1.0000, gradient_norm=0.0550, 
[2025-09-11 23:55:23,344][__main__][INFO] - Train, Round 357: loss=0.0011, accuracy=1.0000, gradient_norm=0.0570, 
[2025-09-11 23:55:23,344][__main__][INFO] - Train, Round 358: loss=0.0011, accuracy=1.0000, gradient_norm=0.0556, 
[2025-09-11 23:55:23,344][__main__][INFO] - Train, Round 359: loss=0.0011, accuracy=1.0000, gradient_norm=0.0561, 
[2025-09-11 23:55:23,344][__main__][INFO] - Train, Round 360: loss=0.0011, accuracy=1.0000, gradient_norm=0.0547, 
[2025-09-11 23:55:23,344][__main__][INFO] - Train, Round 361: loss=0.0011, accuracy=1.0000, gradient_norm=0.0564, 
[2025-09-11 23:55:23,344][__main__][INFO] - Train, Round 362: loss=0.0011, accuracy=1.0000, gradient_norm=0.0542, 
[2025-09-11 23:55:23,344][__main__][INFO] - Train, Round 363: loss=0.0010, accuracy=1.0000, gradient_norm=0.0559, 
[2025-09-11 23:55:23,344][__main__][INFO] - Train, Round 364: loss=0.0010, accuracy=1.0000, gradient_norm=0.0551, 
[2025-09-11 23:55:23,344][__main__][INFO] - Train, Round 365: loss=0.0010, accuracy=1.0000, gradient_norm=0.0532, 
[2025-09-11 23:55:23,344][__main__][INFO] - Train, Round 366: loss=0.0010, accuracy=1.0000, gradient_norm=0.0550, 
[2025-09-11 23:55:23,344][__main__][INFO] - Train, Round 367: loss=0.0010, accuracy=1.0000, gradient_norm=0.0542, 
[2025-09-11 23:55:23,344][__main__][INFO] - Train, Round 368: loss=0.0010, accuracy=1.0000, gradient_norm=0.0545, 
[2025-09-11 23:55:23,344][__main__][INFO] - Train, Round 369: loss=0.0010, accuracy=1.0000, gradient_norm=0.0559, 
[2025-09-11 23:55:23,344][__main__][INFO] - Train, Round 370: loss=0.0010, accuracy=1.0000, gradient_norm=0.0525, 
[2025-09-11 23:55:23,344][__main__][INFO] - Train, Round 371: loss=0.0010, accuracy=1.0000, gradient_norm=0.0525, 
[2025-09-11 23:55:23,344][__main__][INFO] - Train, Round 372: loss=0.0010, accuracy=1.0000, gradient_norm=0.0526, 
[2025-09-11 23:55:23,344][__main__][INFO] - Train, Round 373: loss=0.0010, accuracy=1.0000, gradient_norm=0.0538, 
[2025-09-11 23:55:23,344][__main__][INFO] - Train, Round 374: loss=0.0010, accuracy=1.0000, gradient_norm=0.0531, 
[2025-09-11 23:55:23,344][__main__][INFO] - Train, Round 375: loss=0.0010, accuracy=1.0000, gradient_norm=0.0536, 
[2025-09-11 23:55:23,344][__main__][INFO] - Train, Round 376: loss=0.0010, accuracy=1.0000, gradient_norm=0.0547, 
[2025-09-11 23:55:23,344][__main__][INFO] - Train, Round 377: loss=0.0010, accuracy=1.0000, gradient_norm=0.0545, 
[2025-09-11 23:55:23,344][__main__][INFO] - Train, Round 378: loss=0.0010, accuracy=1.0000, gradient_norm=0.0527, 
[2025-09-11 23:55:23,344][__main__][INFO] - Train, Round 379: loss=0.0010, accuracy=1.0000, gradient_norm=0.0530, 
[2025-09-11 23:55:23,344][__main__][INFO] - Train, Round 380: loss=0.0010, accuracy=1.0000, gradient_norm=0.0512, 
[2025-09-11 23:55:23,344][__main__][INFO] - Train, Round 381: loss=0.0010, accuracy=1.0000, gradient_norm=0.0528, 
[2025-09-11 23:55:23,344][__main__][INFO] - Train, Round 382: loss=0.0010, accuracy=1.0000, gradient_norm=0.0538, 
[2025-09-11 23:55:23,344][__main__][INFO] - Train, Round 383: loss=0.0010, accuracy=1.0000, gradient_norm=0.0517, 
[2025-09-11 23:55:23,344][__main__][INFO] - Train, Round 384: loss=0.0010, accuracy=1.0000, gradient_norm=0.0536, 
[2025-09-11 23:55:23,344][__main__][INFO] - Train, Round 385: loss=0.0010, accuracy=1.0000, gradient_norm=0.0492, 
[2025-09-11 23:55:23,344][__main__][INFO] - Train, Round 386: loss=0.0010, accuracy=1.0000, gradient_norm=0.0538, 
[2025-09-11 23:55:23,344][__main__][INFO] - Train, Round 387: loss=0.0010, accuracy=1.0000, gradient_norm=0.0521, 
[2025-09-11 23:55:23,344][__main__][INFO] - Train, Round 388: loss=0.0010, accuracy=1.0000, gradient_norm=0.0510, 
[2025-09-11 23:55:23,344][__main__][INFO] - Train, Round 389: loss=0.0010, accuracy=1.0000, gradient_norm=0.0496, 
[2025-09-11 23:55:23,344][__main__][INFO] - Train, Round 390: loss=0.0010, accuracy=1.0000, gradient_norm=0.0511, 
[2025-09-11 23:55:23,344][__main__][INFO] - Train, Round 391: loss=0.0010, accuracy=1.0000, gradient_norm=0.0501, 
[2025-09-11 23:55:23,344][__main__][INFO] - Train, Round 392: loss=0.0010, accuracy=1.0000, gradient_norm=0.0530, 
[2025-09-11 23:55:23,344][__main__][INFO] - Train, Round 393: loss=0.0010, accuracy=1.0000, gradient_norm=0.0517, 
[2025-09-11 23:55:23,344][__main__][INFO] - Train, Round 394: loss=0.0010, accuracy=1.0000, gradient_norm=0.0517, 
[2025-09-11 23:55:23,344][__main__][INFO] - Train, Round 395: loss=0.0010, accuracy=1.0000, gradient_norm=0.0503, 
[2025-09-11 23:55:23,344][__main__][INFO] - Train, Round 396: loss=0.0010, accuracy=1.0000, gradient_norm=0.0497, 
[2025-09-11 23:55:23,344][__main__][INFO] - Train, Round 397: loss=0.0010, accuracy=1.0000, gradient_norm=0.0496, 
[2025-09-11 23:55:23,344][__main__][INFO] - Train, Round 398: loss=0.0010, accuracy=1.0000, gradient_norm=0.0488, 
[2025-09-11 23:55:23,344][__main__][INFO] - Train, Round 399: loss=0.0010, accuracy=1.0000, gradient_norm=0.0478, 
[2025-09-11 23:55:23,344][__main__][INFO] - Train, Round 400: loss=0.0010, accuracy=1.0000, gradient_norm=0.0511, 
[2025-09-11 23:55:23,344][__main__][INFO] - Train, Round 401: loss=0.0010, accuracy=1.0000, gradient_norm=0.0511, 
[2025-09-11 23:55:23,344][__main__][INFO] - Train, Round 402: loss=0.0009, accuracy=1.0000, gradient_norm=0.0481, 
[2025-09-11 23:55:23,344][__main__][INFO] - Train, Round 403: loss=0.0009, accuracy=1.0000, gradient_norm=0.0488, 
[2025-09-11 23:55:23,344][__main__][INFO] - Train, Round 404: loss=0.0009, accuracy=1.0000, gradient_norm=0.0496, 
[2025-09-11 23:55:23,344][__main__][INFO] - Train, Round 405: loss=0.0009, accuracy=1.0000, gradient_norm=0.0488, 
[2025-09-11 23:55:23,344][__main__][INFO] - Train, Round 406: loss=0.0009, accuracy=1.0000, gradient_norm=0.0499, 
[2025-09-11 23:55:23,344][__main__][INFO] - Train, Round 407: loss=0.0009, accuracy=1.0000, gradient_norm=0.0493, 
[2025-09-11 23:55:23,344][__main__][INFO] - Train, Round 408: loss=0.0009, accuracy=1.0000, gradient_norm=0.0499, 
[2025-09-11 23:55:23,344][__main__][INFO] - Train, Round 409: loss=0.0009, accuracy=1.0000, gradient_norm=0.0502, 
[2025-09-11 23:55:23,345][__main__][INFO] - Train, Round 410: loss=0.0009, accuracy=1.0000, gradient_norm=0.0466, 
[2025-09-11 23:55:23,345][__main__][INFO] - Train, Round 411: loss=0.0009, accuracy=1.0000, gradient_norm=0.0498, 
[2025-09-11 23:55:23,345][__main__][INFO] - Train, Round 412: loss=0.0009, accuracy=1.0000, gradient_norm=0.0480, 
[2025-09-11 23:55:23,345][__main__][INFO] - Train, Round 413: loss=0.0009, accuracy=1.0000, gradient_norm=0.0486, 
[2025-09-11 23:55:23,345][__main__][INFO] - Train, Round 414: loss=0.0009, accuracy=1.0000, gradient_norm=0.0501, 
[2025-09-11 23:55:23,345][__main__][INFO] - Train, Round 415: loss=0.0009, accuracy=1.0000, gradient_norm=0.0498, 
[2025-09-11 23:55:23,345][__main__][INFO] - Train, Round 416: loss=0.0009, accuracy=1.0000, gradient_norm=0.0487, 
[2025-09-11 23:55:23,345][__main__][INFO] - Train, Round 417: loss=0.0009, accuracy=1.0000, gradient_norm=0.0496, 
[2025-09-11 23:55:23,345][__main__][INFO] - Train, Round 418: loss=0.0009, accuracy=1.0000, gradient_norm=0.0486, 
[2025-09-11 23:55:23,345][__main__][INFO] - Train, Round 419: loss=0.0009, accuracy=1.0000, gradient_norm=0.0489, 
[2025-09-11 23:55:23,345][__main__][INFO] - Train, Round 420: loss=0.0009, accuracy=1.0000, gradient_norm=0.0485, 
[2025-09-11 23:55:23,345][__main__][INFO] - Train, Round 421: loss=0.0009, accuracy=1.0000, gradient_norm=0.0483, 
[2025-09-11 23:55:23,345][__main__][INFO] - Train, Round 422: loss=0.0009, accuracy=1.0000, gradient_norm=0.0468, 
[2025-09-11 23:55:23,345][__main__][INFO] - Train, Round 423: loss=0.0009, accuracy=1.0000, gradient_norm=0.0483, 
[2025-09-11 23:55:23,345][__main__][INFO] - Train, Round 424: loss=0.0009, accuracy=1.0000, gradient_norm=0.0489, 
[2025-09-11 23:55:23,345][__main__][INFO] - Train, Round 425: loss=0.0009, accuracy=1.0000, gradient_norm=0.0473, 
[2025-09-11 23:55:23,345][__main__][INFO] - Train, Round 426: loss=0.0009, accuracy=1.0000, gradient_norm=0.0474, 
[2025-09-11 23:55:23,345][__main__][INFO] - Train, Round 427: loss=0.0009, accuracy=1.0000, gradient_norm=0.0466, 
[2025-09-11 23:55:23,345][__main__][INFO] - Train, Round 428: loss=0.0009, accuracy=1.0000, gradient_norm=0.0472, 
[2025-09-11 23:55:23,345][__main__][INFO] - Train, Round 429: loss=0.0009, accuracy=1.0000, gradient_norm=0.0482, 
[2025-09-11 23:55:23,345][__main__][INFO] - Train, Round 430: loss=0.0009, accuracy=1.0000, gradient_norm=0.0483, 
[2025-09-11 23:55:23,345][__main__][INFO] - Train, Round 431: loss=0.0009, accuracy=1.0000, gradient_norm=0.0462, 
[2025-09-11 23:55:23,345][__main__][INFO] - Train, Round 432: loss=0.0009, accuracy=1.0000, gradient_norm=0.0477, 
[2025-09-11 23:55:23,345][__main__][INFO] - Train, Round 433: loss=0.0009, accuracy=1.0000, gradient_norm=0.0470, 
[2025-09-11 23:55:23,345][__main__][INFO] - Train, Round 434: loss=0.0009, accuracy=1.0000, gradient_norm=0.0469, 
[2025-09-11 23:55:23,345][__main__][INFO] - Train, Round 435: loss=0.0009, accuracy=1.0000, gradient_norm=0.0475, 
[2025-09-11 23:55:23,345][__main__][INFO] - Train, Round 436: loss=0.0009, accuracy=1.0000, gradient_norm=0.0474, 
[2025-09-11 23:55:23,345][__main__][INFO] - Train, Round 437: loss=0.0009, accuracy=1.0000, gradient_norm=0.0465, 
[2025-09-11 23:55:23,345][__main__][INFO] - Train, Round 438: loss=0.0009, accuracy=1.0000, gradient_norm=0.0474, 
[2025-09-11 23:55:23,345][__main__][INFO] - Train, Round 439: loss=0.0009, accuracy=1.0000, gradient_norm=0.0451, 
[2025-09-11 23:55:23,345][__main__][INFO] - Train, Round 440: loss=0.0009, accuracy=1.0000, gradient_norm=0.0472, 
[2025-09-11 23:55:23,345][__main__][INFO] - Train, Round 441: loss=0.0009, accuracy=1.0000, gradient_norm=0.0461, 
[2025-09-11 23:55:23,345][__main__][INFO] - Train, Round 442: loss=0.0009, accuracy=1.0000, gradient_norm=0.0453, 
[2025-09-11 23:55:23,345][__main__][INFO] - Train, Round 443: loss=0.0009, accuracy=1.0000, gradient_norm=0.0454, 
[2025-09-11 23:55:23,345][__main__][INFO] - Train, Round 444: loss=0.0009, accuracy=1.0000, gradient_norm=0.0470, 
[2025-09-11 23:55:23,345][__main__][INFO] - Train, Round 445: loss=0.0009, accuracy=1.0000, gradient_norm=0.0456, 
[2025-09-11 23:55:23,345][__main__][INFO] - Train, Round 446: loss=0.0009, accuracy=1.0000, gradient_norm=0.0456, 
[2025-09-11 23:55:23,345][__main__][INFO] - Train, Round 447: loss=0.0009, accuracy=1.0000, gradient_norm=0.0486, 
[2025-09-11 23:55:23,345][__main__][INFO] - Train, Round 448: loss=0.0009, accuracy=1.0000, gradient_norm=0.0452, 
[2025-09-11 23:55:23,345][__main__][INFO] - Train, Round 449: loss=0.0009, accuracy=1.0000, gradient_norm=0.0454, 
[2025-09-11 23:55:23,345][__main__][INFO] - Train, Round 450: loss=0.0009, accuracy=1.0000, gradient_norm=0.0450, 
[2025-09-11 23:55:23,345][__main__][INFO] - Train, Round 451: loss=0.0009, accuracy=1.0000, gradient_norm=0.0434, 
[2025-09-11 23:55:23,345][__main__][INFO] - Train, Round 452: loss=0.0009, accuracy=1.0000, gradient_norm=0.0459, 
[2025-09-11 23:55:23,345][__main__][INFO] - Train, Round 453: loss=0.0009, accuracy=1.0000, gradient_norm=0.0453, 
[2025-09-11 23:55:23,345][__main__][INFO] - Train, Round 454: loss=0.0009, accuracy=1.0000, gradient_norm=0.0444, 
[2025-09-11 23:55:23,345][__main__][INFO] - Train, Round 455: loss=0.0009, accuracy=1.0000, gradient_norm=0.0433, 
[2025-09-11 23:55:23,345][__main__][INFO] - Train, Round 456: loss=0.0009, accuracy=1.0000, gradient_norm=0.0451, 
[2025-09-11 23:55:23,345][__main__][INFO] - Train, Round 457: loss=0.0009, accuracy=1.0000, gradient_norm=0.0457, 
[2025-09-11 23:55:23,345][__main__][INFO] - Train, Round 458: loss=0.0009, accuracy=1.0000, gradient_norm=0.0456, 
[2025-09-11 23:55:23,345][__main__][INFO] - Train, Round 459: loss=0.0008, accuracy=1.0000, gradient_norm=0.0450, 
[2025-09-11 23:55:23,345][__main__][INFO] - Train, Round 460: loss=0.0008, accuracy=1.0000, gradient_norm=0.0452, 
[2025-09-11 23:55:23,345][__main__][INFO] - Train, Round 461: loss=0.0008, accuracy=1.0000, gradient_norm=0.0440, 
[2025-09-11 23:55:23,345][__main__][INFO] - Train, Round 462: loss=0.0008, accuracy=1.0000, gradient_norm=0.0444, 
[2025-09-11 23:55:23,345][__main__][INFO] - Train, Round 463: loss=0.0008, accuracy=1.0000, gradient_norm=0.0447, 
[2025-09-11 23:55:23,345][__main__][INFO] - Train, Round 464: loss=0.0008, accuracy=1.0000, gradient_norm=0.0455, 
[2025-09-11 23:55:23,345][__main__][INFO] - Train, Round 465: loss=0.0008, accuracy=1.0000, gradient_norm=0.0437, 
[2025-09-11 23:55:23,345][__main__][INFO] - Train, Round 466: loss=0.0008, accuracy=1.0000, gradient_norm=0.0442, 
[2025-09-11 23:55:23,345][__main__][INFO] - Train, Round 467: loss=0.0008, accuracy=1.0000, gradient_norm=0.0456, 
[2025-09-11 23:55:23,345][__main__][INFO] - Train, Round 468: loss=0.0008, accuracy=1.0000, gradient_norm=0.0442, 
[2025-09-11 23:55:23,345][__main__][INFO] - Train, Round 469: loss=0.0008, accuracy=1.0000, gradient_norm=0.0434, 
[2025-09-11 23:55:23,345][__main__][INFO] - Train, Round 470: loss=0.0008, accuracy=1.0000, gradient_norm=0.0436, 
[2025-09-11 23:55:23,345][__main__][INFO] - Train, Round 471: loss=0.0008, accuracy=1.0000, gradient_norm=0.0432, 
[2025-09-11 23:55:23,345][__main__][INFO] - Train, Round 472: loss=0.0008, accuracy=1.0000, gradient_norm=0.0438, 
[2025-09-11 23:55:23,345][__main__][INFO] - Train, Round 473: loss=0.0008, accuracy=1.0000, gradient_norm=0.0438, 
[2025-09-11 23:55:23,345][__main__][INFO] - Train, Round 474: loss=0.0008, accuracy=1.0000, gradient_norm=0.0445, 
[2025-09-11 23:55:23,345][__main__][INFO] - Train, Round 475: loss=0.0008, accuracy=1.0000, gradient_norm=0.0430, 
[2025-09-11 23:55:23,345][__main__][INFO] - Train, Round 476: loss=0.0008, accuracy=1.0000, gradient_norm=0.0426, 
[2025-09-11 23:55:23,345][__main__][INFO] - Train, Round 477: loss=0.0008, accuracy=1.0000, gradient_norm=0.0442, 
[2025-09-11 23:55:23,345][__main__][INFO] - Train, Round 478: loss=0.0008, accuracy=1.0000, gradient_norm=0.0434, 
[2025-09-11 23:55:23,345][__main__][INFO] - Train, Round 479: loss=0.0008, accuracy=1.0000, gradient_norm=0.0456, 
[2025-09-11 23:55:23,345][__main__][INFO] - Train, Round 480: loss=0.0008, accuracy=1.0000, gradient_norm=0.0437, 
[2025-09-11 23:55:23,345][__main__][INFO] - Train, Round 481: loss=0.0008, accuracy=1.0000, gradient_norm=0.0425, 
[2025-09-11 23:55:23,346][__main__][INFO] - Train, Round 482: loss=0.0008, accuracy=1.0000, gradient_norm=0.0438, 
[2025-09-11 23:55:23,346][__main__][INFO] - Train, Round 483: loss=0.0008, accuracy=1.0000, gradient_norm=0.0448, 
[2025-09-11 23:55:23,346][__main__][INFO] - Train, Round 484: loss=0.0008, accuracy=1.0000, gradient_norm=0.0451, 
[2025-09-11 23:55:23,346][__main__][INFO] - Train, Round 485: loss=0.0008, accuracy=1.0000, gradient_norm=0.0437, 
[2025-09-11 23:55:23,346][__main__][INFO] - Train, Round 486: loss=0.0008, accuracy=1.0000, gradient_norm=0.0460, 
[2025-09-11 23:55:23,346][__main__][INFO] - Train, Round 487: loss=0.0008, accuracy=1.0000, gradient_norm=0.0432, 
[2025-09-11 23:55:23,346][__main__][INFO] - Train, Round 488: loss=0.0008, accuracy=1.0000, gradient_norm=0.0422, 
[2025-09-11 23:55:23,346][__main__][INFO] - Train, Round 489: loss=0.0008, accuracy=1.0000, gradient_norm=0.0429, 
[2025-09-11 23:55:23,346][__main__][INFO] - Train, Round 490: loss=0.0008, accuracy=1.0000, gradient_norm=0.0434, 
[2025-09-11 23:55:23,346][__main__][INFO] - Train, Round 491: loss=0.0008, accuracy=1.0000, gradient_norm=0.0436, 
[2025-09-11 23:55:23,346][__main__][INFO] - Train, Round 492: loss=0.0008, accuracy=1.0000, gradient_norm=0.0431, 
[2025-09-11 23:55:23,346][__main__][INFO] - Train, Round 493: loss=0.0008, accuracy=1.0000, gradient_norm=0.0440, 
[2025-09-11 23:55:23,346][__main__][INFO] - Train, Round 494: loss=0.0008, accuracy=1.0000, gradient_norm=0.0425, 
[2025-09-11 23:55:23,346][__main__][INFO] - Train, Round 495: loss=0.0008, accuracy=1.0000, gradient_norm=0.0426, 
[2025-09-11 23:55:23,346][__main__][INFO] - Train, Round 496: loss=0.0008, accuracy=1.0000, gradient_norm=0.0418, 
[2025-09-11 23:55:23,346][__main__][INFO] - Train, Round 497: loss=0.0008, accuracy=1.0000, gradient_norm=0.0425, 
[2025-09-11 23:55:23,346][__main__][INFO] - Train, Round 498: loss=0.0008, accuracy=1.0000, gradient_norm=0.0438, 
[2025-09-11 23:55:23,346][__main__][INFO] - Train, Round 499: loss=0.0008, accuracy=1.0000, gradient_norm=0.0417, 
[2025-09-11 23:55:23,346][__main__][INFO] - Train, Round 500: loss=0.0008, accuracy=1.0000, gradient_norm=0.0434, 
[2025-09-11 23:55:23,346][__main__][INFO] - Test, Round 001: loss=2.3046, accuracy=0.1062, 
[2025-09-11 23:55:23,346][__main__][INFO] - Test, Round 002: loss=2.3035, accuracy=0.1115, 
[2025-09-11 23:55:23,346][__main__][INFO] - Test, Round 003: loss=2.3024, accuracy=0.1168, 
[2025-09-11 23:55:23,346][__main__][INFO] - Test, Round 004: loss=2.3014, accuracy=0.1224, 
[2025-09-11 23:55:23,346][__main__][INFO] - Test, Round 005: loss=2.3004, accuracy=0.1248, 
[2025-09-11 23:55:23,346][__main__][INFO] - Test, Round 006: loss=2.2994, accuracy=0.1282, 
[2025-09-11 23:55:23,346][__main__][INFO] - Test, Round 007: loss=2.2984, accuracy=0.1295, 
[2025-09-11 23:55:23,346][__main__][INFO] - Test, Round 008: loss=2.2974, accuracy=0.1304, 
[2025-09-11 23:55:23,346][__main__][INFO] - Test, Round 009: loss=2.2963, accuracy=0.1301, 
[2025-09-11 23:55:23,346][__main__][INFO] - Test, Round 010: loss=2.2952, accuracy=0.1321, 
[2025-09-11 23:55:23,346][__main__][INFO] - Test, Round 011: loss=2.2939, accuracy=0.1325, 
[2025-09-11 23:55:23,346][__main__][INFO] - Test, Round 012: loss=2.2925, accuracy=0.1366, 
[2025-09-11 23:55:23,346][__main__][INFO] - Test, Round 013: loss=2.2910, accuracy=0.1387, 
[2025-09-11 23:55:23,346][__main__][INFO] - Test, Round 014: loss=2.2892, accuracy=0.1405, 
[2025-09-11 23:55:23,346][__main__][INFO] - Test, Round 015: loss=2.2872, accuracy=0.1423, 
[2025-09-11 23:55:23,346][__main__][INFO] - Test, Round 016: loss=2.2848, accuracy=0.1450, 
[2025-09-11 23:55:23,346][__main__][INFO] - Test, Round 017: loss=2.2818, accuracy=0.1483, 
[2025-09-11 23:55:23,346][__main__][INFO] - Test, Round 018: loss=2.2783, accuracy=0.1514, 
[2025-09-11 23:55:23,346][__main__][INFO] - Test, Round 019: loss=2.2739, accuracy=0.1553, 
[2025-09-11 23:55:23,346][__main__][INFO] - Test, Round 020: loss=2.2685, accuracy=0.1602, 
[2025-09-11 23:55:23,346][__main__][INFO] - Test, Round 021: loss=2.2617, accuracy=0.1636, 
[2025-09-11 23:55:23,346][__main__][INFO] - Test, Round 022: loss=2.2532, accuracy=0.1677, 
[2025-09-11 23:55:23,346][__main__][INFO] - Test, Round 023: loss=2.2425, accuracy=0.1709, 
[2025-09-11 23:55:23,346][__main__][INFO] - Test, Round 024: loss=2.2293, accuracy=0.1757, 
[2025-09-11 23:55:23,346][__main__][INFO] - Test, Round 025: loss=2.2135, accuracy=0.1828, 
[2025-09-11 23:55:23,346][__main__][INFO] - Test, Round 026: loss=2.1957, accuracy=0.1902, 
[2025-09-11 23:55:23,346][__main__][INFO] - Test, Round 027: loss=2.1768, accuracy=0.1961, 
[2025-09-11 23:55:23,346][__main__][INFO] - Test, Round 028: loss=2.1580, accuracy=0.2024, 
[2025-09-11 23:55:23,346][__main__][INFO] - Test, Round 029: loss=2.1392, accuracy=0.2141, 
[2025-09-11 23:55:23,346][__main__][INFO] - Test, Round 030: loss=2.1209, accuracy=0.2279, 
[2025-09-11 23:55:23,346][__main__][INFO] - Test, Round 031: loss=2.1033, accuracy=0.2351, 
[2025-09-11 23:55:23,346][__main__][INFO] - Test, Round 032: loss=2.0853, accuracy=0.2502, 
[2025-09-11 23:55:23,346][__main__][INFO] - Test, Round 033: loss=2.0699, accuracy=0.2590, 
[2025-09-11 23:55:23,346][__main__][INFO] - Test, Round 034: loss=2.0507, accuracy=0.2714, 
[2025-09-11 23:55:23,346][__main__][INFO] - Test, Round 035: loss=2.0414, accuracy=0.2753, 
[2025-09-11 23:55:23,346][__main__][INFO] - Test, Round 036: loss=2.0223, accuracy=0.2739, 
[2025-09-11 23:55:23,346][__main__][INFO] - Test, Round 037: loss=2.0166, accuracy=0.2784, 
[2025-09-11 23:55:23,346][__main__][INFO] - Test, Round 038: loss=2.0024, accuracy=0.2829, 
[2025-09-11 23:55:23,346][__main__][INFO] - Test, Round 039: loss=2.0079, accuracy=0.2849, 
[2025-09-11 23:55:23,346][__main__][INFO] - Test, Round 040: loss=1.9814, accuracy=0.2928, 
[2025-09-11 23:55:23,346][__main__][INFO] - Test, Round 041: loss=1.9770, accuracy=0.2970, 
[2025-09-11 23:55:23,346][__main__][INFO] - Test, Round 042: loss=1.9994, accuracy=0.2908, 
[2025-09-11 23:55:23,346][__main__][INFO] - Test, Round 043: loss=1.9810, accuracy=0.2896, 
[2025-09-11 23:55:23,346][__main__][INFO] - Test, Round 044: loss=1.9647, accuracy=0.2971, 
[2025-09-11 23:55:23,346][__main__][INFO] - Test, Round 045: loss=1.9995, accuracy=0.2799, 
[2025-09-11 23:55:23,346][__main__][INFO] - Test, Round 046: loss=1.9715, accuracy=0.2955, 
[2025-09-11 23:55:23,346][__main__][INFO] - Test, Round 047: loss=2.0084, accuracy=0.2961, 
[2025-09-11 23:55:23,346][__main__][INFO] - Test, Round 048: loss=2.0056, accuracy=0.2926, 
[2025-09-11 23:55:23,346][__main__][INFO] - Test, Round 049: loss=1.9797, accuracy=0.2985, 
[2025-09-11 23:55:23,346][__main__][INFO] - Test, Round 050: loss=1.9795, accuracy=0.2966, 
[2025-09-11 23:55:23,346][__main__][INFO] - Test, Round 051: loss=2.0360, accuracy=0.2964, 
[2025-09-11 23:55:23,346][__main__][INFO] - Test, Round 052: loss=1.9512, accuracy=0.3070, 
[2025-09-11 23:55:23,346][__main__][INFO] - Test, Round 053: loss=2.0244, accuracy=0.2875, 
[2025-09-11 23:55:23,346][__main__][INFO] - Test, Round 054: loss=2.0448, accuracy=0.2903, 
[2025-09-11 23:55:23,346][__main__][INFO] - Test, Round 055: loss=2.0192, accuracy=0.2996, 
[2025-09-11 23:55:23,347][__main__][INFO] - Test, Round 056: loss=1.9844, accuracy=0.3026, 
[2025-09-11 23:55:23,347][__main__][INFO] - Test, Round 057: loss=1.9930, accuracy=0.3025, 
[2025-09-11 23:55:23,347][__main__][INFO] - Test, Round 058: loss=2.0580, accuracy=0.2915, 
[2025-09-11 23:55:23,347][__main__][INFO] - Test, Round 059: loss=2.0072, accuracy=0.3105, 
[2025-09-11 23:55:23,347][__main__][INFO] - Test, Round 060: loss=2.1643, accuracy=0.2844, 
[2025-09-11 23:55:23,347][__main__][INFO] - Test, Round 061: loss=2.0543, accuracy=0.2993, 
[2025-09-11 23:55:23,347][__main__][INFO] - Test, Round 062: loss=2.0587, accuracy=0.3090, 
[2025-09-11 23:55:23,347][__main__][INFO] - Test, Round 063: loss=2.0102, accuracy=0.3255, 
[2025-09-11 23:55:23,347][__main__][INFO] - Test, Round 064: loss=2.1217, accuracy=0.2920, 
[2025-09-11 23:55:23,347][__main__][INFO] - Test, Round 065: loss=2.0701, accuracy=0.3061, 
[2025-09-11 23:55:23,347][__main__][INFO] - Test, Round 066: loss=2.1545, accuracy=0.3074, 
[2025-09-11 23:55:23,347][__main__][INFO] - Test, Round 067: loss=2.1999, accuracy=0.2963, 
[2025-09-11 23:55:23,347][__main__][INFO] - Test, Round 068: loss=2.1394, accuracy=0.3216, 
[2025-09-11 23:55:23,347][__main__][INFO] - Test, Round 069: loss=2.2429, accuracy=0.3068, 
[2025-09-11 23:55:23,347][__main__][INFO] - Test, Round 070: loss=2.3134, accuracy=0.3000, 
[2025-09-11 23:55:23,347][__main__][INFO] - Test, Round 071: loss=2.6280, accuracy=0.2588, 
[2025-09-11 23:55:23,347][__main__][INFO] - Test, Round 072: loss=2.3311, accuracy=0.3050, 
[2025-09-11 23:55:23,347][__main__][INFO] - Test, Round 073: loss=2.5186, accuracy=0.2781, 
[2025-09-11 23:55:23,347][__main__][INFO] - Test, Round 074: loss=2.3317, accuracy=0.2951, 
[2025-09-11 23:55:23,347][__main__][INFO] - Test, Round 075: loss=2.3116, accuracy=0.3045, 
[2025-09-11 23:55:23,347][__main__][INFO] - Test, Round 076: loss=2.3589, accuracy=0.3065, 
[2025-09-11 23:55:23,347][__main__][INFO] - Test, Round 077: loss=2.3522, accuracy=0.3084, 
[2025-09-11 23:55:23,347][__main__][INFO] - Test, Round 078: loss=2.6825, accuracy=0.2679, 
[2025-09-11 23:55:23,347][__main__][INFO] - Test, Round 079: loss=2.4281, accuracy=0.3116, 
[2025-09-11 23:55:23,347][__main__][INFO] - Test, Round 080: loss=2.6322, accuracy=0.3030, 
[2025-09-11 23:55:23,347][__main__][INFO] - Test, Round 081: loss=2.5835, accuracy=0.3128, 
[2025-09-11 23:55:23,347][__main__][INFO] - Test, Round 082: loss=2.7941, accuracy=0.2922, 
[2025-09-11 23:55:23,347][__main__][INFO] - Test, Round 083: loss=2.8886, accuracy=0.2760, 
[2025-09-11 23:55:23,347][__main__][INFO] - Test, Round 084: loss=2.6909, accuracy=0.3057, 
[2025-09-11 23:55:23,347][__main__][INFO] - Test, Round 085: loss=3.4406, accuracy=0.2738, 
[2025-09-11 23:55:23,347][__main__][INFO] - Test, Round 086: loss=2.7185, accuracy=0.3189, 
[2025-09-11 23:55:23,347][__main__][INFO] - Test, Round 087: loss=2.9135, accuracy=0.3056, 
[2025-09-11 23:55:23,347][__main__][INFO] - Test, Round 088: loss=3.1222, accuracy=0.2952, 
[2025-09-11 23:55:23,347][__main__][INFO] - Test, Round 089: loss=3.0120, accuracy=0.3064, 
[2025-09-11 23:55:23,347][__main__][INFO] - Test, Round 090: loss=3.1008, accuracy=0.3145, 
[2025-09-11 23:55:23,347][__main__][INFO] - Test, Round 091: loss=3.0791, accuracy=0.3170, 
[2025-09-11 23:55:23,347][__main__][INFO] - Test, Round 092: loss=3.3704, accuracy=0.3043, 
[2025-09-11 23:55:23,347][__main__][INFO] - Test, Round 093: loss=3.2559, accuracy=0.3123, 
[2025-09-11 23:55:23,347][__main__][INFO] - Test, Round 094: loss=3.3701, accuracy=0.3150, 
[2025-09-11 23:55:23,347][__main__][INFO] - Test, Round 095: loss=3.3230, accuracy=0.3261, 
[2025-09-11 23:55:23,347][__main__][INFO] - Test, Round 096: loss=3.6940, accuracy=0.3093, 
[2025-09-11 23:55:23,347][__main__][INFO] - Test, Round 097: loss=3.4496, accuracy=0.3111, 
[2025-09-11 23:55:23,347][__main__][INFO] - Test, Round 098: loss=3.6172, accuracy=0.3063, 
[2025-09-11 23:55:23,347][__main__][INFO] - Test, Round 099: loss=3.6444, accuracy=0.3218, 
[2025-09-11 23:55:23,347][__main__][INFO] - Test, Round 100: loss=3.5324, accuracy=0.3252, 
[2025-09-11 23:55:23,347][__main__][INFO] - Test, Round 101: loss=3.6574, accuracy=0.3245, 
[2025-09-11 23:55:23,347][__main__][INFO] - Test, Round 102: loss=3.9020, accuracy=0.3150, 
[2025-09-11 23:55:23,347][__main__][INFO] - Test, Round 103: loss=3.6693, accuracy=0.3271, 
[2025-09-11 23:55:23,347][__main__][INFO] - Test, Round 104: loss=3.8149, accuracy=0.3248, 
[2025-09-11 23:55:23,347][__main__][INFO] - Test, Round 105: loss=3.9273, accuracy=0.3266, 
[2025-09-11 23:55:23,347][__main__][INFO] - Test, Round 106: loss=3.9426, accuracy=0.3279, 
[2025-09-11 23:55:23,347][__main__][INFO] - Test, Round 107: loss=4.0368, accuracy=0.3287, 
[2025-09-11 23:55:23,347][__main__][INFO] - Test, Round 108: loss=4.0812, accuracy=0.3281, 
[2025-09-11 23:55:23,347][__main__][INFO] - Test, Round 109: loss=4.1704, accuracy=0.3220, 
[2025-09-11 23:55:23,347][__main__][INFO] - Test, Round 110: loss=4.2106, accuracy=0.3259, 
[2025-09-11 23:55:23,347][__main__][INFO] - Test, Round 111: loss=4.2706, accuracy=0.3258, 
[2025-09-11 23:55:23,347][__main__][INFO] - Test, Round 112: loss=4.3413, accuracy=0.3241, 
[2025-09-11 23:55:23,347][__main__][INFO] - Test, Round 113: loss=4.3795, accuracy=0.3229, 
[2025-09-11 23:55:23,347][__main__][INFO] - Test, Round 114: loss=4.4147, accuracy=0.3248, 
[2025-09-11 23:55:23,347][__main__][INFO] - Test, Round 115: loss=4.4673, accuracy=0.3248, 
[2025-09-11 23:55:23,347][__main__][INFO] - Test, Round 116: loss=4.5083, accuracy=0.3252, 
[2025-09-11 23:55:23,347][__main__][INFO] - Test, Round 117: loss=4.5680, accuracy=0.3240, 
[2025-09-11 23:55:23,347][__main__][INFO] - Test, Round 118: loss=4.5957, accuracy=0.3254, 
[2025-09-11 23:55:23,347][__main__][INFO] - Test, Round 119: loss=4.6291, accuracy=0.3250, 
[2025-09-11 23:55:23,347][__main__][INFO] - Test, Round 120: loss=4.6735, accuracy=0.3233, 
[2025-09-11 23:55:23,347][__main__][INFO] - Test, Round 121: loss=4.7196, accuracy=0.3241, 
[2025-09-11 23:55:23,347][__main__][INFO] - Test, Round 122: loss=4.7445, accuracy=0.3253, 
[2025-09-11 23:55:23,347][__main__][INFO] - Test, Round 123: loss=4.7785, accuracy=0.3233, 
[2025-09-11 23:55:23,347][__main__][INFO] - Test, Round 124: loss=4.8125, accuracy=0.3234, 
[2025-09-11 23:55:23,347][__main__][INFO] - Test, Round 125: loss=4.8443, accuracy=0.3247, 
[2025-09-11 23:55:23,347][__main__][INFO] - Test, Round 126: loss=4.8732, accuracy=0.3228, 
[2025-09-11 23:55:23,347][__main__][INFO] - Test, Round 127: loss=4.9088, accuracy=0.3238, 
[2025-09-11 23:55:23,347][__main__][INFO] - Test, Round 128: loss=4.9415, accuracy=0.3231, 
[2025-09-11 23:55:23,347][__main__][INFO] - Test, Round 129: loss=4.9612, accuracy=0.3222, 
[2025-09-11 23:55:23,348][__main__][INFO] - Test, Round 130: loss=4.9915, accuracy=0.3238, 
[2025-09-11 23:55:23,348][__main__][INFO] - Test, Round 131: loss=5.0201, accuracy=0.3233, 
[2025-09-11 23:55:23,348][__main__][INFO] - Test, Round 132: loss=5.0413, accuracy=0.3222, 
[2025-09-11 23:55:23,348][__main__][INFO] - Test, Round 133: loss=5.0669, accuracy=0.3233, 
[2025-09-11 23:55:23,348][__main__][INFO] - Test, Round 134: loss=5.0955, accuracy=0.3242, 
[2025-09-11 23:55:23,348][__main__][INFO] - Test, Round 135: loss=5.1175, accuracy=0.3246, 
[2025-09-11 23:55:23,348][__main__][INFO] - Test, Round 136: loss=5.1449, accuracy=0.3231, 
[2025-09-11 23:55:23,348][__main__][INFO] - Test, Round 137: loss=5.1616, accuracy=0.3239, 
[2025-09-11 23:55:23,348][__main__][INFO] - Test, Round 138: loss=5.1836, accuracy=0.3246, 
[2025-09-11 23:55:23,348][__main__][INFO] - Test, Round 139: loss=5.2057, accuracy=0.3225, 
[2025-09-11 23:55:23,348][__main__][INFO] - Test, Round 140: loss=5.2286, accuracy=0.3239, 
[2025-09-11 23:55:23,348][__main__][INFO] - Test, Round 141: loss=5.2488, accuracy=0.3240, 
[2025-09-11 23:55:23,348][__main__][INFO] - Test, Round 142: loss=5.2682, accuracy=0.3235, 
[2025-09-11 23:55:23,348][__main__][INFO] - Test, Round 143: loss=5.2828, accuracy=0.3235, 
[2025-09-11 23:55:23,348][__main__][INFO] - Test, Round 144: loss=5.3046, accuracy=0.3232, 
[2025-09-11 23:55:23,348][__main__][INFO] - Test, Round 145: loss=5.3209, accuracy=0.3237, 
[2025-09-11 23:55:23,348][__main__][INFO] - Test, Round 146: loss=5.3420, accuracy=0.3235, 
[2025-09-11 23:55:23,348][__main__][INFO] - Test, Round 147: loss=5.3619, accuracy=0.3229, 
[2025-09-11 23:55:23,348][__main__][INFO] - Test, Round 148: loss=5.3764, accuracy=0.3231, 
[2025-09-11 23:55:23,348][__main__][INFO] - Test, Round 149: loss=5.3983, accuracy=0.3227, 
[2025-09-11 23:55:23,348][__main__][INFO] - Test, Round 150: loss=5.4117, accuracy=0.3229, 
[2025-09-11 23:55:23,348][__main__][INFO] - Test, Round 151: loss=5.4283, accuracy=0.3232, 
[2025-09-11 23:55:23,348][__main__][INFO] - Test, Round 152: loss=5.4439, accuracy=0.3228, 
[2025-09-11 23:55:23,348][__main__][INFO] - Test, Round 153: loss=5.4589, accuracy=0.3239, 
[2025-09-11 23:55:23,348][__main__][INFO] - Test, Round 154: loss=5.4755, accuracy=0.3234, 
[2025-09-11 23:55:23,348][__main__][INFO] - Test, Round 155: loss=5.4924, accuracy=0.3244, 
[2025-09-11 23:55:23,348][__main__][INFO] - Test, Round 156: loss=5.5063, accuracy=0.3219, 
[2025-09-11 23:55:23,348][__main__][INFO] - Test, Round 157: loss=5.5229, accuracy=0.3230, 
[2025-09-11 23:55:23,348][__main__][INFO] - Test, Round 158: loss=5.5335, accuracy=0.3226, 
[2025-09-11 23:55:23,348][__main__][INFO] - Test, Round 159: loss=5.5471, accuracy=0.3241, 
[2025-09-11 23:55:23,348][__main__][INFO] - Test, Round 160: loss=5.5622, accuracy=0.3232, 
[2025-09-11 23:55:23,348][__main__][INFO] - Test, Round 161: loss=5.5725, accuracy=0.3235, 
[2025-09-11 23:55:23,348][__main__][INFO] - Test, Round 162: loss=5.5901, accuracy=0.3234, 
[2025-09-11 23:55:23,348][__main__][INFO] - Test, Round 163: loss=5.6008, accuracy=0.3233, 
[2025-09-11 23:55:23,348][__main__][INFO] - Test, Round 164: loss=5.6126, accuracy=0.3233, 
[2025-09-11 23:55:23,348][__main__][INFO] - Test, Round 165: loss=5.6261, accuracy=0.3225, 
[2025-09-11 23:55:23,348][__main__][INFO] - Test, Round 166: loss=5.6363, accuracy=0.3235, 
[2025-09-11 23:55:23,348][__main__][INFO] - Test, Round 167: loss=5.6498, accuracy=0.3224, 
[2025-09-11 23:55:23,348][__main__][INFO] - Test, Round 168: loss=5.6620, accuracy=0.3250, 
[2025-09-11 23:55:23,348][__main__][INFO] - Test, Round 169: loss=5.6754, accuracy=0.3235, 
[2025-09-11 23:55:23,348][__main__][INFO] - Test, Round 170: loss=5.6867, accuracy=0.3221, 
[2025-09-11 23:55:23,348][__main__][INFO] - Test, Round 171: loss=5.6979, accuracy=0.3238, 
[2025-09-11 23:55:23,348][__main__][INFO] - Test, Round 172: loss=5.7069, accuracy=0.3234, 
[2025-09-11 23:55:23,348][__main__][INFO] - Test, Round 173: loss=5.7179, accuracy=0.3228, 
[2025-09-11 23:55:23,348][__main__][INFO] - Test, Round 174: loss=5.7302, accuracy=0.3239, 
[2025-09-11 23:55:23,348][__main__][INFO] - Test, Round 175: loss=5.7406, accuracy=0.3229, 
[2025-09-11 23:55:23,348][__main__][INFO] - Test, Round 176: loss=5.7518, accuracy=0.3225, 
[2025-09-11 23:55:23,348][__main__][INFO] - Test, Round 177: loss=5.7623, accuracy=0.3228, 
[2025-09-11 23:55:23,348][__main__][INFO] - Test, Round 178: loss=5.7712, accuracy=0.3230, 
[2025-09-11 23:55:23,348][__main__][INFO] - Test, Round 179: loss=5.7817, accuracy=0.3230, 
[2025-09-11 23:55:23,348][__main__][INFO] - Test, Round 180: loss=5.7925, accuracy=0.3224, 
[2025-09-11 23:55:23,348][__main__][INFO] - Test, Round 181: loss=5.8029, accuracy=0.3232, 
[2025-09-11 23:55:23,348][__main__][INFO] - Test, Round 182: loss=5.8113, accuracy=0.3231, 
[2025-09-11 23:55:23,348][__main__][INFO] - Test, Round 183: loss=5.8197, accuracy=0.3234, 
[2025-09-11 23:55:23,348][__main__][INFO] - Test, Round 184: loss=5.8304, accuracy=0.3235, 
[2025-09-11 23:55:23,348][__main__][INFO] - Test, Round 185: loss=5.8402, accuracy=0.3229, 
[2025-09-11 23:55:23,348][__main__][INFO] - Test, Round 186: loss=5.8495, accuracy=0.3235, 
[2025-09-11 23:55:23,348][__main__][INFO] - Test, Round 187: loss=5.8590, accuracy=0.3223, 
[2025-09-11 23:55:23,348][__main__][INFO] - Test, Round 188: loss=5.8656, accuracy=0.3226, 
[2025-09-11 23:55:23,348][__main__][INFO] - Test, Round 189: loss=5.8756, accuracy=0.3232, 
[2025-09-11 23:55:23,348][__main__][INFO] - Test, Round 190: loss=5.8827, accuracy=0.3231, 
[2025-09-11 23:55:23,348][__main__][INFO] - Test, Round 191: loss=5.8903, accuracy=0.3227, 
[2025-09-11 23:55:23,348][__main__][INFO] - Test, Round 192: loss=5.9002, accuracy=0.3229, 
[2025-09-11 23:55:23,348][__main__][INFO] - Test, Round 193: loss=5.9070, accuracy=0.3226, 
[2025-09-11 23:55:23,348][__main__][INFO] - Test, Round 194: loss=5.9163, accuracy=0.3231, 
[2025-09-11 23:55:23,348][__main__][INFO] - Test, Round 195: loss=5.9233, accuracy=0.3229, 
[2025-09-11 23:55:23,348][__main__][INFO] - Test, Round 196: loss=5.9290, accuracy=0.3233, 
[2025-09-11 23:55:23,348][__main__][INFO] - Test, Round 197: loss=5.9397, accuracy=0.3225, 
[2025-09-11 23:55:23,348][__main__][INFO] - Test, Round 198: loss=5.9456, accuracy=0.3227, 
[2025-09-11 23:55:23,348][__main__][INFO] - Test, Round 199: loss=5.9563, accuracy=0.3230, 
[2025-09-11 23:55:23,348][__main__][INFO] - Test, Round 200: loss=5.9621, accuracy=0.3232, 
[2025-09-11 23:55:23,348][__main__][INFO] - Test, Round 201: loss=5.9699, accuracy=0.3219, 
[2025-09-11 23:55:23,348][__main__][INFO] - Test, Round 202: loss=5.9759, accuracy=0.3228, 
[2025-09-11 23:55:23,348][__main__][INFO] - Test, Round 203: loss=5.9835, accuracy=0.3224, 
[2025-09-11 23:55:23,349][__main__][INFO] - Test, Round 204: loss=5.9923, accuracy=0.3219, 
[2025-09-11 23:55:23,349][__main__][INFO] - Test, Round 205: loss=5.9977, accuracy=0.3227, 
[2025-09-11 23:55:23,349][__main__][INFO] - Test, Round 206: loss=6.0060, accuracy=0.3237, 
[2025-09-11 23:55:23,349][__main__][INFO] - Test, Round 207: loss=6.0128, accuracy=0.3232, 
[2025-09-11 23:55:23,349][__main__][INFO] - Test, Round 208: loss=6.0176, accuracy=0.3223, 
[2025-09-11 23:55:23,349][__main__][INFO] - Test, Round 209: loss=6.0245, accuracy=0.3229, 
[2025-09-11 23:55:23,349][__main__][INFO] - Test, Round 210: loss=6.0318, accuracy=0.3228, 
[2025-09-11 23:55:23,349][__main__][INFO] - Test, Round 211: loss=6.0384, accuracy=0.3223, 
[2025-09-11 23:55:23,349][__main__][INFO] - Test, Round 212: loss=6.0440, accuracy=0.3227, 
[2025-09-11 23:55:23,349][__main__][INFO] - Test, Round 213: loss=6.0510, accuracy=0.3226, 
[2025-09-11 23:55:23,349][__main__][INFO] - Test, Round 214: loss=6.0579, accuracy=0.3227, 
[2025-09-11 23:55:23,349][__main__][INFO] - Test, Round 215: loss=6.0636, accuracy=0.3226, 
[2025-09-11 23:55:23,349][__main__][INFO] - Test, Round 216: loss=6.0693, accuracy=0.3228, 
[2025-09-11 23:55:23,349][__main__][INFO] - Test, Round 217: loss=6.0774, accuracy=0.3228, 
[2025-09-11 23:55:23,349][__main__][INFO] - Test, Round 218: loss=6.0806, accuracy=0.3233, 
[2025-09-11 23:55:23,349][__main__][INFO] - Test, Round 219: loss=6.0874, accuracy=0.3222, 
[2025-09-11 23:55:23,349][__main__][INFO] - Test, Round 220: loss=6.0943, accuracy=0.3223, 
[2025-09-11 23:55:23,349][__main__][INFO] - Test, Round 221: loss=6.0989, accuracy=0.3237, 
[2025-09-11 23:55:23,349][__main__][INFO] - Test, Round 222: loss=6.1045, accuracy=0.3229, 
[2025-09-11 23:55:23,349][__main__][INFO] - Test, Round 223: loss=6.1106, accuracy=0.3230, 
[2025-09-11 23:55:23,349][__main__][INFO] - Test, Round 224: loss=6.1163, accuracy=0.3219, 
[2025-09-11 23:55:23,349][__main__][INFO] - Test, Round 225: loss=6.1205, accuracy=0.3224, 
[2025-09-11 23:55:23,349][__main__][INFO] - Test, Round 226: loss=6.1276, accuracy=0.3220, 
[2025-09-11 23:55:23,349][__main__][INFO] - Test, Round 227: loss=6.1329, accuracy=0.3227, 
[2025-09-11 23:55:23,349][__main__][INFO] - Test, Round 228: loss=6.1385, accuracy=0.3226, 
[2025-09-11 23:55:23,349][__main__][INFO] - Test, Round 229: loss=6.1450, accuracy=0.3228, 
[2025-09-11 23:55:23,349][__main__][INFO] - Test, Round 230: loss=6.1486, accuracy=0.3227, 
[2025-09-11 23:55:23,349][__main__][INFO] - Test, Round 231: loss=6.1551, accuracy=0.3229, 
[2025-09-11 23:55:23,349][__main__][INFO] - Test, Round 232: loss=6.1582, accuracy=0.3229, 
[2025-09-11 23:55:23,349][__main__][INFO] - Test, Round 233: loss=6.1628, accuracy=0.3231, 
[2025-09-11 23:55:23,349][__main__][INFO] - Test, Round 234: loss=6.1689, accuracy=0.3229, 
[2025-09-11 23:55:23,349][__main__][INFO] - Test, Round 235: loss=6.1752, accuracy=0.3230, 
[2025-09-11 23:55:23,349][__main__][INFO] - Test, Round 236: loss=6.1798, accuracy=0.3226, 
[2025-09-11 23:55:23,349][__main__][INFO] - Test, Round 237: loss=6.1846, accuracy=0.3226, 
[2025-09-11 23:55:23,349][__main__][INFO] - Test, Round 238: loss=6.1889, accuracy=0.3228, 
[2025-09-11 23:55:23,349][__main__][INFO] - Test, Round 239: loss=6.1931, accuracy=0.3226, 
[2025-09-11 23:55:23,349][__main__][INFO] - Test, Round 240: loss=6.1979, accuracy=0.3226, 
[2025-09-11 23:55:23,349][__main__][INFO] - Test, Round 241: loss=6.2025, accuracy=0.3226, 
[2025-09-11 23:55:23,349][__main__][INFO] - Test, Round 242: loss=6.2071, accuracy=0.3225, 
[2025-09-11 23:55:23,349][__main__][INFO] - Test, Round 243: loss=6.2143, accuracy=0.3229, 
[2025-09-11 23:55:23,349][__main__][INFO] - Test, Round 244: loss=6.2178, accuracy=0.3232, 
[2025-09-11 23:55:23,349][__main__][INFO] - Test, Round 245: loss=6.2222, accuracy=0.3220, 
[2025-09-11 23:55:23,349][__main__][INFO] - Test, Round 246: loss=6.2270, accuracy=0.3227, 
[2025-09-11 23:55:23,349][__main__][INFO] - Test, Round 247: loss=6.2302, accuracy=0.3222, 
[2025-09-11 23:55:23,349][__main__][INFO] - Test, Round 248: loss=6.2362, accuracy=0.3219, 
[2025-09-11 23:55:23,349][__main__][INFO] - Test, Round 249: loss=6.2386, accuracy=0.3227, 
[2025-09-11 23:55:23,349][__main__][INFO] - Test, Round 250: loss=6.2424, accuracy=0.3234, 
[2025-09-11 23:55:23,349][__main__][INFO] - Test, Round 251: loss=6.2483, accuracy=0.3221, 
[2025-09-11 23:55:23,349][__main__][INFO] - Test, Round 252: loss=6.2518, accuracy=0.3226, 
[2025-09-11 23:55:23,349][__main__][INFO] - Test, Round 253: loss=6.2561, accuracy=0.3221, 
[2025-09-11 23:55:23,349][__main__][INFO] - Test, Round 254: loss=6.2600, accuracy=0.3223, 
[2025-09-11 23:55:23,349][__main__][INFO] - Test, Round 255: loss=6.2640, accuracy=0.3226, 
[2025-09-11 23:55:23,349][__main__][INFO] - Test, Round 256: loss=6.2681, accuracy=0.3226, 
[2025-09-11 23:55:23,349][__main__][INFO] - Test, Round 257: loss=6.2720, accuracy=0.3221, 
[2025-09-11 23:55:23,349][__main__][INFO] - Test, Round 258: loss=6.2756, accuracy=0.3225, 
[2025-09-11 23:55:23,349][__main__][INFO] - Test, Round 259: loss=6.2795, accuracy=0.3231, 
[2025-09-11 23:55:23,349][__main__][INFO] - Test, Round 260: loss=6.2845, accuracy=0.3225, 
[2025-09-11 23:55:23,349][__main__][INFO] - Test, Round 261: loss=6.2880, accuracy=0.3225, 
[2025-09-11 23:55:23,349][__main__][INFO] - Test, Round 262: loss=6.2912, accuracy=0.3227, 
[2025-09-11 23:55:23,349][__main__][INFO] - Test, Round 263: loss=6.2939, accuracy=0.3220, 
[2025-09-11 23:55:23,349][__main__][INFO] - Test, Round 264: loss=6.2991, accuracy=0.3222, 
[2025-09-11 23:55:23,349][__main__][INFO] - Test, Round 265: loss=6.3021, accuracy=0.3217, 
[2025-09-11 23:55:23,349][__main__][INFO] - Test, Round 266: loss=6.3060, accuracy=0.3225, 
[2025-09-11 23:55:23,349][__main__][INFO] - Test, Round 267: loss=6.3096, accuracy=0.3220, 
[2025-09-11 23:55:23,349][__main__][INFO] - Test, Round 268: loss=6.3149, accuracy=0.3222, 
[2025-09-11 23:55:23,349][__main__][INFO] - Test, Round 269: loss=6.3177, accuracy=0.3228, 
[2025-09-11 23:55:23,349][__main__][INFO] - Test, Round 270: loss=6.3210, accuracy=0.3217, 
[2025-09-11 23:55:23,349][__main__][INFO] - Test, Round 271: loss=6.3248, accuracy=0.3225, 
[2025-09-11 23:55:23,349][__main__][INFO] - Test, Round 272: loss=6.3278, accuracy=0.3227, 
[2025-09-11 23:55:23,349][__main__][INFO] - Test, Round 273: loss=6.3323, accuracy=0.3222, 
[2025-09-11 23:55:23,349][__main__][INFO] - Test, Round 274: loss=6.3338, accuracy=0.3221, 
[2025-09-11 23:55:23,349][__main__][INFO] - Test, Round 275: loss=6.3371, accuracy=0.3217, 
[2025-09-11 23:55:23,349][__main__][INFO] - Test, Round 276: loss=6.3410, accuracy=0.3221, 
[2025-09-11 23:55:23,349][__main__][INFO] - Test, Round 277: loss=6.3440, accuracy=0.3222, 
[2025-09-11 23:55:23,349][__main__][INFO] - Test, Round 278: loss=6.3475, accuracy=0.3222, 
[2025-09-11 23:55:23,349][__main__][INFO] - Test, Round 279: loss=6.3498, accuracy=0.3220, 
[2025-09-11 23:55:23,350][__main__][INFO] - Test, Round 280: loss=6.3529, accuracy=0.3220, 
[2025-09-11 23:55:23,350][__main__][INFO] - Test, Round 281: loss=6.3574, accuracy=0.3220, 
[2025-09-11 23:55:23,350][__main__][INFO] - Test, Round 282: loss=6.3598, accuracy=0.3222, 
[2025-09-11 23:55:23,350][__main__][INFO] - Test, Round 283: loss=6.3640, accuracy=0.3225, 
[2025-09-11 23:55:23,350][__main__][INFO] - Test, Round 284: loss=6.3667, accuracy=0.3221, 
[2025-09-11 23:55:23,350][__main__][INFO] - Test, Round 285: loss=6.3698, accuracy=0.3217, 
[2025-09-11 23:55:23,350][__main__][INFO] - Test, Round 286: loss=6.3720, accuracy=0.3222, 
[2025-09-11 23:55:23,350][__main__][INFO] - Test, Round 287: loss=6.3760, accuracy=0.3218, 
[2025-09-11 23:55:23,350][__main__][INFO] - Test, Round 288: loss=6.3784, accuracy=0.3221, 
[2025-09-11 23:55:23,350][__main__][INFO] - Test, Round 289: loss=6.3819, accuracy=0.3221, 
[2025-09-11 23:55:23,350][__main__][INFO] - Test, Round 290: loss=6.3853, accuracy=0.3219, 
[2025-09-11 23:55:23,350][__main__][INFO] - Test, Round 291: loss=6.3877, accuracy=0.3217, 
[2025-09-11 23:55:23,350][__main__][INFO] - Test, Round 292: loss=6.3900, accuracy=0.3217, 
[2025-09-11 23:55:23,350][__main__][INFO] - Test, Round 293: loss=6.3933, accuracy=0.3224, 
[2025-09-11 23:55:23,350][__main__][INFO] - Test, Round 294: loss=6.3965, accuracy=0.3218, 
[2025-09-11 23:55:23,350][__main__][INFO] - Test, Round 295: loss=6.3992, accuracy=0.3218, 
[2025-09-11 23:55:23,350][__main__][INFO] - Test, Round 296: loss=6.4015, accuracy=0.3212, 
[2025-09-11 23:55:23,350][__main__][INFO] - Test, Round 297: loss=6.4049, accuracy=0.3216, 
[2025-09-11 23:55:23,350][__main__][INFO] - Test, Round 298: loss=6.4069, accuracy=0.3220, 
[2025-09-11 23:55:23,350][__main__][INFO] - Test, Round 299: loss=6.4105, accuracy=0.3216, 
[2025-09-11 23:55:23,350][__main__][INFO] - Test, Round 300: loss=6.4128, accuracy=0.3217, 
[2025-09-11 23:55:23,350][__main__][INFO] - Test, Round 301: loss=6.4147, accuracy=0.3222, 
[2025-09-11 23:55:23,350][__main__][INFO] - Test, Round 302: loss=6.4175, accuracy=0.3219, 
[2025-09-11 23:55:23,350][__main__][INFO] - Test, Round 303: loss=6.4209, accuracy=0.3219, 
[2025-09-11 23:55:23,350][__main__][INFO] - Test, Round 304: loss=6.4230, accuracy=0.3221, 
[2025-09-11 23:55:23,350][__main__][INFO] - Test, Round 305: loss=6.4257, accuracy=0.3222, 
[2025-09-11 23:55:23,350][__main__][INFO] - Test, Round 306: loss=6.4288, accuracy=0.3217, 
[2025-09-11 23:55:23,350][__main__][INFO] - Test, Round 307: loss=6.4308, accuracy=0.3220, 
[2025-09-11 23:55:23,350][__main__][INFO] - Test, Round 308: loss=6.4318, accuracy=0.3220, 
[2025-09-11 23:55:23,350][__main__][INFO] - Test, Round 309: loss=6.4365, accuracy=0.3221, 
[2025-09-11 23:55:23,350][__main__][INFO] - Test, Round 310: loss=6.4380, accuracy=0.3219, 
[2025-09-11 23:55:23,350][__main__][INFO] - Test, Round 311: loss=6.4410, accuracy=0.3220, 
[2025-09-11 23:55:23,350][__main__][INFO] - Test, Round 312: loss=6.4429, accuracy=0.3216, 
[2025-09-11 23:55:23,350][__main__][INFO] - Test, Round 313: loss=6.4448, accuracy=0.3214, 
[2025-09-11 23:55:23,350][__main__][INFO] - Test, Round 314: loss=6.4471, accuracy=0.3219, 
[2025-09-11 23:55:23,350][__main__][INFO] - Test, Round 315: loss=6.4495, accuracy=0.3219, 
[2025-09-11 23:55:23,350][__main__][INFO] - Test, Round 316: loss=6.4527, accuracy=0.3215, 
[2025-09-11 23:55:23,350][__main__][INFO] - Test, Round 317: loss=6.4540, accuracy=0.3218, 
[2025-09-11 23:55:23,350][__main__][INFO] - Test, Round 318: loss=6.4576, accuracy=0.3219, 
[2025-09-11 23:55:23,350][__main__][INFO] - Test, Round 319: loss=6.4591, accuracy=0.3220, 
[2025-09-11 23:55:23,350][__main__][INFO] - Test, Round 320: loss=6.4619, accuracy=0.3216, 
[2025-09-11 23:55:23,350][__main__][INFO] - Test, Round 321: loss=6.4635, accuracy=0.3224, 
[2025-09-11 23:55:23,350][__main__][INFO] - Test, Round 322: loss=6.4660, accuracy=0.3221, 
[2025-09-11 23:55:23,350][__main__][INFO] - Test, Round 323: loss=6.4679, accuracy=0.3211, 
[2025-09-11 23:55:23,350][__main__][INFO] - Test, Round 324: loss=6.4704, accuracy=0.3216, 
[2025-09-11 23:55:23,350][__main__][INFO] - Test, Round 325: loss=6.4719, accuracy=0.3219, 
[2025-09-11 23:55:23,350][__main__][INFO] - Test, Round 326: loss=6.4744, accuracy=0.3218, 
[2025-09-11 23:55:23,350][__main__][INFO] - Test, Round 327: loss=6.4771, accuracy=0.3216, 
[2025-09-11 23:55:23,350][__main__][INFO] - Test, Round 328: loss=6.4784, accuracy=0.3215, 
[2025-09-11 23:55:23,350][__main__][INFO] - Test, Round 329: loss=6.4814, accuracy=0.3222, 
[2025-09-11 23:55:23,350][__main__][INFO] - Test, Round 330: loss=6.4832, accuracy=0.3219, 
[2025-09-11 23:55:23,350][__main__][INFO] - Test, Round 331: loss=6.4850, accuracy=0.3215, 
[2025-09-11 23:55:23,350][__main__][INFO] - Test, Round 332: loss=6.4869, accuracy=0.3221, 
[2025-09-11 23:55:23,350][__main__][INFO] - Test, Round 333: loss=6.4888, accuracy=0.3217, 
[2025-09-11 23:55:23,350][__main__][INFO] - Test, Round 334: loss=6.4913, accuracy=0.3216, 
[2025-09-11 23:55:23,350][__main__][INFO] - Test, Round 335: loss=6.4931, accuracy=0.3220, 
[2025-09-11 23:55:23,350][__main__][INFO] - Test, Round 336: loss=6.4956, accuracy=0.3218, 
[2025-09-11 23:55:23,350][__main__][INFO] - Test, Round 337: loss=6.4981, accuracy=0.3213, 
[2025-09-11 23:55:23,350][__main__][INFO] - Test, Round 338: loss=6.4994, accuracy=0.3218, 
[2025-09-11 23:55:23,350][__main__][INFO] - Test, Round 339: loss=6.5012, accuracy=0.3220, 
[2025-09-11 23:55:23,350][__main__][INFO] - Test, Round 340: loss=6.5024, accuracy=0.3217, 
[2025-09-11 23:55:23,350][__main__][INFO] - Test, Round 341: loss=6.5048, accuracy=0.3220, 
[2025-09-11 23:55:23,350][__main__][INFO] - Test, Round 342: loss=6.5059, accuracy=0.3217, 
[2025-09-11 23:55:23,350][__main__][INFO] - Test, Round 343: loss=6.5082, accuracy=0.3217, 
[2025-09-11 23:55:23,350][__main__][INFO] - Test, Round 344: loss=6.5095, accuracy=0.3219, 
[2025-09-11 23:55:23,350][__main__][INFO] - Test, Round 345: loss=6.5121, accuracy=0.3218, 
[2025-09-11 23:55:23,350][__main__][INFO] - Test, Round 346: loss=6.5135, accuracy=0.3219, 
[2025-09-11 23:55:23,350][__main__][INFO] - Test, Round 347: loss=6.5152, accuracy=0.3215, 
[2025-09-11 23:55:23,350][__main__][INFO] - Test, Round 348: loss=6.5181, accuracy=0.3216, 
[2025-09-11 23:55:23,350][__main__][INFO] - Test, Round 349: loss=6.5200, accuracy=0.3217, 
[2025-09-11 23:55:23,350][__main__][INFO] - Test, Round 350: loss=6.5216, accuracy=0.3218, 
[2025-09-11 23:55:23,350][__main__][INFO] - Test, Round 351: loss=6.5223, accuracy=0.3219, 
[2025-09-11 23:55:23,350][__main__][INFO] - Test, Round 352: loss=6.5246, accuracy=0.3215, 
[2025-09-11 23:55:23,350][__main__][INFO] - Test, Round 353: loss=6.5258, accuracy=0.3224, 
[2025-09-11 23:55:23,350][__main__][INFO] - Test, Round 354: loss=6.5287, accuracy=0.3217, 
[2025-09-11 23:55:23,351][__main__][INFO] - Test, Round 355: loss=6.5295, accuracy=0.3214, 
[2025-09-11 23:55:23,351][__main__][INFO] - Test, Round 356: loss=6.5316, accuracy=0.3222, 
[2025-09-11 23:55:23,351][__main__][INFO] - Test, Round 357: loss=6.5326, accuracy=0.3221, 
[2025-09-11 23:55:23,351][__main__][INFO] - Test, Round 358: loss=6.5345, accuracy=0.3218, 
[2025-09-11 23:55:23,351][__main__][INFO] - Test, Round 359: loss=6.5355, accuracy=0.3218, 
[2025-09-11 23:55:23,351][__main__][INFO] - Test, Round 360: loss=6.5371, accuracy=0.3219, 
[2025-09-11 23:55:23,351][__main__][INFO] - Test, Round 361: loss=6.5391, accuracy=0.3216, 
[2025-09-11 23:55:23,351][__main__][INFO] - Test, Round 362: loss=6.5407, accuracy=0.3217, 
[2025-09-11 23:55:23,351][__main__][INFO] - Test, Round 363: loss=6.5417, accuracy=0.3215, 
[2025-09-11 23:55:23,351][__main__][INFO] - Test, Round 364: loss=6.5438, accuracy=0.3215, 
[2025-09-11 23:55:23,351][__main__][INFO] - Test, Round 365: loss=6.5453, accuracy=0.3220, 
[2025-09-11 23:55:23,351][__main__][INFO] - Test, Round 366: loss=6.5472, accuracy=0.3216, 
[2025-09-11 23:55:23,351][__main__][INFO] - Test, Round 367: loss=6.5483, accuracy=0.3209, 
[2025-09-11 23:55:23,351][__main__][INFO] - Test, Round 368: loss=6.5495, accuracy=0.3214, 
[2025-09-11 23:55:23,351][__main__][INFO] - Test, Round 369: loss=6.5519, accuracy=0.3215, 
[2025-09-11 23:55:23,351][__main__][INFO] - Test, Round 370: loss=6.5527, accuracy=0.3214, 
[2025-09-11 23:55:23,351][__main__][INFO] - Test, Round 371: loss=6.5539, accuracy=0.3214, 
[2025-09-11 23:55:23,351][__main__][INFO] - Test, Round 372: loss=6.5550, accuracy=0.3218, 
[2025-09-11 23:55:23,351][__main__][INFO] - Test, Round 373: loss=6.5565, accuracy=0.3216, 
[2025-09-11 23:55:23,351][__main__][INFO] - Test, Round 374: loss=6.5580, accuracy=0.3210, 
[2025-09-11 23:55:23,351][__main__][INFO] - Test, Round 375: loss=6.5599, accuracy=0.3222, 
[2025-09-11 23:55:23,351][__main__][INFO] - Test, Round 376: loss=6.5614, accuracy=0.3210, 
[2025-09-11 23:55:23,351][__main__][INFO] - Test, Round 377: loss=6.5620, accuracy=0.3217, 
[2025-09-11 23:55:23,351][__main__][INFO] - Test, Round 378: loss=6.5642, accuracy=0.3215, 
[2025-09-11 23:55:23,351][__main__][INFO] - Test, Round 379: loss=6.5660, accuracy=0.3213, 
[2025-09-11 23:55:23,351][__main__][INFO] - Test, Round 380: loss=6.5662, accuracy=0.3217, 
[2025-09-11 23:55:23,351][__main__][INFO] - Test, Round 381: loss=6.5682, accuracy=0.3214, 
[2025-09-11 23:55:23,351][__main__][INFO] - Test, Round 382: loss=6.5697, accuracy=0.3212, 
[2025-09-11 23:55:23,351][__main__][INFO] - Test, Round 383: loss=6.5709, accuracy=0.3211, 
[2025-09-11 23:55:23,351][__main__][INFO] - Test, Round 384: loss=6.5724, accuracy=0.3208, 
[2025-09-11 23:55:23,351][__main__][INFO] - Test, Round 385: loss=6.5736, accuracy=0.3211, 
[2025-09-11 23:55:23,351][__main__][INFO] - Test, Round 386: loss=6.5754, accuracy=0.3212, 
[2025-09-11 23:55:23,351][__main__][INFO] - Test, Round 387: loss=6.5755, accuracy=0.3216, 
[2025-09-11 23:55:23,351][__main__][INFO] - Test, Round 388: loss=6.5775, accuracy=0.3212, 
[2025-09-11 23:55:23,351][__main__][INFO] - Test, Round 389: loss=6.5788, accuracy=0.3210, 
[2025-09-11 23:55:23,351][__main__][INFO] - Test, Round 390: loss=6.5803, accuracy=0.3212, 
[2025-09-11 23:55:23,351][__main__][INFO] - Test, Round 391: loss=6.5809, accuracy=0.3211, 
[2025-09-11 23:55:23,351][__main__][INFO] - Test, Round 392: loss=6.5830, accuracy=0.3210, 
[2025-09-11 23:55:23,351][__main__][INFO] - Test, Round 393: loss=6.5840, accuracy=0.3212, 
[2025-09-11 23:55:23,351][__main__][INFO] - Test, Round 394: loss=6.5841, accuracy=0.3208, 
[2025-09-11 23:55:23,351][__main__][INFO] - Test, Round 395: loss=6.5859, accuracy=0.3215, 
[2025-09-11 23:55:23,351][__main__][INFO] - Test, Round 396: loss=6.5868, accuracy=0.3211, 
[2025-09-11 23:55:23,351][__main__][INFO] - Test, Round 397: loss=6.5881, accuracy=0.3215, 
[2025-09-11 23:55:23,351][__main__][INFO] - Test, Round 398: loss=6.5898, accuracy=0.3212, 
[2025-09-11 23:55:23,351][__main__][INFO] - Test, Round 399: loss=6.5909, accuracy=0.3210, 
[2025-09-11 23:55:23,351][__main__][INFO] - Test, Round 400: loss=6.5927, accuracy=0.3210, 
[2025-09-11 23:55:23,351][__main__][INFO] - Test, Round 401: loss=6.5937, accuracy=0.3215, 
[2025-09-11 23:55:23,351][__main__][INFO] - Test, Round 402: loss=6.5944, accuracy=0.3208, 
[2025-09-11 23:55:23,351][__main__][INFO] - Test, Round 403: loss=6.5950, accuracy=0.3208, 
[2025-09-11 23:55:23,351][__main__][INFO] - Test, Round 404: loss=6.5968, accuracy=0.3215, 
[2025-09-11 23:55:23,351][__main__][INFO] - Test, Round 405: loss=6.5981, accuracy=0.3214, 
[2025-09-11 23:55:23,351][__main__][INFO] - Test, Round 406: loss=6.5984, accuracy=0.3216, 
[2025-09-11 23:55:23,351][__main__][INFO] - Test, Round 407: loss=6.6002, accuracy=0.3210, 
[2025-09-11 23:55:23,351][__main__][INFO] - Test, Round 408: loss=6.6014, accuracy=0.3210, 
[2025-09-11 23:55:23,351][__main__][INFO] - Test, Round 409: loss=6.6021, accuracy=0.3210, 
[2025-09-11 23:55:23,351][__main__][INFO] - Test, Round 410: loss=6.6028, accuracy=0.3215, 
[2025-09-11 23:55:23,351][__main__][INFO] - Test, Round 411: loss=6.6044, accuracy=0.3210, 
[2025-09-11 23:55:23,351][__main__][INFO] - Test, Round 412: loss=6.6058, accuracy=0.3211, 
[2025-09-11 23:55:23,351][__main__][INFO] - Test, Round 413: loss=6.6058, accuracy=0.3210, 
[2025-09-11 23:55:23,351][__main__][INFO] - Test, Round 414: loss=6.6074, accuracy=0.3208, 
[2025-09-11 23:55:23,351][__main__][INFO] - Test, Round 415: loss=6.6078, accuracy=0.3216, 
[2025-09-11 23:55:23,351][__main__][INFO] - Test, Round 416: loss=6.6089, accuracy=0.3214, 
[2025-09-11 23:55:23,351][__main__][INFO] - Test, Round 417: loss=6.6109, accuracy=0.3207, 
[2025-09-11 23:55:23,351][__main__][INFO] - Test, Round 418: loss=6.6105, accuracy=0.3211, 
[2025-09-11 23:55:23,351][__main__][INFO] - Test, Round 419: loss=6.6129, accuracy=0.3208, 
[2025-09-11 23:55:23,351][__main__][INFO] - Test, Round 420: loss=6.6135, accuracy=0.3207, 
[2025-09-11 23:55:23,351][__main__][INFO] - Test, Round 421: loss=6.6139, accuracy=0.3205, 
[2025-09-11 23:55:23,351][__main__][INFO] - Test, Round 422: loss=6.6155, accuracy=0.3209, 
[2025-09-11 23:55:23,351][__main__][INFO] - Test, Round 423: loss=6.6160, accuracy=0.3207, 
[2025-09-11 23:55:23,351][__main__][INFO] - Test, Round 424: loss=6.6167, accuracy=0.3210, 
[2025-09-11 23:55:23,351][__main__][INFO] - Test, Round 425: loss=6.6173, accuracy=0.3210, 
[2025-09-11 23:55:23,351][__main__][INFO] - Test, Round 426: loss=6.6180, accuracy=0.3212, 
[2025-09-11 23:55:23,351][__main__][INFO] - Test, Round 427: loss=6.6191, accuracy=0.3208, 
[2025-09-11 23:55:23,351][__main__][INFO] - Test, Round 428: loss=6.6208, accuracy=0.3206, 
[2025-09-11 23:55:23,351][__main__][INFO] - Test, Round 429: loss=6.6214, accuracy=0.3210, 
[2025-09-11 23:55:23,352][__main__][INFO] - Test, Round 430: loss=6.6228, accuracy=0.3210, 
[2025-09-11 23:55:23,352][__main__][INFO] - Test, Round 431: loss=6.6231, accuracy=0.3212, 
[2025-09-11 23:55:23,352][__main__][INFO] - Test, Round 432: loss=6.6243, accuracy=0.3210, 
[2025-09-11 23:55:23,352][__main__][INFO] - Test, Round 433: loss=6.6253, accuracy=0.3210, 
[2025-09-11 23:55:23,352][__main__][INFO] - Test, Round 434: loss=6.6260, accuracy=0.3209, 
[2025-09-11 23:55:23,352][__main__][INFO] - Test, Round 435: loss=6.6267, accuracy=0.3209, 
[2025-09-11 23:55:23,352][__main__][INFO] - Test, Round 436: loss=6.6271, accuracy=0.3208, 
[2025-09-11 23:55:23,352][__main__][INFO] - Test, Round 437: loss=6.6277, accuracy=0.3213, 
[2025-09-11 23:55:23,352][__main__][INFO] - Test, Round 438: loss=6.6289, accuracy=0.3212, 
[2025-09-11 23:55:23,352][__main__][INFO] - Test, Round 439: loss=6.6299, accuracy=0.3213, 
[2025-09-11 23:55:23,352][__main__][INFO] - Test, Round 440: loss=6.6309, accuracy=0.3208, 
[2025-09-11 23:55:23,352][__main__][INFO] - Test, Round 441: loss=6.6321, accuracy=0.3202, 
[2025-09-11 23:55:23,352][__main__][INFO] - Test, Round 442: loss=6.6325, accuracy=0.3207, 
[2025-09-11 23:55:23,352][__main__][INFO] - Test, Round 443: loss=6.6333, accuracy=0.3210, 
[2025-09-11 23:55:23,352][__main__][INFO] - Test, Round 444: loss=6.6344, accuracy=0.3208, 
[2025-09-11 23:55:23,352][__main__][INFO] - Test, Round 445: loss=6.6352, accuracy=0.3213, 
[2025-09-11 23:55:23,352][__main__][INFO] - Test, Round 446: loss=6.6361, accuracy=0.3212, 
[2025-09-11 23:55:23,352][__main__][INFO] - Test, Round 447: loss=6.6359, accuracy=0.3215, 
[2025-09-11 23:55:23,352][__main__][INFO] - Test, Round 448: loss=6.6360, accuracy=0.3208, 
[2025-09-11 23:55:23,352][__main__][INFO] - Test, Round 449: loss=6.6380, accuracy=0.3205, 
[2025-09-11 23:55:23,352][__main__][INFO] - Test, Round 450: loss=6.6390, accuracy=0.3212, 
[2025-09-11 23:55:23,352][__main__][INFO] - Test, Round 451: loss=6.6395, accuracy=0.3208, 
[2025-09-11 23:55:23,352][__main__][INFO] - Test, Round 452: loss=6.6400, accuracy=0.3207, 
[2025-09-11 23:55:23,352][__main__][INFO] - Test, Round 453: loss=6.6403, accuracy=0.3209, 
[2025-09-11 23:55:23,352][__main__][INFO] - Test, Round 454: loss=6.6414, accuracy=0.3209, 
[2025-09-11 23:55:23,352][__main__][INFO] - Test, Round 455: loss=6.6423, accuracy=0.3212, 
[2025-09-11 23:55:23,352][__main__][INFO] - Test, Round 456: loss=6.6424, accuracy=0.3213, 
[2025-09-11 23:55:23,352][__main__][INFO] - Test, Round 457: loss=6.6430, accuracy=0.3208, 
[2025-09-11 23:55:23,352][__main__][INFO] - Test, Round 458: loss=6.6439, accuracy=0.3212, 
[2025-09-11 23:55:23,352][__main__][INFO] - Test, Round 459: loss=6.6449, accuracy=0.3207, 
[2025-09-11 23:55:23,352][__main__][INFO] - Test, Round 460: loss=6.6462, accuracy=0.3213, 
[2025-09-11 23:55:23,352][__main__][INFO] - Test, Round 461: loss=6.6461, accuracy=0.3210, 
[2025-09-11 23:55:23,352][__main__][INFO] - Test, Round 462: loss=6.6459, accuracy=0.3213, 
[2025-09-11 23:55:23,352][__main__][INFO] - Test, Round 463: loss=6.6470, accuracy=0.3209, 
[2025-09-11 23:55:23,352][__main__][INFO] - Test, Round 464: loss=6.6482, accuracy=0.3212, 
[2025-09-11 23:55:23,352][__main__][INFO] - Test, Round 465: loss=6.6495, accuracy=0.3209, 
[2025-09-11 23:55:23,352][__main__][INFO] - Test, Round 466: loss=6.6496, accuracy=0.3207, 
[2025-09-11 23:55:23,352][__main__][INFO] - Test, Round 467: loss=6.6498, accuracy=0.3208, 
[2025-09-11 23:55:23,352][__main__][INFO] - Test, Round 468: loss=6.6510, accuracy=0.3207, 
[2025-09-11 23:55:23,352][__main__][INFO] - Test, Round 469: loss=6.6516, accuracy=0.3213, 
[2025-09-11 23:55:23,352][__main__][INFO] - Test, Round 470: loss=6.6525, accuracy=0.3205, 
[2025-09-11 23:55:23,352][__main__][INFO] - Test, Round 471: loss=6.6528, accuracy=0.3206, 
[2025-09-11 23:55:23,352][__main__][INFO] - Test, Round 472: loss=6.6532, accuracy=0.3212, 
[2025-09-11 23:55:23,352][__main__][INFO] - Test, Round 473: loss=6.6536, accuracy=0.3210, 
[2025-09-11 23:55:23,352][__main__][INFO] - Test, Round 474: loss=6.6543, accuracy=0.3207, 
[2025-09-11 23:55:23,352][__main__][INFO] - Test, Round 475: loss=6.6547, accuracy=0.3210, 
[2025-09-11 23:55:23,352][__main__][INFO] - Test, Round 476: loss=6.6559, accuracy=0.3209, 
[2025-09-11 23:55:23,352][__main__][INFO] - Test, Round 477: loss=6.6555, accuracy=0.3209, 
[2025-09-11 23:55:23,352][__main__][INFO] - Test, Round 478: loss=6.6568, accuracy=0.3210, 
[2025-09-11 23:55:23,352][__main__][INFO] - Test, Round 479: loss=6.6575, accuracy=0.3208, 
[2025-09-11 23:55:23,352][__main__][INFO] - Test, Round 480: loss=6.6569, accuracy=0.3208, 
[2025-09-11 23:55:23,352][__main__][INFO] - Test, Round 481: loss=6.6587, accuracy=0.3208, 
[2025-09-11 23:55:23,352][__main__][INFO] - Test, Round 482: loss=6.6592, accuracy=0.3212, 
[2025-09-11 23:55:23,352][__main__][INFO] - Test, Round 483: loss=6.6597, accuracy=0.3209, 
[2025-09-11 23:55:23,352][__main__][INFO] - Test, Round 484: loss=6.6599, accuracy=0.3208, 
[2025-09-11 23:55:23,352][__main__][INFO] - Test, Round 485: loss=6.6603, accuracy=0.3210, 
[2025-09-11 23:55:23,352][__main__][INFO] - Test, Round 486: loss=6.6614, accuracy=0.3211, 
[2025-09-11 23:55:23,352][__main__][INFO] - Test, Round 487: loss=6.6619, accuracy=0.3210, 
[2025-09-11 23:55:23,352][__main__][INFO] - Test, Round 488: loss=6.6629, accuracy=0.3209, 
[2025-09-11 23:55:23,352][__main__][INFO] - Test, Round 489: loss=6.6627, accuracy=0.3208, 
[2025-09-11 23:55:23,352][__main__][INFO] - Test, Round 490: loss=6.6638, accuracy=0.3213, 
[2025-09-11 23:55:23,352][__main__][INFO] - Test, Round 491: loss=6.6634, accuracy=0.3208, 
[2025-09-11 23:55:23,352][__main__][INFO] - Test, Round 492: loss=6.6648, accuracy=0.3211, 
[2025-09-11 23:55:23,352][__main__][INFO] - Test, Round 493: loss=6.6650, accuracy=0.3211, 
[2025-09-11 23:55:23,352][__main__][INFO] - Test, Round 494: loss=6.6660, accuracy=0.3212, 
[2025-09-11 23:55:23,352][__main__][INFO] - Test, Round 495: loss=6.6656, accuracy=0.3206, 
[2025-09-11 23:55:23,352][__main__][INFO] - Test, Round 496: loss=6.6668, accuracy=0.3207, 
[2025-09-11 23:55:23,352][__main__][INFO] - Test, Round 497: loss=6.6672, accuracy=0.3212, 
[2025-09-11 23:55:23,352][__main__][INFO] - Test, Round 498: loss=6.6675, accuracy=0.3210, 
[2025-09-11 23:55:23,352][__main__][INFO] - Test, Round 499: loss=6.6677, accuracy=0.3208, 
[2025-09-11 23:55:23,352][__main__][INFO] - Test, Round 500: loss=6.6682, accuracy=0.3209, 
