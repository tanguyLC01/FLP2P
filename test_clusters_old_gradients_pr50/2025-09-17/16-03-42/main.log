[2025-09-17 16:03:47,275][flp2p.graph_runner][INFO] - Train, Round 0 : loss => 2.2609554290771485,  accuracy: 0.20811111111111114, gradient_norm : 0.24286071063503933
[2025-09-17 16:03:47,936][flp2p.graph_runner][INFO] - Test, Round 0 : loss => 2.150061873842773,  accuracy: 0.26961770623742454
[2025-09-17 16:03:50,443][flp2p.graph_runner][INFO] - Train, Round 1 : loss => 2.1677353594700497,  accuracy: 0.23183333333333334, gradient_norm : 0.3281676732827966
[2025-09-17 16:03:51,028][flp2p.graph_runner][INFO] - Test, Round 1 : loss => 1.9722289345873674,  accuracy: 0.29577464788732394
[2025-09-17 16:03:53,815][flp2p.graph_runner][INFO] - Train, Round 2 : loss => 2.0428227446296,  accuracy: 0.27146464646464646, gradient_norm : 0.275263097940189
[2025-09-17 16:03:54,437][flp2p.graph_runner][INFO] - Test, Round 2 : loss => 1.9878237923854905,  accuracy: 0.3123861566484517
[2025-09-17 16:03:56,922][flp2p.graph_runner][INFO] - Train, Round 3 : loss => 1.8929921573400497,  accuracy: 0.31272222222222223, gradient_norm : 0.3491411461787812
[2025-09-17 16:03:57,514][flp2p.graph_runner][INFO] - Test, Round 3 : loss => 1.7343179001184756,  accuracy: 0.35728542914171657
[2025-09-17 16:04:00,317][flp2p.graph_runner][INFO] - Train, Round 4 : loss => 1.7257640092661886,  accuracy: 0.39116161616161615, gradient_norm : 0.41973979311486437
[2025-09-17 16:04:00,917][flp2p.graph_runner][INFO] - Test, Round 4 : loss => 1.5427284551797993,  accuracy: 0.4397810218978102
[2025-09-17 16:04:03,730][flp2p.graph_runner][INFO] - Train, Round 5 : loss => 1.9466639555764922,  accuracy: 0.3107070707070707, gradient_norm : 0.28547224951628913
[2025-09-17 16:04:04,376][flp2p.graph_runner][INFO] - Test, Round 5 : loss => 1.8608026460124405,  accuracy: 0.35876475930971846
[2025-09-17 16:04:06,921][flp2p.graph_runner][INFO] - Train, Round 6 : loss => 1.7519300479690234,  accuracy: 0.36883333333333335, gradient_norm : 0.30282298909187755
[2025-09-17 16:04:07,475][flp2p.graph_runner][INFO] - Test, Round 6 : loss => 1.6860172900405679,  accuracy: 0.4075924075924076
[2025-09-17 16:04:10,306][flp2p.graph_runner][INFO] - Train, Round 7 : loss => 1.7106740790786166,  accuracy: 0.422020202020202, gradient_norm : 0.4590153790302914
[2025-09-17 16:04:10,947][flp2p.graph_runner][INFO] - Test, Round 7 : loss => 1.4040211402975462,  accuracy: 0.47518382352941174
[2025-09-17 16:04:13,733][flp2p.graph_runner][INFO] - Train, Round 8 : loss => 1.6167554648085074,  accuracy: 0.436010101010101, gradient_norm : 0.4687106100062332
[2025-09-17 16:04:14,387][flp2p.graph_runner][INFO] - Test, Round 8 : loss => 1.3258917787144526,  accuracy: 0.5
[2025-09-17 16:04:17,190][flp2p.graph_runner][INFO] - Train, Round 9 : loss => 1.537845425714146,  accuracy: 0.4553030303030302, gradient_norm : 0.45946586620321417
[2025-09-17 16:04:17,794][flp2p.graph_runner][INFO] - Test, Round 9 : loss => 1.31172356828707,  accuracy: 0.5388127853881278
[2025-09-17 16:04:21,346][flp2p.graph_runner][INFO] - Train, Round 10 : loss => 1.6147371697606463,  accuracy: 0.4251010101010101, gradient_norm : 0.3756670017898322
[2025-09-17 16:04:22,247][flp2p.graph_runner][INFO] - Test, Round 10 : loss => 1.5123666840791703,  accuracy: 0.48818181818181816
[2025-09-17 16:04:26,112][flp2p.graph_runner][INFO] - Train, Round 11 : loss => 1.628314599214178,  accuracy: 0.44328282828282833, gradient_norm : 0.4457729173345607
[2025-09-17 16:04:27,028][flp2p.graph_runner][INFO] - Test, Round 11 : loss => 1.4022742301931364,  accuracy: 0.5191256830601093
[2025-09-17 16:04:30,557][flp2p.graph_runner][INFO] - Train, Round 12 : loss => 1.4589828275369876,  accuracy: 0.4662626262626262, gradient_norm : 0.3647261940061703
[2025-09-17 16:04:31,206][flp2p.graph_runner][INFO] - Test, Round 12 : loss => 1.4513498386547092,  accuracy: 0.45196706312900276
[2025-09-17 16:04:34,035][flp2p.graph_runner][INFO] - Train, Round 13 : loss => 1.4648338105190883,  accuracy: 0.4697474747474747, gradient_norm : 0.3433025650300941
[2025-09-17 16:04:34,680][flp2p.graph_runner][INFO] - Test, Round 13 : loss => 1.3949580263458907,  accuracy: 0.5096065873741995
[2025-09-17 16:04:37,483][flp2p.graph_runner][INFO] - Train, Round 14 : loss => 1.7386607000773604,  accuracy: 0.37696969696969707, gradient_norm : 0.3251379462795422
[2025-09-17 16:04:38,147][flp2p.graph_runner][INFO] - Test, Round 14 : loss => 1.6356899401534608,  accuracy: 0.44871794871794873
[2025-09-17 16:04:40,971][flp2p.graph_runner][INFO] - Train, Round 15 : loss => 1.5013759380940235,  accuracy: 0.4643434343434342, gradient_norm : 0.4566854168673285
[2025-09-17 16:04:41,626][flp2p.graph_runner][INFO] - Test, Round 15 : loss => 1.3070581151452256,  accuracy: 0.5247252747252747
[2025-09-17 16:04:44,894][flp2p.graph_runner][INFO] - Train, Round 16 : loss => 1.2489386928578219,  accuracy: 0.5484999999999999, gradient_norm : 0.4337219766585358
[2025-09-17 16:04:45,743][flp2p.graph_runner][INFO] - Test, Round 16 : loss => 1.1073341835552537,  accuracy: 0.5925553319919518
[2025-09-17 16:04:48,865][flp2p.graph_runner][INFO] - Train, Round 17 : loss => 1.4482277024875987,  accuracy: 0.5057575757575757, gradient_norm : 0.4972600956695745
[2025-09-17 16:04:49,517][flp2p.graph_runner][INFO] - Test, Round 17 : loss => 1.217494470810173,  accuracy: 0.5505925250683683
[2025-09-17 16:04:52,150][flp2p.graph_runner][INFO] - Train, Round 18 : loss => 1.087074881196022,  accuracy: 0.5838333333333333, gradient_norm : 0.42659251729828496
[2025-09-17 16:04:52,717][flp2p.graph_runner][INFO] - Test, Round 18 : loss => 0.9931900033142526,  accuracy: 0.6248746238716149
[2025-09-17 16:04:55,555][flp2p.graph_runner][INFO] - Train, Round 19 : loss => 1.4153405970244697,  accuracy: 0.49666666666666676, gradient_norm : 0.43120801183865454
[2025-09-17 16:04:56,228][flp2p.graph_runner][INFO] - Test, Round 19 : loss => 1.2741195596072263,  accuracy: 0.5375457875457875
[2025-09-17 16:04:59,061][flp2p.graph_runner][INFO] - Train, Round 20 : loss => 1.2772340600011927,  accuracy: 0.5474242424242424, gradient_norm : 0.3707354829516717
[2025-09-17 16:04:59,836][flp2p.graph_runner][INFO] - Test, Round 20 : loss => 1.19852877009493,  accuracy: 0.5663311985361391
[2025-09-17 16:05:02,882][flp2p.graph_runner][INFO] - Train, Round 21 : loss => 1.288576772741296,  accuracy: 0.5305555555555554, gradient_norm : 0.3957330164555948
[2025-09-17 16:05:03,600][flp2p.graph_runner][INFO] - Test, Round 21 : loss => 1.2238768688573018,  accuracy: 0.5574817518248175
[2025-09-17 16:05:06,760][flp2p.graph_runner][INFO] - Train, Round 22 : loss => 0.9799576149971196,  accuracy: 0.6506060606060606, gradient_norm : 0.3968431233860434
[2025-09-17 16:05:07,552][flp2p.graph_runner][INFO] - Test, Round 22 : loss => 0.9571553509257156,  accuracy: 0.6538812785388128
[2025-09-17 16:05:10,764][flp2p.graph_runner][INFO] - Train, Round 23 : loss => 1.1197108038447119,  accuracy: 0.5843434343434343, gradient_norm : 0.33677316516047157
[2025-09-17 16:05:11,550][flp2p.graph_runner][INFO] - Test, Round 23 : loss => 1.1458162203199365,  accuracy: 0.5782250686184812
[2025-09-17 16:05:14,684][flp2p.graph_runner][INFO] - Train, Round 24 : loss => 1.0592788547729002,  accuracy: 0.6122222222222221, gradient_norm : 0.37068301852225977
[2025-09-17 16:05:15,480][flp2p.graph_runner][INFO] - Test, Round 24 : loss => 1.144756109531277,  accuracy: 0.5974382433668801
[2025-09-17 16:05:18,557][flp2p.graph_runner][INFO] - Train, Round 25 : loss => 0.9074844718656757,  accuracy: 0.6614141414141415, gradient_norm : 0.3810917327435513
[2025-09-17 16:05:19,387][flp2p.graph_runner][INFO] - Test, Round 25 : loss => 1.0138550191927866,  accuracy: 0.6278434940855323
[2025-09-17 16:05:22,570][flp2p.graph_runner][INFO] - Train, Round 26 : loss => 0.8213555533777583,  accuracy: 0.6872222222222223, gradient_norm : 0.31817418077931536
[2025-09-17 16:05:23,396][flp2p.graph_runner][INFO] - Test, Round 26 : loss => 0.9252719414255974,  accuracy: 0.6623970722781336
[2025-09-17 16:05:26,521][flp2p.graph_runner][INFO] - Train, Round 27 : loss => 1.0493642879029115,  accuracy: 0.605959595959596, gradient_norm : 0.3519092185585743
[2025-09-17 16:05:27,317][flp2p.graph_runner][INFO] - Test, Round 27 : loss => 1.0796852634578455,  accuracy: 0.5974499089253188
[2025-09-17 16:05:30,483][flp2p.graph_runner][INFO] - Train, Round 28 : loss => 0.761557026399356,  accuracy: 0.7081313131313132, gradient_norm : 0.32281398153743823
[2025-09-17 16:05:31,297][flp2p.graph_runner][INFO] - Test, Round 28 : loss => 0.9168079266954576,  accuracy: 0.6618049225159526
[2025-09-17 16:05:34,467][flp2p.graph_runner][INFO] - Train, Round 29 : loss => 1.0248783206623613,  accuracy: 0.6297474747474747, gradient_norm : 0.3348670465010943
[2025-09-17 16:05:35,308][flp2p.graph_runner][INFO] - Test, Round 29 : loss => 1.0804850952547105,  accuracy: 0.6202185792349727
[2025-09-17 16:05:38,168][flp2p.graph_runner][INFO] - Train, Round 30 : loss => 0.855157150713106,  accuracy: 0.6832222222222222, gradient_norm : 0.33552706264600074
[2025-09-17 16:05:38,887][flp2p.graph_runner][INFO] - Test, Round 30 : loss => 0.9603717103152571,  accuracy: 0.6513026052104208
[2025-09-17 16:05:41,775][flp2p.graph_runner][INFO] - Train, Round 31 : loss => 0.9670531870797276,  accuracy: 0.6471666666666668, gradient_norm : 0.31898627617842307
[2025-09-17 16:05:42,516][flp2p.graph_runner][INFO] - Test, Round 31 : loss => 1.1050183587524307,  accuracy: 0.6285140562248996
[2025-09-17 16:05:45,659][flp2p.graph_runner][INFO] - Train, Round 32 : loss => 0.9131970028138974,  accuracy: 0.6536868686868686, gradient_norm : 0.35146579454377636
[2025-09-17 16:05:46,496][flp2p.graph_runner][INFO] - Test, Round 32 : loss => 0.976794308315129,  accuracy: 0.6374429223744292
[2025-09-17 16:05:49,366][flp2p.graph_runner][INFO] - Train, Round 33 : loss => 1.070850218925625,  accuracy: 0.6123888888888889, gradient_norm : 0.36690092582758915
[2025-09-17 16:05:50,104][flp2p.graph_runner][INFO] - Test, Round 33 : loss => 1.094697672009468,  accuracy: 0.607
[2025-09-17 16:05:53,285][flp2p.graph_runner][INFO] - Train, Round 34 : loss => 0.8285482446747747,  accuracy: 0.6807070707070707, gradient_norm : 0.33648038938503577
[2025-09-17 16:05:54,097][flp2p.graph_runner][INFO] - Test, Round 34 : loss => 0.9947659069801862,  accuracy: 0.6492673992673993
[2025-09-17 16:05:57,242][flp2p.graph_runner][INFO] - Train, Round 35 : loss => 0.8922138939696279,  accuracy: 0.6695959595959595, gradient_norm : 0.29915616245843907
[2025-09-17 16:05:58,064][flp2p.graph_runner][INFO] - Test, Round 35 : loss => 1.025795111373135,  accuracy: 0.6465753424657534
[2025-09-17 16:06:01,235][flp2p.graph_runner][INFO] - Train, Round 36 : loss => 0.8109034792237887,  accuracy: 0.6947979797979799, gradient_norm : 0.3181590409560002
[2025-09-17 16:06:02,047][flp2p.graph_runner][INFO] - Test, Round 36 : loss => 0.9935507194092285,  accuracy: 0.6328767123287671
[2025-09-17 16:06:05,193][flp2p.graph_runner][INFO] - Train, Round 37 : loss => 0.817515131814236,  accuracy: 0.6989898989898989, gradient_norm : 0.30164144914302465
[2025-09-17 16:06:06,005][flp2p.graph_runner][INFO] - Test, Round 37 : loss => 0.9831624516784818,  accuracy: 0.6636197440585009
[2025-09-17 16:06:09,233][flp2p.graph_runner][INFO] - Train, Round 38 : loss => 0.9817437981114243,  accuracy: 0.6334848484848485, gradient_norm : 0.3976038147284495
[2025-09-17 16:06:10,057][flp2p.graph_runner][INFO] - Test, Round 38 : loss => 1.040779422133998,  accuracy: 0.6429872495446266
[2025-09-17 16:06:12,869][flp2p.graph_runner][INFO] - Train, Round 39 : loss => 0.6339686402600879,  accuracy: 0.7547777777777777, gradient_norm : 0.2583619178914768
[2025-09-17 16:06:13,613][flp2p.graph_runner][INFO] - Test, Round 39 : loss => 0.917973591726546,  accuracy: 0.6980942828485457
[2025-09-17 16:06:16,819][flp2p.graph_runner][INFO] - Train, Round 40 : loss => 1.020662208365728,  accuracy: 0.6316666666666665, gradient_norm : 0.280440766289543
[2025-09-17 16:06:17,639][flp2p.graph_runner][INFO] - Test, Round 40 : loss => 1.2009675853710244,  accuracy: 0.6096014492753623
[2025-09-17 16:06:20,783][flp2p.graph_runner][INFO] - Train, Round 41 : loss => 0.791352094133441,  accuracy: 0.7029797979797977, gradient_norm : 0.30265024429048554
[2025-09-17 16:06:21,629][flp2p.graph_runner][INFO] - Test, Round 41 : loss => 1.0426129197334721,  accuracy: 0.6122262773722628
[2025-09-17 16:06:24,794][flp2p.graph_runner][INFO] - Train, Round 42 : loss => 0.8832156859719279,  accuracy: 0.6765151515151515, gradient_norm : 0.3494557928507871
[2025-09-17 16:06:25,605][flp2p.graph_runner][INFO] - Test, Round 42 : loss => 0.9945228559013685,  accuracy: 0.6535288725939505
[2025-09-17 16:06:28,768][flp2p.graph_runner][INFO] - Train, Round 43 : loss => 0.8770541431671572,  accuracy: 0.6726262626262626, gradient_norm : 0.32824898994300283
[2025-09-17 16:06:29,592][flp2p.graph_runner][INFO] - Test, Round 43 : loss => 1.0762066966208228,  accuracy: 0.6468978102189781
[2025-09-17 16:06:32,474][flp2p.graph_runner][INFO] - Train, Round 44 : loss => 0.7230426750308834,  accuracy: 0.7236666666666667, gradient_norm : 0.2927923236614853
[2025-09-17 16:06:33,225][flp2p.graph_runner][INFO] - Test, Round 44 : loss => 0.9674294216848498,  accuracy: 0.6804020100502512
[2025-09-17 16:06:36,375][flp2p.graph_runner][INFO] - Train, Round 45 : loss => 0.8162538059041694,  accuracy: 0.700050505050505, gradient_norm : 0.3057652200532394
[2025-09-17 16:06:37,212][flp2p.graph_runner][INFO] - Test, Round 45 : loss => 0.9771660279124272,  accuracy: 0.6805807622504537
[2025-09-17 16:06:40,387][flp2p.graph_runner][INFO] - Train, Round 46 : loss => 0.6557318036384514,  accuracy: 0.7467676767676769, gradient_norm : 0.25207091607590276
[2025-09-17 16:06:41,205][flp2p.graph_runner][INFO] - Test, Round 46 : loss => 0.9084724607594284,  accuracy: 0.7097069597069597
[2025-09-17 16:06:44,408][flp2p.graph_runner][INFO] - Train, Round 47 : loss => 0.7652213747094555,  accuracy: 0.7108080808080809, gradient_norm : 0.2825875178810741
[2025-09-17 16:06:45,244][flp2p.graph_runner][INFO] - Test, Round 47 : loss => 0.9891955865335552,  accuracy: 0.6767399267399268
[2025-09-17 16:06:48,397][flp2p.graph_runner][INFO] - Train, Round 48 : loss => 0.7524129657789176,  accuracy: 0.7151010101010101, gradient_norm : 0.27741949179572595
[2025-09-17 16:06:49,200][flp2p.graph_runner][INFO] - Test, Round 48 : loss => 1.0009098270931747,  accuracy: 0.6775956284153005
[2025-09-17 16:06:52,383][flp2p.graph_runner][INFO] - Train, Round 49 : loss => 0.6723944421462489,  accuracy: 0.7352020202020202, gradient_norm : 0.25392017886051277
[2025-09-17 16:06:53,223][flp2p.graph_runner][INFO] - Test, Round 49 : loss => 0.926813123737528,  accuracy: 0.6967213114754098
[2025-09-17 16:06:56,110][flp2p.graph_runner][INFO] - Train, Round 50 : loss => 0.6054256680151836,  accuracy: 0.7642222222222221, gradient_norm : 0.2157933794875669
[2025-09-17 16:06:56,858][flp2p.graph_runner][INFO] - Test, Round 50 : loss => 0.9231023251470666,  accuracy: 0.7112676056338029
[2025-09-17 16:07:00,070][flp2p.graph_runner][INFO] - Train, Round 51 : loss => 0.7258487705832742,  accuracy: 0.7181818181818181, gradient_norm : 0.2755563004673442
[2025-09-17 16:07:00,911][flp2p.graph_runner][INFO] - Test, Round 51 : loss => 0.9709050971172601,  accuracy: 0.6735985533453888
[2025-09-17 16:07:04,100][flp2p.graph_runner][INFO] - Train, Round 52 : loss => 0.5493524454606372,  accuracy: 0.7859090909090908, gradient_norm : 0.2163046007706092
[2025-09-17 16:07:04,950][flp2p.graph_runner][INFO] - Test, Round 52 : loss => 0.8904034162524843,  accuracy: 0.7135036496350365
[2025-09-17 16:07:08,109][flp2p.graph_runner][INFO] - Train, Round 53 : loss => 0.5925498586673656,  accuracy: 0.7711111111111111, gradient_norm : 0.2311532634968754
[2025-09-17 16:07:08,935][flp2p.graph_runner][INFO] - Test, Round 53 : loss => 0.9387187309542673,  accuracy: 0.6993583868011
[2025-09-17 16:07:12,087][flp2p.graph_runner][INFO] - Train, Round 54 : loss => 0.6529839162499588,  accuracy: 0.7436868686868685, gradient_norm : 0.24440423413620074
[2025-09-17 16:07:12,910][flp2p.graph_runner][INFO] - Test, Round 54 : loss => 1.0108480606047472,  accuracy: 0.666971637694419
[2025-09-17 16:07:16,071][flp2p.graph_runner][INFO] - Train, Round 55 : loss => 0.6263336659357072,  accuracy: 0.7551515151515151, gradient_norm : 0.23172130410449937
[2025-09-17 16:07:16,882][flp2p.graph_runner][INFO] - Test, Round 55 : loss => 0.9056609512188217,  accuracy: 0.7127272727272728
[2025-09-17 16:07:19,721][flp2p.graph_runner][INFO] - Train, Round 56 : loss => 0.6046672286613224,  accuracy: 0.7676111111111111, gradient_norm : 0.23932849844156356
[2025-09-17 16:07:20,485][flp2p.graph_runner][INFO] - Test, Round 56 : loss => 0.9004061231641856,  accuracy: 0.7191574724172518
[2025-09-17 16:07:23,662][flp2p.graph_runner][INFO] - Train, Round 57 : loss => 0.6596268100566566,  accuracy: 0.7452525252525253, gradient_norm : 0.2663161634461588
[2025-09-17 16:07:24,484][flp2p.graph_runner][INFO] - Test, Round 57 : loss => 1.0069270558437726,  accuracy: 0.666970802919708
[2025-09-17 16:07:27,655][flp2p.graph_runner][INFO] - Train, Round 58 : loss => 0.566497289474214,  accuracy: 0.781969696969697, gradient_norm : 0.226004422739829
[2025-09-17 16:07:28,500][flp2p.graph_runner][INFO] - Test, Round 58 : loss => 0.983270869632598,  accuracy: 0.6987295825771325
[2025-09-17 16:07:31,661][flp2p.graph_runner][INFO] - Train, Round 59 : loss => 0.5889401881631894,  accuracy: 0.7708080808080809, gradient_norm : 0.21898148256846203
[2025-09-17 16:07:32,502][flp2p.graph_runner][INFO] - Test, Round 59 : loss => 0.9770326910503141,  accuracy: 0.6958105646630237
[2025-09-17 16:07:35,701][flp2p.graph_runner][INFO] - Train, Round 60 : loss => 0.6021209244088933,  accuracy: 0.7682323232323232, gradient_norm : 0.21078233225746426
[2025-09-17 16:07:36,544][flp2p.graph_runner][INFO] - Test, Round 60 : loss => 0.9053766738680901,  accuracy: 0.7138964577656676
[2025-09-17 16:07:39,687][flp2p.graph_runner][INFO] - Train, Round 61 : loss => 0.5438672759769498,  accuracy: 0.7905050505050505, gradient_norm : 0.2098147954531959
[2025-09-17 16:07:40,532][flp2p.graph_runner][INFO] - Test, Round 61 : loss => 0.9005879521424417,  accuracy: 0.7239488117001828
[2025-09-17 16:07:43,426][flp2p.graph_runner][INFO] - Train, Round 62 : loss => 0.5821614843448818,  accuracy: 0.7777222222222222, gradient_norm : 0.21896560301550932
[2025-09-17 16:07:44,167][flp2p.graph_runner][INFO] - Test, Round 62 : loss => 0.9386911499739892,  accuracy: 0.7015075376884422
[2025-09-17 16:07:47,320][flp2p.graph_runner][INFO] - Train, Round 63 : loss => 0.560734410030912,  accuracy: 0.7864141414141412, gradient_norm : 0.21057711010755675
[2025-09-17 16:07:48,167][flp2p.graph_runner][INFO] - Test, Round 63 : loss => 0.9093269216513067,  accuracy: 0.7148080438756855
[2025-09-17 16:07:51,348][flp2p.graph_runner][INFO] - Train, Round 64 : loss => 0.5207628398054195,  accuracy: 0.7980808080808082, gradient_norm : 0.2011842410121871
[2025-09-17 16:07:52,166][flp2p.graph_runner][INFO] - Test, Round 64 : loss => 0.9141907024601279,  accuracy: 0.7269406392694064
[2025-09-17 16:07:55,361][flp2p.graph_runner][INFO] - Train, Round 65 : loss => 0.5609268710999679,  accuracy: 0.7825252525252526, gradient_norm : 0.19502244086901788
[2025-09-17 16:07:56,176][flp2p.graph_runner][INFO] - Test, Round 65 : loss => 0.9051944192254195,  accuracy: 0.7235401459854015
[2025-09-17 16:07:59,362][flp2p.graph_runner][INFO] - Train, Round 66 : loss => 0.5425965585754691,  accuracy: 0.7894949494949494, gradient_norm : 0.2218995602728791
[2025-09-17 16:08:00,191][flp2p.graph_runner][INFO] - Test, Round 66 : loss => 0.9669312785883419,  accuracy: 0.7058287795992714
[2025-09-17 16:08:03,348][flp2p.graph_runner][INFO] - Train, Round 67 : loss => 0.5210681892897066,  accuracy: 0.7952525252525253, gradient_norm : 0.19018648981482714
[2025-09-17 16:08:04,171][flp2p.graph_runner][INFO] - Test, Round 67 : loss => 0.902157265977426,  accuracy: 0.7163636363636363
[2025-09-17 16:08:07,346][flp2p.graph_runner][INFO] - Train, Round 68 : loss => 0.5634193663635718,  accuracy: 0.7805555555555554, gradient_norm : 0.208804301151206
[2025-09-17 16:08:08,139][flp2p.graph_runner][INFO] - Test, Round 68 : loss => 0.8761705952019639,  accuracy: 0.7240437158469946
[2025-09-17 16:08:11,316][flp2p.graph_runner][INFO] - Train, Round 69 : loss => 0.7726150615370099,  accuracy: 0.7113131313131312, gradient_norm : 0.22567272414843573
[2025-09-17 16:08:12,148][flp2p.graph_runner][INFO] - Test, Round 69 : loss => 1.069984274869827,  accuracy: 0.6548592188919165
[2025-09-17 16:08:15,026][flp2p.graph_runner][INFO] - Train, Round 70 : loss => 0.5731051636640526,  accuracy: 0.7748333333333334, gradient_norm : 0.22303034508269995
[2025-09-17 16:08:15,779][flp2p.graph_runner][INFO] - Test, Round 70 : loss => 0.8442441683206985,  accuracy: 0.7442786069651741
[2025-09-17 16:08:18,938][flp2p.graph_runner][INFO] - Train, Round 71 : loss => 0.501116444930526,  accuracy: 0.8017171717171717, gradient_norm : 0.18527897179671007
[2025-09-17 16:08:19,752][flp2p.graph_runner][INFO] - Test, Round 71 : loss => 0.8625287146078072,  accuracy: 0.7364130434782609
[2025-09-17 16:08:22,612][flp2p.graph_runner][INFO] - Train, Round 72 : loss => 0.47904622254980495,  accuracy: 0.8152222222222223, gradient_norm : 0.1962182805709335
[2025-09-17 16:08:23,352][flp2p.graph_runner][INFO] - Test, Round 72 : loss => 0.967226573300721,  accuracy: 0.7185929648241206
[2025-09-17 16:08:26,540][flp2p.graph_runner][INFO] - Train, Round 73 : loss => 0.5377268581983641,  accuracy: 0.7864646464646464, gradient_norm : 0.2104160935263198
[2025-09-17 16:08:27,374][flp2p.graph_runner][INFO] - Test, Round 73 : loss => 0.9163893793784353,  accuracy: 0.7071167883211679
[2025-09-17 16:08:30,528][flp2p.graph_runner][INFO] - Train, Round 74 : loss => 0.6010962127262963,  accuracy: 0.7697474747474747, gradient_norm : 0.2249592106428918
[2025-09-17 16:08:31,355][flp2p.graph_runner][INFO] - Test, Round 74 : loss => 0.9602100533041833,  accuracy: 0.7119565217391305
[2025-09-17 16:08:34,569][flp2p.graph_runner][INFO] - Train, Round 75 : loss => 0.5178621916998633,  accuracy: 0.7953030303030303, gradient_norm : 0.2022299055119646
[2025-09-17 16:08:35,395][flp2p.graph_runner][INFO] - Test, Round 75 : loss => 0.896567503440434,  accuracy: 0.7158371040723982
[2025-09-17 16:08:38,489][flp2p.graph_runner][INFO] - Train, Round 76 : loss => 0.5102505798313138,  accuracy: 0.8012626262626262, gradient_norm : 0.1953398883614209
[2025-09-17 16:08:39,318][flp2p.graph_runner][INFO] - Test, Round 76 : loss => 0.8885344663167108,  accuracy: 0.719634703196347
[2025-09-17 16:08:42,490][flp2p.graph_runner][INFO] - Train, Round 77 : loss => 0.5265806799533135,  accuracy: 0.7938888888888888, gradient_norm : 0.21338328354896238
[2025-09-17 16:08:43,320][flp2p.graph_runner][INFO] - Test, Round 77 : loss => 0.9752270535559114,  accuracy: 0.6916058394160584
[2025-09-17 16:08:46,478][flp2p.graph_runner][INFO] - Train, Round 78 : loss => 0.4907951551521282,  accuracy: 0.8101515151515152, gradient_norm : 0.2105159150171483
[2025-09-17 16:08:47,287][flp2p.graph_runner][INFO] - Test, Round 78 : loss => 0.9628245273373858,  accuracy: 0.7071167883211679
[2025-09-17 16:08:50,453][flp2p.graph_runner][INFO] - Train, Round 79 : loss => 0.5159009683850331,  accuracy: 0.8017676767676767, gradient_norm : 0.20106685590438264
[2025-09-17 16:08:51,264][flp2p.graph_runner][INFO] - Test, Round 79 : loss => 0.9163585794200845,  accuracy: 0.726691042047532
[2025-09-17 16:08:54,404][flp2p.graph_runner][INFO] - Train, Round 80 : loss => 0.5367824472496233,  accuracy: 0.7937373737373737, gradient_norm : 0.20585499828277404
[2025-09-17 16:08:55,241][flp2p.graph_runner][INFO] - Test, Round 80 : loss => 0.9950846062940939,  accuracy: 0.7024567788898999
[2025-09-17 16:08:58,399][flp2p.graph_runner][INFO] - Train, Round 81 : loss => 0.4569168889729719,  accuracy: 0.8174242424242424, gradient_norm : 0.1706096951134797
[2025-09-17 16:08:59,259][flp2p.graph_runner][INFO] - Test, Round 81 : loss => 0.8949257950532382,  accuracy: 0.736986301369863
[2025-09-17 16:09:02,409][flp2p.graph_runner][INFO] - Train, Round 82 : loss => 0.46105391153325787,  accuracy: 0.8196464646464647, gradient_norm : 0.19409516544313166
[2025-09-17 16:09:03,240][flp2p.graph_runner][INFO] - Test, Round 82 : loss => 0.9277546500975268,  accuracy: 0.7306642402183804
[2025-09-17 16:09:06,109][flp2p.graph_runner][INFO] - Train, Round 83 : loss => 0.4737073852694205,  accuracy: 0.8132777777777779, gradient_norm : 0.17821570608282047
[2025-09-17 16:09:06,848][flp2p.graph_runner][INFO] - Test, Round 83 : loss => 0.9170491689834932,  accuracy: 0.7242424242424242
[2025-09-17 16:09:10,010][flp2p.graph_runner][INFO] - Train, Round 84 : loss => 0.3966795296577567,  accuracy: 0.8462626262626263, gradient_norm : 0.18030758321223847
[2025-09-17 16:09:10,839][flp2p.graph_runner][INFO] - Test, Round 84 : loss => 0.9328359062659642,  accuracy: 0.7323556370302475
[2025-09-17 16:09:13,947][flp2p.graph_runner][INFO] - Train, Round 85 : loss => 0.45604231629779063,  accuracy: 0.8169696969696971, gradient_norm : 0.17123105151379267
[2025-09-17 16:09:14,778][flp2p.graph_runner][INFO] - Test, Round 85 : loss => 0.8673326219053573,  accuracy: 0.7488584474885844
[2025-09-17 16:09:17,955][flp2p.graph_runner][INFO] - Train, Round 86 : loss => 0.4327724668749425,  accuracy: 0.8338383838383838, gradient_norm : 0.18676434812341078
[2025-09-17 16:09:18,776][flp2p.graph_runner][INFO] - Test, Round 86 : loss => 0.930619301563861,  accuracy: 0.7221206581352834
[2025-09-17 16:09:21,677][flp2p.graph_runner][INFO] - Train, Round 87 : loss => 0.5395078653003536,  accuracy: 0.7925, gradient_norm : 0.2205731179297969
[2025-09-17 16:09:22,458][flp2p.graph_runner][INFO] - Test, Round 87 : loss => 0.9663992308841679,  accuracy: 0.7124248496993988
[2025-09-17 16:09:25,646][flp2p.graph_runner][INFO] - Train, Round 88 : loss => 0.4360934364976305,  accuracy: 0.8315656565656566, gradient_norm : 0.17278730334291778
[2025-09-17 16:09:26,458][flp2p.graph_runner][INFO] - Test, Round 88 : loss => 0.8661858214635283,  accuracy: 0.7388127853881279
[2025-09-17 16:09:29,667][flp2p.graph_runner][INFO] - Train, Round 89 : loss => 0.4612702187320048,  accuracy: 0.8193939393939393, gradient_norm : 0.191758152060869
[2025-09-17 16:09:30,499][flp2p.graph_runner][INFO] - Test, Round 89 : loss => 0.9141169687699188,  accuracy: 0.7263636363636363
[2025-09-17 16:09:33,674][flp2p.graph_runner][INFO] - Train, Round 90 : loss => 0.4783883203523697,  accuracy: 0.8148484848484848, gradient_norm : 0.20131031237962532
[2025-09-17 16:09:34,500][flp2p.graph_runner][INFO] - Test, Round 90 : loss => 0.9938351176070648,  accuracy: 0.7232304900181489
[2025-09-17 16:09:37,644][flp2p.graph_runner][INFO] - Train, Round 91 : loss => 0.421303241549454,  accuracy: 0.8314646464646464, gradient_norm : 0.1795312906430004
[2025-09-17 16:09:38,466][flp2p.graph_runner][INFO] - Test, Round 91 : loss => 0.9527380310366751,  accuracy: 0.7385740402193784
[2025-09-17 16:09:41,637][flp2p.graph_runner][INFO] - Train, Round 92 : loss => 0.45587312859261697,  accuracy: 0.8186363636363636, gradient_norm : 0.18455780705694141
[2025-09-17 16:09:42,467][flp2p.graph_runner][INFO] - Test, Round 92 : loss => 0.9061364452819426,  accuracy: 0.7323049001814882
[2025-09-17 16:09:45,647][flp2p.graph_runner][INFO] - Train, Round 93 : loss => 0.4879188700985678,  accuracy: 0.8085858585858586, gradient_norm : 0.19960569820282592
[2025-09-17 16:09:46,453][flp2p.graph_runner][INFO] - Test, Round 93 : loss => 0.946899062797536,  accuracy: 0.7381386861313869
[2025-09-17 16:09:49,621][flp2p.graph_runner][INFO] - Train, Round 94 : loss => 0.434450039084715,  accuracy: 0.8278282828282827, gradient_norm : 0.16603217502919965
[2025-09-17 16:09:50,449][flp2p.graph_runner][INFO] - Test, Round 94 : loss => 0.8796326807420108,  accuracy: 0.7302452316076294
[2025-09-17 16:09:53,626][flp2p.graph_runner][INFO] - Train, Round 95 : loss => 0.4275091285668058,  accuracy: 0.8330808080808081, gradient_norm : 0.16784651348648869
[2025-09-17 16:09:54,448][flp2p.graph_runner][INFO] - Test, Round 95 : loss => 0.8891221199330149,  accuracy: 0.7493188010899182
[2025-09-17 16:09:57,621][flp2p.graph_runner][INFO] - Train, Round 96 : loss => 0.4442072620546572,  accuracy: 0.8262121212121212, gradient_norm : 0.1768865283597562
[2025-09-17 16:09:58,448][flp2p.graph_runner][INFO] - Test, Round 96 : loss => 0.8759660044698602,  accuracy: 0.7445255474452555
[2025-09-17 16:10:01,614][flp2p.graph_runner][INFO] - Train, Round 97 : loss => 0.45725698160809275,  accuracy: 0.826010101010101, gradient_norm : 0.19756854005094046
[2025-09-17 16:10:02,413][flp2p.graph_runner][INFO] - Test, Round 97 : loss => 0.9365684235074224,  accuracy: 0.7281021897810219
[2025-09-17 16:10:05,581][flp2p.graph_runner][INFO] - Train, Round 98 : loss => 0.42340319610256383,  accuracy: 0.8319696969696969, gradient_norm : 0.17386339566787515
[2025-09-17 16:10:06,427][flp2p.graph_runner][INFO] - Test, Round 98 : loss => 0.9487135797875113,  accuracy: 0.7223744292237443
[2025-09-17 16:10:09,588][flp2p.graph_runner][INFO] - Train, Round 99 : loss => 0.41795979896783,  accuracy: 0.8352525252525252, gradient_norm : 0.1740220089259121
[2025-09-17 16:10:10,432][flp2p.graph_runner][INFO] - Test, Round 99 : loss => 0.8831372085914733,  accuracy: 0.7447774750227066
[2025-09-17 16:10:13,270][flp2p.graph_runner][INFO] - Train, Round 100 : loss => 0.43286004494293595,  accuracy: 0.8272777777777778, gradient_norm : 0.1793172729447679
[2025-09-17 16:10:14,010][flp2p.graph_runner][INFO] - Test, Round 100 : loss => 1.0112854476958033,  accuracy: 0.7086693548387096
[2025-09-17 16:10:17,168][flp2p.graph_runner][INFO] - Train, Round 101 : loss => 0.3619143280596428,  accuracy: 0.8605555555555556, gradient_norm : 0.1710940038155018
[2025-09-17 16:10:18,008][flp2p.graph_runner][INFO] - Test, Round 101 : loss => 0.9656306287177158,  accuracy: 0.7452229299363057
[2025-09-17 16:10:20,854][flp2p.graph_runner][INFO] - Train, Round 102 : loss => 0.3973412820805,  accuracy: 0.8435555555555556, gradient_norm : 0.16984482169323573
[2025-09-17 16:10:21,606][flp2p.graph_runner][INFO] - Test, Round 102 : loss => 0.8971092644704872,  accuracy: 0.7359437751004017
[2025-09-17 16:10:24,794][flp2p.graph_runner][INFO] - Train, Round 103 : loss => 0.45228157865732294,  accuracy: 0.8247979797979799, gradient_norm : 0.17777705589392362
[2025-09-17 16:10:25,620][flp2p.graph_runner][INFO] - Test, Round 103 : loss => 0.9162352117122419,  accuracy: 0.7312614259597806
[2025-09-17 16:10:28,795][flp2p.graph_runner][INFO] - Train, Round 104 : loss => 0.40582410518056183,  accuracy: 0.8426767676767677, gradient_norm : 0.17516302390221272
[2025-09-17 16:10:29,612][flp2p.graph_runner][INFO] - Test, Round 104 : loss => 0.9021781753346013,  accuracy: 0.7420526793823796
[2025-09-17 16:10:32,770][flp2p.graph_runner][INFO] - Train, Round 105 : loss => 0.4041998561536082,  accuracy: 0.8374242424242423, gradient_norm : 0.17619431178391637
[2025-09-17 16:10:33,591][flp2p.graph_runner][INFO] - Test, Round 105 : loss => 0.9330613384119916,  accuracy: 0.72975432211101
[2025-09-17 16:10:36,777][flp2p.graph_runner][INFO] - Train, Round 106 : loss => 0.40955517496631455,  accuracy: 0.8382828282828283, gradient_norm : 0.1665894170165573
[2025-09-17 16:10:37,603][flp2p.graph_runner][INFO] - Test, Round 106 : loss => 0.8820050766708654,  accuracy: 0.7541284403669725
[2025-09-17 16:10:40,475][flp2p.graph_runner][INFO] - Train, Round 107 : loss => 0.34937225128223265,  accuracy: 0.856611111111111, gradient_norm : 0.15171527404904622
[2025-09-17 16:10:41,210][flp2p.graph_runner][INFO] - Test, Round 107 : loss => 0.928540785247891,  accuracy: 0.7464788732394366
[2025-09-17 16:10:44,399][flp2p.graph_runner][INFO] - Train, Round 108 : loss => 0.4285715626765468,  accuracy: 0.8332323232323232, gradient_norm : 0.18636325340748153
[2025-09-17 16:10:45,229][flp2p.graph_runner][INFO] - Test, Round 108 : loss => 0.9983784762712625,  accuracy: 0.7252747252747253
[2025-09-17 16:10:48,402][flp2p.graph_runner][INFO] - Train, Round 109 : loss => 0.44934558477649594,  accuracy: 0.825858585858586, gradient_norm : 0.18709617447781463
[2025-09-17 16:10:49,256][flp2p.graph_runner][INFO] - Test, Round 109 : loss => 0.911397819395366,  accuracy: 0.7330895795246801
[2025-09-17 16:10:52,425][flp2p.graph_runner][INFO] - Train, Round 110 : loss => 0.4239167610843486,  accuracy: 0.8325757575757577, gradient_norm : 0.16892522087792605
[2025-09-17 16:10:53,241][flp2p.graph_runner][INFO] - Test, Round 110 : loss => 0.9679076274145734,  accuracy: 0.7327272727272728
[2025-09-17 16:10:56,389][flp2p.graph_runner][INFO] - Train, Round 111 : loss => 0.3846015459840904,  accuracy: 0.8470707070707071, gradient_norm : 0.18059030183199595
[2025-09-17 16:10:57,208][flp2p.graph_runner][INFO] - Test, Round 111 : loss => 0.8863043209026449,  accuracy: 0.7557392102846648
[2025-09-17 16:11:00,372][flp2p.graph_runner][INFO] - Train, Round 112 : loss => 0.40931239394880264,  accuracy: 0.8400505050505052, gradient_norm : 0.17621320352458675
[2025-09-17 16:11:01,186][flp2p.graph_runner][INFO] - Test, Round 112 : loss => 0.957597372824209,  accuracy: 0.7274384685505926
[2025-09-17 16:11:04,348][flp2p.graph_runner][INFO] - Train, Round 113 : loss => 0.40166297972177606,  accuracy: 0.8425252525252525, gradient_norm : 0.18704692398197098
[2025-09-17 16:11:05,188][flp2p.graph_runner][INFO] - Test, Round 113 : loss => 0.9717715997181156,  accuracy: 0.73
[2025-09-17 16:11:08,093][flp2p.graph_runner][INFO] - Train, Round 114 : loss => 0.37292881143794754,  accuracy: 0.850888888888889, gradient_norm : 0.15967913752933766
[2025-09-17 16:11:08,880][flp2p.graph_runner][INFO] - Test, Round 114 : loss => 0.920738612951887,  accuracy: 0.755533199195171
[2025-09-17 16:11:12,054][flp2p.graph_runner][INFO] - Train, Round 115 : loss => 0.40672590755346005,  accuracy: 0.8433838383838385, gradient_norm : 0.17622102767102865
[2025-09-17 16:11:12,882][flp2p.graph_runner][INFO] - Test, Round 115 : loss => 0.8732716279167522,  accuracy: 0.7552703941338221
[2025-09-17 16:11:15,770][flp2p.graph_runner][INFO] - Train, Round 116 : loss => 0.3498919549145406,  accuracy: 0.8659444444444445, gradient_norm : 0.1800850966325965
[2025-09-17 16:11:16,544][flp2p.graph_runner][INFO] - Test, Round 116 : loss => 0.9791719636105629,  accuracy: 0.7399598393574297
[2025-09-17 16:11:19,727][flp2p.graph_runner][INFO] - Train, Round 117 : loss => 0.42071340341831137,  accuracy: 0.8342424242424242, gradient_norm : 0.18738300312287
[2025-09-17 16:11:20,572][flp2p.graph_runner][INFO] - Test, Round 117 : loss => 0.988544468260332,  accuracy: 0.7288444040036397
[2025-09-17 16:11:23,685][flp2p.graph_runner][INFO] - Train, Round 118 : loss => 0.40358240515656,  accuracy: 0.8436363636363636, gradient_norm : 0.17186474920329584
[2025-09-17 16:11:24,506][flp2p.graph_runner][INFO] - Test, Round 118 : loss => 0.9316059034385961,  accuracy: 0.7463369963369964
[2025-09-17 16:11:27,658][flp2p.graph_runner][INFO] - Train, Round 119 : loss => 0.3640877388106949,  accuracy: 0.8561616161616161, gradient_norm : 0.1591579189729687
[2025-09-17 16:11:28,496][flp2p.graph_runner][INFO] - Test, Round 119 : loss => 0.9322233116408006,  accuracy: 0.7458715596330275
[2025-09-17 16:11:31,663][flp2p.graph_runner][INFO] - Train, Round 120 : loss => 0.36688548177934893,  accuracy: 0.8586363636363635, gradient_norm : 0.19176729696710068
[2025-09-17 16:11:32,495][flp2p.graph_runner][INFO] - Test, Round 120 : loss => 0.8976275648606485,  accuracy: 0.740673339399454
[2025-09-17 16:11:35,676][flp2p.graph_runner][INFO] - Train, Round 121 : loss => 0.3574338791504102,  accuracy: 0.8591919191919194, gradient_norm : 0.1622674834380207
[2025-09-17 16:11:36,529][flp2p.graph_runner][INFO] - Test, Round 121 : loss => 0.9064468243078554,  accuracy: 0.7625570776255708
[2025-09-17 16:11:39,708][flp2p.graph_runner][INFO] - Train, Round 122 : loss => 0.37696951043275695,  accuracy: 0.8505555555555555, gradient_norm : 0.17692558646865122
[2025-09-17 16:11:40,512][flp2p.graph_runner][INFO] - Test, Round 122 : loss => 0.9689966148981765,  accuracy: 0.7388127853881279
[2025-09-17 16:11:43,693][flp2p.graph_runner][INFO] - Train, Round 123 : loss => 0.3605786519088283,  accuracy: 0.857777777777778, gradient_norm : 0.17062681851129557
[2025-09-17 16:11:44,513][flp2p.graph_runner][INFO] - Test, Round 123 : loss => 0.9538574482219807,  accuracy: 0.7472826086956522
[2025-09-17 16:11:47,692][flp2p.graph_runner][INFO] - Train, Round 124 : loss => 0.3798385139670052,  accuracy: 0.8525252525252525, gradient_norm : 0.17423701896642366
[2025-09-17 16:11:48,519][flp2p.graph_runner][INFO] - Test, Round 124 : loss => 0.9535293380962261,  accuracy: 0.7358834244080146
[2025-09-17 16:11:51,689][flp2p.graph_runner][INFO] - Train, Round 125 : loss => 0.41374371559947826,  accuracy: 0.8357070707070707, gradient_norm : 0.17062109957710417
[2025-09-17 16:11:52,521][flp2p.graph_runner][INFO] - Test, Round 125 : loss => 0.9270914330531698,  accuracy: 0.7119266055045872
[2025-09-17 16:11:55,680][flp2p.graph_runner][INFO] - Train, Round 126 : loss => 0.3403902534433159,  accuracy: 0.8634343434343433, gradient_norm : 0.16206717129697504
[2025-09-17 16:11:56,525][flp2p.graph_runner][INFO] - Test, Round 126 : loss => 0.9346942474772205,  accuracy: 0.7591575091575091
[2025-09-17 16:11:59,724][flp2p.graph_runner][INFO] - Train, Round 127 : loss => 0.3620187555605133,  accuracy: 0.8578282828282827, gradient_norm : 0.17063832319328115
[2025-09-17 16:12:00,563][flp2p.graph_runner][INFO] - Test, Round 127 : loss => 0.9589362216402184,  accuracy: 0.7490909090909091
[2025-09-17 16:12:03,716][flp2p.graph_runner][INFO] - Train, Round 128 : loss => 0.35363912630809036,  accuracy: 0.8615656565656565, gradient_norm : 0.17758232117536318
[2025-09-17 16:12:04,528][flp2p.graph_runner][INFO] - Test, Round 128 : loss => 0.9736739140244589,  accuracy: 0.740673339399454
[2025-09-17 16:12:07,690][flp2p.graph_runner][INFO] - Train, Round 129 : loss => 0.3726695870317626,  accuracy: 0.857020202020202, gradient_norm : 0.17269150146429432
[2025-09-17 16:12:08,552][flp2p.graph_runner][INFO] - Test, Round 129 : loss => 0.9373716308790094,  accuracy: 0.7592760180995475
[2025-09-17 16:12:11,767][flp2p.graph_runner][INFO] - Train, Round 130 : loss => 0.3667635493064925,  accuracy: 0.8572222222222222, gradient_norm : 0.1712207912218275
[2025-09-17 16:12:12,569][flp2p.graph_runner][INFO] - Test, Round 130 : loss => 0.9140748720622888,  accuracy: 0.7422586520947176
[2025-09-17 16:12:15,718][flp2p.graph_runner][INFO] - Train, Round 131 : loss => 0.3567182193829715,  accuracy: 0.8633333333333333, gradient_norm : 0.18211799824563152
[2025-09-17 16:12:16,575][flp2p.graph_runner][INFO] - Test, Round 131 : loss => 0.918717493314995,  accuracy: 0.7486338797814208
[2025-09-17 16:12:19,788][flp2p.graph_runner][INFO] - Train, Round 132 : loss => 0.38928287434984776,  accuracy: 0.8460101010101011, gradient_norm : 0.18221372202037128
[2025-09-17 16:12:20,617][flp2p.graph_runner][INFO] - Test, Round 132 : loss => 0.9431764215747588,  accuracy: 0.7436131386861314
[2025-09-17 16:12:23,803][flp2p.graph_runner][INFO] - Train, Round 133 : loss => 0.3429603032631624,  accuracy: 0.8665151515151515, gradient_norm : 0.1626521362835539
[2025-09-17 16:12:24,651][flp2p.graph_runner][INFO] - Test, Round 133 : loss => 0.9247096035407206,  accuracy: 0.7486437613019892
[2025-09-17 16:12:27,522][flp2p.graph_runner][INFO] - Train, Round 134 : loss => 0.29157940816010525,  accuracy: 0.8852222222222221, gradient_norm : 0.1646979162766152
[2025-09-17 16:12:28,295][flp2p.graph_runner][INFO] - Test, Round 134 : loss => 0.9785522384805841,  accuracy: 0.7637637637637638
[2025-09-17 16:12:31,464][flp2p.graph_runner][INFO] - Train, Round 135 : loss => 0.30110199670620424,  accuracy: 0.8809595959595959, gradient_norm : 0.1712827606271861
[2025-09-17 16:12:32,261][flp2p.graph_runner][INFO] - Test, Round 135 : loss => 0.9204815693703604,  accuracy: 0.7623400365630713
[2025-09-17 16:12:35,432][flp2p.graph_runner][INFO] - Train, Round 136 : loss => 0.3679157416392682,  accuracy: 0.8536868686868686, gradient_norm : 0.16118111942757848
[2025-09-17 16:12:36,270][flp2p.graph_runner][INFO] - Test, Round 136 : loss => 0.922079744232688,  accuracy: 0.7506824385805277
[2025-09-17 16:12:39,450][flp2p.graph_runner][INFO] - Train, Round 137 : loss => 0.31729301334593346,  accuracy: 0.8787373737373738, gradient_norm : 0.16381012620167695
[2025-09-17 16:12:40,283][flp2p.graph_runner][INFO] - Test, Round 137 : loss => 1.0260763739342014,  accuracy: 0.7178082191780822
[2025-09-17 16:12:43,463][flp2p.graph_runner][INFO] - Train, Round 138 : loss => 0.3437207821677668,  accuracy: 0.8632828282828282, gradient_norm : 0.17610551754809756
[2025-09-17 16:12:44,302][flp2p.graph_runner][INFO] - Test, Round 138 : loss => 0.9237297544777664,  accuracy: 0.7452402538531279
[2025-09-17 16:12:47,471][flp2p.graph_runner][INFO] - Train, Round 139 : loss => 0.41661088883745173,  accuracy: 0.8350505050505052, gradient_norm : 0.18800975817962715
[2025-09-17 16:12:48,337][flp2p.graph_runner][INFO] - Test, Round 139 : loss => 0.9964211088516851,  accuracy: 0.7235401459854015
[2025-09-17 16:12:51,495][flp2p.graph_runner][INFO] - Train, Round 140 : loss => 0.36457272385523537,  accuracy: 0.8565151515151517, gradient_norm : 0.18298331797143066
[2025-09-17 16:12:52,327][flp2p.graph_runner][INFO] - Test, Round 140 : loss => 0.9677997148729279,  accuracy: 0.7196007259528131
[2025-09-17 16:12:55,527][flp2p.graph_runner][INFO] - Train, Round 141 : loss => 0.33896974390595325,  accuracy: 0.8669191919191919, gradient_norm : 0.16719158234906642
[2025-09-17 16:12:56,383][flp2p.graph_runner][INFO] - Test, Round 141 : loss => 0.9853124057457328,  accuracy: 0.7278538812785388
[2025-09-17 16:12:59,556][flp2p.graph_runner][INFO] - Train, Round 142 : loss => 0.30679027854576735,  accuracy: 0.8790909090909091, gradient_norm : 0.16786718647452664
[2025-09-17 16:13:00,384][flp2p.graph_runner][INFO] - Test, Round 142 : loss => 0.9652261783136479,  accuracy: 0.7486338797814208
[2025-09-17 16:13:03,534][flp2p.graph_runner][INFO] - Train, Round 143 : loss => 0.33123415728441613,  accuracy: 0.8709595959595959, gradient_norm : 0.15900026021965324
[2025-09-17 16:13:04,362][flp2p.graph_runner][INFO] - Test, Round 143 : loss => 0.9877237997466907,  accuracy: 0.7347310847766636
[2025-09-17 16:13:07,515][flp2p.graph_runner][INFO] - Train, Round 144 : loss => 0.2894615056735898,  accuracy: 0.8891919191919192, gradient_norm : 0.16083302412678843
[2025-09-17 16:13:08,332][flp2p.graph_runner][INFO] - Test, Round 144 : loss => 0.9641431371098784,  accuracy: 0.7634703196347032
[2025-09-17 16:13:11,470][flp2p.graph_runner][INFO] - Train, Round 145 : loss => 0.3239275646845768,  accuracy: 0.8754040404040404, gradient_norm : 0.18370386049738574
[2025-09-17 16:13:12,300][flp2p.graph_runner][INFO] - Test, Round 145 : loss => 0.9628189903989894,  accuracy: 0.7622504537205081
[2025-09-17 16:13:15,479][flp2p.graph_runner][INFO] - Train, Round 146 : loss => 0.31387930024870775,  accuracy: 0.8767171717171718, gradient_norm : 0.1676474831848769
[2025-09-17 16:13:16,328][flp2p.graph_runner][INFO] - Test, Round 146 : loss => 0.9360510898623496,  accuracy: 0.737943585077343
[2025-09-17 16:13:19,493][flp2p.graph_runner][INFO] - Train, Round 147 : loss => 0.27612650592227783,  accuracy: 0.8967676767676767, gradient_norm : 0.16592673434955188
[2025-09-17 16:13:20,325][flp2p.graph_runner][INFO] - Test, Round 147 : loss => 0.9705530742352659,  accuracy: 0.7727272727272727
[2025-09-17 16:13:23,493][flp2p.graph_runner][INFO] - Train, Round 148 : loss => 0.3029050495473105,  accuracy: 0.8841919191919191, gradient_norm : 0.18387271569103
[2025-09-17 16:13:24,330][flp2p.graph_runner][INFO] - Test, Round 148 : loss => 0.963753016816971,  accuracy: 0.7397260273972602
[2025-09-17 16:13:27,466][flp2p.graph_runner][INFO] - Train, Round 149 : loss => 0.3473318107092244,  accuracy: 0.86479797979798, gradient_norm : 0.16152972481786723
[2025-09-17 16:13:28,304][flp2p.graph_runner][INFO] - Test, Round 149 : loss => 0.9603622443702099,  accuracy: 0.7445255474452555
[2025-09-17 16:13:31,494][flp2p.graph_runner][INFO] - Train, Round 150 : loss => 0.2782744742053072,  accuracy: 0.889949494949495, gradient_norm : 0.167800869948413
[2025-09-17 16:13:32,301][flp2p.graph_runner][INFO] - Test, Round 150 : loss => 1.002166522548064,  accuracy: 0.7641165755919854
[2025-09-17 16:13:35,488][flp2p.graph_runner][INFO] - Train, Round 151 : loss => 0.26069445775070615,  accuracy: 0.8976262626262625, gradient_norm : 0.1577634967737203
[2025-09-17 16:13:36,321][flp2p.graph_runner][INFO] - Test, Round 151 : loss => 0.8740574289333615,  accuracy: 0.7779799818016379
[2025-09-17 16:13:39,494][flp2p.graph_runner][INFO] - Train, Round 152 : loss => 0.36690850401981595,  accuracy: 0.8523737373737372, gradient_norm : 0.16470742587638584
[2025-09-17 16:13:40,354][flp2p.graph_runner][INFO] - Test, Round 152 : loss => 0.9594155290328404,  accuracy: 0.7398897058823529
[2025-09-17 16:13:43,487][flp2p.graph_runner][INFO] - Train, Round 153 : loss => 0.3098158319270376,  accuracy: 0.8791414141414142, gradient_norm : 0.18255813216814393
[2025-09-17 16:13:44,308][flp2p.graph_runner][INFO] - Test, Round 153 : loss => 0.9844957249583013,  accuracy: 0.7586837294332724
[2025-09-17 16:13:47,512][flp2p.graph_runner][INFO] - Train, Round 154 : loss => 0.3023077424780213,  accuracy: 0.8808080808080809, gradient_norm : 0.17445153261107407
[2025-09-17 16:13:48,366][flp2p.graph_runner][INFO] - Test, Round 154 : loss => 0.9262058893278065,  accuracy: 0.7675478577939836
[2025-09-17 16:13:51,279][flp2p.graph_runner][INFO] - Train, Round 155 : loss => 0.2959435135119323,  accuracy: 0.886111111111111, gradient_norm : 0.17253960593422757
[2025-09-17 16:13:52,008][flp2p.graph_runner][INFO] - Test, Round 155 : loss => 1.0066851063156463,  accuracy: 0.7429718875502008
[2025-09-17 16:13:55,165][flp2p.graph_runner][INFO] - Train, Round 156 : loss => 0.2858782817957421,  accuracy: 0.8847979797979797, gradient_norm : 0.16184117331349868
[2025-09-17 16:13:55,966][flp2p.graph_runner][INFO] - Test, Round 156 : loss => 0.9416188652893871,  accuracy: 0.7493138151875571
[2025-09-17 16:13:59,103][flp2p.graph_runner][INFO] - Train, Round 157 : loss => 0.2873571189921098,  accuracy: 0.8885353535353535, gradient_norm : 0.16669466504330704
[2025-09-17 16:13:59,919][flp2p.graph_runner][INFO] - Test, Round 157 : loss => 0.9503198675033074,  accuracy: 0.7338782924613987
[2025-09-17 16:14:03,094][flp2p.graph_runner][INFO] - Train, Round 158 : loss => 0.30019880944959315,  accuracy: 0.8824242424242424, gradient_norm : 0.16755917291919245
[2025-09-17 16:14:03,927][flp2p.graph_runner][INFO] - Test, Round 158 : loss => 0.9830619783564047,  accuracy: 0.7345454545454545
[2025-09-17 16:14:07,102][flp2p.graph_runner][INFO] - Train, Round 159 : loss => 0.2698989086396041,  accuracy: 0.8952525252525254, gradient_norm : 0.16044452596726508
[2025-09-17 16:14:07,932][flp2p.graph_runner][INFO] - Test, Round 159 : loss => 0.945813506068102,  accuracy: 0.7531876138433515
[2025-09-17 16:14:11,098][flp2p.graph_runner][INFO] - Train, Round 160 : loss => 0.29863380652369054,  accuracy: 0.8877777777777778, gradient_norm : 0.1642687630055156
[2025-09-17 16:14:11,934][flp2p.graph_runner][INFO] - Test, Round 160 : loss => 0.9596621432254667,  accuracy: 0.7506799637352675
[2025-09-17 16:14:15,090][flp2p.graph_runner][INFO] - Train, Round 161 : loss => 0.30757480084390737,  accuracy: 0.8792929292929292, gradient_norm : 0.16981606993698764
[2025-09-17 16:14:15,930][flp2p.graph_runner][INFO] - Test, Round 161 : loss => 0.9730370197238758,  accuracy: 0.7484105358764759
[2025-09-17 16:14:19,059][flp2p.graph_runner][INFO] - Train, Round 162 : loss => 0.2869780080071712,  accuracy: 0.8892929292929292, gradient_norm : 0.16172476279670134
[2025-09-17 16:14:19,940][flp2p.graph_runner][INFO] - Test, Round 162 : loss => 1.0515858511656686,  accuracy: 0.746800731261426
[2025-09-17 16:14:23,173][flp2p.graph_runner][INFO] - Train, Round 163 : loss => 0.2878498285692,  accuracy: 0.8913636363636364, gradient_norm : 0.1667153113413894
[2025-09-17 16:14:24,020][flp2p.graph_runner][INFO] - Test, Round 163 : loss => 0.9236346674701779,  accuracy: 0.7592425608656447
[2025-09-17 16:14:27,272][flp2p.graph_runner][INFO] - Train, Round 164 : loss => 0.27516106379977523,  accuracy: 0.8920707070707071, gradient_norm : 0.16205655697346832
[2025-09-17 16:14:28,125][flp2p.graph_runner][INFO] - Test, Round 164 : loss => 0.9616383774036711,  accuracy: 0.7381818181818182
[2025-09-17 16:14:31,272][flp2p.graph_runner][INFO] - Train, Round 165 : loss => 0.2540960901075309,  accuracy: 0.9018686868686872, gradient_norm : 0.15708577395435752
[2025-09-17 16:14:32,114][flp2p.graph_runner][INFO] - Test, Round 165 : loss => 1.0059531359780918,  accuracy: 0.7572727272727273
[2025-09-17 16:14:35,274][flp2p.graph_runner][INFO] - Train, Round 166 : loss => 0.2726433851842034,  accuracy: 0.8942929292929291, gradient_norm : 0.163403705272299
[2025-09-17 16:14:36,096][flp2p.graph_runner][INFO] - Test, Round 166 : loss => 0.9422429353268927,  accuracy: 0.7552130553037172
[2025-09-17 16:14:38,939][flp2p.graph_runner][INFO] - Train, Round 167 : loss => 0.2437756106826661,  accuracy: 0.9083333333333332, gradient_norm : 0.14597377371797185
[2025-09-17 16:14:39,699][flp2p.graph_runner][INFO] - Test, Round 167 : loss => 1.0329543622294861,  accuracy: 0.7608040201005025
[2025-09-17 16:14:42,901][flp2p.graph_runner][INFO] - Train, Round 168 : loss => 0.3027411949125301,  accuracy: 0.8809595959595957, gradient_norm : 0.15886152645231516
[2025-09-17 16:14:43,744][flp2p.graph_runner][INFO] - Test, Round 168 : loss => 0.963440761775953,  accuracy: 0.7525206232813932
[2025-09-17 16:14:46,936][flp2p.graph_runner][INFO] - Train, Round 169 : loss => 0.2680199645804378,  accuracy: 0.8962121212121212, gradient_norm : 0.16043817595221974
[2025-09-17 16:14:47,767][flp2p.graph_runner][INFO] - Test, Round 169 : loss => 0.9793274757660687,  accuracy: 0.7470319634703196
[2025-09-17 16:14:50,967][flp2p.graph_runner][INFO] - Train, Round 170 : loss => 0.28384378501956015,  accuracy: 0.8883333333333332, gradient_norm : 0.15994893418104655
[2025-09-17 16:14:51,833][flp2p.graph_runner][INFO] - Test, Round 170 : loss => 0.9200548063029782,  accuracy: 0.7693710118505014
[2025-09-17 16:14:55,065][flp2p.graph_runner][INFO] - Train, Round 171 : loss => 0.2751901032929626,  accuracy: 0.8936868686868688, gradient_norm : 0.16794657351399198
[2025-09-17 16:14:55,940][flp2p.graph_runner][INFO] - Test, Round 171 : loss => 0.9956666701366785,  accuracy: 0.7310155535224153
[2025-09-17 16:14:59,204][flp2p.graph_runner][INFO] - Train, Round 172 : loss => 0.2589816671150016,  accuracy: 0.9011616161616162, gradient_norm : 0.1592471148756239
[2025-09-17 16:15:00,062][flp2p.graph_runner][INFO] - Test, Round 172 : loss => 0.9187346390257143,  accuracy: 0.7607305936073059
[2025-09-17 16:15:03,324][flp2p.graph_runner][INFO] - Train, Round 173 : loss => 0.3117163594319815,  accuracy: 0.8776262626262628, gradient_norm : 0.16467852430149046
[2025-09-17 16:15:04,166][flp2p.graph_runner][INFO] - Test, Round 173 : loss => 1.001904896459623,  accuracy: 0.7442922374429224
[2025-09-17 16:15:07,397][flp2p.graph_runner][INFO] - Train, Round 174 : loss => 0.2518242361446689,  accuracy: 0.9027272727272727, gradient_norm : 0.1701554796802636
[2025-09-17 16:15:08,262][flp2p.graph_runner][INFO] - Test, Round 174 : loss => 0.9478263492611322,  accuracy: 0.7609090909090909
[2025-09-17 16:15:11,509][flp2p.graph_runner][INFO] - Train, Round 175 : loss => 0.2036115146741257,  accuracy: 0.9234848484848485, gradient_norm : 0.15830010710558903
[2025-09-17 16:15:12,340][flp2p.graph_runner][INFO] - Test, Round 175 : loss => 1.0161884757966049,  accuracy: 0.7556973564266181
[2025-09-17 16:15:15,227][flp2p.graph_runner][INFO] - Train, Round 176 : loss => 0.25380572430931353,  accuracy: 0.9008333333333334, gradient_norm : 0.1644008034271189
[2025-09-17 16:15:15,989][flp2p.graph_runner][INFO] - Test, Round 176 : loss => 1.0572753827813397,  accuracy: 0.7372116349047142
[2025-09-17 16:15:19,210][flp2p.graph_runner][INFO] - Train, Round 177 : loss => 0.22172281171318017,  accuracy: 0.918131313131313, gradient_norm : 0.15608015921331564
[2025-09-17 16:15:20,084][flp2p.graph_runner][INFO] - Test, Round 177 : loss => 0.9826858245434552,  accuracy: 0.7901459854014599
[2025-09-17 16:15:22,985][flp2p.graph_runner][INFO] - Train, Round 178 : loss => 0.24415344158679242,  accuracy: 0.9087777777777777, gradient_norm : 0.161440049251279
[2025-09-17 16:15:23,751][flp2p.graph_runner][INFO] - Test, Round 178 : loss => 1.0172209730717459,  accuracy: 0.7542627883650953
[2025-09-17 16:15:26,946][flp2p.graph_runner][INFO] - Train, Round 179 : loss => 0.23610366760822768,  accuracy: 0.9105050505050505, gradient_norm : 0.1671609207532888
[2025-09-17 16:15:27,785][flp2p.graph_runner][INFO] - Test, Round 179 : loss => 0.9669143855844163,  accuracy: 0.7595978062157221
[2025-09-17 16:15:30,708][flp2p.graph_runner][INFO] - Train, Round 180 : loss => 0.22641628172768832,  accuracy: 0.9154999999999999, gradient_norm : 0.14929199305572818
[2025-09-17 16:15:31,468][flp2p.graph_runner][INFO] - Test, Round 180 : loss => 0.9645936927145395,  accuracy: 0.7545271629778671
[2025-09-17 16:15:34,618][flp2p.graph_runner][INFO] - Train, Round 181 : loss => 0.23433910349677084,  accuracy: 0.9091414141414143, gradient_norm : 0.15361303189328715
[2025-09-17 16:15:35,477][flp2p.graph_runner][INFO] - Test, Round 181 : loss => 1.0372515279680627,  accuracy: 0.7406392694063927
[2025-09-17 16:15:38,734][flp2p.graph_runner][INFO] - Train, Round 182 : loss => 0.2331963565675048,  accuracy: 0.9089393939393938, gradient_norm : 0.17590158407663967
[2025-09-17 16:15:39,589][flp2p.graph_runner][INFO] - Test, Round 182 : loss => 0.9910798589280255,  accuracy: 0.7520435967302452
[2025-09-17 16:15:42,852][flp2p.graph_runner][INFO] - Train, Round 183 : loss => 0.22567140522033063,  accuracy: 0.9139393939393939, gradient_norm : 0.17583882535503617
[2025-09-17 16:15:43,707][flp2p.graph_runner][INFO] - Test, Round 183 : loss => 1.0895913436954074,  accuracy: 0.7316176470588235
[2025-09-17 16:15:46,953][flp2p.graph_runner][INFO] - Train, Round 184 : loss => 0.2790481186483455,  accuracy: 0.8930808080808081, gradient_norm : 0.16568820673429935
[2025-09-17 16:15:47,801][flp2p.graph_runner][INFO] - Test, Round 184 : loss => 0.940546310092745,  accuracy: 0.7543538038496792
[2025-09-17 16:15:50,959][flp2p.graph_runner][INFO] - Train, Round 185 : loss => 0.1792008575582286,  accuracy: 0.9341919191919191, gradient_norm : 0.1627083298458157
[2025-09-17 16:15:51,833][flp2p.graph_runner][INFO] - Test, Round 185 : loss => 1.004412157097396,  accuracy: 0.76993583868011
[2025-09-17 16:15:55,061][flp2p.graph_runner][INFO] - Train, Round 186 : loss => 0.22145393340800762,  accuracy: 0.9162626262626262, gradient_norm : 0.16484671465682654
[2025-09-17 16:15:55,919][flp2p.graph_runner][INFO] - Test, Round 186 : loss => 0.9572717806183433,  accuracy: 0.7748404740200547
[2025-09-17 16:15:58,815][flp2p.graph_runner][INFO] - Train, Round 187 : loss => 0.22180564391128427,  accuracy: 0.9167222222222223, gradient_norm : 0.1539553941701733
[2025-09-17 16:15:59,577][flp2p.graph_runner][INFO] - Test, Round 187 : loss => 0.9594942450013165,  accuracy: 0.7814702920443102
[2025-09-17 16:16:02,786][flp2p.graph_runner][INFO] - Train, Round 188 : loss => 0.2490176922006702,  accuracy: 0.903131313131313, gradient_norm : 0.1626470488772834
[2025-09-17 16:16:03,635][flp2p.graph_runner][INFO] - Test, Round 188 : loss => 0.9861033425155393,  accuracy: 0.7434030937215651
[2025-09-17 16:16:06,579][flp2p.graph_runner][INFO] - Train, Round 189 : loss => 0.23475429460688854,  accuracy: 0.9092222222222222, gradient_norm : 0.15061043483465514
[2025-09-17 16:16:07,349][flp2p.graph_runner][INFO] - Test, Round 189 : loss => 0.9782485994152866,  accuracy: 0.7467597208374875
[2025-09-17 16:16:10,543][flp2p.graph_runner][INFO] - Train, Round 190 : loss => 0.19911994196547972,  accuracy: 0.9271717171717173, gradient_norm : 0.14893245774418112
[2025-09-17 16:16:11,390][flp2p.graph_runner][INFO] - Test, Round 190 : loss => 0.9893868355557608,  accuracy: 0.7702825888787602
[2025-09-17 16:16:14,611][flp2p.graph_runner][INFO] - Train, Round 191 : loss => 0.2639866066769173,  accuracy: 0.901010101010101, gradient_norm : 0.1609240791955365
[2025-09-17 16:16:15,483][flp2p.graph_runner][INFO] - Test, Round 191 : loss => 0.9640061566146462,  accuracy: 0.7577696526508226
[2025-09-17 16:16:18,692][flp2p.graph_runner][INFO] - Train, Round 192 : loss => 0.24523405728927394,  accuracy: 0.9071212121212122, gradient_norm : 0.1638419975648687
[2025-09-17 16:16:19,555][flp2p.graph_runner][INFO] - Test, Round 192 : loss => 0.9695808630752476,  accuracy: 0.7447584320875114
[2025-09-17 16:16:22,779][flp2p.graph_runner][INFO] - Train, Round 193 : loss => 0.175197594627036,  accuracy: 0.9337373737373736, gradient_norm : 0.1550079584183435
[2025-09-17 16:16:23,616][flp2p.graph_runner][INFO] - Test, Round 193 : loss => 1.0068580705325172,  accuracy: 0.7861060329067642
[2025-09-17 16:16:26,856][flp2p.graph_runner][INFO] - Train, Round 194 : loss => 0.23893252145620256,  accuracy: 0.9085353535353535, gradient_norm : 0.163182404139674
[2025-09-17 16:16:27,708][flp2p.graph_runner][INFO] - Test, Round 194 : loss => 1.0045066075576832,  accuracy: 0.742520398912058
[2025-09-17 16:16:30,938][flp2p.graph_runner][INFO] - Train, Round 195 : loss => 0.21013220870859875,  accuracy: 0.9206060606060606, gradient_norm : 0.16564805554120263
[2025-09-17 16:16:31,817][flp2p.graph_runner][INFO] - Test, Round 195 : loss => 1.0414736807481808,  accuracy: 0.7418181818181818
[2025-09-17 16:16:35,022][flp2p.graph_runner][INFO] - Train, Round 196 : loss => 0.2010069602007482,  accuracy: 0.9264646464646463, gradient_norm : 0.1604442912861107
[2025-09-17 16:16:35,857][flp2p.graph_runner][INFO] - Test, Round 196 : loss => 0.967235437330579,  accuracy: 0.7717690192483959
[2025-09-17 16:16:39,095][flp2p.graph_runner][INFO] - Train, Round 197 : loss => 0.22553543053389039,  accuracy: 0.914949494949495, gradient_norm : 0.16705257187171493
[2025-09-17 16:16:39,946][flp2p.graph_runner][INFO] - Test, Round 197 : loss => 1.0607655862568721,  accuracy: 0.7447584320875114
[2025-09-17 16:16:43,204][flp2p.graph_runner][INFO] - Train, Round 198 : loss => 0.2599283407570083,  accuracy: 0.8995454545454545, gradient_norm : 0.1609709645162449
[2025-09-17 16:16:44,062][flp2p.graph_runner][INFO] - Test, Round 198 : loss => 0.9593503801433296,  accuracy: 0.7612076852698993
[2025-09-17 16:16:47,251][flp2p.graph_runner][INFO] - Train, Round 199 : loss => 0.20992947344678792,  accuracy: 0.9189898989898991, gradient_norm : 0.16230508644358513
[2025-09-17 16:16:48,110][flp2p.graph_runner][INFO] - Test, Round 199 : loss => 1.0193191016101402,  accuracy: 0.7680365296803653
[2025-09-17 16:16:48,110][__main__][INFO] - Train, Round 001: loss=2.2610, accuracy=0.2081, gradient_norm=0.2429, 
[2025-09-17 16:16:48,110][__main__][INFO] - Train, Round 002: loss=2.1677, accuracy=0.2318, gradient_norm=0.3282, 
[2025-09-17 16:16:48,111][__main__][INFO] - Train, Round 003: loss=2.0428, accuracy=0.2715, gradient_norm=0.2753, 
[2025-09-17 16:16:48,111][__main__][INFO] - Train, Round 004: loss=1.8930, accuracy=0.3127, gradient_norm=0.3491, 
[2025-09-17 16:16:48,111][__main__][INFO] - Train, Round 005: loss=1.7258, accuracy=0.3912, gradient_norm=0.4197, 
[2025-09-17 16:16:48,111][__main__][INFO] - Train, Round 006: loss=1.9467, accuracy=0.3107, gradient_norm=0.2855, 
[2025-09-17 16:16:48,111][__main__][INFO] - Train, Round 007: loss=1.7519, accuracy=0.3688, gradient_norm=0.3028, 
[2025-09-17 16:16:48,111][__main__][INFO] - Train, Round 008: loss=1.7107, accuracy=0.4220, gradient_norm=0.4590, 
[2025-09-17 16:16:48,111][__main__][INFO] - Train, Round 009: loss=1.6168, accuracy=0.4360, gradient_norm=0.4687, 
[2025-09-17 16:16:48,111][__main__][INFO] - Train, Round 010: loss=1.5378, accuracy=0.4553, gradient_norm=0.4595, 
[2025-09-17 16:16:48,111][__main__][INFO] - Train, Round 011: loss=1.6147, accuracy=0.4251, gradient_norm=0.3757, 
[2025-09-17 16:16:48,111][__main__][INFO] - Train, Round 012: loss=1.6283, accuracy=0.4433, gradient_norm=0.4458, 
[2025-09-17 16:16:48,111][__main__][INFO] - Train, Round 013: loss=1.4590, accuracy=0.4663, gradient_norm=0.3647, 
[2025-09-17 16:16:48,111][__main__][INFO] - Train, Round 014: loss=1.4648, accuracy=0.4697, gradient_norm=0.3433, 
[2025-09-17 16:16:48,111][__main__][INFO] - Train, Round 015: loss=1.7387, accuracy=0.3770, gradient_norm=0.3251, 
[2025-09-17 16:16:48,111][__main__][INFO] - Train, Round 016: loss=1.5014, accuracy=0.4643, gradient_norm=0.4567, 
[2025-09-17 16:16:48,111][__main__][INFO] - Train, Round 017: loss=1.2489, accuracy=0.5485, gradient_norm=0.4337, 
[2025-09-17 16:16:48,111][__main__][INFO] - Train, Round 018: loss=1.4482, accuracy=0.5058, gradient_norm=0.4973, 
[2025-09-17 16:16:48,111][__main__][INFO] - Train, Round 019: loss=1.0871, accuracy=0.5838, gradient_norm=0.4266, 
[2025-09-17 16:16:48,111][__main__][INFO] - Train, Round 020: loss=1.4153, accuracy=0.4967, gradient_norm=0.4312, 
[2025-09-17 16:16:48,111][__main__][INFO] - Train, Round 021: loss=1.2772, accuracy=0.5474, gradient_norm=0.3707, 
[2025-09-17 16:16:48,111][__main__][INFO] - Train, Round 022: loss=1.2886, accuracy=0.5306, gradient_norm=0.3957, 
[2025-09-17 16:16:48,111][__main__][INFO] - Train, Round 023: loss=0.9800, accuracy=0.6506, gradient_norm=0.3968, 
[2025-09-17 16:16:48,111][__main__][INFO] - Train, Round 024: loss=1.1197, accuracy=0.5843, gradient_norm=0.3368, 
[2025-09-17 16:16:48,111][__main__][INFO] - Train, Round 025: loss=1.0593, accuracy=0.6122, gradient_norm=0.3707, 
[2025-09-17 16:16:48,111][__main__][INFO] - Train, Round 026: loss=0.9075, accuracy=0.6614, gradient_norm=0.3811, 
[2025-09-17 16:16:48,111][__main__][INFO] - Train, Round 027: loss=0.8214, accuracy=0.6872, gradient_norm=0.3182, 
[2025-09-17 16:16:48,111][__main__][INFO] - Train, Round 028: loss=1.0494, accuracy=0.6060, gradient_norm=0.3519, 
[2025-09-17 16:16:48,111][__main__][INFO] - Train, Round 029: loss=0.7616, accuracy=0.7081, gradient_norm=0.3228, 
[2025-09-17 16:16:48,111][__main__][INFO] - Train, Round 030: loss=1.0249, accuracy=0.6297, gradient_norm=0.3349, 
[2025-09-17 16:16:48,111][__main__][INFO] - Train, Round 031: loss=0.8552, accuracy=0.6832, gradient_norm=0.3355, 
[2025-09-17 16:16:48,111][__main__][INFO] - Train, Round 032: loss=0.9671, accuracy=0.6472, gradient_norm=0.3190, 
[2025-09-17 16:16:48,111][__main__][INFO] - Train, Round 033: loss=0.9132, accuracy=0.6537, gradient_norm=0.3515, 
[2025-09-17 16:16:48,111][__main__][INFO] - Train, Round 034: loss=1.0709, accuracy=0.6124, gradient_norm=0.3669, 
[2025-09-17 16:16:48,111][__main__][INFO] - Train, Round 035: loss=0.8285, accuracy=0.6807, gradient_norm=0.3365, 
[2025-09-17 16:16:48,111][__main__][INFO] - Train, Round 036: loss=0.8922, accuracy=0.6696, gradient_norm=0.2992, 
[2025-09-17 16:16:48,111][__main__][INFO] - Train, Round 037: loss=0.8109, accuracy=0.6948, gradient_norm=0.3182, 
[2025-09-17 16:16:48,111][__main__][INFO] - Train, Round 038: loss=0.8175, accuracy=0.6990, gradient_norm=0.3016, 
[2025-09-17 16:16:48,111][__main__][INFO] - Train, Round 039: loss=0.9817, accuracy=0.6335, gradient_norm=0.3976, 
[2025-09-17 16:16:48,111][__main__][INFO] - Train, Round 040: loss=0.6340, accuracy=0.7548, gradient_norm=0.2584, 
[2025-09-17 16:16:48,111][__main__][INFO] - Train, Round 041: loss=1.0207, accuracy=0.6317, gradient_norm=0.2804, 
[2025-09-17 16:16:48,111][__main__][INFO] - Train, Round 042: loss=0.7914, accuracy=0.7030, gradient_norm=0.3027, 
[2025-09-17 16:16:48,111][__main__][INFO] - Train, Round 043: loss=0.8832, accuracy=0.6765, gradient_norm=0.3495, 
[2025-09-17 16:16:48,111][__main__][INFO] - Train, Round 044: loss=0.8771, accuracy=0.6726, gradient_norm=0.3282, 
[2025-09-17 16:16:48,111][__main__][INFO] - Train, Round 045: loss=0.7230, accuracy=0.7237, gradient_norm=0.2928, 
[2025-09-17 16:16:48,111][__main__][INFO] - Train, Round 046: loss=0.8163, accuracy=0.7001, gradient_norm=0.3058, 
[2025-09-17 16:16:48,111][__main__][INFO] - Train, Round 047: loss=0.6557, accuracy=0.7468, gradient_norm=0.2521, 
[2025-09-17 16:16:48,111][__main__][INFO] - Train, Round 048: loss=0.7652, accuracy=0.7108, gradient_norm=0.2826, 
[2025-09-17 16:16:48,111][__main__][INFO] - Train, Round 049: loss=0.7524, accuracy=0.7151, gradient_norm=0.2774, 
[2025-09-17 16:16:48,111][__main__][INFO] - Train, Round 050: loss=0.6724, accuracy=0.7352, gradient_norm=0.2539, 
[2025-09-17 16:16:48,111][__main__][INFO] - Train, Round 051: loss=0.6054, accuracy=0.7642, gradient_norm=0.2158, 
[2025-09-17 16:16:48,111][__main__][INFO] - Train, Round 052: loss=0.7258, accuracy=0.7182, gradient_norm=0.2756, 
[2025-09-17 16:16:48,111][__main__][INFO] - Train, Round 053: loss=0.5494, accuracy=0.7859, gradient_norm=0.2163, 
[2025-09-17 16:16:48,111][__main__][INFO] - Train, Round 054: loss=0.5925, accuracy=0.7711, gradient_norm=0.2312, 
[2025-09-17 16:16:48,111][__main__][INFO] - Train, Round 055: loss=0.6530, accuracy=0.7437, gradient_norm=0.2444, 
[2025-09-17 16:16:48,111][__main__][INFO] - Train, Round 056: loss=0.6263, accuracy=0.7552, gradient_norm=0.2317, 
[2025-09-17 16:16:48,111][__main__][INFO] - Train, Round 057: loss=0.6047, accuracy=0.7676, gradient_norm=0.2393, 
[2025-09-17 16:16:48,111][__main__][INFO] - Train, Round 058: loss=0.6596, accuracy=0.7453, gradient_norm=0.2663, 
[2025-09-17 16:16:48,111][__main__][INFO] - Train, Round 059: loss=0.5665, accuracy=0.7820, gradient_norm=0.2260, 
[2025-09-17 16:16:48,111][__main__][INFO] - Train, Round 060: loss=0.5889, accuracy=0.7708, gradient_norm=0.2190, 
[2025-09-17 16:16:48,111][__main__][INFO] - Train, Round 061: loss=0.6021, accuracy=0.7682, gradient_norm=0.2108, 
[2025-09-17 16:16:48,112][__main__][INFO] - Train, Round 062: loss=0.5439, accuracy=0.7905, gradient_norm=0.2098, 
[2025-09-17 16:16:48,112][__main__][INFO] - Train, Round 063: loss=0.5822, accuracy=0.7777, gradient_norm=0.2190, 
[2025-09-17 16:16:48,112][__main__][INFO] - Train, Round 064: loss=0.5607, accuracy=0.7864, gradient_norm=0.2106, 
[2025-09-17 16:16:48,112][__main__][INFO] - Train, Round 065: loss=0.5208, accuracy=0.7981, gradient_norm=0.2012, 
[2025-09-17 16:16:48,112][__main__][INFO] - Train, Round 066: loss=0.5609, accuracy=0.7825, gradient_norm=0.1950, 
[2025-09-17 16:16:48,112][__main__][INFO] - Train, Round 067: loss=0.5426, accuracy=0.7895, gradient_norm=0.2219, 
[2025-09-17 16:16:48,112][__main__][INFO] - Train, Round 068: loss=0.5211, accuracy=0.7953, gradient_norm=0.1902, 
[2025-09-17 16:16:48,112][__main__][INFO] - Train, Round 069: loss=0.5634, accuracy=0.7806, gradient_norm=0.2088, 
[2025-09-17 16:16:48,112][__main__][INFO] - Train, Round 070: loss=0.7726, accuracy=0.7113, gradient_norm=0.2257, 
[2025-09-17 16:16:48,112][__main__][INFO] - Train, Round 071: loss=0.5731, accuracy=0.7748, gradient_norm=0.2230, 
[2025-09-17 16:16:48,112][__main__][INFO] - Train, Round 072: loss=0.5011, accuracy=0.8017, gradient_norm=0.1853, 
[2025-09-17 16:16:48,112][__main__][INFO] - Train, Round 073: loss=0.4790, accuracy=0.8152, gradient_norm=0.1962, 
[2025-09-17 16:16:48,112][__main__][INFO] - Train, Round 074: loss=0.5377, accuracy=0.7865, gradient_norm=0.2104, 
[2025-09-17 16:16:48,112][__main__][INFO] - Train, Round 075: loss=0.6011, accuracy=0.7697, gradient_norm=0.2250, 
[2025-09-17 16:16:48,112][__main__][INFO] - Train, Round 076: loss=0.5179, accuracy=0.7953, gradient_norm=0.2022, 
[2025-09-17 16:16:48,112][__main__][INFO] - Train, Round 077: loss=0.5103, accuracy=0.8013, gradient_norm=0.1953, 
[2025-09-17 16:16:48,112][__main__][INFO] - Train, Round 078: loss=0.5266, accuracy=0.7939, gradient_norm=0.2134, 
[2025-09-17 16:16:48,112][__main__][INFO] - Train, Round 079: loss=0.4908, accuracy=0.8102, gradient_norm=0.2105, 
[2025-09-17 16:16:48,112][__main__][INFO] - Train, Round 080: loss=0.5159, accuracy=0.8018, gradient_norm=0.2011, 
[2025-09-17 16:16:48,112][__main__][INFO] - Train, Round 081: loss=0.5368, accuracy=0.7937, gradient_norm=0.2059, 
[2025-09-17 16:16:48,112][__main__][INFO] - Train, Round 082: loss=0.4569, accuracy=0.8174, gradient_norm=0.1706, 
[2025-09-17 16:16:48,112][__main__][INFO] - Train, Round 083: loss=0.4611, accuracy=0.8196, gradient_norm=0.1941, 
[2025-09-17 16:16:48,112][__main__][INFO] - Train, Round 084: loss=0.4737, accuracy=0.8133, gradient_norm=0.1782, 
[2025-09-17 16:16:48,112][__main__][INFO] - Train, Round 085: loss=0.3967, accuracy=0.8463, gradient_norm=0.1803, 
[2025-09-17 16:16:48,112][__main__][INFO] - Train, Round 086: loss=0.4560, accuracy=0.8170, gradient_norm=0.1712, 
[2025-09-17 16:16:48,112][__main__][INFO] - Train, Round 087: loss=0.4328, accuracy=0.8338, gradient_norm=0.1868, 
[2025-09-17 16:16:48,112][__main__][INFO] - Train, Round 088: loss=0.5395, accuracy=0.7925, gradient_norm=0.2206, 
[2025-09-17 16:16:48,112][__main__][INFO] - Train, Round 089: loss=0.4361, accuracy=0.8316, gradient_norm=0.1728, 
[2025-09-17 16:16:48,112][__main__][INFO] - Train, Round 090: loss=0.4613, accuracy=0.8194, gradient_norm=0.1918, 
[2025-09-17 16:16:48,112][__main__][INFO] - Train, Round 091: loss=0.4784, accuracy=0.8148, gradient_norm=0.2013, 
[2025-09-17 16:16:48,112][__main__][INFO] - Train, Round 092: loss=0.4213, accuracy=0.8315, gradient_norm=0.1795, 
[2025-09-17 16:16:48,112][__main__][INFO] - Train, Round 093: loss=0.4559, accuracy=0.8186, gradient_norm=0.1846, 
[2025-09-17 16:16:48,112][__main__][INFO] - Train, Round 094: loss=0.4879, accuracy=0.8086, gradient_norm=0.1996, 
[2025-09-17 16:16:48,112][__main__][INFO] - Train, Round 095: loss=0.4345, accuracy=0.8278, gradient_norm=0.1660, 
[2025-09-17 16:16:48,112][__main__][INFO] - Train, Round 096: loss=0.4275, accuracy=0.8331, gradient_norm=0.1678, 
[2025-09-17 16:16:48,112][__main__][INFO] - Train, Round 097: loss=0.4442, accuracy=0.8262, gradient_norm=0.1769, 
[2025-09-17 16:16:48,112][__main__][INFO] - Train, Round 098: loss=0.4573, accuracy=0.8260, gradient_norm=0.1976, 
[2025-09-17 16:16:48,112][__main__][INFO] - Train, Round 099: loss=0.4234, accuracy=0.8320, gradient_norm=0.1739, 
[2025-09-17 16:16:48,112][__main__][INFO] - Train, Round 100: loss=0.4180, accuracy=0.8353, gradient_norm=0.1740, 
[2025-09-17 16:16:48,112][__main__][INFO] - Train, Round 101: loss=0.4329, accuracy=0.8273, gradient_norm=0.1793, 
[2025-09-17 16:16:48,112][__main__][INFO] - Train, Round 102: loss=0.3619, accuracy=0.8606, gradient_norm=0.1711, 
[2025-09-17 16:16:48,112][__main__][INFO] - Train, Round 103: loss=0.3973, accuracy=0.8436, gradient_norm=0.1698, 
[2025-09-17 16:16:48,112][__main__][INFO] - Train, Round 104: loss=0.4523, accuracy=0.8248, gradient_norm=0.1778, 
[2025-09-17 16:16:48,112][__main__][INFO] - Train, Round 105: loss=0.4058, accuracy=0.8427, gradient_norm=0.1752, 
[2025-09-17 16:16:48,112][__main__][INFO] - Train, Round 106: loss=0.4042, accuracy=0.8374, gradient_norm=0.1762, 
[2025-09-17 16:16:48,112][__main__][INFO] - Train, Round 107: loss=0.4096, accuracy=0.8383, gradient_norm=0.1666, 
[2025-09-17 16:16:48,112][__main__][INFO] - Train, Round 108: loss=0.3494, accuracy=0.8566, gradient_norm=0.1517, 
[2025-09-17 16:16:48,112][__main__][INFO] - Train, Round 109: loss=0.4286, accuracy=0.8332, gradient_norm=0.1864, 
[2025-09-17 16:16:48,112][__main__][INFO] - Train, Round 110: loss=0.4493, accuracy=0.8259, gradient_norm=0.1871, 
[2025-09-17 16:16:48,112][__main__][INFO] - Train, Round 111: loss=0.4239, accuracy=0.8326, gradient_norm=0.1689, 
[2025-09-17 16:16:48,112][__main__][INFO] - Train, Round 112: loss=0.3846, accuracy=0.8471, gradient_norm=0.1806, 
[2025-09-17 16:16:48,112][__main__][INFO] - Train, Round 113: loss=0.4093, accuracy=0.8401, gradient_norm=0.1762, 
[2025-09-17 16:16:48,112][__main__][INFO] - Train, Round 114: loss=0.4017, accuracy=0.8425, gradient_norm=0.1870, 
[2025-09-17 16:16:48,112][__main__][INFO] - Train, Round 115: loss=0.3729, accuracy=0.8509, gradient_norm=0.1597, 
[2025-09-17 16:16:48,112][__main__][INFO] - Train, Round 116: loss=0.4067, accuracy=0.8434, gradient_norm=0.1762, 
[2025-09-17 16:16:48,112][__main__][INFO] - Train, Round 117: loss=0.3499, accuracy=0.8659, gradient_norm=0.1801, 
[2025-09-17 16:16:48,112][__main__][INFO] - Train, Round 118: loss=0.4207, accuracy=0.8342, gradient_norm=0.1874, 
[2025-09-17 16:16:48,112][__main__][INFO] - Train, Round 119: loss=0.4036, accuracy=0.8436, gradient_norm=0.1719, 
[2025-09-17 16:16:48,112][__main__][INFO] - Train, Round 120: loss=0.3641, accuracy=0.8562, gradient_norm=0.1592, 
[2025-09-17 16:16:48,112][__main__][INFO] - Train, Round 121: loss=0.3669, accuracy=0.8586, gradient_norm=0.1918, 
[2025-09-17 16:16:48,112][__main__][INFO] - Train, Round 122: loss=0.3574, accuracy=0.8592, gradient_norm=0.1623, 
[2025-09-17 16:16:48,112][__main__][INFO] - Train, Round 123: loss=0.3770, accuracy=0.8506, gradient_norm=0.1769, 
[2025-09-17 16:16:48,113][__main__][INFO] - Train, Round 124: loss=0.3606, accuracy=0.8578, gradient_norm=0.1706, 
[2025-09-17 16:16:48,113][__main__][INFO] - Train, Round 125: loss=0.3798, accuracy=0.8525, gradient_norm=0.1742, 
[2025-09-17 16:16:48,113][__main__][INFO] - Train, Round 126: loss=0.4137, accuracy=0.8357, gradient_norm=0.1706, 
[2025-09-17 16:16:48,113][__main__][INFO] - Train, Round 127: loss=0.3404, accuracy=0.8634, gradient_norm=0.1621, 
[2025-09-17 16:16:48,113][__main__][INFO] - Train, Round 128: loss=0.3620, accuracy=0.8578, gradient_norm=0.1706, 
[2025-09-17 16:16:48,113][__main__][INFO] - Train, Round 129: loss=0.3536, accuracy=0.8616, gradient_norm=0.1776, 
[2025-09-17 16:16:48,113][__main__][INFO] - Train, Round 130: loss=0.3727, accuracy=0.8570, gradient_norm=0.1727, 
[2025-09-17 16:16:48,113][__main__][INFO] - Train, Round 131: loss=0.3668, accuracy=0.8572, gradient_norm=0.1712, 
[2025-09-17 16:16:48,113][__main__][INFO] - Train, Round 132: loss=0.3567, accuracy=0.8633, gradient_norm=0.1821, 
[2025-09-17 16:16:48,113][__main__][INFO] - Train, Round 133: loss=0.3893, accuracy=0.8460, gradient_norm=0.1822, 
[2025-09-17 16:16:48,113][__main__][INFO] - Train, Round 134: loss=0.3430, accuracy=0.8665, gradient_norm=0.1627, 
[2025-09-17 16:16:48,113][__main__][INFO] - Train, Round 135: loss=0.2916, accuracy=0.8852, gradient_norm=0.1647, 
[2025-09-17 16:16:48,113][__main__][INFO] - Train, Round 136: loss=0.3011, accuracy=0.8810, gradient_norm=0.1713, 
[2025-09-17 16:16:48,113][__main__][INFO] - Train, Round 137: loss=0.3679, accuracy=0.8537, gradient_norm=0.1612, 
[2025-09-17 16:16:48,113][__main__][INFO] - Train, Round 138: loss=0.3173, accuracy=0.8787, gradient_norm=0.1638, 
[2025-09-17 16:16:48,113][__main__][INFO] - Train, Round 139: loss=0.3437, accuracy=0.8633, gradient_norm=0.1761, 
[2025-09-17 16:16:48,113][__main__][INFO] - Train, Round 140: loss=0.4166, accuracy=0.8351, gradient_norm=0.1880, 
[2025-09-17 16:16:48,113][__main__][INFO] - Train, Round 141: loss=0.3646, accuracy=0.8565, gradient_norm=0.1830, 
[2025-09-17 16:16:48,113][__main__][INFO] - Train, Round 142: loss=0.3390, accuracy=0.8669, gradient_norm=0.1672, 
[2025-09-17 16:16:48,113][__main__][INFO] - Train, Round 143: loss=0.3068, accuracy=0.8791, gradient_norm=0.1679, 
[2025-09-17 16:16:48,113][__main__][INFO] - Train, Round 144: loss=0.3312, accuracy=0.8710, gradient_norm=0.1590, 
[2025-09-17 16:16:48,113][__main__][INFO] - Train, Round 145: loss=0.2895, accuracy=0.8892, gradient_norm=0.1608, 
[2025-09-17 16:16:48,113][__main__][INFO] - Train, Round 146: loss=0.3239, accuracy=0.8754, gradient_norm=0.1837, 
[2025-09-17 16:16:48,113][__main__][INFO] - Train, Round 147: loss=0.3139, accuracy=0.8767, gradient_norm=0.1676, 
[2025-09-17 16:16:48,113][__main__][INFO] - Train, Round 148: loss=0.2761, accuracy=0.8968, gradient_norm=0.1659, 
[2025-09-17 16:16:48,113][__main__][INFO] - Train, Round 149: loss=0.3029, accuracy=0.8842, gradient_norm=0.1839, 
[2025-09-17 16:16:48,113][__main__][INFO] - Train, Round 150: loss=0.3473, accuracy=0.8648, gradient_norm=0.1615, 
[2025-09-17 16:16:48,113][__main__][INFO] - Train, Round 151: loss=0.2783, accuracy=0.8899, gradient_norm=0.1678, 
[2025-09-17 16:16:48,113][__main__][INFO] - Train, Round 152: loss=0.2607, accuracy=0.8976, gradient_norm=0.1578, 
[2025-09-17 16:16:48,113][__main__][INFO] - Train, Round 153: loss=0.3669, accuracy=0.8524, gradient_norm=0.1647, 
[2025-09-17 16:16:48,113][__main__][INFO] - Train, Round 154: loss=0.3098, accuracy=0.8791, gradient_norm=0.1826, 
[2025-09-17 16:16:48,113][__main__][INFO] - Train, Round 155: loss=0.3023, accuracy=0.8808, gradient_norm=0.1745, 
[2025-09-17 16:16:48,113][__main__][INFO] - Train, Round 156: loss=0.2959, accuracy=0.8861, gradient_norm=0.1725, 
[2025-09-17 16:16:48,113][__main__][INFO] - Train, Round 157: loss=0.2859, accuracy=0.8848, gradient_norm=0.1618, 
[2025-09-17 16:16:48,113][__main__][INFO] - Train, Round 158: loss=0.2874, accuracy=0.8885, gradient_norm=0.1667, 
[2025-09-17 16:16:48,113][__main__][INFO] - Train, Round 159: loss=0.3002, accuracy=0.8824, gradient_norm=0.1676, 
[2025-09-17 16:16:48,113][__main__][INFO] - Train, Round 160: loss=0.2699, accuracy=0.8953, gradient_norm=0.1604, 
[2025-09-17 16:16:48,113][__main__][INFO] - Train, Round 161: loss=0.2986, accuracy=0.8878, gradient_norm=0.1643, 
[2025-09-17 16:16:48,113][__main__][INFO] - Train, Round 162: loss=0.3076, accuracy=0.8793, gradient_norm=0.1698, 
[2025-09-17 16:16:48,113][__main__][INFO] - Train, Round 163: loss=0.2870, accuracy=0.8893, gradient_norm=0.1617, 
[2025-09-17 16:16:48,113][__main__][INFO] - Train, Round 164: loss=0.2878, accuracy=0.8914, gradient_norm=0.1667, 
[2025-09-17 16:16:48,113][__main__][INFO] - Train, Round 165: loss=0.2752, accuracy=0.8921, gradient_norm=0.1621, 
[2025-09-17 16:16:48,113][__main__][INFO] - Train, Round 166: loss=0.2541, accuracy=0.9019, gradient_norm=0.1571, 
[2025-09-17 16:16:48,113][__main__][INFO] - Train, Round 167: loss=0.2726, accuracy=0.8943, gradient_norm=0.1634, 
[2025-09-17 16:16:48,113][__main__][INFO] - Train, Round 168: loss=0.2438, accuracy=0.9083, gradient_norm=0.1460, 
[2025-09-17 16:16:48,113][__main__][INFO] - Train, Round 169: loss=0.3027, accuracy=0.8810, gradient_norm=0.1589, 
[2025-09-17 16:16:48,113][__main__][INFO] - Train, Round 170: loss=0.2680, accuracy=0.8962, gradient_norm=0.1604, 
[2025-09-17 16:16:48,113][__main__][INFO] - Train, Round 171: loss=0.2838, accuracy=0.8883, gradient_norm=0.1599, 
[2025-09-17 16:16:48,113][__main__][INFO] - Train, Round 172: loss=0.2752, accuracy=0.8937, gradient_norm=0.1679, 
[2025-09-17 16:16:48,113][__main__][INFO] - Train, Round 173: loss=0.2590, accuracy=0.9012, gradient_norm=0.1592, 
[2025-09-17 16:16:48,113][__main__][INFO] - Train, Round 174: loss=0.3117, accuracy=0.8776, gradient_norm=0.1647, 
[2025-09-17 16:16:48,113][__main__][INFO] - Train, Round 175: loss=0.2518, accuracy=0.9027, gradient_norm=0.1702, 
[2025-09-17 16:16:48,113][__main__][INFO] - Train, Round 176: loss=0.2036, accuracy=0.9235, gradient_norm=0.1583, 
[2025-09-17 16:16:48,113][__main__][INFO] - Train, Round 177: loss=0.2538, accuracy=0.9008, gradient_norm=0.1644, 
[2025-09-17 16:16:48,113][__main__][INFO] - Train, Round 178: loss=0.2217, accuracy=0.9181, gradient_norm=0.1561, 
[2025-09-17 16:16:48,113][__main__][INFO] - Train, Round 179: loss=0.2442, accuracy=0.9088, gradient_norm=0.1614, 
[2025-09-17 16:16:48,113][__main__][INFO] - Train, Round 180: loss=0.2361, accuracy=0.9105, gradient_norm=0.1672, 
[2025-09-17 16:16:48,113][__main__][INFO] - Train, Round 181: loss=0.2264, accuracy=0.9155, gradient_norm=0.1493, 
[2025-09-17 16:16:48,113][__main__][INFO] - Train, Round 182: loss=0.2343, accuracy=0.9091, gradient_norm=0.1536, 
[2025-09-17 16:16:48,113][__main__][INFO] - Train, Round 183: loss=0.2332, accuracy=0.9089, gradient_norm=0.1759, 
[2025-09-17 16:16:48,113][__main__][INFO] - Train, Round 184: loss=0.2257, accuracy=0.9139, gradient_norm=0.1758, 
[2025-09-17 16:16:48,113][__main__][INFO] - Train, Round 185: loss=0.2790, accuracy=0.8931, gradient_norm=0.1657, 
[2025-09-17 16:16:48,113][__main__][INFO] - Train, Round 186: loss=0.1792, accuracy=0.9342, gradient_norm=0.1627, 
[2025-09-17 16:16:48,113][__main__][INFO] - Train, Round 187: loss=0.2215, accuracy=0.9163, gradient_norm=0.1648, 
[2025-09-17 16:16:48,114][__main__][INFO] - Train, Round 188: loss=0.2218, accuracy=0.9167, gradient_norm=0.1540, 
[2025-09-17 16:16:48,114][__main__][INFO] - Train, Round 189: loss=0.2490, accuracy=0.9031, gradient_norm=0.1626, 
[2025-09-17 16:16:48,114][__main__][INFO] - Train, Round 190: loss=0.2348, accuracy=0.9092, gradient_norm=0.1506, 
[2025-09-17 16:16:48,114][__main__][INFO] - Train, Round 191: loss=0.1991, accuracy=0.9272, gradient_norm=0.1489, 
[2025-09-17 16:16:48,114][__main__][INFO] - Train, Round 192: loss=0.2640, accuracy=0.9010, gradient_norm=0.1609, 
[2025-09-17 16:16:48,114][__main__][INFO] - Train, Round 193: loss=0.2452, accuracy=0.9071, gradient_norm=0.1638, 
[2025-09-17 16:16:48,114][__main__][INFO] - Train, Round 194: loss=0.1752, accuracy=0.9337, gradient_norm=0.1550, 
[2025-09-17 16:16:48,114][__main__][INFO] - Train, Round 195: loss=0.2389, accuracy=0.9085, gradient_norm=0.1632, 
[2025-09-17 16:16:48,114][__main__][INFO] - Train, Round 196: loss=0.2101, accuracy=0.9206, gradient_norm=0.1656, 
[2025-09-17 16:16:48,114][__main__][INFO] - Train, Round 197: loss=0.2010, accuracy=0.9265, gradient_norm=0.1604, 
[2025-09-17 16:16:48,114][__main__][INFO] - Train, Round 198: loss=0.2255, accuracy=0.9149, gradient_norm=0.1671, 
[2025-09-17 16:16:48,114][__main__][INFO] - Train, Round 199: loss=0.2599, accuracy=0.8995, gradient_norm=0.1610, 
[2025-09-17 16:16:48,114][__main__][INFO] - Train, Round 200: loss=0.2099, accuracy=0.9190, gradient_norm=0.1623, 
[2025-09-17 16:16:48,114][__main__][INFO] - Test, Round 001: loss=2.1501, accuracy=0.2696, 
[2025-09-17 16:16:48,114][__main__][INFO] - Test, Round 002: loss=1.9722, accuracy=0.2958, 
[2025-09-17 16:16:48,114][__main__][INFO] - Test, Round 003: loss=1.9878, accuracy=0.3124, 
[2025-09-17 16:16:48,114][__main__][INFO] - Test, Round 004: loss=1.7343, accuracy=0.3573, 
[2025-09-17 16:16:48,114][__main__][INFO] - Test, Round 005: loss=1.5427, accuracy=0.4398, 
[2025-09-17 16:16:48,114][__main__][INFO] - Test, Round 006: loss=1.8608, accuracy=0.3588, 
[2025-09-17 16:16:48,114][__main__][INFO] - Test, Round 007: loss=1.6860, accuracy=0.4076, 
[2025-09-17 16:16:48,114][__main__][INFO] - Test, Round 008: loss=1.4040, accuracy=0.4752, 
[2025-09-17 16:16:48,114][__main__][INFO] - Test, Round 009: loss=1.3259, accuracy=0.5000, 
[2025-09-17 16:16:48,114][__main__][INFO] - Test, Round 010: loss=1.3117, accuracy=0.5388, 
[2025-09-17 16:16:48,114][__main__][INFO] - Test, Round 011: loss=1.5124, accuracy=0.4882, 
[2025-09-17 16:16:48,114][__main__][INFO] - Test, Round 012: loss=1.4023, accuracy=0.5191, 
[2025-09-17 16:16:48,114][__main__][INFO] - Test, Round 013: loss=1.4513, accuracy=0.4520, 
[2025-09-17 16:16:48,114][__main__][INFO] - Test, Round 014: loss=1.3950, accuracy=0.5096, 
[2025-09-17 16:16:48,114][__main__][INFO] - Test, Round 015: loss=1.6357, accuracy=0.4487, 
[2025-09-17 16:16:48,114][__main__][INFO] - Test, Round 016: loss=1.3071, accuracy=0.5247, 
[2025-09-17 16:16:48,114][__main__][INFO] - Test, Round 017: loss=1.1073, accuracy=0.5926, 
[2025-09-17 16:16:48,114][__main__][INFO] - Test, Round 018: loss=1.2175, accuracy=0.5506, 
[2025-09-17 16:16:48,114][__main__][INFO] - Test, Round 019: loss=0.9932, accuracy=0.6249, 
[2025-09-17 16:16:48,114][__main__][INFO] - Test, Round 020: loss=1.2741, accuracy=0.5375, 
[2025-09-17 16:16:48,114][__main__][INFO] - Test, Round 021: loss=1.1985, accuracy=0.5663, 
[2025-09-17 16:16:48,114][__main__][INFO] - Test, Round 022: loss=1.2239, accuracy=0.5575, 
[2025-09-17 16:16:48,114][__main__][INFO] - Test, Round 023: loss=0.9572, accuracy=0.6539, 
[2025-09-17 16:16:48,114][__main__][INFO] - Test, Round 024: loss=1.1458, accuracy=0.5782, 
[2025-09-17 16:16:48,114][__main__][INFO] - Test, Round 025: loss=1.1448, accuracy=0.5974, 
[2025-09-17 16:16:48,114][__main__][INFO] - Test, Round 026: loss=1.0139, accuracy=0.6278, 
[2025-09-17 16:16:48,114][__main__][INFO] - Test, Round 027: loss=0.9253, accuracy=0.6624, 
[2025-09-17 16:16:48,114][__main__][INFO] - Test, Round 028: loss=1.0797, accuracy=0.5974, 
[2025-09-17 16:16:48,114][__main__][INFO] - Test, Round 029: loss=0.9168, accuracy=0.6618, 
[2025-09-17 16:16:48,114][__main__][INFO] - Test, Round 030: loss=1.0805, accuracy=0.6202, 
[2025-09-17 16:16:48,114][__main__][INFO] - Test, Round 031: loss=0.9604, accuracy=0.6513, 
[2025-09-17 16:16:48,114][__main__][INFO] - Test, Round 032: loss=1.1050, accuracy=0.6285, 
[2025-09-17 16:16:48,114][__main__][INFO] - Test, Round 033: loss=0.9768, accuracy=0.6374, 
[2025-09-17 16:16:48,114][__main__][INFO] - Test, Round 034: loss=1.0947, accuracy=0.6070, 
[2025-09-17 16:16:48,114][__main__][INFO] - Test, Round 035: loss=0.9948, accuracy=0.6493, 
[2025-09-17 16:16:48,114][__main__][INFO] - Test, Round 036: loss=1.0258, accuracy=0.6466, 
[2025-09-17 16:16:48,114][__main__][INFO] - Test, Round 037: loss=0.9936, accuracy=0.6329, 
[2025-09-17 16:16:48,114][__main__][INFO] - Test, Round 038: loss=0.9832, accuracy=0.6636, 
[2025-09-17 16:16:48,114][__main__][INFO] - Test, Round 039: loss=1.0408, accuracy=0.6430, 
[2025-09-17 16:16:48,114][__main__][INFO] - Test, Round 040: loss=0.9180, accuracy=0.6981, 
[2025-09-17 16:16:48,114][__main__][INFO] - Test, Round 041: loss=1.2010, accuracy=0.6096, 
[2025-09-17 16:16:48,114][__main__][INFO] - Test, Round 042: loss=1.0426, accuracy=0.6122, 
[2025-09-17 16:16:48,114][__main__][INFO] - Test, Round 043: loss=0.9945, accuracy=0.6535, 
[2025-09-17 16:16:48,114][__main__][INFO] - Test, Round 044: loss=1.0762, accuracy=0.6469, 
[2025-09-17 16:16:48,114][__main__][INFO] - Test, Round 045: loss=0.9674, accuracy=0.6804, 
[2025-09-17 16:16:48,114][__main__][INFO] - Test, Round 046: loss=0.9772, accuracy=0.6806, 
[2025-09-17 16:16:48,114][__main__][INFO] - Test, Round 047: loss=0.9085, accuracy=0.7097, 
[2025-09-17 16:16:48,114][__main__][INFO] - Test, Round 048: loss=0.9892, accuracy=0.6767, 
[2025-09-17 16:16:48,114][__main__][INFO] - Test, Round 049: loss=1.0009, accuracy=0.6776, 
[2025-09-17 16:16:48,114][__main__][INFO] - Test, Round 050: loss=0.9268, accuracy=0.6967, 
[2025-09-17 16:16:48,115][__main__][INFO] - Test, Round 051: loss=0.9231, accuracy=0.7113, 
[2025-09-17 16:16:48,115][__main__][INFO] - Test, Round 052: loss=0.9709, accuracy=0.6736, 
[2025-09-17 16:16:48,115][__main__][INFO] - Test, Round 053: loss=0.8904, accuracy=0.7135, 
[2025-09-17 16:16:48,115][__main__][INFO] - Test, Round 054: loss=0.9387, accuracy=0.6994, 
[2025-09-17 16:16:48,115][__main__][INFO] - Test, Round 055: loss=1.0108, accuracy=0.6670, 
[2025-09-17 16:16:48,115][__main__][INFO] - Test, Round 056: loss=0.9057, accuracy=0.7127, 
[2025-09-17 16:16:48,115][__main__][INFO] - Test, Round 057: loss=0.9004, accuracy=0.7192, 
[2025-09-17 16:16:48,115][__main__][INFO] - Test, Round 058: loss=1.0069, accuracy=0.6670, 
[2025-09-17 16:16:48,115][__main__][INFO] - Test, Round 059: loss=0.9833, accuracy=0.6987, 
[2025-09-17 16:16:48,115][__main__][INFO] - Test, Round 060: loss=0.9770, accuracy=0.6958, 
[2025-09-17 16:16:48,115][__main__][INFO] - Test, Round 061: loss=0.9054, accuracy=0.7139, 
[2025-09-17 16:16:48,115][__main__][INFO] - Test, Round 062: loss=0.9006, accuracy=0.7239, 
[2025-09-17 16:16:48,115][__main__][INFO] - Test, Round 063: loss=0.9387, accuracy=0.7015, 
[2025-09-17 16:16:48,115][__main__][INFO] - Test, Round 064: loss=0.9093, accuracy=0.7148, 
[2025-09-17 16:16:48,115][__main__][INFO] - Test, Round 065: loss=0.9142, accuracy=0.7269, 
[2025-09-17 16:16:48,115][__main__][INFO] - Test, Round 066: loss=0.9052, accuracy=0.7235, 
[2025-09-17 16:16:48,115][__main__][INFO] - Test, Round 067: loss=0.9669, accuracy=0.7058, 
[2025-09-17 16:16:48,115][__main__][INFO] - Test, Round 068: loss=0.9022, accuracy=0.7164, 
[2025-09-17 16:16:48,115][__main__][INFO] - Test, Round 069: loss=0.8762, accuracy=0.7240, 
[2025-09-17 16:16:48,115][__main__][INFO] - Test, Round 070: loss=1.0700, accuracy=0.6549, 
[2025-09-17 16:16:48,115][__main__][INFO] - Test, Round 071: loss=0.8442, accuracy=0.7443, 
[2025-09-17 16:16:48,115][__main__][INFO] - Test, Round 072: loss=0.8625, accuracy=0.7364, 
[2025-09-17 16:16:48,115][__main__][INFO] - Test, Round 073: loss=0.9672, accuracy=0.7186, 
[2025-09-17 16:16:48,115][__main__][INFO] - Test, Round 074: loss=0.9164, accuracy=0.7071, 
[2025-09-17 16:16:48,115][__main__][INFO] - Test, Round 075: loss=0.9602, accuracy=0.7120, 
[2025-09-17 16:16:48,115][__main__][INFO] - Test, Round 076: loss=0.8966, accuracy=0.7158, 
[2025-09-17 16:16:48,115][__main__][INFO] - Test, Round 077: loss=0.8885, accuracy=0.7196, 
[2025-09-17 16:16:48,115][__main__][INFO] - Test, Round 078: loss=0.9752, accuracy=0.6916, 
[2025-09-17 16:16:48,115][__main__][INFO] - Test, Round 079: loss=0.9628, accuracy=0.7071, 
[2025-09-17 16:16:48,115][__main__][INFO] - Test, Round 080: loss=0.9164, accuracy=0.7267, 
[2025-09-17 16:16:48,115][__main__][INFO] - Test, Round 081: loss=0.9951, accuracy=0.7025, 
[2025-09-17 16:16:48,115][__main__][INFO] - Test, Round 082: loss=0.8949, accuracy=0.7370, 
[2025-09-17 16:16:48,115][__main__][INFO] - Test, Round 083: loss=0.9278, accuracy=0.7307, 
[2025-09-17 16:16:48,115][__main__][INFO] - Test, Round 084: loss=0.9170, accuracy=0.7242, 
[2025-09-17 16:16:48,115][__main__][INFO] - Test, Round 085: loss=0.9328, accuracy=0.7324, 
[2025-09-17 16:16:48,115][__main__][INFO] - Test, Round 086: loss=0.8673, accuracy=0.7489, 
[2025-09-17 16:16:48,115][__main__][INFO] - Test, Round 087: loss=0.9306, accuracy=0.7221, 
[2025-09-17 16:16:48,115][__main__][INFO] - Test, Round 088: loss=0.9664, accuracy=0.7124, 
[2025-09-17 16:16:48,115][__main__][INFO] - Test, Round 089: loss=0.8662, accuracy=0.7388, 
[2025-09-17 16:16:48,115][__main__][INFO] - Test, Round 090: loss=0.9141, accuracy=0.7264, 
[2025-09-17 16:16:48,115][__main__][INFO] - Test, Round 091: loss=0.9938, accuracy=0.7232, 
[2025-09-17 16:16:48,115][__main__][INFO] - Test, Round 092: loss=0.9527, accuracy=0.7386, 
[2025-09-17 16:16:48,115][__main__][INFO] - Test, Round 093: loss=0.9061, accuracy=0.7323, 
[2025-09-17 16:16:48,115][__main__][INFO] - Test, Round 094: loss=0.9469, accuracy=0.7381, 
[2025-09-17 16:16:48,115][__main__][INFO] - Test, Round 095: loss=0.8796, accuracy=0.7302, 
[2025-09-17 16:16:48,115][__main__][INFO] - Test, Round 096: loss=0.8891, accuracy=0.7493, 
[2025-09-17 16:16:48,115][__main__][INFO] - Test, Round 097: loss=0.8760, accuracy=0.7445, 
[2025-09-17 16:16:48,115][__main__][INFO] - Test, Round 098: loss=0.9366, accuracy=0.7281, 
[2025-09-17 16:16:48,115][__main__][INFO] - Test, Round 099: loss=0.9487, accuracy=0.7224, 
[2025-09-17 16:16:48,115][__main__][INFO] - Test, Round 100: loss=0.8831, accuracy=0.7448, 
[2025-09-17 16:16:48,115][__main__][INFO] - Test, Round 101: loss=1.0113, accuracy=0.7087, 
[2025-09-17 16:16:48,115][__main__][INFO] - Test, Round 102: loss=0.9656, accuracy=0.7452, 
[2025-09-17 16:16:48,115][__main__][INFO] - Test, Round 103: loss=0.8971, accuracy=0.7359, 
[2025-09-17 16:16:48,115][__main__][INFO] - Test, Round 104: loss=0.9162, accuracy=0.7313, 
[2025-09-17 16:16:48,115][__main__][INFO] - Test, Round 105: loss=0.9022, accuracy=0.7421, 
[2025-09-17 16:16:48,115][__main__][INFO] - Test, Round 106: loss=0.9331, accuracy=0.7298, 
[2025-09-17 16:16:48,115][__main__][INFO] - Test, Round 107: loss=0.8820, accuracy=0.7541, 
[2025-09-17 16:16:48,115][__main__][INFO] - Test, Round 108: loss=0.9285, accuracy=0.7465, 
[2025-09-17 16:16:48,115][__main__][INFO] - Test, Round 109: loss=0.9984, accuracy=0.7253, 
[2025-09-17 16:16:48,115][__main__][INFO] - Test, Round 110: loss=0.9114, accuracy=0.7331, 
[2025-09-17 16:16:48,115][__main__][INFO] - Test, Round 111: loss=0.9679, accuracy=0.7327, 
[2025-09-17 16:16:48,115][__main__][INFO] - Test, Round 112: loss=0.8863, accuracy=0.7557, 
[2025-09-17 16:16:48,115][__main__][INFO] - Test, Round 113: loss=0.9576, accuracy=0.7274, 
[2025-09-17 16:16:48,115][__main__][INFO] - Test, Round 114: loss=0.9718, accuracy=0.7300, 
[2025-09-17 16:16:48,116][__main__][INFO] - Test, Round 115: loss=0.9207, accuracy=0.7555, 
[2025-09-17 16:16:48,116][__main__][INFO] - Test, Round 116: loss=0.8733, accuracy=0.7553, 
[2025-09-17 16:16:48,116][__main__][INFO] - Test, Round 117: loss=0.9792, accuracy=0.7400, 
[2025-09-17 16:16:48,116][__main__][INFO] - Test, Round 118: loss=0.9885, accuracy=0.7288, 
[2025-09-17 16:16:48,116][__main__][INFO] - Test, Round 119: loss=0.9316, accuracy=0.7463, 
[2025-09-17 16:16:48,116][__main__][INFO] - Test, Round 120: loss=0.9322, accuracy=0.7459, 
[2025-09-17 16:16:48,116][__main__][INFO] - Test, Round 121: loss=0.8976, accuracy=0.7407, 
[2025-09-17 16:16:48,116][__main__][INFO] - Test, Round 122: loss=0.9064, accuracy=0.7626, 
[2025-09-17 16:16:48,116][__main__][INFO] - Test, Round 123: loss=0.9690, accuracy=0.7388, 
[2025-09-17 16:16:48,116][__main__][INFO] - Test, Round 124: loss=0.9539, accuracy=0.7473, 
[2025-09-17 16:16:48,116][__main__][INFO] - Test, Round 125: loss=0.9535, accuracy=0.7359, 
[2025-09-17 16:16:48,116][__main__][INFO] - Test, Round 126: loss=0.9271, accuracy=0.7119, 
[2025-09-17 16:16:48,116][__main__][INFO] - Test, Round 127: loss=0.9347, accuracy=0.7592, 
[2025-09-17 16:16:48,116][__main__][INFO] - Test, Round 128: loss=0.9589, accuracy=0.7491, 
[2025-09-17 16:16:48,116][__main__][INFO] - Test, Round 129: loss=0.9737, accuracy=0.7407, 
[2025-09-17 16:16:48,116][__main__][INFO] - Test, Round 130: loss=0.9374, accuracy=0.7593, 
[2025-09-17 16:16:48,116][__main__][INFO] - Test, Round 131: loss=0.9141, accuracy=0.7423, 
[2025-09-17 16:16:48,116][__main__][INFO] - Test, Round 132: loss=0.9187, accuracy=0.7486, 
[2025-09-17 16:16:48,116][__main__][INFO] - Test, Round 133: loss=0.9432, accuracy=0.7436, 
[2025-09-17 16:16:48,116][__main__][INFO] - Test, Round 134: loss=0.9247, accuracy=0.7486, 
[2025-09-17 16:16:48,116][__main__][INFO] - Test, Round 135: loss=0.9786, accuracy=0.7638, 
[2025-09-17 16:16:48,116][__main__][INFO] - Test, Round 136: loss=0.9205, accuracy=0.7623, 
[2025-09-17 16:16:48,116][__main__][INFO] - Test, Round 137: loss=0.9221, accuracy=0.7507, 
[2025-09-17 16:16:48,116][__main__][INFO] - Test, Round 138: loss=1.0261, accuracy=0.7178, 
[2025-09-17 16:16:48,116][__main__][INFO] - Test, Round 139: loss=0.9237, accuracy=0.7452, 
[2025-09-17 16:16:48,116][__main__][INFO] - Test, Round 140: loss=0.9964, accuracy=0.7235, 
[2025-09-17 16:16:48,116][__main__][INFO] - Test, Round 141: loss=0.9678, accuracy=0.7196, 
[2025-09-17 16:16:48,116][__main__][INFO] - Test, Round 142: loss=0.9853, accuracy=0.7279, 
[2025-09-17 16:16:48,116][__main__][INFO] - Test, Round 143: loss=0.9652, accuracy=0.7486, 
[2025-09-17 16:16:48,116][__main__][INFO] - Test, Round 144: loss=0.9877, accuracy=0.7347, 
[2025-09-17 16:16:48,116][__main__][INFO] - Test, Round 145: loss=0.9641, accuracy=0.7635, 
[2025-09-17 16:16:48,116][__main__][INFO] - Test, Round 146: loss=0.9628, accuracy=0.7623, 
[2025-09-17 16:16:48,116][__main__][INFO] - Test, Round 147: loss=0.9361, accuracy=0.7379, 
[2025-09-17 16:16:48,116][__main__][INFO] - Test, Round 148: loss=0.9706, accuracy=0.7727, 
[2025-09-17 16:16:48,116][__main__][INFO] - Test, Round 149: loss=0.9638, accuracy=0.7397, 
[2025-09-17 16:16:48,116][__main__][INFO] - Test, Round 150: loss=0.9604, accuracy=0.7445, 
[2025-09-17 16:16:48,116][__main__][INFO] - Test, Round 151: loss=1.0022, accuracy=0.7641, 
[2025-09-17 16:16:48,116][__main__][INFO] - Test, Round 152: loss=0.8741, accuracy=0.7780, 
[2025-09-17 16:16:48,116][__main__][INFO] - Test, Round 153: loss=0.9594, accuracy=0.7399, 
[2025-09-17 16:16:48,116][__main__][INFO] - Test, Round 154: loss=0.9845, accuracy=0.7587, 
[2025-09-17 16:16:48,116][__main__][INFO] - Test, Round 155: loss=0.9262, accuracy=0.7675, 
[2025-09-17 16:16:48,116][__main__][INFO] - Test, Round 156: loss=1.0067, accuracy=0.7430, 
[2025-09-17 16:16:48,116][__main__][INFO] - Test, Round 157: loss=0.9416, accuracy=0.7493, 
[2025-09-17 16:16:48,116][__main__][INFO] - Test, Round 158: loss=0.9503, accuracy=0.7339, 
[2025-09-17 16:16:48,116][__main__][INFO] - Test, Round 159: loss=0.9831, accuracy=0.7345, 
[2025-09-17 16:16:48,116][__main__][INFO] - Test, Round 160: loss=0.9458, accuracy=0.7532, 
[2025-09-17 16:16:48,116][__main__][INFO] - Test, Round 161: loss=0.9597, accuracy=0.7507, 
[2025-09-17 16:16:48,116][__main__][INFO] - Test, Round 162: loss=0.9730, accuracy=0.7484, 
[2025-09-17 16:16:48,116][__main__][INFO] - Test, Round 163: loss=1.0516, accuracy=0.7468, 
[2025-09-17 16:16:48,116][__main__][INFO] - Test, Round 164: loss=0.9236, accuracy=0.7592, 
[2025-09-17 16:16:48,116][__main__][INFO] - Test, Round 165: loss=0.9616, accuracy=0.7382, 
[2025-09-17 16:16:48,116][__main__][INFO] - Test, Round 166: loss=1.0060, accuracy=0.7573, 
[2025-09-17 16:16:48,116][__main__][INFO] - Test, Round 167: loss=0.9422, accuracy=0.7552, 
[2025-09-17 16:16:48,116][__main__][INFO] - Test, Round 168: loss=1.0330, accuracy=0.7608, 
[2025-09-17 16:16:48,116][__main__][INFO] - Test, Round 169: loss=0.9634, accuracy=0.7525, 
[2025-09-17 16:16:48,116][__main__][INFO] - Test, Round 170: loss=0.9793, accuracy=0.7470, 
[2025-09-17 16:16:48,116][__main__][INFO] - Test, Round 171: loss=0.9201, accuracy=0.7694, 
[2025-09-17 16:16:48,116][__main__][INFO] - Test, Round 172: loss=0.9957, accuracy=0.7310, 
[2025-09-17 16:16:48,116][__main__][INFO] - Test, Round 173: loss=0.9187, accuracy=0.7607, 
[2025-09-17 16:16:48,116][__main__][INFO] - Test, Round 174: loss=1.0019, accuracy=0.7443, 
[2025-09-17 16:16:48,116][__main__][INFO] - Test, Round 175: loss=0.9478, accuracy=0.7609, 
[2025-09-17 16:16:48,116][__main__][INFO] - Test, Round 176: loss=1.0162, accuracy=0.7557, 
[2025-09-17 16:16:48,116][__main__][INFO] - Test, Round 177: loss=1.0573, accuracy=0.7372, 
[2025-09-17 16:16:48,116][__main__][INFO] - Test, Round 178: loss=0.9827, accuracy=0.7901, 
[2025-09-17 16:16:48,116][__main__][INFO] - Test, Round 179: loss=1.0172, accuracy=0.7543, 
[2025-09-17 16:16:48,117][__main__][INFO] - Test, Round 180: loss=0.9669, accuracy=0.7596, 
[2025-09-17 16:16:48,117][__main__][INFO] - Test, Round 181: loss=0.9646, accuracy=0.7545, 
[2025-09-17 16:16:48,117][__main__][INFO] - Test, Round 182: loss=1.0373, accuracy=0.7406, 
[2025-09-17 16:16:48,117][__main__][INFO] - Test, Round 183: loss=0.9911, accuracy=0.7520, 
[2025-09-17 16:16:48,117][__main__][INFO] - Test, Round 184: loss=1.0896, accuracy=0.7316, 
[2025-09-17 16:16:48,117][__main__][INFO] - Test, Round 185: loss=0.9405, accuracy=0.7544, 
[2025-09-17 16:16:48,117][__main__][INFO] - Test, Round 186: loss=1.0044, accuracy=0.7699, 
[2025-09-17 16:16:48,117][__main__][INFO] - Test, Round 187: loss=0.9573, accuracy=0.7748, 
[2025-09-17 16:16:48,117][__main__][INFO] - Test, Round 188: loss=0.9595, accuracy=0.7815, 
[2025-09-17 16:16:48,117][__main__][INFO] - Test, Round 189: loss=0.9861, accuracy=0.7434, 
[2025-09-17 16:16:48,117][__main__][INFO] - Test, Round 190: loss=0.9782, accuracy=0.7468, 
[2025-09-17 16:16:48,117][__main__][INFO] - Test, Round 191: loss=0.9894, accuracy=0.7703, 
[2025-09-17 16:16:48,117][__main__][INFO] - Test, Round 192: loss=0.9640, accuracy=0.7578, 
[2025-09-17 16:16:48,117][__main__][INFO] - Test, Round 193: loss=0.9696, accuracy=0.7448, 
[2025-09-17 16:16:48,117][__main__][INFO] - Test, Round 194: loss=1.0069, accuracy=0.7861, 
[2025-09-17 16:16:48,117][__main__][INFO] - Test, Round 195: loss=1.0045, accuracy=0.7425, 
[2025-09-17 16:16:48,117][__main__][INFO] - Test, Round 196: loss=1.0415, accuracy=0.7418, 
[2025-09-17 16:16:48,117][__main__][INFO] - Test, Round 197: loss=0.9672, accuracy=0.7718, 
[2025-09-17 16:16:48,117][__main__][INFO] - Test, Round 198: loss=1.0608, accuracy=0.7448, 
[2025-09-17 16:16:48,117][__main__][INFO] - Test, Round 199: loss=0.9594, accuracy=0.7612, 
[2025-09-17 16:16:48,117][__main__][INFO] - Test, Round 200: loss=1.0193, accuracy=0.7680, 
