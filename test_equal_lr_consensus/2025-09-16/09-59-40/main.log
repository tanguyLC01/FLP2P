[2025-09-16 09:59:54,262][flp2p.graph_runner][INFO] - Train, Round 0 : loss => 2.3048175781965257,  accuracy: 0.1055, gradient_norm : 0.23842065734638415
[2025-09-16 10:00:18,588][flp2p.graph_runner][INFO] - Test, Round 0 : loss => 2.304754534327984,  accuracy: 0.1075
[2025-09-16 10:00:34,999][flp2p.graph_runner][INFO] - Train, Round 1 : loss => 2.304842571616173,  accuracy: 0.10556, gradient_norm : 0.24311006263557164
[2025-09-16 10:01:01,569][flp2p.graph_runner][INFO] - Test, Round 1 : loss => 2.304743252015114,  accuracy: 0.1076
[2025-09-16 10:01:19,020][flp2p.graph_runner][INFO] - Train, Round 2 : loss => 2.30482892036438,  accuracy: 0.10562, gradient_norm : 0.23730231624936482
[2025-09-16 10:01:44,815][flp2p.graph_runner][INFO] - Test, Round 2 : loss => 2.3047316689133646,  accuracy: 0.1077
[2025-09-16 10:02:02,600][flp2p.graph_runner][INFO] - Train, Round 3 : loss => 2.30481625944376,  accuracy: 0.10572, gradient_norm : 0.24062582058333706
[2025-09-16 10:02:27,978][flp2p.graph_runner][INFO] - Test, Round 3 : loss => 2.304720542359352,  accuracy: 0.1079
[2025-09-16 10:02:45,648][flp2p.graph_runner][INFO] - Train, Round 4 : loss => 2.3047936761379244,  accuracy: 0.1058, gradient_norm : 0.23863088908002467
[2025-09-16 10:03:11,463][flp2p.graph_runner][INFO] - Test, Round 4 : loss => 2.3047094898104667,  accuracy: 0.1081
[2025-09-16 10:03:29,376][flp2p.graph_runner][INFO] - Train, Round 5 : loss => 2.3047676119208336,  accuracy: 0.10588, gradient_norm : 0.24426154677011946
[2025-09-16 10:03:55,100][flp2p.graph_runner][INFO] - Test, Round 5 : loss => 2.3046982234597206,  accuracy: 0.1084
[2025-09-16 10:04:12,999][flp2p.graph_runner][INFO] - Train, Round 6 : loss => 2.304767075181007,  accuracy: 0.10592, gradient_norm : 0.24349388250044396
[2025-09-16 10:04:38,797][flp2p.graph_runner][INFO] - Test, Round 6 : loss => 2.3046871374607085,  accuracy: 0.1084
[2025-09-16 10:04:56,795][flp2p.graph_runner][INFO] - Train, Round 7 : loss => 2.304786133766174,  accuracy: 0.10592, gradient_norm : 0.23952914391329602
[2025-09-16 10:05:22,633][flp2p.graph_runner][INFO] - Test, Round 7 : loss => 2.3046757460713385,  accuracy: 0.1087
[2025-09-16 10:05:40,206][flp2p.graph_runner][INFO] - Train, Round 8 : loss => 2.3047312408685685,  accuracy: 0.10598, gradient_norm : 0.23463916845712923
[2025-09-16 10:06:06,391][flp2p.graph_runner][INFO] - Test, Round 8 : loss => 2.3046643609642983,  accuracy: 0.1087
[2025-09-16 10:06:24,258][flp2p.graph_runner][INFO] - Train, Round 9 : loss => 2.3047336959838867,  accuracy: 0.10604, gradient_norm : 0.23778765027851026
[2025-09-16 10:06:49,756][flp2p.graph_runner][INFO] - Test, Round 9 : loss => 2.3046529886960982,  accuracy: 0.1088
[2025-09-16 10:07:07,602][flp2p.graph_runner][INFO] - Train, Round 10 : loss => 2.3046875062584875,  accuracy: 0.10602, gradient_norm : 0.24122620402715458
[2025-09-16 10:07:33,686][flp2p.graph_runner][INFO] - Test, Round 10 : loss => 2.3046421337366105,  accuracy: 0.1088
[2025-09-16 10:07:51,371][flp2p.graph_runner][INFO] - Train, Round 11 : loss => 2.304755453467369,  accuracy: 0.10618, gradient_norm : 0.2434265529464918
[2025-09-16 10:08:16,902][flp2p.graph_runner][INFO] - Test, Round 11 : loss => 2.3046304444074632,  accuracy: 0.1089
[2025-09-16 10:08:34,503][flp2p.graph_runner][INFO] - Train, Round 12 : loss => 2.304652819931507,  accuracy: 0.10632, gradient_norm : 0.2437873815237994
[2025-09-16 10:09:00,251][flp2p.graph_runner][INFO] - Test, Round 12 : loss => 2.3046196319699286,  accuracy: 0.1091
[2025-09-16 10:09:18,012][flp2p.graph_runner][INFO] - Train, Round 13 : loss => 2.304700514674187,  accuracy: 0.10636, gradient_norm : 0.23625859393335752
[2025-09-16 10:09:43,952][flp2p.graph_runner][INFO] - Test, Round 13 : loss => 2.3046080393314363,  accuracy: 0.1091
[2025-09-16 10:10:01,834][flp2p.graph_runner][INFO] - Train, Round 14 : loss => 2.304658833742142,  accuracy: 0.10646, gradient_norm : 0.2409511682268397
[2025-09-16 10:10:27,782][flp2p.graph_runner][INFO] - Test, Round 14 : loss => 2.3045968779325485,  accuracy: 0.1095
[2025-09-16 10:10:45,645][flp2p.graph_runner][INFO] - Train, Round 15 : loss => 2.3046657437086107,  accuracy: 0.10656, gradient_norm : 0.23869414801351893
[2025-09-16 10:11:11,490][flp2p.graph_runner][INFO] - Test, Round 15 : loss => 2.3045853840231896,  accuracy: 0.1099
[2025-09-16 10:11:29,412][flp2p.graph_runner][INFO] - Train, Round 16 : loss => 2.304644478261471,  accuracy: 0.1066, gradient_norm : 0.24297968578281604
[2025-09-16 10:11:55,221][flp2p.graph_runner][INFO] - Test, Round 16 : loss => 2.304574141907692,  accuracy: 0.1098
[2025-09-16 10:12:13,154][flp2p.graph_runner][INFO] - Train, Round 17 : loss => 2.304628294706345,  accuracy: 0.10688, gradient_norm : 0.24107934939273687
[2025-09-16 10:12:39,187][flp2p.graph_runner][INFO] - Test, Round 17 : loss => 2.3045630297541617,  accuracy: 0.1095
[2025-09-16 10:12:57,494][flp2p.graph_runner][INFO] - Train, Round 18 : loss => 2.3046046549081804,  accuracy: 0.10702, gradient_norm : 0.23921696539246223
[2025-09-16 10:13:23,305][flp2p.graph_runner][INFO] - Test, Round 18 : loss => 2.304551989245415,  accuracy: 0.1098
[2025-09-16 10:13:41,154][flp2p.graph_runner][INFO] - Train, Round 19 : loss => 2.3046302565932275,  accuracy: 0.10704, gradient_norm : 0.2396391774320497
[2025-09-16 10:14:06,871][flp2p.graph_runner][INFO] - Test, Round 19 : loss => 2.3045407784700394,  accuracy: 0.1097
[2025-09-16 10:14:24,736][flp2p.graph_runner][INFO] - Train, Round 20 : loss => 2.3046054670214655,  accuracy: 0.1071, gradient_norm : 0.24630913321216907
[2025-09-16 10:14:51,593][flp2p.graph_runner][INFO] - Test, Round 20 : loss => 2.3045293115496635,  accuracy: 0.1097
[2025-09-16 10:15:09,742][flp2p.graph_runner][INFO] - Train, Round 21 : loss => 2.304527652859688,  accuracy: 0.1072, gradient_norm : 0.24070218135522353
[2025-09-16 10:15:37,121][flp2p.graph_runner][INFO] - Test, Round 21 : loss => 2.304518552899361,  accuracy: 0.11
[2025-09-16 10:15:55,376][flp2p.graph_runner][INFO] - Train, Round 22 : loss => 2.3045533415675163,  accuracy: 0.10724, gradient_norm : 0.24210145935072266
[2025-09-16 10:16:23,205][flp2p.graph_runner][INFO] - Test, Round 22 : loss => 2.304507681453228,  accuracy: 0.1103
[2025-09-16 10:16:41,299][flp2p.graph_runner][INFO] - Train, Round 23 : loss => 2.304484794139862,  accuracy: 0.10726, gradient_norm : 0.242026943162204
[2025-09-16 10:17:08,968][flp2p.graph_runner][INFO] - Test, Round 23 : loss => 2.3044971412301063,  accuracy: 0.1104
[2025-09-16 10:17:27,260][flp2p.graph_runner][INFO] - Train, Round 24 : loss => 2.304514294266701,  accuracy: 0.10734, gradient_norm : 0.2365523162475804
[2025-09-16 10:17:56,268][flp2p.graph_runner][INFO] - Test, Round 24 : loss => 2.3044861406326294,  accuracy: 0.1106
[2025-09-16 10:18:14,637][flp2p.graph_runner][INFO] - Train, Round 25 : loss => 2.3045195427536966,  accuracy: 0.10762, gradient_norm : 0.23908203270894288
[2025-09-16 10:18:42,535][flp2p.graph_runner][INFO] - Test, Round 25 : loss => 2.3044750479221343,  accuracy: 0.1111
[2025-09-16 10:19:00,720][flp2p.graph_runner][INFO] - Train, Round 26 : loss => 2.3044972115755082,  accuracy: 0.1078, gradient_norm : 0.23866913336091167
[2025-09-16 10:19:28,674][flp2p.graph_runner][INFO] - Test, Round 26 : loss => 2.304464093887806,  accuracy: 0.1113
[2025-09-16 10:19:46,701][flp2p.graph_runner][INFO] - Train, Round 27 : loss => 2.304506159722805,  accuracy: 0.10794, gradient_norm : 0.24092185732422183
[2025-09-16 10:20:14,935][flp2p.graph_runner][INFO] - Test, Round 27 : loss => 2.3044528089284895,  accuracy: 0.1113
[2025-09-16 10:20:32,968][flp2p.graph_runner][INFO] - Train, Round 28 : loss => 2.304513866305351,  accuracy: 0.10804, gradient_norm : 0.234893188874315
[2025-09-16 10:21:01,847][flp2p.graph_runner][INFO] - Test, Round 28 : loss => 2.3044414842247964,  accuracy: 0.1118
[2025-09-16 10:21:20,176][flp2p.graph_runner][INFO] - Train, Round 29 : loss => 2.30444849640131,  accuracy: 0.10808, gradient_norm : 0.2394892145102625
[2025-09-16 10:21:48,292][flp2p.graph_runner][INFO] - Test, Round 29 : loss => 2.3044306262135508,  accuracy: 0.1118
[2025-09-16 10:22:06,354][flp2p.graph_runner][INFO] - Train, Round 30 : loss => 2.304467276632786,  accuracy: 0.10814, gradient_norm : 0.2388561252735517
[2025-09-16 10:22:34,759][flp2p.graph_runner][INFO] - Test, Round 30 : loss => 2.304419525194168,  accuracy: 0.112
[2025-09-16 10:22:53,239][flp2p.graph_runner][INFO] - Train, Round 31 : loss => 2.304483809173107,  accuracy: 0.10822, gradient_norm : 0.2433093062753549
[2025-09-16 10:23:22,775][flp2p.graph_runner][INFO] - Test, Round 31 : loss => 2.3044080471754076,  accuracy: 0.1122
[2025-09-16 10:23:41,245][flp2p.graph_runner][INFO] - Train, Round 32 : loss => 2.3044750729203223,  accuracy: 0.10836, gradient_norm : 0.24415522289561664
[2025-09-16 10:24:10,942][flp2p.graph_runner][INFO] - Test, Round 32 : loss => 2.304396567630768,  accuracy: 0.1123
[2025-09-16 10:24:29,447][flp2p.graph_runner][INFO] - Train, Round 33 : loss => 2.3044225937128067,  accuracy: 0.10844, gradient_norm : 0.23389580343657396
[2025-09-16 10:24:58,865][flp2p.graph_runner][INFO] - Test, Round 33 : loss => 2.3043856911540033,  accuracy: 0.1123
[2025-09-16 10:25:17,168][flp2p.graph_runner][INFO] - Train, Round 34 : loss => 2.3044007772207262,  accuracy: 0.1085, gradient_norm : 0.24620207959483564
[2025-09-16 10:25:46,299][flp2p.graph_runner][INFO] - Test, Round 34 : loss => 2.3043747923016547,  accuracy: 0.1124
[2025-09-16 10:26:04,525][flp2p.graph_runner][INFO] - Train, Round 35 : loss => 2.304395206272602,  accuracy: 0.10858, gradient_norm : 0.23792364359280654
[2025-09-16 10:26:35,072][flp2p.graph_runner][INFO] - Test, Round 35 : loss => 2.3043637588977814,  accuracy: 0.1125
[2025-09-16 10:26:53,257][flp2p.graph_runner][INFO] - Train, Round 36 : loss => 2.304419315457344,  accuracy: 0.10862, gradient_norm : 0.24146532476857313
[2025-09-16 10:27:22,724][flp2p.graph_runner][INFO] - Test, Round 36 : loss => 2.304352324962616,  accuracy: 0.1126
[2025-09-16 10:27:40,803][flp2p.graph_runner][INFO] - Train, Round 37 : loss => 2.3043816715478895,  accuracy: 0.10882, gradient_norm : 0.23658021931646608
[2025-09-16 10:28:10,249][flp2p.graph_runner][INFO] - Test, Round 37 : loss => 2.3043411606431006,  accuracy: 0.1127
[2025-09-16 10:28:28,489][flp2p.graph_runner][INFO] - Train, Round 38 : loss => 2.304346345961094,  accuracy: 0.10896, gradient_norm : 0.23798188369159864
[2025-09-16 10:28:57,946][flp2p.graph_runner][INFO] - Test, Round 38 : loss => 2.3043302639365195,  accuracy: 0.1127
[2025-09-16 10:29:16,206][flp2p.graph_runner][INFO] - Train, Round 39 : loss => 2.3043750885128973,  accuracy: 0.10918, gradient_norm : 0.2371932111097815
[2025-09-16 10:29:45,616][flp2p.graph_runner][INFO] - Test, Round 39 : loss => 2.304319268465042,  accuracy: 0.1127
[2025-09-16 10:30:03,774][flp2p.graph_runner][INFO] - Train, Round 40 : loss => 2.3043592795729637,  accuracy: 0.10938, gradient_norm : 0.2419810978455578
[2025-09-16 10:30:32,954][flp2p.graph_runner][INFO] - Test, Round 40 : loss => 2.3043077054023744,  accuracy: 0.1128
[2025-09-16 10:30:50,976][flp2p.graph_runner][INFO] - Train, Round 41 : loss => 2.3043533766269686,  accuracy: 0.10952, gradient_norm : 0.23777581995506789
[2025-09-16 10:31:20,317][flp2p.graph_runner][INFO] - Test, Round 41 : loss => 2.3042965240359305,  accuracy: 0.113
[2025-09-16 10:31:38,421][flp2p.graph_runner][INFO] - Train, Round 42 : loss => 2.3043072444200514,  accuracy: 0.1097, gradient_norm : 0.23916482484750193
[2025-09-16 10:32:07,719][flp2p.graph_runner][INFO] - Test, Round 42 : loss => 2.304285710811615,  accuracy: 0.1132
[2025-09-16 10:32:25,914][flp2p.graph_runner][INFO] - Train, Round 43 : loss => 2.304317380785942,  accuracy: 0.10982, gradient_norm : 0.24254276318910523
[2025-09-16 10:32:55,679][flp2p.graph_runner][INFO] - Test, Round 43 : loss => 2.3042742626786232,  accuracy: 0.1137
[2025-09-16 10:33:13,991][flp2p.graph_runner][INFO] - Train, Round 44 : loss => 2.304298809468746,  accuracy: 0.10992, gradient_norm : 0.2426477372264641
[2025-09-16 10:33:43,088][flp2p.graph_runner][INFO] - Test, Round 44 : loss => 2.3042631644368172,  accuracy: 0.1137
[2025-09-16 10:34:01,393][flp2p.graph_runner][INFO] - Train, Round 45 : loss => 2.3042806935310365,  accuracy: 0.1101, gradient_norm : 0.23863944369003678
[2025-09-16 10:34:31,069][flp2p.graph_runner][INFO] - Test, Round 45 : loss => 2.3042522195100785,  accuracy: 0.1137
[2025-09-16 10:34:49,302][flp2p.graph_runner][INFO] - Train, Round 46 : loss => 2.3042665773630144,  accuracy: 0.11012, gradient_norm : 0.2420817001125507
[2025-09-16 10:35:18,932][flp2p.graph_runner][INFO] - Test, Round 46 : loss => 2.304241106450558,  accuracy: 0.1136
[2025-09-16 10:35:37,106][flp2p.graph_runner][INFO] - Train, Round 47 : loss => 2.3042540353536607,  accuracy: 0.11022, gradient_norm : 0.23512925409297744
[2025-09-16 10:36:06,637][flp2p.graph_runner][INFO] - Test, Round 47 : loss => 2.304230266082287,  accuracy: 0.1137
[2025-09-16 10:36:24,949][flp2p.graph_runner][INFO] - Train, Round 48 : loss => 2.304252201616764,  accuracy: 0.11044, gradient_norm : 0.23583318450019206
[2025-09-16 10:36:54,883][flp2p.graph_runner][INFO] - Test, Round 48 : loss => 2.3042191710829734,  accuracy: 0.1138
[2025-09-16 10:37:12,985][flp2p.graph_runner][INFO] - Train, Round 49 : loss => 2.3042295762896536,  accuracy: 0.11066, gradient_norm : 0.23669493223506655
[2025-09-16 10:37:42,770][flp2p.graph_runner][INFO] - Test, Round 49 : loss => 2.3042082128047943,  accuracy: 0.1138
[2025-09-16 10:37:42,790][__main__][INFO] - Train, Round 001: loss=2.3048, accuracy=0.1055, gradient_norm=0.2384, 
[2025-09-16 10:37:42,791][__main__][INFO] - Train, Round 002: loss=2.3048, accuracy=0.1056, gradient_norm=0.2431, 
[2025-09-16 10:37:42,791][__main__][INFO] - Train, Round 003: loss=2.3048, accuracy=0.1056, gradient_norm=0.2373, 
[2025-09-16 10:37:42,791][__main__][INFO] - Train, Round 004: loss=2.3048, accuracy=0.1057, gradient_norm=0.2406, 
[2025-09-16 10:37:42,791][__main__][INFO] - Train, Round 005: loss=2.3048, accuracy=0.1058, gradient_norm=0.2386, 
[2025-09-16 10:37:42,791][__main__][INFO] - Train, Round 006: loss=2.3048, accuracy=0.1059, gradient_norm=0.2443, 
[2025-09-16 10:37:42,791][__main__][INFO] - Train, Round 007: loss=2.3048, accuracy=0.1059, gradient_norm=0.2435, 
[2025-09-16 10:37:42,791][__main__][INFO] - Train, Round 008: loss=2.3048, accuracy=0.1059, gradient_norm=0.2395, 
[2025-09-16 10:37:42,791][__main__][INFO] - Train, Round 009: loss=2.3047, accuracy=0.1060, gradient_norm=0.2346, 
[2025-09-16 10:37:42,791][__main__][INFO] - Train, Round 010: loss=2.3047, accuracy=0.1060, gradient_norm=0.2378, 
[2025-09-16 10:37:42,791][__main__][INFO] - Train, Round 011: loss=2.3047, accuracy=0.1060, gradient_norm=0.2412, 
[2025-09-16 10:37:42,791][__main__][INFO] - Train, Round 012: loss=2.3048, accuracy=0.1062, gradient_norm=0.2434, 
[2025-09-16 10:37:42,791][__main__][INFO] - Train, Round 013: loss=2.3047, accuracy=0.1063, gradient_norm=0.2438, 
[2025-09-16 10:37:42,791][__main__][INFO] - Train, Round 014: loss=2.3047, accuracy=0.1064, gradient_norm=0.2363, 
[2025-09-16 10:37:42,791][__main__][INFO] - Train, Round 015: loss=2.3047, accuracy=0.1065, gradient_norm=0.2410, 
[2025-09-16 10:37:42,791][__main__][INFO] - Train, Round 016: loss=2.3047, accuracy=0.1066, gradient_norm=0.2387, 
[2025-09-16 10:37:42,791][__main__][INFO] - Train, Round 017: loss=2.3046, accuracy=0.1066, gradient_norm=0.2430, 
[2025-09-16 10:37:42,791][__main__][INFO] - Train, Round 018: loss=2.3046, accuracy=0.1069, gradient_norm=0.2411, 
[2025-09-16 10:37:42,791][__main__][INFO] - Train, Round 019: loss=2.3046, accuracy=0.1070, gradient_norm=0.2392, 
[2025-09-16 10:37:42,791][__main__][INFO] - Train, Round 020: loss=2.3046, accuracy=0.1070, gradient_norm=0.2396, 
[2025-09-16 10:37:42,791][__main__][INFO] - Train, Round 021: loss=2.3046, accuracy=0.1071, gradient_norm=0.2463, 
[2025-09-16 10:37:42,791][__main__][INFO] - Train, Round 022: loss=2.3045, accuracy=0.1072, gradient_norm=0.2407, 
[2025-09-16 10:37:42,791][__main__][INFO] - Train, Round 023: loss=2.3046, accuracy=0.1072, gradient_norm=0.2421, 
[2025-09-16 10:37:42,791][__main__][INFO] - Train, Round 024: loss=2.3045, accuracy=0.1073, gradient_norm=0.2420, 
[2025-09-16 10:37:42,791][__main__][INFO] - Train, Round 025: loss=2.3045, accuracy=0.1073, gradient_norm=0.2366, 
[2025-09-16 10:37:42,791][__main__][INFO] - Train, Round 026: loss=2.3045, accuracy=0.1076, gradient_norm=0.2391, 
[2025-09-16 10:37:42,791][__main__][INFO] - Train, Round 027: loss=2.3045, accuracy=0.1078, gradient_norm=0.2387, 
[2025-09-16 10:37:42,791][__main__][INFO] - Train, Round 028: loss=2.3045, accuracy=0.1079, gradient_norm=0.2409, 
[2025-09-16 10:37:42,791][__main__][INFO] - Train, Round 029: loss=2.3045, accuracy=0.1080, gradient_norm=0.2349, 
[2025-09-16 10:37:42,791][__main__][INFO] - Train, Round 030: loss=2.3044, accuracy=0.1081, gradient_norm=0.2395, 
[2025-09-16 10:37:42,791][__main__][INFO] - Train, Round 031: loss=2.3045, accuracy=0.1081, gradient_norm=0.2389, 
[2025-09-16 10:37:42,791][__main__][INFO] - Train, Round 032: loss=2.3045, accuracy=0.1082, gradient_norm=0.2433, 
[2025-09-16 10:37:42,791][__main__][INFO] - Train, Round 033: loss=2.3045, accuracy=0.1084, gradient_norm=0.2442, 
[2025-09-16 10:37:42,791][__main__][INFO] - Train, Round 034: loss=2.3044, accuracy=0.1084, gradient_norm=0.2339, 
[2025-09-16 10:37:42,791][__main__][INFO] - Train, Round 035: loss=2.3044, accuracy=0.1085, gradient_norm=0.2462, 
[2025-09-16 10:37:42,791][__main__][INFO] - Train, Round 036: loss=2.3044, accuracy=0.1086, gradient_norm=0.2379, 
[2025-09-16 10:37:42,791][__main__][INFO] - Train, Round 037: loss=2.3044, accuracy=0.1086, gradient_norm=0.2415, 
[2025-09-16 10:37:42,791][__main__][INFO] - Train, Round 038: loss=2.3044, accuracy=0.1088, gradient_norm=0.2366, 
[2025-09-16 10:37:42,791][__main__][INFO] - Train, Round 039: loss=2.3043, accuracy=0.1090, gradient_norm=0.2380, 
[2025-09-16 10:37:42,791][__main__][INFO] - Train, Round 040: loss=2.3044, accuracy=0.1092, gradient_norm=0.2372, 
[2025-09-16 10:37:42,791][__main__][INFO] - Train, Round 041: loss=2.3044, accuracy=0.1094, gradient_norm=0.2420, 
[2025-09-16 10:37:42,791][__main__][INFO] - Train, Round 042: loss=2.3044, accuracy=0.1095, gradient_norm=0.2378, 
[2025-09-16 10:37:42,791][__main__][INFO] - Train, Round 043: loss=2.3043, accuracy=0.1097, gradient_norm=0.2392, 
[2025-09-16 10:37:42,791][__main__][INFO] - Train, Round 044: loss=2.3043, accuracy=0.1098, gradient_norm=0.2425, 
[2025-09-16 10:37:42,791][__main__][INFO] - Train, Round 045: loss=2.3043, accuracy=0.1099, gradient_norm=0.2426, 
[2025-09-16 10:37:42,791][__main__][INFO] - Train, Round 046: loss=2.3043, accuracy=0.1101, gradient_norm=0.2386, 
[2025-09-16 10:37:42,791][__main__][INFO] - Train, Round 047: loss=2.3043, accuracy=0.1101, gradient_norm=0.2421, 
[2025-09-16 10:37:42,791][__main__][INFO] - Train, Round 048: loss=2.3043, accuracy=0.1102, gradient_norm=0.2351, 
[2025-09-16 10:37:42,791][__main__][INFO] - Train, Round 049: loss=2.3043, accuracy=0.1104, gradient_norm=0.2358, 
[2025-09-16 10:37:42,791][__main__][INFO] - Train, Round 050: loss=2.3042, accuracy=0.1107, gradient_norm=0.2367, 
[2025-09-16 10:37:42,791][__main__][INFO] - Test, Round 001: loss=2.3048, accuracy=0.1075, 
[2025-09-16 10:37:42,791][__main__][INFO] - Test, Round 002: loss=2.3047, accuracy=0.1076, 
[2025-09-16 10:37:42,791][__main__][INFO] - Test, Round 003: loss=2.3047, accuracy=0.1077, 
[2025-09-16 10:37:42,792][__main__][INFO] - Test, Round 004: loss=2.3047, accuracy=0.1079, 
[2025-09-16 10:37:42,792][__main__][INFO] - Test, Round 005: loss=2.3047, accuracy=0.1081, 
[2025-09-16 10:37:42,792][__main__][INFO] - Test, Round 006: loss=2.3047, accuracy=0.1084, 
[2025-09-16 10:37:42,792][__main__][INFO] - Test, Round 007: loss=2.3047, accuracy=0.1084, 
[2025-09-16 10:37:42,792][__main__][INFO] - Test, Round 008: loss=2.3047, accuracy=0.1087, 
[2025-09-16 10:37:42,792][__main__][INFO] - Test, Round 009: loss=2.3047, accuracy=0.1087, 
[2025-09-16 10:37:42,792][__main__][INFO] - Test, Round 010: loss=2.3047, accuracy=0.1088, 
[2025-09-16 10:37:42,792][__main__][INFO] - Test, Round 011: loss=2.3046, accuracy=0.1088, 
[2025-09-16 10:37:42,792][__main__][INFO] - Test, Round 012: loss=2.3046, accuracy=0.1089, 
[2025-09-16 10:37:42,792][__main__][INFO] - Test, Round 013: loss=2.3046, accuracy=0.1091, 
[2025-09-16 10:37:42,792][__main__][INFO] - Test, Round 014: loss=2.3046, accuracy=0.1091, 
[2025-09-16 10:37:42,792][__main__][INFO] - Test, Round 015: loss=2.3046, accuracy=0.1095, 
[2025-09-16 10:37:42,792][__main__][INFO] - Test, Round 016: loss=2.3046, accuracy=0.1099, 
[2025-09-16 10:37:42,792][__main__][INFO] - Test, Round 017: loss=2.3046, accuracy=0.1098, 
[2025-09-16 10:37:42,792][__main__][INFO] - Test, Round 018: loss=2.3046, accuracy=0.1095, 
[2025-09-16 10:37:42,792][__main__][INFO] - Test, Round 019: loss=2.3046, accuracy=0.1098, 
[2025-09-16 10:37:42,792][__main__][INFO] - Test, Round 020: loss=2.3045, accuracy=0.1097, 
[2025-09-16 10:37:42,792][__main__][INFO] - Test, Round 021: loss=2.3045, accuracy=0.1097, 
[2025-09-16 10:37:42,792][__main__][INFO] - Test, Round 022: loss=2.3045, accuracy=0.1100, 
[2025-09-16 10:37:42,792][__main__][INFO] - Test, Round 023: loss=2.3045, accuracy=0.1103, 
[2025-09-16 10:37:42,792][__main__][INFO] - Test, Round 024: loss=2.3045, accuracy=0.1104, 
[2025-09-16 10:37:42,792][__main__][INFO] - Test, Round 025: loss=2.3045, accuracy=0.1106, 
[2025-09-16 10:37:42,792][__main__][INFO] - Test, Round 026: loss=2.3045, accuracy=0.1111, 
[2025-09-16 10:37:42,792][__main__][INFO] - Test, Round 027: loss=2.3045, accuracy=0.1113, 
[2025-09-16 10:37:42,792][__main__][INFO] - Test, Round 028: loss=2.3045, accuracy=0.1113, 
[2025-09-16 10:37:42,792][__main__][INFO] - Test, Round 029: loss=2.3044, accuracy=0.1118, 
[2025-09-16 10:37:42,792][__main__][INFO] - Test, Round 030: loss=2.3044, accuracy=0.1118, 
[2025-09-16 10:37:42,792][__main__][INFO] - Test, Round 031: loss=2.3044, accuracy=0.1120, 
[2025-09-16 10:37:42,792][__main__][INFO] - Test, Round 032: loss=2.3044, accuracy=0.1122, 
[2025-09-16 10:37:42,792][__main__][INFO] - Test, Round 033: loss=2.3044, accuracy=0.1123, 
[2025-09-16 10:37:42,792][__main__][INFO] - Test, Round 034: loss=2.3044, accuracy=0.1123, 
[2025-09-16 10:37:42,792][__main__][INFO] - Test, Round 035: loss=2.3044, accuracy=0.1124, 
[2025-09-16 10:37:42,792][__main__][INFO] - Test, Round 036: loss=2.3044, accuracy=0.1125, 
[2025-09-16 10:37:42,792][__main__][INFO] - Test, Round 037: loss=2.3044, accuracy=0.1126, 
[2025-09-16 10:37:42,792][__main__][INFO] - Test, Round 038: loss=2.3043, accuracy=0.1127, 
[2025-09-16 10:37:42,792][__main__][INFO] - Test, Round 039: loss=2.3043, accuracy=0.1127, 
[2025-09-16 10:37:42,792][__main__][INFO] - Test, Round 040: loss=2.3043, accuracy=0.1127, 
[2025-09-16 10:37:42,792][__main__][INFO] - Test, Round 041: loss=2.3043, accuracy=0.1128, 
[2025-09-16 10:37:42,792][__main__][INFO] - Test, Round 042: loss=2.3043, accuracy=0.1130, 
[2025-09-16 10:37:42,792][__main__][INFO] - Test, Round 043: loss=2.3043, accuracy=0.1132, 
[2025-09-16 10:37:42,792][__main__][INFO] - Test, Round 044: loss=2.3043, accuracy=0.1137, 
[2025-09-16 10:37:42,792][__main__][INFO] - Test, Round 045: loss=2.3043, accuracy=0.1137, 
[2025-09-16 10:37:42,792][__main__][INFO] - Test, Round 046: loss=2.3043, accuracy=0.1137, 
[2025-09-16 10:37:42,792][__main__][INFO] - Test, Round 047: loss=2.3042, accuracy=0.1136, 
[2025-09-16 10:37:42,792][__main__][INFO] - Test, Round 048: loss=2.3042, accuracy=0.1137, 
[2025-09-16 10:37:42,792][__main__][INFO] - Test, Round 049: loss=2.3042, accuracy=0.1138, 
[2025-09-16 10:37:42,792][__main__][INFO] - Test, Round 050: loss=2.3042, accuracy=0.1138, 
