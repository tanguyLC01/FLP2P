[2025-09-15 18:05:55,382][flp2p.graph_runner][INFO] - Train, Round 0 : loss => 2.3039342094461124,  accuracy: 0.11318, gradient_norm : 0.23871509089489396
[2025-09-15 18:06:15,334][flp2p.graph_runner][INFO] - Test, Round 0 : loss => 2.302448552572727,  accuracy: 0.1311
[2025-09-15 18:06:59,470][flp2p.graph_runner][INFO] - Train, Round 1 : loss => 2.3013369992375377,  accuracy: 0.1309666666666667, gradient_norm : 0.2435960799267747
[2025-09-15 18:07:21,388][flp2p.graph_runner][INFO] - Test, Round 1 : loss => 2.3002794004917146,  accuracy: 0.1346
[2025-09-15 18:08:12,264][flp2p.graph_runner][INFO] - Train, Round 2 : loss => 2.298847511510055,  accuracy: 0.13585333333333327, gradient_norm : 0.2425427670881346
[2025-09-15 18:08:34,708][flp2p.graph_runner][INFO] - Test, Round 2 : loss => 2.2981366994976997,  accuracy: 0.1325
[2025-09-15 18:09:26,522][flp2p.graph_runner][INFO] - Train, Round 3 : loss => 2.2963433306415872,  accuracy: 0.13742000000000001, gradient_norm : 0.25628601298751696
[2025-09-15 18:09:48,419][flp2p.graph_runner][INFO] - Test, Round 3 : loss => 2.2959332350373267,  accuracy: 0.1352
[2025-09-15 18:10:38,335][flp2p.graph_runner][INFO] - Train, Round 4 : loss => 2.293727556367715,  accuracy: 0.1404, gradient_norm : 0.2570456347201716
[2025-09-15 18:11:00,417][flp2p.graph_runner][INFO] - Test, Round 4 : loss => 2.2936383613705633,  accuracy: 0.1368
[2025-09-15 18:11:51,255][flp2p.graph_runner][INFO] - Train, Round 5 : loss => 2.2909405950705213,  accuracy: 0.14298, gradient_norm : 0.2679373363206727
[2025-09-15 18:12:13,309][flp2p.graph_runner][INFO] - Test, Round 5 : loss => 2.291095317721367,  accuracy: 0.1386
[2025-09-15 18:13:04,172][flp2p.graph_runner][INFO] - Train, Round 6 : loss => 2.2878323341409366,  accuracy: 0.14671333333333328, gradient_norm : 0.2840672044616661
[2025-09-15 18:13:26,078][flp2p.graph_runner][INFO] - Test, Round 6 : loss => 2.2882173727631567,  accuracy: 0.1437
[2025-09-15 18:14:16,805][flp2p.graph_runner][INFO] - Train, Round 7 : loss => 2.2843296289443966,  accuracy: 0.15097333333333335, gradient_norm : 0.3059036811481609
[2025-09-15 18:14:38,975][flp2p.graph_runner][INFO] - Test, Round 7 : loss => 2.284859893500805,  accuracy: 0.1475
[2025-09-15 18:15:29,802][flp2p.graph_runner][INFO] - Train, Round 8 : loss => 2.2801552908619245,  accuracy: 0.15520666666666663, gradient_norm : 0.3257421177793249
[2025-09-15 18:15:52,205][flp2p.graph_runner][INFO] - Test, Round 8 : loss => 2.280922178173065,  accuracy: 0.1535
[2025-09-15 18:16:43,489][flp2p.graph_runner][INFO] - Train, Round 9 : loss => 2.2752983019749333,  accuracy: 0.15994666666666676, gradient_norm : 0.35630812913404397
[2025-09-15 18:17:05,771][flp2p.graph_runner][INFO] - Test, Round 9 : loss => 2.276165808069706,  accuracy: 0.1559
[2025-09-15 18:17:57,312][flp2p.graph_runner][INFO] - Train, Round 10 : loss => 2.2693802339831994,  accuracy: 0.16514666666666672, gradient_norm : 0.38702761411358144
[2025-09-15 18:18:19,378][flp2p.graph_runner][INFO] - Test, Round 10 : loss => 2.2703291837096216,  accuracy: 0.1609
[2025-09-15 18:19:10,116][flp2p.graph_runner][INFO] - Train, Round 11 : loss => 2.2620482089122147,  accuracy: 0.1730266666666667, gradient_norm : 0.43103029669886717
[2025-09-15 18:19:32,498][flp2p.graph_runner][INFO] - Test, Round 11 : loss => 2.2631273899197577,  accuracy: 0.1687
[2025-09-15 18:20:23,870][flp2p.graph_runner][INFO] - Train, Round 12 : loss => 2.252980717917284,  accuracy: 0.1812666666666667, gradient_norm : 0.4967070220375354
[2025-09-15 18:20:46,151][flp2p.graph_runner][INFO] - Test, Round 12 : loss => 2.25428481580019,  accuracy: 0.1799
[2025-09-15 18:21:37,779][flp2p.graph_runner][INFO] - Train, Round 13 : loss => 2.241737694044907,  accuracy: 0.18973999999999996, gradient_norm : 0.5634631662258683
[2025-09-15 18:21:59,864][flp2p.graph_runner][INFO] - Test, Round 13 : loss => 2.2430918449640274,  accuracy: 0.1872
[2025-09-15 18:22:49,916][flp2p.graph_runner][INFO] - Train, Round 14 : loss => 2.227421104212601,  accuracy: 0.2001133333333333, gradient_norm : 0.6263067966122842
[2025-09-15 18:23:11,927][flp2p.graph_runner][INFO] - Test, Round 14 : loss => 2.2289390699982645,  accuracy: 0.1923
[2025-09-15 18:24:02,494][flp2p.graph_runner][INFO] - Train, Round 15 : loss => 2.2091575337449716,  accuracy: 0.20987333333333333, gradient_norm : 0.7280518314204077
[2025-09-15 18:24:24,775][flp2p.graph_runner][INFO] - Test, Round 15 : loss => 2.2117074314832688,  accuracy: 0.2008
[2025-09-15 18:25:15,796][flp2p.graph_runner][INFO] - Train, Round 16 : loss => 2.187036426464716,  accuracy: 0.21887999999999996, gradient_norm : 0.863196016551681
[2025-09-15 18:25:37,515][flp2p.graph_runner][INFO] - Test, Round 16 : loss => 2.191548239898682,  accuracy: 0.2116
[2025-09-15 18:26:27,045][flp2p.graph_runner][INFO] - Train, Round 17 : loss => 2.1611233899494007,  accuracy: 0.2295133333333334, gradient_norm : 0.9951415729909313
[2025-09-15 18:26:49,062][flp2p.graph_runner][INFO] - Test, Round 17 : loss => 2.169028091716766,  accuracy: 0.2146
[2025-09-15 18:27:40,147][flp2p.graph_runner][INFO] - Train, Round 18 : loss => 2.1319982382655143,  accuracy: 0.23946666666666658, gradient_norm : 1.1938028426732998
[2025-09-15 18:28:02,391][flp2p.graph_runner][INFO] - Test, Round 18 : loss => 2.147095133340359,  accuracy: 0.2259
[2025-09-15 18:28:53,451][flp2p.graph_runner][INFO] - Train, Round 19 : loss => 2.1027454524238904,  accuracy: 0.2524933333333333, gradient_norm : 1.3599303262889972
[2025-09-15 18:29:15,830][flp2p.graph_runner][INFO] - Test, Round 19 : loss => 2.1262824935734272,  accuracy: 0.2297
[2025-09-15 18:30:06,988][flp2p.graph_runner][INFO] - Train, Round 20 : loss => 2.0747086896499005,  accuracy: 0.26214000000000004, gradient_norm : 1.521402503988229
[2025-09-15 18:30:29,078][flp2p.graph_runner][INFO] - Test, Round 20 : loss => 2.108062986660004,  accuracy: 0.238
[2025-09-15 18:31:19,902][flp2p.graph_runner][INFO] - Train, Round 21 : loss => 2.0481807385881736,  accuracy: 0.2747533333333333, gradient_norm : 1.6134976971046666
[2025-09-15 18:31:41,952][flp2p.graph_runner][INFO] - Test, Round 21 : loss => 2.0886651309430597,  accuracy: 0.2487
[2025-09-15 18:32:32,927][flp2p.graph_runner][INFO] - Train, Round 22 : loss => 2.021727987130483,  accuracy: 0.2872600000000001, gradient_norm : 1.8861734929679557
[2025-09-15 18:32:55,106][flp2p.graph_runner][INFO] - Test, Round 22 : loss => 2.0731588691711424,  accuracy: 0.2556
[2025-09-15 18:33:46,023][flp2p.graph_runner][INFO] - Train, Round 23 : loss => 1.9961067724724613,  accuracy: 0.2945333333333332, gradient_norm : 2.068033012254733
[2025-09-15 18:34:08,168][flp2p.graph_runner][INFO] - Test, Round 23 : loss => 2.055070655030012,  accuracy: 0.2665
[2025-09-15 18:34:59,255][flp2p.graph_runner][INFO] - Train, Round 24 : loss => 1.9693563123544053,  accuracy: 0.30588, gradient_norm : 2.0902284818987136
[2025-09-15 18:35:21,691][flp2p.graph_runner][INFO] - Test, Round 24 : loss => 2.0381152293145655,  accuracy: 0.2758
[2025-09-15 18:36:13,244][flp2p.graph_runner][INFO] - Train, Round 25 : loss => 1.9424070819218955,  accuracy: 0.3140666666666667, gradient_norm : 2.335688118523019
[2025-09-15 18:36:35,163][flp2p.graph_runner][INFO] - Test, Round 25 : loss => 2.0224702927351,  accuracy: 0.275
[2025-09-15 18:37:25,433][flp2p.graph_runner][INFO] - Train, Round 26 : loss => 1.9160116781294345,  accuracy: 0.32218, gradient_norm : 2.4448667259036507
[2025-09-15 18:37:47,286][flp2p.graph_runner][INFO] - Test, Round 26 : loss => 2.009830652433634,  accuracy: 0.2765
[2025-09-15 18:38:37,459][flp2p.graph_runner][INFO] - Train, Round 27 : loss => 1.8905576722323891,  accuracy: 0.33106, gradient_norm : 2.535052493334814
[2025-09-15 18:38:59,463][flp2p.graph_runner][INFO] - Test, Round 27 : loss => 1.9938339215815066,  accuracy: 0.2795
[2025-09-15 18:39:50,603][flp2p.graph_runner][INFO] - Train, Round 28 : loss => 1.865557650427025,  accuracy: 0.3396599999999999, gradient_norm : 2.798526387301962
[2025-09-15 18:40:12,697][flp2p.graph_runner][INFO] - Test, Round 28 : loss => 1.983826410138607,  accuracy: 0.2871
[2025-09-15 18:41:03,666][flp2p.graph_runner][INFO] - Train, Round 29 : loss => 1.8433581121265894,  accuracy: 0.3457933333333333, gradient_norm : 3.037973665331484
[2025-09-15 18:41:26,210][flp2p.graph_runner][INFO] - Test, Round 29 : loss => 1.9690704263389112,  accuracy: 0.2933
[2025-09-15 18:42:17,412][flp2p.graph_runner][INFO] - Train, Round 30 : loss => 1.822090499947468,  accuracy: 0.35232000000000013, gradient_norm : 3.028717048711791
[2025-09-15 18:42:39,771][flp2p.graph_runner][INFO] - Test, Round 30 : loss => 1.9749929697155952,  accuracy: 0.2849
[2025-09-15 18:43:31,596][flp2p.graph_runner][INFO] - Train, Round 31 : loss => 1.8014771458506578,  accuracy: 0.3603333333333333, gradient_norm : 3.138052362875444
[2025-09-15 18:43:53,247][flp2p.graph_runner][INFO] - Test, Round 31 : loss => 1.9839632607340814,  accuracy: 0.2854
[2025-09-15 18:44:43,279][flp2p.graph_runner][INFO] - Train, Round 32 : loss => 1.7830625139673548,  accuracy: 0.36633999999999994, gradient_norm : 3.2599412870102085
[2025-09-15 18:45:05,273][flp2p.graph_runner][INFO] - Test, Round 32 : loss => 2.0115760319411753,  accuracy: 0.2917
[2025-09-15 18:45:55,573][flp2p.graph_runner][INFO] - Train, Round 33 : loss => 1.763619595170021,  accuracy: 0.3739466666666667, gradient_norm : 3.41211324171239
[2025-09-15 18:46:17,646][flp2p.graph_runner][INFO] - Test, Round 33 : loss => 1.9538673853337765,  accuracy: 0.2999
[2025-09-15 18:47:08,029][flp2p.graph_runner][INFO] - Train, Round 34 : loss => 1.7430905442436537,  accuracy: 0.38263333333333305, gradient_norm : 3.4371303479598576
[2025-09-15 18:47:30,061][flp2p.graph_runner][INFO] - Test, Round 34 : loss => 1.9790579356312752,  accuracy: 0.2976
[2025-09-15 18:48:20,765][flp2p.graph_runner][INFO] - Train, Round 35 : loss => 1.7236448753376805,  accuracy: 0.3887933333333334, gradient_norm : 3.5569419878209634
[2025-09-15 18:48:43,037][flp2p.graph_runner][INFO] - Test, Round 35 : loss => 1.9705300830721855,  accuracy: 0.2934
[2025-09-15 18:49:33,868][flp2p.graph_runner][INFO] - Train, Round 36 : loss => 1.701402027457952,  accuracy: 0.39717333333333327, gradient_norm : 3.5899319505608283
[2025-09-15 18:49:56,225][flp2p.graph_runner][INFO] - Test, Round 36 : loss => 1.9252059111535549,  accuracy: 0.3055
[2025-09-15 18:50:47,082][flp2p.graph_runner][INFO] - Train, Round 37 : loss => 1.6771127575139206,  accuracy: 0.4063666666666665, gradient_norm : 3.721019023519237
[2025-09-15 18:51:09,310][flp2p.graph_runner][INFO] - Test, Round 37 : loss => 1.92694367518425,  accuracy: 0.3045
[2025-09-15 18:52:00,369][flp2p.graph_runner][INFO] - Train, Round 38 : loss => 1.6552343938251346,  accuracy: 0.41530666666666677, gradient_norm : 3.8976441822554047
[2025-09-15 18:52:22,471][flp2p.graph_runner][INFO] - Test, Round 38 : loss => 1.9316465839624406,  accuracy: 0.3126
[2025-09-15 18:53:13,197][flp2p.graph_runner][INFO] - Train, Round 39 : loss => 1.6335720100998878,  accuracy: 0.42351333333333335, gradient_norm : 4.07024621322067
[2025-09-15 18:53:35,149][flp2p.graph_runner][INFO] - Test, Round 39 : loss => 1.9239769646167755,  accuracy: 0.3107
[2025-09-15 18:54:25,872][flp2p.graph_runner][INFO] - Train, Round 40 : loss => 1.6109296789268663,  accuracy: 0.43232666666666664, gradient_norm : 4.2487836112104285
[2025-09-15 18:54:48,217][flp2p.graph_runner][INFO] - Test, Round 40 : loss => 1.9189666142761708,  accuracy: 0.3212
[2025-09-15 18:55:39,166][flp2p.graph_runner][INFO] - Train, Round 41 : loss => 1.5849010770519578,  accuracy: 0.44186, gradient_norm : 4.428992181230332
[2025-09-15 18:56:01,383][flp2p.graph_runner][INFO] - Test, Round 41 : loss => 1.994248603463173,  accuracy: 0.3061
[2025-09-15 18:56:52,398][flp2p.graph_runner][INFO] - Train, Round 42 : loss => 1.566944790184497,  accuracy: 0.44860000000000017, gradient_norm : 4.375644767348973
[2025-09-15 18:57:14,240][flp2p.graph_runner][INFO] - Test, Round 42 : loss => 2.0619348020136354,  accuracy: 0.2875
[2025-09-15 18:58:04,518][flp2p.graph_runner][INFO] - Train, Round 43 : loss => 1.5445864174266657,  accuracy: 0.45882666666666677, gradient_norm : 4.5397797317272515
[2025-09-15 18:58:26,821][flp2p.graph_runner][INFO] - Test, Round 43 : loss => 1.89432981107831,  accuracy: 0.3261
[2025-09-15 18:59:18,181][flp2p.graph_runner][INFO] - Train, Round 44 : loss => 1.5098364526530106,  accuracy: 0.47066, gradient_norm : 4.809001146137194
[2025-09-15 18:59:40,452][flp2p.graph_runner][INFO] - Test, Round 44 : loss => 2.0146893549442293,  accuracy: 0.3044
[2025-09-15 19:00:31,641][flp2p.graph_runner][INFO] - Train, Round 45 : loss => 1.487303244968255,  accuracy: 0.47722000000000014, gradient_norm : 4.828384586110501
[2025-09-15 19:00:53,826][flp2p.graph_runner][INFO] - Test, Round 45 : loss => 1.9326966152071954,  accuracy: 0.3199
[2025-09-15 19:01:44,560][flp2p.graph_runner][INFO] - Train, Round 46 : loss => 1.4579256192346413,  accuracy: 0.4880933333333332, gradient_norm : 4.843810434768542
[2025-09-15 19:02:06,712][flp2p.graph_runner][INFO] - Test, Round 46 : loss => 1.8865976785123348,  accuracy: 0.3355
[2025-09-15 19:02:57,453][flp2p.graph_runner][INFO] - Train, Round 47 : loss => 1.4258658506472903,  accuracy: 0.5000333333333333, gradient_norm : 5.169060475359112
[2025-09-15 19:03:19,367][flp2p.graph_runner][INFO] - Test, Round 47 : loss => 1.9506080135643482,  accuracy: 0.3233
[2025-09-15 19:04:09,773][flp2p.graph_runner][INFO] - Train, Round 48 : loss => 1.401131321912011,  accuracy: 0.5089800000000001, gradient_norm : 5.325609985002677
[2025-09-15 19:04:31,801][flp2p.graph_runner][INFO] - Test, Round 48 : loss => 1.8990899949848652,  accuracy: 0.3359
[2025-09-15 19:05:22,074][flp2p.graph_runner][INFO] - Train, Round 49 : loss => 1.3714169427255785,  accuracy: 0.5199933333333334, gradient_norm : 5.729069408335497
[2025-09-15 19:05:44,082][flp2p.graph_runner][INFO] - Test, Round 49 : loss => 1.9496330823600292,  accuracy: 0.3285
[2025-09-15 19:06:34,686][flp2p.graph_runner][INFO] - Train, Round 50 : loss => 1.3426743173847595,  accuracy: 0.5302266666666665, gradient_norm : 5.275077495547605
[2025-09-15 19:06:57,148][flp2p.graph_runner][INFO] - Test, Round 50 : loss => 2.0541912414431573,  accuracy: 0.3132
[2025-09-15 19:07:48,416][flp2p.graph_runner][INFO] - Train, Round 51 : loss => 1.3181884665290513,  accuracy: 0.5403666666666666, gradient_norm : 5.4682064199084435
[2025-09-15 19:08:10,554][flp2p.graph_runner][INFO] - Test, Round 51 : loss => 1.9814094961345197,  accuracy: 0.3306
[2025-09-15 19:09:01,027][flp2p.graph_runner][INFO] - Train, Round 52 : loss => 1.2834338434537245,  accuracy: 0.5530999999999998, gradient_norm : 5.961543986877854
[2025-09-15 19:09:23,521][flp2p.graph_runner][INFO] - Test, Round 52 : loss => 2.0265876505315306,  accuracy: 0.3259
[2025-09-15 19:10:14,995][flp2p.graph_runner][INFO] - Train, Round 53 : loss => 1.2566956942528484,  accuracy: 0.5619600000000002, gradient_norm : 5.995273133629982
[2025-09-15 19:10:37,344][flp2p.graph_runner][INFO] - Test, Round 53 : loss => 2.151673434048891,  accuracy: 0.3176
[2025-09-15 19:11:28,922][flp2p.graph_runner][INFO] - Train, Round 54 : loss => 1.2285890988012154,  accuracy: 0.5715799999999998, gradient_norm : 6.1134909468539025
[2025-09-15 19:11:51,281][flp2p.graph_runner][INFO] - Test, Round 54 : loss => 2.0337155656039716,  accuracy: 0.3227
[2025-09-15 19:12:43,193][flp2p.graph_runner][INFO] - Train, Round 55 : loss => 1.1904027543713651,  accuracy: 0.5871533333333334, gradient_norm : 6.38408086271567
[2025-09-15 19:13:05,506][flp2p.graph_runner][INFO] - Test, Round 55 : loss => 2.0266582755565645,  accuracy: 0.3334
[2025-09-15 19:13:56,942][flp2p.graph_runner][INFO] - Train, Round 56 : loss => 1.1547589427481095,  accuracy: 0.6012333333333332, gradient_norm : 6.406357764786155
[2025-09-15 19:14:18,927][flp2p.graph_runner][INFO] - Test, Round 56 : loss => 2.0860917088806628,  accuracy: 0.3277
[2025-09-15 19:15:10,105][flp2p.graph_runner][INFO] - Train, Round 57 : loss => 1.128091948976119,  accuracy: 0.6120666666666663, gradient_norm : 6.833771101215555
[2025-09-15 19:15:32,397][flp2p.graph_runner][INFO] - Test, Round 57 : loss => 2.0837270140051842,  accuracy: 0.3317
[2025-09-15 19:16:23,984][flp2p.graph_runner][INFO] - Train, Round 58 : loss => 1.0893709924320378,  accuracy: 0.6248400000000001, gradient_norm : 6.579271287102061
[2025-09-15 19:16:46,466][flp2p.graph_runner][INFO] - Test, Round 58 : loss => 2.2179632847607134,  accuracy: 0.3071
[2025-09-15 19:17:37,862][flp2p.graph_runner][INFO] - Train, Round 59 : loss => 1.0602707329640784,  accuracy: 0.63802, gradient_norm : 6.683036105430054
[2025-09-15 19:18:00,357][flp2p.graph_runner][INFO] - Test, Round 59 : loss => 2.115909254181385,  accuracy: 0.3304
[2025-09-15 19:18:51,706][flp2p.graph_runner][INFO] - Train, Round 60 : loss => 1.0231582071632146,  accuracy: 0.6499133333333331, gradient_norm : 6.819193416706116
[2025-09-15 19:19:14,032][flp2p.graph_runner][INFO] - Test, Round 60 : loss => 2.105618044668436,  accuracy: 0.3237
[2025-09-15 19:20:05,359][flp2p.graph_runner][INFO] - Train, Round 61 : loss => 0.9795775081465637,  accuracy: 0.6655533333333334, gradient_norm : 7.227153894359752
[2025-09-15 19:20:27,710][flp2p.graph_runner][INFO] - Test, Round 61 : loss => 2.197405427747965,  accuracy: 0.3273
[2025-09-15 19:21:19,103][flp2p.graph_runner][INFO] - Train, Round 62 : loss => 0.9456695761034888,  accuracy: 0.6790466666666669, gradient_norm : 7.031960387918441
[2025-09-15 19:21:41,362][flp2p.graph_runner][INFO] - Test, Round 62 : loss => 2.69631447660923,  accuracy: 0.273
[2025-09-15 19:22:32,790][flp2p.graph_runner][INFO] - Train, Round 63 : loss => 0.9394808947046598,  accuracy: 0.6847866666666668, gradient_norm : 7.038351202073148
[2025-09-15 19:22:55,149][flp2p.graph_runner][INFO] - Test, Round 63 : loss => 2.2251916297137737,  accuracy: 0.3285
[2025-09-15 19:23:46,901][flp2p.graph_runner][INFO] - Train, Round 64 : loss => 0.8743894203628106,  accuracy: 0.7070466666666668, gradient_norm : 6.973601354407274
[2025-09-15 19:24:09,124][flp2p.graph_runner][INFO] - Test, Round 64 : loss => 2.2732367354750633,  accuracy: 0.3321
[2025-09-15 19:24:59,391][flp2p.graph_runner][INFO] - Train, Round 65 : loss => 0.832858482152223,  accuracy: 0.7229599999999998, gradient_norm : 7.160781476751314
[2025-09-15 19:25:21,397][flp2p.graph_runner][INFO] - Test, Round 65 : loss => 2.5634612590789794,  accuracy: 0.2922
[2025-09-15 19:26:11,866][flp2p.graph_runner][INFO] - Train, Round 66 : loss => 0.8127298462763426,  accuracy: 0.7333600000000001, gradient_norm : 7.590068227457123
[2025-09-15 19:26:33,994][flp2p.graph_runner][INFO] - Test, Round 66 : loss => 2.6365538923084735,  accuracy: 0.2973
[2025-09-15 19:27:24,666][flp2p.graph_runner][INFO] - Train, Round 67 : loss => 0.7745794353634115,  accuracy: 0.7493000000000003, gradient_norm : 7.59816948300145
[2025-09-15 19:27:46,827][flp2p.graph_runner][INFO] - Test, Round 67 : loss => 2.346370476704836,  accuracy: 0.318
[2025-09-15 19:28:37,554][flp2p.graph_runner][INFO] - Train, Round 68 : loss => 0.7128462941199544,  accuracy: 0.7715399999999999, gradient_norm : 6.7935627539238785
[2025-09-15 19:28:59,916][flp2p.graph_runner][INFO] - Test, Round 68 : loss => 2.405426832306385,  accuracy: 0.3237
[2025-09-15 19:29:51,442][flp2p.graph_runner][INFO] - Train, Round 69 : loss => 0.685090655783812,  accuracy: 0.7808933333333334, gradient_norm : 7.683685159758975
[2025-09-15 19:30:13,930][flp2p.graph_runner][INFO] - Test, Round 69 : loss => 2.598904134982824,  accuracy: 0.3215
[2025-09-15 19:31:05,682][flp2p.graph_runner][INFO] - Train, Round 70 : loss => 0.6383744988838831,  accuracy: 0.800846666666667, gradient_norm : 7.7958033837080425
[2025-09-15 19:31:27,685][flp2p.graph_runner][INFO] - Test, Round 70 : loss => 2.4458958097696306,  accuracy: 0.3342
[2025-09-15 19:32:18,051][flp2p.graph_runner][INFO] - Train, Round 71 : loss => 0.5971303701711198,  accuracy: 0.8167600000000002, gradient_norm : 7.570734290892358
[2025-09-15 19:32:40,318][flp2p.graph_runner][INFO] - Test, Round 71 : loss => 2.6437968347847463,  accuracy: 0.3337
[2025-09-15 19:33:31,678][flp2p.graph_runner][INFO] - Train, Round 72 : loss => 0.5638069292778769,  accuracy: 0.8288066666666665, gradient_norm : 7.342785856506667
[2025-09-15 19:33:53,990][flp2p.graph_runner][INFO] - Test, Round 72 : loss => 2.9587005203008654,  accuracy: 0.3036
[2025-09-15 19:34:45,209][flp2p.graph_runner][INFO] - Train, Round 73 : loss => 0.5458728720309832,  accuracy: 0.8366933333333333, gradient_norm : 7.176009091168698
[2025-09-15 19:35:07,625][flp2p.graph_runner][INFO] - Test, Round 73 : loss => 3.2241663832247256,  accuracy: 0.2968
[2025-09-15 19:35:59,253][flp2p.graph_runner][INFO] - Train, Round 74 : loss => 0.5314707119576632,  accuracy: 0.8490999999999997, gradient_norm : 6.449661229665136
[2025-09-15 19:36:21,695][flp2p.graph_runner][INFO] - Test, Round 74 : loss => 2.737701414489746,  accuracy: 0.3388
[2025-09-15 19:37:13,534][flp2p.graph_runner][INFO] - Train, Round 75 : loss => 0.4427554209964971,  accuracy: 0.8766400000000002, gradient_norm : 7.090734960200503
[2025-09-15 19:37:35,544][flp2p.graph_runner][INFO] - Test, Round 75 : loss => 2.916443670219183,  accuracy: 0.3169
[2025-09-15 19:38:25,543][flp2p.graph_runner][INFO] - Train, Round 76 : loss => 0.4392062278836965,  accuracy: 0.8793133333333334, gradient_norm : 6.021825582622029
[2025-09-15 19:38:47,703][flp2p.graph_runner][INFO] - Test, Round 76 : loss => 2.7297276160776613,  accuracy: 0.3332
[2025-09-15 19:39:38,271][flp2p.graph_runner][INFO] - Train, Round 77 : loss => 0.36652420300059035,  accuracy: 0.9053066666666668, gradient_norm : 5.763942051333812
[2025-09-15 19:40:00,362][flp2p.graph_runner][INFO] - Test, Round 77 : loss => 2.7178900872886183,  accuracy: 0.3363
[2025-09-15 19:40:51,435][flp2p.graph_runner][INFO] - Train, Round 78 : loss => 0.33508598860042793,  accuracy: 0.9178799999999998, gradient_norm : 6.13652184636255
[2025-09-15 19:41:13,830][flp2p.graph_runner][INFO] - Test, Round 78 : loss => 2.9054709417164326,  accuracy: 0.3315
[2025-09-15 19:42:05,041][flp2p.graph_runner][INFO] - Train, Round 79 : loss => 0.3126731994003058,  accuracy: 0.9236533333333329, gradient_norm : 5.485345122158261
[2025-09-15 19:42:27,273][flp2p.graph_runner][INFO] - Test, Round 79 : loss => 2.9129616525709627,  accuracy: 0.3465
[2025-09-15 19:43:18,716][flp2p.graph_runner][INFO] - Train, Round 80 : loss => 0.2909942113608122,  accuracy: 0.9323266666666665, gradient_norm : 4.685419905005546
[2025-09-15 19:43:41,026][flp2p.graph_runner][INFO] - Test, Round 80 : loss => 2.9654315975904466,  accuracy: 0.3429
[2025-09-15 19:44:32,432][flp2p.graph_runner][INFO] - Train, Round 81 : loss => 0.2579828756519903,  accuracy: 0.9443800000000001, gradient_norm : 5.03151408561256
[2025-09-15 19:44:54,625][flp2p.graph_runner][INFO] - Test, Round 81 : loss => 3.0437571927845477,  accuracy: 0.3243
[2025-09-15 19:45:45,793][flp2p.graph_runner][INFO] - Train, Round 82 : loss => 0.23354367267340415,  accuracy: 0.9517600000000002, gradient_norm : 4.141334970169155
[2025-09-15 19:46:08,153][flp2p.graph_runner][INFO] - Test, Round 82 : loss => 3.0627937396109104,  accuracy: 0.3393
[2025-09-15 19:46:59,555][flp2p.graph_runner][INFO] - Train, Round 83 : loss => 0.20255168291429684,  accuracy: 0.9628666666666668, gradient_norm : 4.541734195832004
[2025-09-15 19:47:21,640][flp2p.graph_runner][INFO] - Test, Round 83 : loss => 3.287130267703533,  accuracy: 0.337
[2025-09-15 19:48:13,094][flp2p.graph_runner][INFO] - Train, Round 84 : loss => 0.21617945077052966,  accuracy: 0.9564533333333334, gradient_norm : 3.5380183674004586
[2025-09-15 19:48:34,737][flp2p.graph_runner][INFO] - Test, Round 84 : loss => 3.2052785592556,  accuracy: 0.3393
[2025-09-15 19:49:24,573][flp2p.graph_runner][INFO] - Train, Round 85 : loss => 0.17462775557301943,  accuracy: 0.9700733333333329, gradient_norm : 3.2800565353290057
[2025-09-15 19:49:47,070][flp2p.graph_runner][INFO] - Test, Round 85 : loss => 3.130596245598793,  accuracy: 0.3444
[2025-09-15 19:50:38,408][flp2p.graph_runner][INFO] - Train, Round 86 : loss => 0.15169227523108325,  accuracy: 0.9787133333333338, gradient_norm : 3.3982653948869865
[2025-09-15 19:51:00,482][flp2p.graph_runner][INFO] - Test, Round 86 : loss => 3.3058730076193807,  accuracy: 0.3393
[2025-09-15 19:51:51,373][flp2p.graph_runner][INFO] - Train, Round 87 : loss => 0.1489252704909693,  accuracy: 0.9774666666666667, gradient_norm : 2.612985351061409
[2025-09-15 19:52:13,496][flp2p.graph_runner][INFO] - Test, Round 87 : loss => 3.238326914548874,  accuracy: 0.3541
[2025-09-15 19:53:03,888][flp2p.graph_runner][INFO] - Train, Round 88 : loss => 0.10500511686628063,  accuracy: 0.9895200000000003, gradient_norm : 2.247718225046822
[2025-09-15 19:53:26,112][flp2p.graph_runner][INFO] - Test, Round 88 : loss => 3.2954115724563597,  accuracy: 0.3463
[2025-09-15 19:54:17,079][flp2p.graph_runner][INFO] - Train, Round 89 : loss => 0.09845700946791718,  accuracy: 0.9904466666666668, gradient_norm : 2.3200481288530845
[2025-09-15 19:54:39,461][flp2p.graph_runner][INFO] - Test, Round 89 : loss => 3.387220586311817,  accuracy: 0.344
[2025-09-15 19:55:30,557][flp2p.graph_runner][INFO] - Train, Round 90 : loss => 0.07152426801233862,  accuracy: 0.9959333333333331, gradient_norm : 1.9887799297917712
[2025-09-15 19:55:52,585][flp2p.graph_runner][INFO] - Test, Round 90 : loss => 3.4370067500710486,  accuracy: 0.3542
[2025-09-15 19:56:42,787][flp2p.graph_runner][INFO] - Train, Round 91 : loss => 0.07683383985344942,  accuracy: 0.9936799999999999, gradient_norm : 2.092665250456185
[2025-09-15 19:57:04,550][flp2p.graph_runner][INFO] - Test, Round 91 : loss => 3.4811557347774507,  accuracy: 0.3502
[2025-09-15 19:57:55,000][flp2p.graph_runner][INFO] - Train, Round 92 : loss => 0.06104852205685652,  accuracy: 0.9964466666666661, gradient_norm : 1.74302837129971
[2025-09-15 19:58:17,210][flp2p.graph_runner][INFO] - Test, Round 92 : loss => 3.5738310292243956,  accuracy: 0.3443
[2025-09-15 19:59:08,290][flp2p.graph_runner][INFO] - Train, Round 93 : loss => 0.051907232356412966,  accuracy: 0.9977266666666663, gradient_norm : 1.7773948659683256
[2025-09-15 19:59:30,609][flp2p.graph_runner][INFO] - Test, Round 93 : loss => 3.6348678423047067,  accuracy: 0.3489
[2025-09-15 20:00:21,901][flp2p.graph_runner][INFO] - Train, Round 94 : loss => 0.05796663430208963,  accuracy: 0.9961599999999994, gradient_norm : 1.411675485808576
[2025-09-15 20:00:44,084][flp2p.graph_runner][INFO] - Test, Round 94 : loss => 3.6263428426742554,  accuracy: 0.3435
[2025-09-15 20:01:35,360][flp2p.graph_runner][INFO] - Train, Round 95 : loss => 0.04141844474322473,  accuracy: 0.9989733333333333, gradient_norm : 1.3335013330539278
[2025-09-15 20:01:57,664][flp2p.graph_runner][INFO] - Test, Round 95 : loss => 3.6758374989509583,  accuracy: 0.3454
[2025-09-15 20:02:48,886][flp2p.graph_runner][INFO] - Train, Round 96 : loss => 0.04124307776801288,  accuracy: 0.9981866666666664, gradient_norm : 1.2748946003107484
[2025-09-15 20:03:11,167][flp2p.graph_runner][INFO] - Test, Round 96 : loss => 3.7292861360907557,  accuracy: 0.3502
[2025-09-15 20:04:02,716][flp2p.graph_runner][INFO] - Train, Round 97 : loss => 0.04241878289341303,  accuracy: 0.9976733333333334, gradient_norm : 1.1582253368676365
[2025-09-15 20:04:24,739][flp2p.graph_runner][INFO] - Test, Round 97 : loss => 3.7470868392825127,  accuracy: 0.3512
[2025-09-15 20:05:16,404][flp2p.graph_runner][INFO] - Train, Round 98 : loss => 0.03294920672623751,  accuracy: 0.9990599999999998, gradient_norm : 1.2241011719487904
[2025-09-15 20:05:38,471][flp2p.graph_runner][INFO] - Test, Round 98 : loss => 3.843500427699089,  accuracy: 0.3518
[2025-09-15 20:06:29,046][flp2p.graph_runner][INFO] - Train, Round 99 : loss => 0.04465761343056028,  accuracy: 0.9965866666666667, gradient_norm : 0.9638650704789927
[2025-09-15 20:06:51,022][flp2p.graph_runner][INFO] - Test, Round 99 : loss => 3.8289931132912636,  accuracy: 0.3511
[2025-09-15 20:07:42,054][flp2p.graph_runner][INFO] - Train, Round 100 : loss => 0.035644275554611034,  accuracy: 0.9987333333333333, gradient_norm : 1.093394380256185
[2025-09-15 20:08:04,255][flp2p.graph_runner][INFO] - Test, Round 100 : loss => 3.8877499258875847,  accuracy: 0.3485
[2025-09-15 20:08:54,973][flp2p.graph_runner][INFO] - Train, Round 101 : loss => 0.02921359067317098,  accuracy: 0.9988133333333333, gradient_norm : 0.8912737122622475
[2025-09-15 20:09:17,185][flp2p.graph_runner][INFO] - Test, Round 101 : loss => 3.9043105016589164,  accuracy: 0.3481
[2025-09-15 20:10:07,845][flp2p.graph_runner][INFO] - Train, Round 102 : loss => 0.023078042196575558,  accuracy: 0.9999066666666666, gradient_norm : 0.8121748343394767
[2025-09-15 20:10:30,176][flp2p.graph_runner][INFO] - Test, Round 102 : loss => 3.944418234479427,  accuracy: 0.3462
[2025-09-15 20:11:21,248][flp2p.graph_runner][INFO] - Train, Round 103 : loss => 0.021134145259081083,  accuracy: 0.9999666666666667, gradient_norm : 0.8070479323608182
[2025-09-15 20:11:43,525][flp2p.graph_runner][INFO] - Test, Round 103 : loss => 3.9878517365932464,  accuracy: 0.3467
[2025-09-15 20:12:34,879][flp2p.graph_runner][INFO] - Train, Round 104 : loss => 0.0197304969211109,  accuracy: 0.9999266666666665, gradient_norm : 0.7346935476294479
[2025-09-15 20:12:56,850][flp2p.graph_runner][INFO] - Test, Round 104 : loss => 4.024006064450741,  accuracy: 0.3455
[2025-09-15 20:13:47,831][flp2p.graph_runner][INFO] - Train, Round 105 : loss => 0.018503021096500257,  accuracy: 0.9999533333333335, gradient_norm : 0.7019948460585992
[2025-09-15 20:14:10,008][flp2p.graph_runner][INFO] - Test, Round 105 : loss => 4.063848687875271,  accuracy: 0.3493
[2025-09-15 20:15:01,107][flp2p.graph_runner][INFO] - Train, Round 106 : loss => 0.01724753854447044,  accuracy: 0.9999733333333334, gradient_norm : 0.6406664130064543
[2025-09-15 20:15:23,350][flp2p.graph_runner][INFO] - Test, Round 106 : loss => 4.094216706049442,  accuracy: 0.3478
[2025-09-15 20:16:14,867][flp2p.graph_runner][INFO] - Train, Round 107 : loss => 0.016271256734229005,  accuracy: 0.9999933333333334, gradient_norm : 0.6141294041817102
[2025-09-15 20:16:37,073][flp2p.graph_runner][INFO] - Test, Round 107 : loss => 4.121591427898407,  accuracy: 0.3472
[2025-09-15 20:17:27,952][flp2p.graph_runner][INFO] - Train, Round 108 : loss => 0.015278029637411237,  accuracy: 0.9999933333333334, gradient_norm : 0.604090226130552
[2025-09-15 20:17:50,246][flp2p.graph_runner][INFO] - Test, Round 108 : loss => 4.158833302140236,  accuracy: 0.3491
[2025-09-15 20:18:41,362][flp2p.graph_runner][INFO] - Train, Round 109 : loss => 0.014489933449076484,  accuracy: 1.0, gradient_norm : 0.5838371436031526
[2025-09-15 20:19:03,518][flp2p.graph_runner][INFO] - Test, Round 109 : loss => 4.184953653359413,  accuracy: 0.3509
[2025-09-15 20:19:54,815][flp2p.graph_runner][INFO] - Train, Round 110 : loss => 0.013778145987695709,  accuracy: 1.0, gradient_norm : 0.5285744273464533
[2025-09-15 20:20:16,997][flp2p.graph_runner][INFO] - Test, Round 110 : loss => 4.212831682634354,  accuracy: 0.348
[2025-09-15 20:21:08,416][flp2p.graph_runner][INFO] - Train, Round 111 : loss => 0.013072882037959066,  accuracy: 0.9999933333333334, gradient_norm : 0.5152402531203939
[2025-09-15 20:21:30,764][flp2p.graph_runner][INFO] - Test, Round 111 : loss => 4.250284248125553,  accuracy: 0.3448
[2025-09-15 20:22:21,854][flp2p.graph_runner][INFO] - Train, Round 112 : loss => 0.012469452367319412,  accuracy: 1.0, gradient_norm : 0.4952113190463521
[2025-09-15 20:22:44,049][flp2p.graph_runner][INFO] - Test, Round 112 : loss => 4.266712011921406,  accuracy: 0.3469
[2025-09-15 20:23:35,001][flp2p.graph_runner][INFO] - Train, Round 113 : loss => 0.011865022620574263,  accuracy: 1.0, gradient_norm : 0.4990529696608887
[2025-09-15 20:23:57,158][flp2p.graph_runner][INFO] - Test, Round 113 : loss => 4.293740814840794,  accuracy: 0.3474
[2025-09-15 20:24:48,146][flp2p.graph_runner][INFO] - Train, Round 114 : loss => 0.01133484220287452,  accuracy: 1.0, gradient_norm : 0.4250974467729735
[2025-09-15 20:25:10,724][flp2p.graph_runner][INFO] - Test, Round 114 : loss => 4.317819312334061,  accuracy: 0.3493
[2025-09-15 20:26:02,356][flp2p.graph_runner][INFO] - Train, Round 115 : loss => 0.010846041915356182,  accuracy: 1.0, gradient_norm : 0.45058244624793725
[2025-09-15 20:26:24,722][flp2p.graph_runner][INFO] - Test, Round 115 : loss => 4.345281654596329,  accuracy: 0.3469
[2025-09-15 20:27:16,521][flp2p.graph_runner][INFO] - Train, Round 116 : loss => 0.010423668844935792,  accuracy: 1.0, gradient_norm : 0.41184803883380583
[2025-09-15 20:27:39,000][flp2p.graph_runner][INFO] - Test, Round 116 : loss => 4.370001338267326,  accuracy: 0.3482
[2025-09-15 20:28:29,027][flp2p.graph_runner][INFO] - Train, Round 117 : loss => 0.009965436762237607,  accuracy: 1.0, gradient_norm : 0.3968192881818488
[2025-09-15 20:28:49,137][flp2p.graph_runner][INFO] - Test, Round 117 : loss => 4.390406600928307,  accuracy: 0.3482
[2025-09-15 20:29:37,973][flp2p.graph_runner][INFO] - Train, Round 118 : loss => 0.009587816309649498,  accuracy: 1.0, gradient_norm : 0.3985240045861252
[2025-09-15 20:29:58,603][flp2p.graph_runner][INFO] - Test, Round 118 : loss => 4.41760398761034,  accuracy: 0.3468
[2025-09-15 20:30:47,425][flp2p.graph_runner][INFO] - Train, Round 119 : loss => 0.009226453397423033,  accuracy: 1.0, gradient_norm : 0.3886255753003101
[2025-09-15 20:31:08,166][flp2p.graph_runner][INFO] - Test, Round 119 : loss => 4.439699497258663,  accuracy: 0.35
[2025-09-15 20:31:56,288][flp2p.graph_runner][INFO] - Train, Round 120 : loss => 0.008960675604563826,  accuracy: 0.9999733333333334, gradient_norm : 0.3628303865834914
[2025-09-15 20:32:16,746][flp2p.graph_runner][INFO] - Test, Round 120 : loss => 4.462588172376156,  accuracy: 0.3464
[2025-09-15 20:33:05,557][flp2p.graph_runner][INFO] - Train, Round 121 : loss => 0.008569251493706059,  accuracy: 1.0, gradient_norm : 0.3494633514406697
[2025-09-15 20:33:26,027][flp2p.graph_runner][INFO] - Test, Round 121 : loss => 4.480630651307106,  accuracy: 0.3478
[2025-09-15 20:34:15,105][flp2p.graph_runner][INFO] - Train, Round 122 : loss => 0.008266315613679277,  accuracy: 1.0, gradient_norm : 0.36722525838972636
[2025-09-15 20:34:35,549][flp2p.graph_runner][INFO] - Test, Round 122 : loss => 4.501747957193851,  accuracy: 0.3472
[2025-09-15 20:35:24,849][flp2p.graph_runner][INFO] - Train, Round 123 : loss => 0.00797660359792644,  accuracy: 1.0, gradient_norm : 0.3365733503539336
[2025-09-15 20:35:45,159][flp2p.graph_runner][INFO] - Test, Round 123 : loss => 4.5201363451123235,  accuracy: 0.3482
[2025-09-15 20:36:33,482][flp2p.graph_runner][INFO] - Train, Round 124 : loss => 0.007729832629653779,  accuracy: 1.0, gradient_norm : 0.3390992371070618
[2025-09-15 20:36:53,813][flp2p.graph_runner][INFO] - Test, Round 124 : loss => 4.539782169997692,  accuracy: 0.3482
[2025-09-15 20:37:42,182][flp2p.graph_runner][INFO] - Train, Round 125 : loss => 0.007467448726917307,  accuracy: 1.0, gradient_norm : 0.3215666592059314
[2025-09-15 20:38:02,092][flp2p.graph_runner][INFO] - Test, Round 125 : loss => 4.557096715581417,  accuracy: 0.3487
[2025-09-15 20:38:49,671][flp2p.graph_runner][INFO] - Train, Round 126 : loss => 0.007225701834055754,  accuracy: 1.0, gradient_norm : 0.31608425902692117
[2025-09-15 20:39:09,897][flp2p.graph_runner][INFO] - Test, Round 126 : loss => 4.576406501162052,  accuracy: 0.3477
[2025-09-15 20:39:58,896][flp2p.graph_runner][INFO] - Train, Round 127 : loss => 0.007001437997678297,  accuracy: 1.0, gradient_norm : 0.2910356754269727
[2025-09-15 20:40:19,166][flp2p.graph_runner][INFO] - Test, Round 127 : loss => 4.593685697615147,  accuracy: 0.3477
[2025-09-15 20:41:07,179][flp2p.graph_runner][INFO] - Train, Round 128 : loss => 0.006789963525661738,  accuracy: 1.0, gradient_norm : 0.28963393693212947
[2025-09-15 20:41:27,646][flp2p.graph_runner][INFO] - Test, Round 128 : loss => 4.612676656830311,  accuracy: 0.347
[2025-09-15 20:42:16,254][flp2p.graph_runner][INFO] - Train, Round 129 : loss => 0.006593786138885965,  accuracy: 1.0, gradient_norm : 0.2833571664206403
[2025-09-15 20:42:36,232][flp2p.graph_runner][INFO] - Test, Round 129 : loss => 4.628415546512604,  accuracy: 0.3479
[2025-09-15 20:43:24,055][flp2p.graph_runner][INFO] - Train, Round 130 : loss => 0.006401729060162327,  accuracy: 1.0, gradient_norm : 0.2790826259785462
[2025-09-15 20:43:43,770][flp2p.graph_runner][INFO] - Test, Round 130 : loss => 4.6474010714530944,  accuracy: 0.3472
[2025-09-15 20:44:31,119][flp2p.graph_runner][INFO] - Train, Round 131 : loss => 0.006225199129645865,  accuracy: 1.0, gradient_norm : 0.25783861658157753
[2025-09-15 20:44:51,549][flp2p.graph_runner][INFO] - Test, Round 131 : loss => 4.659625306046009,  accuracy: 0.3462
[2025-09-15 20:45:40,603][flp2p.graph_runner][INFO] - Train, Round 132 : loss => 0.006049515980121215,  accuracy: 1.0, gradient_norm : 0.2595019659958936
[2025-09-15 20:46:00,842][flp2p.graph_runner][INFO] - Test, Round 132 : loss => 4.679162955570221,  accuracy: 0.3489
[2025-09-15 20:46:49,294][flp2p.graph_runner][INFO] - Train, Round 133 : loss => 0.005888488380587659,  accuracy: 1.0, gradient_norm : 0.2584935622987321
[2025-09-15 20:47:09,256][flp2p.graph_runner][INFO] - Test, Round 133 : loss => 4.694913736772537,  accuracy: 0.3485
[2025-09-15 20:47:57,778][flp2p.graph_runner][INFO] - Train, Round 134 : loss => 0.005733701122808272,  accuracy: 1.0, gradient_norm : 0.24953066014982273
[2025-09-15 20:48:17,532][flp2p.graph_runner][INFO] - Test, Round 134 : loss => 4.707950239205361,  accuracy: 0.3481
[2025-09-15 20:49:04,797][flp2p.graph_runner][INFO] - Train, Round 135 : loss => 0.005578557978054356,  accuracy: 1.0, gradient_norm : 0.24528112787787326
[2025-09-15 20:49:25,357][flp2p.graph_runner][INFO] - Test, Round 135 : loss => 4.723108218097686,  accuracy: 0.3464
[2025-09-15 20:50:13,698][flp2p.graph_runner][INFO] - Train, Round 136 : loss => 0.005439886048940633,  accuracy: 1.0, gradient_norm : 0.24202177252882218
[2025-09-15 20:50:34,074][flp2p.graph_runner][INFO] - Test, Round 136 : loss => 4.739826440775395,  accuracy: 0.3472
[2025-09-15 20:51:22,473][flp2p.graph_runner][INFO] - Train, Round 137 : loss => 0.005302925271098502,  accuracy: 1.0, gradient_norm : 0.23806854687735485
[2025-09-15 20:51:43,310][flp2p.graph_runner][INFO] - Test, Round 137 : loss => 4.755913691174984,  accuracy: 0.3488
[2025-09-15 20:52:32,288][flp2p.graph_runner][INFO] - Train, Round 138 : loss => 0.005172177653585094,  accuracy: 1.0, gradient_norm : 0.23111627889331252
[2025-09-15 20:52:53,182][flp2p.graph_runner][INFO] - Test, Round 138 : loss => 4.768581954729557,  accuracy: 0.3476
[2025-09-15 20:53:42,245][flp2p.graph_runner][INFO] - Train, Round 139 : loss => 0.0050478538974615145,  accuracy: 1.0, gradient_norm : 0.21869507133752603
[2025-09-15 20:54:02,671][flp2p.graph_runner][INFO] - Test, Round 139 : loss => 4.784473997223377,  accuracy: 0.3478
[2025-09-15 20:54:51,171][flp2p.graph_runner][INFO] - Train, Round 140 : loss => 0.0049317667711875415,  accuracy: 1.0, gradient_norm : 0.22189166511095568
[2025-09-15 20:55:11,480][flp2p.graph_runner][INFO] - Test, Round 140 : loss => 4.799844313383103,  accuracy: 0.3476
[2025-09-15 20:56:00,146][flp2p.graph_runner][INFO] - Train, Round 141 : loss => 0.004818610060513795,  accuracy: 1.0, gradient_norm : 0.22925000691075695
[2025-09-15 20:56:20,338][flp2p.graph_runner][INFO] - Test, Round 141 : loss => 4.813493212163448,  accuracy: 0.3469
[2025-09-15 20:57:08,394][flp2p.graph_runner][INFO] - Train, Round 142 : loss => 0.00470844435088414,  accuracy: 1.0, gradient_norm : 0.2213453072710454
[2025-09-15 20:57:28,666][flp2p.graph_runner][INFO] - Test, Round 142 : loss => 4.826132913947105,  accuracy: 0.3486
[2025-09-15 20:58:16,979][flp2p.graph_runner][INFO] - Train, Round 143 : loss => 0.0046077505633002145,  accuracy: 1.0, gradient_norm : 0.20279660830696364
[2025-09-15 20:58:37,284][flp2p.graph_runner][INFO] - Test, Round 143 : loss => 4.838446330690384,  accuracy: 0.3476
[2025-09-15 20:59:25,091][flp2p.graph_runner][INFO] - Train, Round 144 : loss => 0.00449398986112404,  accuracy: 1.0, gradient_norm : 0.20587620759079123
[2025-09-15 20:59:45,389][flp2p.graph_runner][INFO] - Test, Round 144 : loss => 4.851138132596016,  accuracy: 0.348
[2025-09-15 21:00:33,050][flp2p.graph_runner][INFO] - Train, Round 145 : loss => 0.004401594276811616,  accuracy: 1.0, gradient_norm : 0.18583249862724263
[2025-09-15 21:00:53,379][flp2p.graph_runner][INFO] - Test, Round 145 : loss => 4.862947884583473,  accuracy: 0.3475
[2025-09-15 21:01:41,289][flp2p.graph_runner][INFO] - Train, Round 146 : loss => 0.004306604737260689,  accuracy: 1.0, gradient_norm : 0.20573845234536006
[2025-09-15 21:02:00,910][flp2p.graph_runner][INFO] - Test, Round 146 : loss => 4.875655763196945,  accuracy: 0.3478
[2025-09-15 21:02:47,705][flp2p.graph_runner][INFO] - Train, Round 147 : loss => 0.0042185450250205245,  accuracy: 1.0, gradient_norm : 0.18625501553586993
[2025-09-15 21:03:08,091][flp2p.graph_runner][INFO] - Test, Round 147 : loss => 4.889431993329525,  accuracy: 0.3477
[2025-09-15 21:03:57,068][flp2p.graph_runner][INFO] - Train, Round 148 : loss => 0.004128483347400715,  accuracy: 1.0, gradient_norm : 0.19633678746432237
[2025-09-15 21:04:17,432][flp2p.graph_runner][INFO] - Test, Round 148 : loss => 4.902011835563183,  accuracy: 0.3465
[2025-09-15 21:05:06,335][flp2p.graph_runner][INFO] - Train, Round 149 : loss => 0.0040493784566448705,  accuracy: 1.0, gradient_norm : 0.18608432589343546
[2025-09-15 21:05:26,654][flp2p.graph_runner][INFO] - Test, Round 149 : loss => 4.911410699415207,  accuracy: 0.3475
[2025-09-15 21:06:14,809][flp2p.graph_runner][INFO] - Train, Round 150 : loss => 0.003967219941453854,  accuracy: 1.0, gradient_norm : 0.17895308026666223
[2025-09-15 21:06:35,133][flp2p.graph_runner][INFO] - Test, Round 150 : loss => 4.922854148924351,  accuracy: 0.3486
[2025-09-15 21:07:23,437][flp2p.graph_runner][INFO] - Train, Round 151 : loss => 0.003887642457266337,  accuracy: 1.0, gradient_norm : 0.18141298514614335
[2025-09-15 21:07:43,853][flp2p.graph_runner][INFO] - Test, Round 151 : loss => 4.936962690758705,  accuracy: 0.3483
[2025-09-15 21:08:32,228][flp2p.graph_runner][INFO] - Train, Round 152 : loss => 0.0038147855681017954,  accuracy: 1.0, gradient_norm : 0.17499353820573785
[2025-09-15 21:08:52,452][flp2p.graph_runner][INFO] - Test, Round 152 : loss => 4.947855799734592,  accuracy: 0.347
[2025-09-15 21:09:40,850][flp2p.graph_runner][INFO] - Train, Round 153 : loss => 0.003741584946255897,  accuracy: 1.0, gradient_norm : 0.16810073085455415
[2025-09-15 21:10:01,574][flp2p.graph_runner][INFO] - Test, Round 153 : loss => 4.959370103013516,  accuracy: 0.3473
[2025-09-15 21:10:50,388][flp2p.graph_runner][INFO] - Train, Round 154 : loss => 0.00366974987341867,  accuracy: 1.0, gradient_norm : 0.1718778738535596
[2025-09-15 21:11:10,621][flp2p.graph_runner][INFO] - Test, Round 154 : loss => 4.970521185612679,  accuracy: 0.3477
[2025-09-15 21:11:58,000][flp2p.graph_runner][INFO] - Train, Round 155 : loss => 0.003605473178516453,  accuracy: 1.0, gradient_norm : 0.17150727312756409
[2025-09-15 21:12:18,432][flp2p.graph_runner][INFO] - Test, Round 155 : loss => 4.979258040845394,  accuracy: 0.3479
[2025-09-15 21:13:06,248][flp2p.graph_runner][INFO] - Train, Round 156 : loss => 0.0035392699960114737,  accuracy: 1.0, gradient_norm : 0.1688811919887479
[2025-09-15 21:13:26,611][flp2p.graph_runner][INFO] - Test, Round 156 : loss => 4.99150360351801,  accuracy: 0.3478
[2025-09-15 21:14:14,950][flp2p.graph_runner][INFO] - Train, Round 157 : loss => 0.0034717100691341323,  accuracy: 1.0, gradient_norm : 0.162652749341716
[2025-09-15 21:14:35,109][flp2p.graph_runner][INFO] - Test, Round 157 : loss => 5.000611685621738,  accuracy: 0.3482
[2025-09-15 21:15:23,586][flp2p.graph_runner][INFO] - Train, Round 158 : loss => 0.0034108100665616796,  accuracy: 1.0, gradient_norm : 0.1555690836641713
[2025-09-15 21:15:43,788][flp2p.graph_runner][INFO] - Test, Round 158 : loss => 5.012394201278687,  accuracy: 0.3474
[2025-09-15 21:16:32,182][flp2p.graph_runner][INFO] - Train, Round 159 : loss => 0.003351106708626806,  accuracy: 1.0, gradient_norm : 0.15398269802254352
[2025-09-15 21:16:52,845][flp2p.graph_runner][INFO] - Test, Round 159 : loss => 5.02597532696724,  accuracy: 0.3471
[2025-09-15 21:17:41,666][flp2p.graph_runner][INFO] - Train, Round 160 : loss => 0.003296712732505207,  accuracy: 1.0, gradient_norm : 0.1596074738301997
[2025-09-15 21:18:02,488][flp2p.graph_runner][INFO] - Test, Round 160 : loss => 5.033430009365082,  accuracy: 0.3477
[2025-09-15 21:18:51,682][flp2p.graph_runner][INFO] - Train, Round 161 : loss => 0.0032400392579438626,  accuracy: 1.0, gradient_norm : 0.14528402756946737
[2025-09-15 21:19:11,850][flp2p.graph_runner][INFO] - Test, Round 161 : loss => 5.043835017502308,  accuracy: 0.3481
[2025-09-15 21:20:00,580][flp2p.graph_runner][INFO] - Train, Round 162 : loss => 0.0031836600654060023,  accuracy: 1.0, gradient_norm : 0.14589507754747838
[2025-09-15 21:20:20,500][flp2p.graph_runner][INFO] - Test, Round 162 : loss => 5.053825350880623,  accuracy: 0.3475
[2025-09-15 21:21:08,261][flp2p.graph_runner][INFO] - Train, Round 163 : loss => 0.0031310374309153587,  accuracy: 1.0, gradient_norm : 0.14249203353370585
[2025-09-15 21:21:28,653][flp2p.graph_runner][INFO] - Test, Round 163 : loss => 5.062846649503708,  accuracy: 0.3481
[2025-09-15 21:22:17,169][flp2p.graph_runner][INFO] - Train, Round 164 : loss => 0.00308088925036524,  accuracy: 1.0, gradient_norm : 0.1381697249082691
[2025-09-15 21:22:37,647][flp2p.graph_runner][INFO] - Test, Round 164 : loss => 5.072963850224018,  accuracy: 0.3486
[2025-09-15 21:23:26,158][flp2p.graph_runner][INFO] - Train, Round 165 : loss => 0.0030320379656041036,  accuracy: 1.0, gradient_norm : 0.14074925832726462
[2025-09-15 21:23:46,286][flp2p.graph_runner][INFO] - Test, Round 165 : loss => 5.082229608976841,  accuracy: 0.3475
[2025-09-15 21:24:33,949][flp2p.graph_runner][INFO] - Train, Round 166 : loss => 0.002985814432565046,  accuracy: 1.0, gradient_norm : 0.13929017452374415
[2025-09-15 21:24:54,388][flp2p.graph_runner][INFO] - Test, Round 166 : loss => 5.0920008733987805,  accuracy: 0.3468
[2025-09-15 21:25:43,079][flp2p.graph_runner][INFO] - Train, Round 167 : loss => 0.00293741114801378,  accuracy: 1.0, gradient_norm : 0.13571124292258405
[2025-09-15 21:26:03,411][flp2p.graph_runner][INFO] - Test, Round 167 : loss => 5.100405434119701,  accuracy: 0.3472
[2025-09-15 21:26:52,063][flp2p.graph_runner][INFO] - Train, Round 168 : loss => 0.0028927621005277603,  accuracy: 1.0, gradient_norm : 0.13532923491353416
[2025-09-15 21:27:12,189][flp2p.graph_runner][INFO] - Test, Round 168 : loss => 5.110650851559639,  accuracy: 0.3474
[2025-09-15 21:27:59,813][flp2p.graph_runner][INFO] - Train, Round 169 : loss => 0.0028458901968648814,  accuracy: 1.0, gradient_norm : 0.13821424881186795
[2025-09-15 21:28:20,199][flp2p.graph_runner][INFO] - Test, Round 169 : loss => 5.119936495935917,  accuracy: 0.3477
[2025-09-15 21:29:08,627][flp2p.graph_runner][INFO] - Train, Round 170 : loss => 0.002803954423240308,  accuracy: 1.0, gradient_norm : 0.1330882607156696
[2025-09-15 21:29:29,055][flp2p.graph_runner][INFO] - Test, Round 170 : loss => 5.128598869824409,  accuracy: 0.3467
[2025-09-15 21:30:17,762][flp2p.graph_runner][INFO] - Train, Round 171 : loss => 0.002762635314720683,  accuracy: 1.0, gradient_norm : 0.12576653088662307
[2025-09-15 21:30:37,906][flp2p.graph_runner][INFO] - Test, Round 171 : loss => 5.136897384059429,  accuracy: 0.3465
[2025-09-15 21:31:25,632][flp2p.graph_runner][INFO] - Train, Round 172 : loss => 0.002721292441177259,  accuracy: 1.0, gradient_norm : 0.1320666814750851
[2025-09-15 21:31:46,097][flp2p.graph_runner][INFO] - Test, Round 172 : loss => 5.146060766077041,  accuracy: 0.3475
[2025-09-15 21:32:34,539][flp2p.graph_runner][INFO] - Train, Round 173 : loss => 0.002682018567090078,  accuracy: 1.0, gradient_norm : 0.1241986982639424
[2025-09-15 21:32:54,598][flp2p.graph_runner][INFO] - Test, Round 173 : loss => 5.155860171210766,  accuracy: 0.3479
[2025-09-15 21:33:42,687][flp2p.graph_runner][INFO] - Train, Round 174 : loss => 0.00264499017095659,  accuracy: 1.0, gradient_norm : 0.12449788029961495
[2025-09-15 21:34:03,218][flp2p.graph_runner][INFO] - Test, Round 174 : loss => 5.162368666958809,  accuracy: 0.3481
[2025-09-15 21:34:51,887][flp2p.graph_runner][INFO] - Train, Round 175 : loss => 0.002606609769039399,  accuracy: 1.0, gradient_norm : 0.12266198038057576
[2025-09-15 21:35:12,360][flp2p.graph_runner][INFO] - Test, Round 175 : loss => 5.1707035958886145,  accuracy: 0.3474
[2025-09-15 21:36:00,794][flp2p.graph_runner][INFO] - Train, Round 176 : loss => 0.0025695729216386105,  accuracy: 1.0, gradient_norm : 0.11879585311115788
[2025-09-15 21:36:21,509][flp2p.graph_runner][INFO] - Test, Round 176 : loss => 5.179978184103966,  accuracy: 0.3475
[2025-09-15 21:37:10,407][flp2p.graph_runner][INFO] - Train, Round 177 : loss => 0.0025344165267112352,  accuracy: 1.0, gradient_norm : 0.12425304807981123
[2025-09-15 21:37:30,714][flp2p.graph_runner][INFO] - Test, Round 177 : loss => 5.186284739804268,  accuracy: 0.3474
[2025-09-15 21:38:18,561][flp2p.graph_runner][INFO] - Train, Round 178 : loss => 0.0025001670657366047,  accuracy: 1.0, gradient_norm : 0.12275359144893129
[2025-09-15 21:38:38,703][flp2p.graph_runner][INFO] - Test, Round 178 : loss => 5.197071298420429,  accuracy: 0.3468
[2025-09-15 21:39:27,375][flp2p.graph_runner][INFO] - Train, Round 179 : loss => 0.0024682482731683796,  accuracy: 1.0, gradient_norm : 0.11450205795213403
[2025-09-15 21:39:47,340][flp2p.graph_runner][INFO] - Test, Round 179 : loss => 5.204474634540081,  accuracy: 0.3464
[2025-09-15 21:40:34,957][flp2p.graph_runner][INFO] - Train, Round 180 : loss => 0.0024325870770553603,  accuracy: 1.0, gradient_norm : 0.11769591771623764
[2025-09-15 21:40:55,565][flp2p.graph_runner][INFO] - Test, Round 180 : loss => 5.212988614332676,  accuracy: 0.3473
[2025-09-15 21:41:44,584][flp2p.graph_runner][INFO] - Train, Round 181 : loss => 0.002399667974629361,  accuracy: 1.0, gradient_norm : 0.11701744643919355
[2025-09-15 21:42:05,015][flp2p.graph_runner][INFO] - Test, Round 181 : loss => 5.220372531545162,  accuracy: 0.3469
[2025-09-15 21:42:53,940][flp2p.graph_runner][INFO] - Train, Round 182 : loss => 0.002368321019748692,  accuracy: 1.0, gradient_norm : 0.11076214617498424
[2025-09-15 21:43:14,710][flp2p.graph_runner][INFO] - Test, Round 182 : loss => 5.229240699410439,  accuracy: 0.3474
[2025-09-15 21:44:03,258][flp2p.graph_runner][INFO] - Train, Round 183 : loss => 0.0023364399238198527,  accuracy: 1.0, gradient_norm : 0.11485820843407982
[2025-09-15 21:44:23,534][flp2p.graph_runner][INFO] - Test, Round 183 : loss => 5.2344103188872335,  accuracy: 0.347
[2025-09-15 21:45:12,176][flp2p.graph_runner][INFO] - Train, Round 184 : loss => 0.002308857882841645,  accuracy: 1.0, gradient_norm : 0.11098568647737674
[2025-09-15 21:45:32,414][flp2p.graph_runner][INFO] - Test, Round 184 : loss => 5.242394995009899,  accuracy: 0.3469
[2025-09-15 21:46:20,510][flp2p.graph_runner][INFO] - Train, Round 185 : loss => 0.002279014214242731,  accuracy: 1.0, gradient_norm : 0.11755828680226309
[2025-09-15 21:46:41,060][flp2p.graph_runner][INFO] - Test, Round 185 : loss => 5.250542967891693,  accuracy: 0.3474
[2025-09-15 21:47:29,207][flp2p.graph_runner][INFO] - Train, Round 186 : loss => 0.0022505237832956475,  accuracy: 1.0, gradient_norm : 0.1066041421920678
[2025-09-15 21:47:49,919][flp2p.graph_runner][INFO] - Test, Round 186 : loss => 5.2589788259506225,  accuracy: 0.3471
[2025-09-15 21:48:38,824][flp2p.graph_runner][INFO] - Train, Round 187 : loss => 0.002221084131597309,  accuracy: 1.0, gradient_norm : 0.10645341330195784
[2025-09-15 21:48:58,897][flp2p.graph_runner][INFO] - Test, Round 187 : loss => 5.265130325853825,  accuracy: 0.347
[2025-09-15 21:49:47,297][flp2p.graph_runner][INFO] - Train, Round 188 : loss => 0.0021942911969866446,  accuracy: 1.0, gradient_norm : 0.10656789294513923
[2025-09-15 21:50:07,507][flp2p.graph_runner][INFO] - Test, Round 188 : loss => 5.274089417338371,  accuracy: 0.3465
[2025-09-15 21:50:56,007][flp2p.graph_runner][INFO] - Train, Round 189 : loss => 0.002168155379331438,  accuracy: 1.0, gradient_norm : 0.1051683038845054
[2025-09-15 21:51:16,161][flp2p.graph_runner][INFO] - Test, Round 189 : loss => 5.279790136194229,  accuracy: 0.3474
[2025-09-15 21:52:05,187][flp2p.graph_runner][INFO] - Train, Round 190 : loss => 0.0021418628825146395,  accuracy: 1.0, gradient_norm : 0.10400942040048325
[2025-09-15 21:52:25,657][flp2p.graph_runner][INFO] - Test, Round 190 : loss => 5.287247150373459,  accuracy: 0.3466
[2025-09-15 21:53:14,927][flp2p.graph_runner][INFO] - Train, Round 191 : loss => 0.0021153158204591203,  accuracy: 1.0, gradient_norm : 0.1043923364361994
[2025-09-15 21:53:35,073][flp2p.graph_runner][INFO] - Test, Round 191 : loss => 5.294620547640323,  accuracy: 0.3475
[2025-09-15 21:54:23,162][flp2p.graph_runner][INFO] - Train, Round 192 : loss => 0.002090800071067255,  accuracy: 1.0, gradient_norm : 0.09788232139325728
[2025-09-15 21:54:43,892][flp2p.graph_runner][INFO] - Test, Round 192 : loss => 5.301907107973099,  accuracy: 0.347
[2025-09-15 21:55:32,436][flp2p.graph_runner][INFO] - Train, Round 193 : loss => 0.0020659237453461783,  accuracy: 1.0, gradient_norm : 0.1048602712030136
[2025-09-15 21:55:52,979][flp2p.graph_runner][INFO] - Test, Round 193 : loss => 5.308401584994793,  accuracy: 0.3462
[2025-09-15 21:56:41,785][flp2p.graph_runner][INFO] - Train, Round 194 : loss => 0.0020430029563916225,  accuracy: 1.0, gradient_norm : 0.10441144717795925
[2025-09-15 21:57:01,701][flp2p.graph_runner][INFO] - Test, Round 194 : loss => 5.316648297607899,  accuracy: 0.3474
[2025-09-15 21:57:49,896][flp2p.graph_runner][INFO] - Train, Round 195 : loss => 0.0020191189337250164,  accuracy: 1.0, gradient_norm : 0.10267789234404366
[2025-09-15 21:58:10,191][flp2p.graph_runner][INFO] - Test, Round 195 : loss => 5.322264809906483,  accuracy: 0.3466
[2025-09-15 21:58:58,596][flp2p.graph_runner][INFO] - Train, Round 196 : loss => 0.0019953616496059113,  accuracy: 1.0, gradient_norm : 0.09633200966142123
[2025-09-15 21:59:19,021][flp2p.graph_runner][INFO] - Test, Round 196 : loss => 5.329578308951855,  accuracy: 0.3466
[2025-09-15 22:00:07,548][flp2p.graph_runner][INFO] - Train, Round 197 : loss => 0.0019734037750458806,  accuracy: 1.0, gradient_norm : 0.09133807349175288
[2025-09-15 22:00:27,762][flp2p.graph_runner][INFO] - Test, Round 197 : loss => 5.3357338188648225,  accuracy: 0.3467
[2025-09-15 22:01:15,650][flp2p.graph_runner][INFO] - Train, Round 198 : loss => 0.0019504275591558928,  accuracy: 1.0, gradient_norm : 0.09611269736178345
[2025-09-15 22:01:35,349][flp2p.graph_runner][INFO] - Test, Round 198 : loss => 5.341994611251354,  accuracy: 0.3464
[2025-09-15 22:02:23,467][flp2p.graph_runner][INFO] - Train, Round 199 : loss => 0.0019294053236080797,  accuracy: 1.0, gradient_norm : 0.09413318767119001
[2025-09-15 22:02:43,691][flp2p.graph_runner][INFO] - Test, Round 199 : loss => 5.349612452995777,  accuracy: 0.347
[2025-09-15 22:03:31,800][flp2p.graph_runner][INFO] - Train, Round 200 : loss => 0.0019079372437651424,  accuracy: 1.0, gradient_norm : 0.09595965497106228
[2025-09-15 22:03:52,362][flp2p.graph_runner][INFO] - Test, Round 200 : loss => 5.355305087256432,  accuracy: 0.3467
[2025-09-15 22:04:40,507][flp2p.graph_runner][INFO] - Train, Round 201 : loss => 0.0018882911177934144,  accuracy: 1.0, gradient_norm : 0.09571946268600053
[2025-09-15 22:05:01,011][flp2p.graph_runner][INFO] - Test, Round 201 : loss => 5.362080008304119,  accuracy: 0.3459
[2025-09-15 22:05:49,469][flp2p.graph_runner][INFO] - Train, Round 202 : loss => 0.0018668918965461972,  accuracy: 1.0, gradient_norm : 0.09214438896103845
[2025-09-15 22:06:09,909][flp2p.graph_runner][INFO] - Test, Round 202 : loss => 5.368957397580147,  accuracy: 0.3477
[2025-09-15 22:06:58,312][flp2p.graph_runner][INFO] - Train, Round 203 : loss => 0.0018467361221458608,  accuracy: 1.0, gradient_norm : 0.09233314237087027
[2025-09-15 22:07:18,934][flp2p.graph_runner][INFO] - Test, Round 203 : loss => 5.376303067600727,  accuracy: 0.3466
[2025-09-15 22:08:07,829][flp2p.graph_runner][INFO] - Train, Round 204 : loss => 0.0018279675158070553,  accuracy: 1.0, gradient_norm : 0.08976543154314508
[2025-09-15 22:08:28,189][flp2p.graph_runner][INFO] - Test, Round 204 : loss => 5.382295514798164,  accuracy: 0.3471
[2025-09-15 22:09:17,330][flp2p.graph_runner][INFO] - Train, Round 205 : loss => 0.0018078604617524734,  accuracy: 1.0, gradient_norm : 0.08707050504039496
[2025-09-15 22:09:37,150][flp2p.graph_runner][INFO] - Test, Round 205 : loss => 5.387666636097431,  accuracy: 0.3463
[2025-09-15 22:10:24,463][flp2p.graph_runner][INFO] - Train, Round 206 : loss => 0.0017900401545436265,  accuracy: 1.0, gradient_norm : 0.08649483177880474
[2025-09-15 22:10:44,741][flp2p.graph_runner][INFO] - Test, Round 206 : loss => 5.393191583943367,  accuracy: 0.3472
[2025-09-15 22:11:33,318][flp2p.graph_runner][INFO] - Train, Round 207 : loss => 0.0017716439716605234,  accuracy: 1.0, gradient_norm : 0.09009735369451458
[2025-09-15 22:11:53,369][flp2p.graph_runner][INFO] - Test, Round 207 : loss => 5.400680185866356,  accuracy: 0.3465
[2025-09-15 22:12:42,329][flp2p.graph_runner][INFO] - Train, Round 208 : loss => 0.0017528982186922801,  accuracy: 1.0, gradient_norm : 0.08998228493360136
[2025-09-15 22:13:02,779][flp2p.graph_runner][INFO] - Test, Round 208 : loss => 5.406505514955521,  accuracy: 0.3466
[2025-09-15 22:13:51,392][flp2p.graph_runner][INFO] - Train, Round 209 : loss => 0.001734130213784131,  accuracy: 1.0, gradient_norm : 0.08616986909579241
[2025-09-15 22:14:11,585][flp2p.graph_runner][INFO] - Test, Round 209 : loss => 5.412224115514755,  accuracy: 0.347
[2025-09-15 22:14:59,394][flp2p.graph_runner][INFO] - Train, Round 210 : loss => 0.0017173559985531029,  accuracy: 1.0, gradient_norm : 0.08368683006380387
[2025-09-15 22:15:19,655][flp2p.graph_runner][INFO] - Test, Round 210 : loss => 5.41792665143013,  accuracy: 0.3467
[2025-09-15 22:16:07,862][flp2p.graph_runner][INFO] - Train, Round 211 : loss => 0.0017012796506120742,  accuracy: 1.0, gradient_norm : 0.08866628093697951
[2025-09-15 22:16:27,915][flp2p.graph_runner][INFO] - Test, Round 211 : loss => 5.424423093819618,  accuracy: 0.3464
[2025-09-15 22:17:15,780][flp2p.graph_runner][INFO] - Train, Round 212 : loss => 0.0016840824897371926,  accuracy: 1.0, gradient_norm : 0.08519066607495444
[2025-09-15 22:17:36,254][flp2p.graph_runner][INFO] - Test, Round 212 : loss => 5.429449163460731,  accuracy: 0.3469
[2025-09-15 22:18:24,142][flp2p.graph_runner][INFO] - Train, Round 213 : loss => 0.0016662522141511244,  accuracy: 1.0, gradient_norm : 0.08077847334542292
[2025-09-15 22:18:44,481][flp2p.graph_runner][INFO] - Test, Round 213 : loss => 5.4358286602258685,  accuracy: 0.3467
[2025-09-15 22:19:32,295][flp2p.graph_runner][INFO] - Train, Round 214 : loss => 0.0016509975179845545,  accuracy: 1.0, gradient_norm : 0.08586679808265338
[2025-09-15 22:19:52,897][flp2p.graph_runner][INFO] - Test, Round 214 : loss => 5.440886534428596,  accuracy: 0.3465
[2025-09-15 22:20:41,636][flp2p.graph_runner][INFO] - Train, Round 215 : loss => 0.0016355069069686572,  accuracy: 1.0, gradient_norm : 0.08157323485951054
[2025-09-15 22:21:02,134][flp2p.graph_runner][INFO] - Test, Round 215 : loss => 5.4472816332936285,  accuracy: 0.3459
[2025-09-15 22:21:51,064][flp2p.graph_runner][INFO] - Train, Round 216 : loss => 0.0016194192949236213,  accuracy: 1.0, gradient_norm : 0.08175977862428323
[2025-09-15 22:22:11,340][flp2p.graph_runner][INFO] - Test, Round 216 : loss => 5.4530177924990655,  accuracy: 0.3454
[2025-09-15 22:22:59,704][flp2p.graph_runner][INFO] - Train, Round 217 : loss => 0.0016043510097127484,  accuracy: 1.0, gradient_norm : 0.08153105583283138
[2025-09-15 22:23:20,198][flp2p.graph_runner][INFO] - Test, Round 217 : loss => 5.459902506768703,  accuracy: 0.3463
[2025-09-15 22:24:08,485][flp2p.graph_runner][INFO] - Train, Round 218 : loss => 0.0015892094284936324,  accuracy: 1.0, gradient_norm : 0.07795127379284715
[2025-09-15 22:24:28,854][flp2p.graph_runner][INFO] - Test, Round 218 : loss => 5.46478970836401,  accuracy: 0.3465
[2025-09-15 22:25:17,685][flp2p.graph_runner][INFO] - Train, Round 219 : loss => 0.001575059451157965,  accuracy: 1.0, gradient_norm : 0.07752931195290104
[2025-09-15 22:25:37,982][flp2p.graph_runner][INFO] - Test, Round 219 : loss => 5.469721310615539,  accuracy: 0.347
[2025-09-15 22:26:26,626][flp2p.graph_runner][INFO] - Train, Round 220 : loss => 0.001560563823077245,  accuracy: 1.0, gradient_norm : 0.07814907858067742
[2025-09-15 22:26:46,675][flp2p.graph_runner][INFO] - Test, Round 220 : loss => 5.474922190749645,  accuracy: 0.3457
[2025-09-15 22:27:35,603][flp2p.graph_runner][INFO] - Train, Round 221 : loss => 0.0015462164875013203,  accuracy: 1.0, gradient_norm : 0.07537536107426608
[2025-09-15 22:27:55,712][flp2p.graph_runner][INFO] - Test, Round 221 : loss => 5.4794863955259325,  accuracy: 0.3462
[2025-09-15 22:28:44,361][flp2p.graph_runner][INFO] - Train, Round 222 : loss => 0.0015314542576864677,  accuracy: 1.0, gradient_norm : 0.07437977144514818
[2025-09-15 22:29:04,674][flp2p.graph_runner][INFO] - Test, Round 222 : loss => 5.485710953485966,  accuracy: 0.3464
[2025-09-15 22:29:53,728][flp2p.graph_runner][INFO] - Train, Round 223 : loss => 0.0015172209652276557,  accuracy: 1.0, gradient_norm : 0.07575470704368886
[2025-09-15 22:30:13,660][flp2p.graph_runner][INFO] - Test, Round 223 : loss => 5.490190522766113,  accuracy: 0.3461
[2025-09-15 22:31:02,159][flp2p.graph_runner][INFO] - Train, Round 224 : loss => 0.0015046784769947405,  accuracy: 1.0, gradient_norm : 0.0779688094184589
[2025-09-15 22:31:22,572][flp2p.graph_runner][INFO] - Test, Round 224 : loss => 5.4960862711310385,  accuracy: 0.346
[2025-09-15 22:32:11,021][flp2p.graph_runner][INFO] - Train, Round 225 : loss => 0.0014903135342319723,  accuracy: 1.0, gradient_norm : 0.07628107913233667
[2025-09-15 22:32:31,000][flp2p.graph_runner][INFO] - Test, Round 225 : loss => 5.500608974134922,  accuracy: 0.3466
[2025-09-15 22:33:19,177][flp2p.graph_runner][INFO] - Train, Round 226 : loss => 0.0014784423673700076,  accuracy: 1.0, gradient_norm : 0.07611182690632531
[2025-09-15 22:33:40,017][flp2p.graph_runner][INFO] - Test, Round 226 : loss => 5.5069016128897665,  accuracy: 0.3451
[2025-09-15 22:34:29,328][flp2p.graph_runner][INFO] - Train, Round 227 : loss => 0.0014651477463000148,  accuracy: 1.0, gradient_norm : 0.07442540908715417
[2025-09-15 22:34:49,651][flp2p.graph_runner][INFO] - Test, Round 227 : loss => 5.51277093834877,  accuracy: 0.3459
[2025-09-15 22:35:38,592][flp2p.graph_runner][INFO] - Train, Round 228 : loss => 0.001452045182595612,  accuracy: 1.0, gradient_norm : 0.07051518974860492
[2025-09-15 22:35:58,624][flp2p.graph_runner][INFO] - Test, Round 228 : loss => 5.518346058988572,  accuracy: 0.346
[2025-09-15 22:36:45,820][flp2p.graph_runner][INFO] - Train, Round 229 : loss => 0.0014399396171211269,  accuracy: 1.0, gradient_norm : 0.07282945235599803
[2025-09-15 22:37:06,194][flp2p.graph_runner][INFO] - Test, Round 229 : loss => 5.523430891823769,  accuracy: 0.3465
[2025-09-15 22:37:54,859][flp2p.graph_runner][INFO] - Train, Round 230 : loss => 0.0014274622820327446,  accuracy: 1.0, gradient_norm : 0.07411424580938998
[2025-09-15 22:38:15,571][flp2p.graph_runner][INFO] - Test, Round 230 : loss => 5.5281721077919,  accuracy: 0.3457
[2025-09-15 22:39:04,065][flp2p.graph_runner][INFO] - Train, Round 231 : loss => 0.001415102218428122,  accuracy: 1.0, gradient_norm : 0.07381430176174661
[2025-09-15 22:39:24,453][flp2p.graph_runner][INFO] - Test, Round 231 : loss => 5.531767313504219,  accuracy: 0.3454
[2025-09-15 22:40:12,772][flp2p.graph_runner][INFO] - Train, Round 232 : loss => 0.0014038162112895713,  accuracy: 1.0, gradient_norm : 0.07160878129006254
[2025-09-15 22:40:33,088][flp2p.graph_runner][INFO] - Test, Round 232 : loss => 5.5382579164505,  accuracy: 0.3458
[2025-09-15 22:41:21,806][flp2p.graph_runner][INFO] - Train, Round 233 : loss => 0.0013911357470108973,  accuracy: 1.0, gradient_norm : 0.06707388679793311
[2025-09-15 22:41:41,960][flp2p.graph_runner][INFO] - Test, Round 233 : loss => 5.542668353462219,  accuracy: 0.346
[2025-09-15 22:42:30,170][flp2p.graph_runner][INFO] - Train, Round 234 : loss => 0.0013796751565920806,  accuracy: 1.0, gradient_norm : 0.07100780205924348
[2025-09-15 22:42:50,557][flp2p.graph_runner][INFO] - Test, Round 234 : loss => 5.548317140471935,  accuracy: 0.3455
[2025-09-15 22:43:39,150][flp2p.graph_runner][INFO] - Train, Round 235 : loss => 0.0013690562538007118,  accuracy: 1.0, gradient_norm : 0.07044540524971665
[2025-09-15 22:43:59,502][flp2p.graph_runner][INFO] - Test, Round 235 : loss => 5.553065323901176,  accuracy: 0.3456
[2025-09-15 22:44:48,021][flp2p.graph_runner][INFO] - Train, Round 236 : loss => 0.001357167467552548,  accuracy: 1.0, gradient_norm : 0.07214844944238355
[2025-09-15 22:45:08,291][flp2p.graph_runner][INFO] - Test, Round 236 : loss => 5.557380864524841,  accuracy: 0.3463
[2025-09-15 22:45:57,277][flp2p.graph_runner][INFO] - Train, Round 237 : loss => 0.0013468687403656076,  accuracy: 1.0, gradient_norm : 0.06943122908105534
[2025-09-15 22:46:17,624][flp2p.graph_runner][INFO] - Test, Round 237 : loss => 5.561282866263389,  accuracy: 0.3457
[2025-09-15 22:47:06,673][flp2p.graph_runner][INFO] - Train, Round 238 : loss => 0.0013354592420849565,  accuracy: 1.0, gradient_norm : 0.06609916131425705
[2025-09-15 22:47:26,986][flp2p.graph_runner][INFO] - Test, Round 238 : loss => 5.567177002298832,  accuracy: 0.3457
[2025-09-15 22:48:16,472][flp2p.graph_runner][INFO] - Train, Round 239 : loss => 0.001324598397695809,  accuracy: 1.0, gradient_norm : 0.06870541419057143
[2025-09-15 22:48:36,510][flp2p.graph_runner][INFO] - Test, Round 239 : loss => 5.572878740608692,  accuracy: 0.3462
[2025-09-15 22:49:24,356][flp2p.graph_runner][INFO] - Train, Round 240 : loss => 0.001314456027224272,  accuracy: 1.0, gradient_norm : 0.06493100600400446
[2025-09-15 22:49:44,607][flp2p.graph_runner][INFO] - Test, Round 240 : loss => 5.576832043373585,  accuracy: 0.3454
[2025-09-15 22:50:32,632][flp2p.graph_runner][INFO] - Train, Round 241 : loss => 0.0013040267955996873,  accuracy: 1.0, gradient_norm : 0.0661435925479648
[2025-09-15 22:50:52,140][flp2p.graph_runner][INFO] - Test, Round 241 : loss => 5.581938335442543,  accuracy: 0.3456
[2025-09-15 22:51:39,051][flp2p.graph_runner][INFO] - Train, Round 242 : loss => 0.0012938429625258623,  accuracy: 1.0, gradient_norm : 0.06410442978990324
[2025-09-15 22:51:59,224][flp2p.graph_runner][INFO] - Test, Round 242 : loss => 5.58562613158226,  accuracy: 0.3456
[2025-09-15 22:52:47,365][flp2p.graph_runner][INFO] - Train, Round 243 : loss => 0.0012831324863266976,  accuracy: 1.0, gradient_norm : 0.06521825846467251
[2025-09-15 22:53:07,752][flp2p.graph_runner][INFO] - Test, Round 243 : loss => 5.590669172370434,  accuracy: 0.346
[2025-09-15 22:53:56,248][flp2p.graph_runner][INFO] - Train, Round 244 : loss => 0.001273818022285317,  accuracy: 1.0, gradient_norm : 0.06623220955842539
[2025-09-15 22:54:16,637][flp2p.graph_runner][INFO] - Test, Round 244 : loss => 5.595706561183929,  accuracy: 0.3457
[2025-09-15 22:55:05,199][flp2p.graph_runner][INFO] - Train, Round 245 : loss => 0.0012644105433234163,  accuracy: 1.0, gradient_norm : 0.06696165660616954
[2025-09-15 22:55:25,371][flp2p.graph_runner][INFO] - Test, Round 245 : loss => 5.599774597358704,  accuracy: 0.3455
[2025-09-15 22:56:13,777][flp2p.graph_runner][INFO] - Train, Round 246 : loss => 0.0012550309287325944,  accuracy: 1.0, gradient_norm : 0.06584047109318225
[2025-09-15 22:56:34,493][flp2p.graph_runner][INFO] - Test, Round 246 : loss => 5.604120393955707,  accuracy: 0.3458
[2025-09-15 22:57:23,563][flp2p.graph_runner][INFO] - Train, Round 247 : loss => 0.0012449089200526943,  accuracy: 1.0, gradient_norm : 0.06563975741672984
[2025-09-15 22:57:43,953][flp2p.graph_runner][INFO] - Test, Round 247 : loss => 5.60907319663763,  accuracy: 0.3461
[2025-09-15 22:58:33,010][flp2p.graph_runner][INFO] - Train, Round 248 : loss => 0.0012353667806019079,  accuracy: 1.0, gradient_norm : 0.06425921092678406
[2025-09-15 22:58:52,796][flp2p.graph_runner][INFO] - Test, Round 248 : loss => 5.61320905828476,  accuracy: 0.3451
[2025-09-15 22:59:40,925][flp2p.graph_runner][INFO] - Train, Round 249 : loss => 0.00122622489281639,  accuracy: 1.0, gradient_norm : 0.06357895546974121
[2025-09-15 23:00:01,609][flp2p.graph_runner][INFO] - Test, Round 249 : loss => 5.617138901603222,  accuracy: 0.346
[2025-09-15 23:00:50,240][flp2p.graph_runner][INFO] - Train, Round 250 : loss => 0.0012172697069036077,  accuracy: 1.0, gradient_norm : 0.06437017387012506
[2025-09-15 23:01:10,659][flp2p.graph_runner][INFO] - Test, Round 250 : loss => 5.621157532668113,  accuracy: 0.3463
[2025-09-15 23:01:59,111][flp2p.graph_runner][INFO] - Train, Round 251 : loss => 0.001208508845278023,  accuracy: 1.0, gradient_norm : 0.0655164461565358
[2025-09-15 23:02:19,457][flp2p.graph_runner][INFO] - Test, Round 251 : loss => 5.625483679103851,  accuracy: 0.3457
[2025-09-15 23:03:07,887][flp2p.graph_runner][INFO] - Train, Round 252 : loss => 0.001199546067097496,  accuracy: 1.0, gradient_norm : 0.062071693440900216
[2025-09-15 23:03:28,489][flp2p.graph_runner][INFO] - Test, Round 252 : loss => 5.630585420274734,  accuracy: 0.3456
[2025-09-15 23:04:17,107][flp2p.graph_runner][INFO] - Train, Round 253 : loss => 0.0011906946357582147,  accuracy: 1.0, gradient_norm : 0.06609883892846585
[2025-09-15 23:04:37,552][flp2p.graph_runner][INFO] - Test, Round 253 : loss => 5.635121929562092,  accuracy: 0.3452
[2025-09-15 23:05:25,953][flp2p.graph_runner][INFO] - Train, Round 254 : loss => 0.0011819423892059908,  accuracy: 1.0, gradient_norm : 0.06089279555616729
[2025-09-15 23:05:46,483][flp2p.graph_runner][INFO] - Test, Round 254 : loss => 5.639436313974858,  accuracy: 0.3457
[2025-09-15 23:06:34,877][flp2p.graph_runner][INFO] - Train, Round 255 : loss => 0.0011735135534399889,  accuracy: 1.0, gradient_norm : 0.05965358402314243
[2025-09-15 23:06:55,285][flp2p.graph_runner][INFO] - Test, Round 255 : loss => 5.643017044007778,  accuracy: 0.3454
[2025-09-15 23:07:43,182][flp2p.graph_runner][INFO] - Train, Round 256 : loss => 0.0011649193180589166,  accuracy: 1.0, gradient_norm : 0.060831037689797966
[2025-09-15 23:08:03,685][flp2p.graph_runner][INFO] - Test, Round 256 : loss => 5.647714190673828,  accuracy: 0.346
[2025-09-15 23:08:51,647][flp2p.graph_runner][INFO] - Train, Round 257 : loss => 0.0011567070109110016,  accuracy: 1.0, gradient_norm : 0.06134976123573944
[2025-09-15 23:09:12,041][flp2p.graph_runner][INFO] - Test, Round 257 : loss => 5.652820238208771,  accuracy: 0.3457
[2025-09-15 23:09:59,834][flp2p.graph_runner][INFO] - Train, Round 258 : loss => 0.0011485344166188344,  accuracy: 1.0, gradient_norm : 0.060005051079808025
[2025-09-15 23:10:19,683][flp2p.graph_runner][INFO] - Test, Round 258 : loss => 5.656599828493595,  accuracy: 0.3453
[2025-09-15 23:11:06,814][flp2p.graph_runner][INFO] - Train, Round 259 : loss => 0.0011404666698460156,  accuracy: 1.0, gradient_norm : 0.05962128117001697
[2025-09-15 23:11:26,914][flp2p.graph_runner][INFO] - Test, Round 259 : loss => 5.660051485025883,  accuracy: 0.3454
[2025-09-15 23:12:15,060][flp2p.graph_runner][INFO] - Train, Round 260 : loss => 0.001132692456473402,  accuracy: 1.0, gradient_norm : 0.06168193709799255
[2025-09-15 23:12:35,441][flp2p.graph_runner][INFO] - Test, Round 260 : loss => 5.664362234520913,  accuracy: 0.3451
[2025-09-15 23:13:23,818][flp2p.graph_runner][INFO] - Train, Round 261 : loss => 0.001124112703910214,  accuracy: 1.0, gradient_norm : 0.05702474792813781
[2025-09-15 23:13:44,061][flp2p.graph_runner][INFO] - Test, Round 261 : loss => 5.668151934659481,  accuracy: 0.3452
[2025-09-15 23:14:32,992][flp2p.graph_runner][INFO] - Train, Round 262 : loss => 0.001117412415745396,  accuracy: 1.0, gradient_norm : 0.0612026623076142
[2025-09-15 23:14:53,509][flp2p.graph_runner][INFO] - Test, Round 262 : loss => 5.673318532407284,  accuracy: 0.3454
[2025-09-15 23:15:42,415][flp2p.graph_runner][INFO] - Train, Round 263 : loss => 0.001109417218734355,  accuracy: 1.0, gradient_norm : 0.05540950095057544
[2025-09-15 23:16:02,513][flp2p.graph_runner][INFO] - Test, Round 263 : loss => 5.676643482744693,  accuracy: 0.345
[2025-09-15 23:16:50,106][flp2p.graph_runner][INFO] - Train, Round 264 : loss => 0.0011019788329819375,  accuracy: 1.0, gradient_norm : 0.059034849416825214
[2025-09-15 23:17:10,204][flp2p.graph_runner][INFO] - Test, Round 264 : loss => 5.680802622807026,  accuracy: 0.3453
[2025-09-15 23:17:58,535][flp2p.graph_runner][INFO] - Train, Round 265 : loss => 0.0010940863089490456,  accuracy: 1.0, gradient_norm : 0.05528358386592095
[2025-09-15 23:18:18,215][flp2p.graph_runner][INFO] - Test, Round 265 : loss => 5.684433426201344,  accuracy: 0.3451
[2025-09-15 23:19:06,150][flp2p.graph_runner][INFO] - Train, Round 266 : loss => 0.001087294686258247,  accuracy: 1.0, gradient_norm : 0.05712567501552164
[2025-09-15 23:19:26,226][flp2p.graph_runner][INFO] - Test, Round 266 : loss => 5.688627241110802,  accuracy: 0.3453
[2025-09-15 23:20:14,246][flp2p.graph_runner][INFO] - Train, Round 267 : loss => 0.001079703942959896,  accuracy: 1.0, gradient_norm : 0.055788681086681516
[2025-09-15 23:20:34,346][flp2p.graph_runner][INFO] - Test, Round 267 : loss => 5.692983243227005,  accuracy: 0.3453
[2025-09-15 23:21:22,802][flp2p.graph_runner][INFO] - Train, Round 268 : loss => 0.0010722390518397632,  accuracy: 1.0, gradient_norm : 0.05350100891150339
[2025-09-15 23:21:43,319][flp2p.graph_runner][INFO] - Test, Round 268 : loss => 5.69643951113224,  accuracy: 0.3456
[2025-09-15 23:22:31,286][flp2p.graph_runner][INFO] - Train, Round 269 : loss => 0.001065286373705021,  accuracy: 1.0, gradient_norm : 0.055250124398218456
[2025-09-15 23:22:51,741][flp2p.graph_runner][INFO] - Test, Round 269 : loss => 5.701414942336083,  accuracy: 0.3452
[2025-09-15 23:23:40,063][flp2p.graph_runner][INFO] - Train, Round 270 : loss => 0.0010584189513125843,  accuracy: 1.0, gradient_norm : 0.0562916771652469
[2025-09-15 23:24:00,377][flp2p.graph_runner][INFO] - Test, Round 270 : loss => 5.705507005393505,  accuracy: 0.3454
[2025-09-15 23:24:48,696][flp2p.graph_runner][INFO] - Train, Round 271 : loss => 0.0010516134635690834,  accuracy: 1.0, gradient_norm : 0.05506717704435537
[2025-09-15 23:25:08,848][flp2p.graph_runner][INFO] - Test, Round 271 : loss => 5.7091874965548515,  accuracy: 0.3454
[2025-09-15 23:25:57,778][flp2p.graph_runner][INFO] - Train, Round 272 : loss => 0.0010445517663902136,  accuracy: 1.0, gradient_norm : 0.05499713999623206
[2025-09-15 23:26:18,221][flp2p.graph_runner][INFO] - Test, Round 272 : loss => 5.712570384573937,  accuracy: 0.3453
[2025-09-15 23:27:07,224][flp2p.graph_runner][INFO] - Train, Round 273 : loss => 0.0010381355441738075,  accuracy: 1.0, gradient_norm : 0.05462986802474273
[2025-09-15 23:27:28,041][flp2p.graph_runner][INFO] - Test, Round 273 : loss => 5.715494606983662,  accuracy: 0.3451
[2025-09-15 23:28:17,242][flp2p.graph_runner][INFO] - Train, Round 274 : loss => 0.001031485007091154,  accuracy: 1.0, gradient_norm : 0.05417632853718851
[2025-09-15 23:28:37,736][flp2p.graph_runner][INFO] - Test, Round 274 : loss => 5.71967307112217,  accuracy: 0.3453
[2025-09-15 23:29:26,009][flp2p.graph_runner][INFO] - Train, Round 275 : loss => 0.0010248575750059288,  accuracy: 1.0, gradient_norm : 0.05255929581143171
[2025-09-15 23:29:46,247][flp2p.graph_runner][INFO] - Test, Round 275 : loss => 5.723656369543075,  accuracy: 0.3455
[2025-09-15 23:30:34,319][flp2p.graph_runner][INFO] - Train, Round 276 : loss => 0.0010187217390436366,  accuracy: 1.0, gradient_norm : 0.051727781574540055
[2025-09-15 23:30:55,017][flp2p.graph_runner][INFO] - Test, Round 276 : loss => 5.727143615913391,  accuracy: 0.3452
[2025-09-15 23:31:43,440][flp2p.graph_runner][INFO] - Train, Round 277 : loss => 0.001011620164232833,  accuracy: 1.0, gradient_norm : 0.05280786375737081
[2025-09-15 23:32:04,042][flp2p.graph_runner][INFO] - Test, Round 277 : loss => 5.731463751530647,  accuracy: 0.3455
[2025-09-15 23:32:52,138][flp2p.graph_runner][INFO] - Train, Round 278 : loss => 0.001005674615395643,  accuracy: 1.0, gradient_norm : 0.05218021080208612
[2025-09-15 23:33:12,771][flp2p.graph_runner][INFO] - Test, Round 278 : loss => 5.734143622791767,  accuracy: 0.3452
[2025-09-15 23:34:01,374][flp2p.graph_runner][INFO] - Train, Round 279 : loss => 0.0009994700404301212,  accuracy: 1.0, gradient_norm : 0.05167879242677507
[2025-09-15 23:34:21,416][flp2p.graph_runner][INFO] - Test, Round 279 : loss => 5.738034154880046,  accuracy: 0.3447
[2025-09-15 23:35:09,109][flp2p.graph_runner][INFO] - Train, Round 280 : loss => 0.0009934239847401232,  accuracy: 1.0, gradient_norm : 0.05263149022287499
[2025-09-15 23:35:29,518][flp2p.graph_runner][INFO] - Test, Round 280 : loss => 5.742110032713414,  accuracy: 0.345
[2025-09-15 23:36:18,054][flp2p.graph_runner][INFO] - Train, Round 281 : loss => 0.0009872159179212755,  accuracy: 1.0, gradient_norm : 0.05202696048300066
[2025-09-15 23:36:38,256][flp2p.graph_runner][INFO] - Test, Round 281 : loss => 5.745847216916085,  accuracy: 0.345
[2025-09-15 23:37:26,499][flp2p.graph_runner][INFO] - Train, Round 282 : loss => 0.0009809474533297663,  accuracy: 1.0, gradient_norm : 0.05110089868877447
[2025-09-15 23:37:46,621][flp2p.graph_runner][INFO] - Test, Round 282 : loss => 5.749616357457638,  accuracy: 0.3447
[2025-09-15 23:38:34,176][flp2p.graph_runner][INFO] - Train, Round 283 : loss => 0.0009753001873226214,  accuracy: 1.0, gradient_norm : 0.05116866905174534
[2025-09-15 23:38:54,370][flp2p.graph_runner][INFO] - Test, Round 283 : loss => 5.7530832030415535,  accuracy: 0.3454
[2025-09-15 23:39:42,153][flp2p.graph_runner][INFO] - Train, Round 284 : loss => 0.0009689759530980762,  accuracy: 1.0, gradient_norm : 0.050627960328800715
[2025-09-15 23:40:02,186][flp2p.graph_runner][INFO] - Test, Round 284 : loss => 5.756297047305107,  accuracy: 0.3448
[2025-09-15 23:40:50,291][flp2p.graph_runner][INFO] - Train, Round 285 : loss => 0.0009633632572270774,  accuracy: 1.0, gradient_norm : 0.05080072132047151
[2025-09-15 23:41:10,338][flp2p.graph_runner][INFO] - Test, Round 285 : loss => 5.760019745779037,  accuracy: 0.3446
[2025-09-15 23:41:57,935][flp2p.graph_runner][INFO] - Train, Round 286 : loss => 0.0009576495745568538,  accuracy: 1.0, gradient_norm : 0.051863850904013464
[2025-09-15 23:42:18,057][flp2p.graph_runner][INFO] - Test, Round 286 : loss => 5.763563698935509,  accuracy: 0.3453
[2025-09-15 23:43:06,151][flp2p.graph_runner][INFO] - Train, Round 287 : loss => 0.0009521448808178924,  accuracy: 1.0, gradient_norm : 0.04855725730891456
[2025-09-15 23:43:26,613][flp2p.graph_runner][INFO] - Test, Round 287 : loss => 5.767051077473163,  accuracy: 0.3449
[2025-09-15 23:44:14,632][flp2p.graph_runner][INFO] - Train, Round 288 : loss => 0.000946323288274774,  accuracy: 1.0, gradient_norm : 0.04953732600526929
[2025-09-15 23:44:34,876][flp2p.graph_runner][INFO] - Test, Round 288 : loss => 5.771288563549518,  accuracy: 0.3452
[2025-09-15 23:45:23,923][flp2p.graph_runner][INFO] - Train, Round 289 : loss => 0.000940889945316788,  accuracy: 1.0, gradient_norm : 0.04710737255728374
[2025-09-15 23:45:44,910][flp2p.graph_runner][INFO] - Test, Round 289 : loss => 5.77289751200676,  accuracy: 0.3451
[2025-09-15 23:46:33,958][flp2p.graph_runner][INFO] - Train, Round 290 : loss => 0.0009353242923922759,  accuracy: 1.0, gradient_norm : 0.05032473238313921
[2025-09-15 23:46:54,444][flp2p.graph_runner][INFO] - Test, Round 290 : loss => 5.776858059608936,  accuracy: 0.3454
[2025-09-15 23:47:43,004][flp2p.graph_runner][INFO] - Train, Round 291 : loss => 0.0009300506083430566,  accuracy: 1.0, gradient_norm : 0.049263070296233835
[2025-09-15 23:48:03,370][flp2p.graph_runner][INFO] - Test, Round 291 : loss => 5.780860626733303,  accuracy: 0.345
[2025-09-15 23:48:51,853][flp2p.graph_runner][INFO] - Train, Round 292 : loss => 0.000924698021093112,  accuracy: 1.0, gradient_norm : 0.0491595003636215
[2025-09-15 23:49:12,564][flp2p.graph_runner][INFO] - Test, Round 292 : loss => 5.784083846819401,  accuracy: 0.3448
[2025-09-15 23:50:01,384][flp2p.graph_runner][INFO] - Train, Round 293 : loss => 0.0009194000365823744,  accuracy: 1.0, gradient_norm : 0.04925714537066494
[2025-09-15 23:50:21,789][flp2p.graph_runner][INFO] - Test, Round 293 : loss => 5.787507218921185,  accuracy: 0.345
[2025-09-15 23:51:10,114][flp2p.graph_runner][INFO] - Train, Round 294 : loss => 0.0009134588448796424,  accuracy: 1.0, gradient_norm : 0.04813340048749335
[2025-09-15 23:51:30,373][flp2p.graph_runner][INFO] - Test, Round 294 : loss => 5.790461597013474,  accuracy: 0.3449
[2025-09-15 23:52:18,932][flp2p.graph_runner][INFO] - Train, Round 295 : loss => 0.0009085927437990904,  accuracy: 1.0, gradient_norm : 0.04957213175506193
[2025-09-15 23:52:39,241][flp2p.graph_runner][INFO] - Test, Round 295 : loss => 5.794460730183125,  accuracy: 0.3449
[2025-09-15 23:53:27,282][flp2p.graph_runner][INFO] - Train, Round 296 : loss => 0.0009036358694963081,  accuracy: 1.0, gradient_norm : 0.047824602563271625
[2025-09-15 23:53:47,556][flp2p.graph_runner][INFO] - Test, Round 296 : loss => 5.797708368384838,  accuracy: 0.3452
[2025-09-15 23:54:35,603][flp2p.graph_runner][INFO] - Train, Round 297 : loss => 0.0008987704779428893,  accuracy: 1.0, gradient_norm : 0.0473575260263029
[2025-09-15 23:54:55,926][flp2p.graph_runner][INFO] - Test, Round 297 : loss => 5.8011272332310675,  accuracy: 0.3451
[2025-09-15 23:55:44,560][flp2p.graph_runner][INFO] - Train, Round 298 : loss => 0.0008935674082507221,  accuracy: 1.0, gradient_norm : 0.048819318724633784
[2025-09-15 23:56:04,897][flp2p.graph_runner][INFO] - Test, Round 298 : loss => 5.8036032304883,  accuracy: 0.3448
[2025-09-15 23:56:53,173][flp2p.graph_runner][INFO] - Train, Round 299 : loss => 0.0008885879056712536,  accuracy: 1.0, gradient_norm : 0.0471974548767282
[2025-09-15 23:57:13,531][flp2p.graph_runner][INFO] - Test, Round 299 : loss => 5.807495896601677,  accuracy: 0.3449
[2025-09-15 23:58:02,111][flp2p.graph_runner][INFO] - Train, Round 300 : loss => 0.0008836850272928131,  accuracy: 1.0, gradient_norm : 0.04492725895692769
[2025-09-15 23:58:22,138][flp2p.graph_runner][INFO] - Test, Round 300 : loss => 5.810431257915496,  accuracy: 0.3449
[2025-09-15 23:59:08,994][flp2p.graph_runner][INFO] - Train, Round 301 : loss => 0.0008789957338982883,  accuracy: 1.0, gradient_norm : 0.04713210715910531
[2025-09-15 23:59:29,248][flp2p.graph_runner][INFO] - Test, Round 301 : loss => 5.8139114035844806,  accuracy: 0.3451
[2025-09-16 00:00:17,749][flp2p.graph_runner][INFO] - Train, Round 302 : loss => 0.0008738673154827361,  accuracy: 1.0, gradient_norm : 0.04540218741304924
[2025-09-16 00:00:38,309][flp2p.graph_runner][INFO] - Test, Round 302 : loss => 5.816577828502655,  accuracy: 0.3455
[2025-09-16 00:01:27,136][flp2p.graph_runner][INFO] - Train, Round 303 : loss => 0.0008691239130227281,  accuracy: 1.0, gradient_norm : 0.045117704164516714
[2025-09-16 00:01:47,441][flp2p.graph_runner][INFO] - Test, Round 303 : loss => 5.8203552637457845,  accuracy: 0.345
[2025-09-16 00:02:36,390][flp2p.graph_runner][INFO] - Train, Round 304 : loss => 0.0008647418574764741,  accuracy: 1.0, gradient_norm : 0.04523889886005078
[2025-09-16 00:02:56,729][flp2p.graph_runner][INFO] - Test, Round 304 : loss => 5.82348636726141,  accuracy: 0.3448
[2025-09-16 00:03:45,410][flp2p.graph_runner][INFO] - Train, Round 305 : loss => 0.0008600498158436191,  accuracy: 1.0, gradient_norm : 0.04572295781522426
[2025-09-16 00:04:06,006][flp2p.graph_runner][INFO] - Test, Round 305 : loss => 5.826739559054375,  accuracy: 0.3452
[2025-09-16 00:04:54,989][flp2p.graph_runner][INFO] - Train, Round 306 : loss => 0.0008551456597342618,  accuracy: 1.0, gradient_norm : 0.048351528046426256
[2025-09-16 00:05:15,509][flp2p.graph_runner][INFO] - Test, Round 306 : loss => 5.830319867944717,  accuracy: 0.345
[2025-09-16 00:06:04,206][flp2p.graph_runner][INFO] - Train, Round 307 : loss => 0.0008507020719116547,  accuracy: 1.0, gradient_norm : 0.04617222018848736
[2025-09-16 00:06:25,085][flp2p.graph_runner][INFO] - Test, Round 307 : loss => 5.832710803687572,  accuracy: 0.3453
[2025-09-16 00:07:14,124][flp2p.graph_runner][INFO] - Train, Round 308 : loss => 0.0008463864428388962,  accuracy: 1.0, gradient_norm : 0.047497611497177045
[2025-09-16 00:07:34,390][flp2p.graph_runner][INFO] - Test, Round 308 : loss => 5.836668325269223,  accuracy: 0.3449
[2025-09-16 00:08:22,235][flp2p.graph_runner][INFO] - Train, Round 309 : loss => 0.000841685989944381,  accuracy: 1.0, gradient_norm : 0.04307329525364367
[2025-09-16 00:08:42,329][flp2p.graph_runner][INFO] - Test, Round 309 : loss => 5.840235925185681,  accuracy: 0.3452
[2025-09-16 00:09:30,074][flp2p.graph_runner][INFO] - Train, Round 310 : loss => 0.0008372934365130885,  accuracy: 1.0, gradient_norm : 0.04530237284173833
[2025-09-16 00:09:50,219][flp2p.graph_runner][INFO] - Test, Round 310 : loss => 5.842433980858326,  accuracy: 0.3448
[2025-09-16 00:10:38,758][flp2p.graph_runner][INFO] - Train, Round 311 : loss => 0.0008326609256134062,  accuracy: 1.0, gradient_norm : 0.04491180739638177
[2025-09-16 00:10:59,123][flp2p.graph_runner][INFO] - Test, Round 311 : loss => 5.8460073057293895,  accuracy: 0.3451
[2025-09-16 00:11:48,165][flp2p.graph_runner][INFO] - Train, Round 312 : loss => 0.000828743230801289,  accuracy: 1.0, gradient_norm : 0.04440777979031319
[2025-09-16 00:12:08,554][flp2p.graph_runner][INFO] - Test, Round 312 : loss => 5.84923669192791,  accuracy: 0.3451
[2025-09-16 00:12:57,465][flp2p.graph_runner][INFO] - Train, Round 313 : loss => 0.0008241444788291117,  accuracy: 1.0, gradient_norm : 0.04361462603378825
[2025-09-16 00:13:17,972][flp2p.graph_runner][INFO] - Test, Round 313 : loss => 5.851739945065975,  accuracy: 0.345
[2025-09-16 00:14:06,492][flp2p.graph_runner][INFO] - Train, Round 314 : loss => 0.0008199143730356202,  accuracy: 1.0, gradient_norm : 0.043642999025033935
[2025-09-16 00:14:26,622][flp2p.graph_runner][INFO] - Test, Round 314 : loss => 5.855378684020042,  accuracy: 0.345
[2025-09-16 00:15:15,372][flp2p.graph_runner][INFO] - Train, Round 315 : loss => 0.00081556860407242,  accuracy: 1.0, gradient_norm : 0.043299107383490194
[2025-09-16 00:15:35,618][flp2p.graph_runner][INFO] - Test, Round 315 : loss => 5.85840242023468,  accuracy: 0.3452
[2025-09-16 00:16:23,927][flp2p.graph_runner][INFO] - Train, Round 316 : loss => 0.0008117530715511141,  accuracy: 1.0, gradient_norm : 0.04357681351338296
[2025-09-16 00:16:44,122][flp2p.graph_runner][INFO] - Test, Round 316 : loss => 5.861730952239037,  accuracy: 0.3449
[2025-09-16 00:17:32,961][flp2p.graph_runner][INFO] - Train, Round 317 : loss => 0.0008072208900436333,  accuracy: 1.0, gradient_norm : 0.04355406514108518
[2025-09-16 00:17:53,236][flp2p.graph_runner][INFO] - Test, Round 317 : loss => 5.8645325530409815,  accuracy: 0.3452
[2025-09-16 00:18:42,317][flp2p.graph_runner][INFO] - Train, Round 318 : loss => 0.0008030280133607451,  accuracy: 1.0, gradient_norm : 0.04222464450358663
[2025-09-16 00:19:02,594][flp2p.graph_runner][INFO] - Test, Round 318 : loss => 5.8670304515004155,  accuracy: 0.3449
[2025-09-16 00:19:51,722][flp2p.graph_runner][INFO] - Train, Round 319 : loss => 0.0007993502050400518,  accuracy: 1.0, gradient_norm : 0.04386362026074898
[2025-09-16 00:20:11,840][flp2p.graph_runner][INFO] - Test, Round 319 : loss => 5.869653346860408,  accuracy: 0.3449
[2025-09-16 00:21:00,640][flp2p.graph_runner][INFO] - Train, Round 320 : loss => 0.0007948644245698236,  accuracy: 1.0, gradient_norm : 0.042342506572694776
[2025-09-16 00:21:20,597][flp2p.graph_runner][INFO] - Test, Round 320 : loss => 5.8730653681755065,  accuracy: 0.345
[2025-09-16 00:22:08,661][flp2p.graph_runner][INFO] - Train, Round 321 : loss => 0.0007915518921557428,  accuracy: 1.0, gradient_norm : 0.04429755044690431
[2025-09-16 00:22:28,818][flp2p.graph_runner][INFO] - Test, Round 321 : loss => 5.876231135070324,  accuracy: 0.345
[2025-09-16 00:23:17,149][flp2p.graph_runner][INFO] - Train, Round 322 : loss => 0.0007873772450814916,  accuracy: 1.0, gradient_norm : 0.0423730306217516
[2025-09-16 00:23:37,484][flp2p.graph_runner][INFO] - Test, Round 322 : loss => 5.879116036748886,  accuracy: 0.3452
[2025-09-16 00:24:25,660][flp2p.graph_runner][INFO] - Train, Round 323 : loss => 0.0007834924308190236,  accuracy: 1.0, gradient_norm : 0.04166386017793514
[2025-09-16 00:24:45,981][flp2p.graph_runner][INFO] - Test, Round 323 : loss => 5.881693117403984,  accuracy: 0.3446
[2025-09-16 00:25:34,233][flp2p.graph_runner][INFO] - Train, Round 324 : loss => 0.0007794680691464842,  accuracy: 1.0, gradient_norm : 0.042603641110281656
[2025-09-16 00:25:54,382][flp2p.graph_runner][INFO] - Test, Round 324 : loss => 5.884674751412868,  accuracy: 0.3448
[2025-09-16 00:26:42,467][flp2p.graph_runner][INFO] - Train, Round 325 : loss => 0.0007760364472051154,  accuracy: 1.0, gradient_norm : 0.04161965152247623
[2025-09-16 00:27:02,891][flp2p.graph_runner][INFO] - Test, Round 325 : loss => 5.887426699185371,  accuracy: 0.3449
[2025-09-16 00:27:51,120][flp2p.graph_runner][INFO] - Train, Round 326 : loss => 0.0007722173158375273,  accuracy: 1.0, gradient_norm : 0.04276287670446531
[2025-09-16 00:28:11,525][flp2p.graph_runner][INFO] - Test, Round 326 : loss => 5.8903168954610825,  accuracy: 0.3445
[2025-09-16 00:28:59,504][flp2p.graph_runner][INFO] - Train, Round 327 : loss => 0.0007684117665485249,  accuracy: 1.0, gradient_norm : 0.041649229645340825
[2025-09-16 00:29:19,998][flp2p.graph_runner][INFO] - Test, Round 327 : loss => 5.892743925261498,  accuracy: 0.3449
[2025-09-16 00:30:08,970][flp2p.graph_runner][INFO] - Train, Round 328 : loss => 0.0007646999354741035,  accuracy: 1.0, gradient_norm : 0.04060706561424038
[2025-09-16 00:30:29,322][flp2p.graph_runner][INFO] - Test, Round 328 : loss => 5.896117247986793,  accuracy: 0.3449
[2025-09-16 00:31:17,418][flp2p.graph_runner][INFO] - Train, Round 329 : loss => 0.0007612413799264081,  accuracy: 1.0, gradient_norm : 0.03981437397093791
[2025-09-16 00:31:37,847][flp2p.graph_runner][INFO] - Test, Round 329 : loss => 5.8990871893882755,  accuracy: 0.3451
[2025-09-16 00:32:26,598][flp2p.graph_runner][INFO] - Train, Round 330 : loss => 0.0007573471410311562,  accuracy: 1.0, gradient_norm : 0.04084523654429288
[2025-09-16 00:32:47,208][flp2p.graph_runner][INFO] - Test, Round 330 : loss => 5.9018623054504395,  accuracy: 0.3448
[2025-09-16 00:33:35,594][flp2p.graph_runner][INFO] - Train, Round 331 : loss => 0.0007535188797313218,  accuracy: 1.0, gradient_norm : 0.040358745262445934
[2025-09-16 00:33:56,133][flp2p.graph_runner][INFO] - Test, Round 331 : loss => 5.90448647018671,  accuracy: 0.3451
[2025-09-16 00:34:44,782][flp2p.graph_runner][INFO] - Train, Round 332 : loss => 0.0007498228754290418,  accuracy: 1.0, gradient_norm : 0.039135788454964356
[2025-09-16 00:35:04,995][flp2p.graph_runner][INFO] - Test, Round 332 : loss => 5.906296679890156,  accuracy: 0.3454
[2025-09-16 00:35:53,257][flp2p.graph_runner][INFO] - Train, Round 333 : loss => 0.0007467015058379425,  accuracy: 1.0, gradient_norm : 0.04082604523798969
[2025-09-16 00:36:13,353][flp2p.graph_runner][INFO] - Test, Round 333 : loss => 5.909829991221428,  accuracy: 0.3448
[2025-09-16 00:37:01,813][flp2p.graph_runner][INFO] - Train, Round 334 : loss => 0.0007430172426757056,  accuracy: 1.0, gradient_norm : 0.04060451201068625
[2025-09-16 00:37:22,545][flp2p.graph_runner][INFO] - Test, Round 334 : loss => 5.9124190494775775,  accuracy: 0.345
[2025-09-16 00:38:10,891][flp2p.graph_runner][INFO] - Train, Round 335 : loss => 0.000740118942873475,  accuracy: 1.0, gradient_norm : 0.04108040282967184
[2025-09-16 00:38:31,100][flp2p.graph_runner][INFO] - Test, Round 335 : loss => 5.915619900035858,  accuracy: 0.3451
[2025-09-16 00:39:19,981][flp2p.graph_runner][INFO] - Train, Round 336 : loss => 0.0007362728472192732,  accuracy: 1.0, gradient_norm : 0.04075206830108771
[2025-09-16 00:39:40,145][flp2p.graph_runner][INFO] - Test, Round 336 : loss => 5.917192075157166,  accuracy: 0.3446
[2025-09-16 00:40:27,629][flp2p.graph_runner][INFO] - Train, Round 337 : loss => 0.0007327424985608859,  accuracy: 1.0, gradient_norm : 0.03991689606569284
[2025-09-16 00:40:48,009][flp2p.graph_runner][INFO] - Test, Round 337 : loss => 5.920944241547584,  accuracy: 0.3447
[2025-09-16 00:41:36,950][flp2p.graph_runner][INFO] - Train, Round 338 : loss => 0.0007293761539282663,  accuracy: 1.0, gradient_norm : 0.03905633887049893
[2025-09-16 00:41:57,366][flp2p.graph_runner][INFO] - Test, Round 338 : loss => 5.9233943234443664,  accuracy: 0.345
[2025-09-16 00:42:46,318][flp2p.graph_runner][INFO] - Train, Round 339 : loss => 0.0007262625036310053,  accuracy: 1.0, gradient_norm : 0.03813850763990254
[2025-09-16 00:43:06,835][flp2p.graph_runner][INFO] - Test, Round 339 : loss => 5.926047440636158,  accuracy: 0.3453
[2025-09-16 00:43:55,408][flp2p.graph_runner][INFO] - Train, Round 340 : loss => 0.0007227505398986976,  accuracy: 1.0, gradient_norm : 0.038773807990670955
[2025-09-16 00:44:15,729][flp2p.graph_runner][INFO] - Test, Round 340 : loss => 5.928684281086921,  accuracy: 0.3449
[2025-09-16 00:45:03,816][flp2p.graph_runner][INFO] - Train, Round 341 : loss => 0.0007194312676074333,  accuracy: 1.0, gradient_norm : 0.03721052335533807
[2025-09-16 00:45:24,199][flp2p.graph_runner][INFO] - Test, Round 341 : loss => 5.931399039661884,  accuracy: 0.3448
[2025-09-16 00:46:13,455][flp2p.graph_runner][INFO] - Train, Round 342 : loss => 0.0007161383532487284,  accuracy: 1.0, gradient_norm : 0.03975942594224736
[2025-09-16 00:46:34,200][flp2p.graph_runner][INFO] - Test, Round 342 : loss => 5.933646869492531,  accuracy: 0.3448
[2025-09-16 00:47:22,956][flp2p.graph_runner][INFO] - Train, Round 343 : loss => 0.0007134743540518684,  accuracy: 1.0, gradient_norm : 0.038997307423925195
[2025-09-16 00:47:43,400][flp2p.graph_runner][INFO] - Test, Round 343 : loss => 5.936240290737152,  accuracy: 0.345
[2025-09-16 00:48:32,212][flp2p.graph_runner][INFO] - Train, Round 344 : loss => 0.000709912927886762,  accuracy: 1.0, gradient_norm : 0.03821964085844282
[2025-09-16 00:48:52,565][flp2p.graph_runner][INFO] - Test, Round 344 : loss => 5.939634398281575,  accuracy: 0.3448
[2025-09-16 00:49:41,412][flp2p.graph_runner][INFO] - Train, Round 345 : loss => 0.0007070145228135517,  accuracy: 1.0, gradient_norm : 0.0384814240905032
[2025-09-16 00:50:01,051][flp2p.graph_runner][INFO] - Test, Round 345 : loss => 5.942163925683499,  accuracy: 0.3447
[2025-09-16 00:50:48,371][flp2p.graph_runner][INFO] - Train, Round 346 : loss => 0.0007037625251541614,  accuracy: 1.0, gradient_norm : 0.038980418770134403
[2025-09-16 00:51:08,217][flp2p.graph_runner][INFO] - Test, Round 346 : loss => 5.944488850700855,  accuracy: 0.345
[2025-09-16 00:51:56,380][flp2p.graph_runner][INFO] - Train, Round 347 : loss => 0.0007001315953675654,  accuracy: 1.0, gradient_norm : 0.0390352907165319
[2025-09-16 00:52:16,367][flp2p.graph_runner][INFO] - Test, Round 347 : loss => 5.9466570236563685,  accuracy: 0.3448
[2025-09-16 00:53:04,656][flp2p.graph_runner][INFO] - Train, Round 348 : loss => 0.0006973972356354352,  accuracy: 1.0, gradient_norm : 0.03790954832005773
[2025-09-16 00:53:25,036][flp2p.graph_runner][INFO] - Test, Round 348 : loss => 5.9496010887026785,  accuracy: 0.3451
[2025-09-16 00:54:13,319][flp2p.graph_runner][INFO] - Train, Round 349 : loss => 0.0006945584720839787,  accuracy: 1.0, gradient_norm : 0.0385424560132628
[2025-09-16 00:54:33,787][flp2p.graph_runner][INFO] - Test, Round 349 : loss => 5.952636531305313,  accuracy: 0.345
[2025-09-16 00:55:21,495][flp2p.graph_runner][INFO] - Train, Round 350 : loss => 0.0006913475526259084,  accuracy: 1.0, gradient_norm : 0.037919873825006015
[2025-09-16 00:55:41,853][flp2p.graph_runner][INFO] - Test, Round 350 : loss => 5.954583663308621,  accuracy: 0.3442
[2025-09-16 00:56:30,458][flp2p.graph_runner][INFO] - Train, Round 351 : loss => 0.000688626017035858,  accuracy: 1.0, gradient_norm : 0.0389021190288608
[2025-09-16 00:56:50,864][flp2p.graph_runner][INFO] - Test, Round 351 : loss => 5.956922267782688,  accuracy: 0.3449
[2025-09-16 00:57:39,670][flp2p.graph_runner][INFO] - Train, Round 352 : loss => 0.0006854327444549806,  accuracy: 1.0, gradient_norm : 0.040130595992430206
[2025-09-16 00:57:59,730][flp2p.graph_runner][INFO] - Test, Round 352 : loss => 5.959306161642075,  accuracy: 0.3449
[2025-09-16 00:58:47,996][flp2p.graph_runner][INFO] - Train, Round 353 : loss => 0.0006824332583588935,  accuracy: 1.0, gradient_norm : 0.03751487048010627
[2025-09-16 00:59:07,888][flp2p.graph_runner][INFO] - Test, Round 353 : loss => 5.962351773750782,  accuracy: 0.3451
[2025-09-16 00:59:56,164][flp2p.graph_runner][INFO] - Train, Round 354 : loss => 0.000679591264136737,  accuracy: 1.0, gradient_norm : 0.03787496741987042
[2025-09-16 01:00:16,633][flp2p.graph_runner][INFO] - Test, Round 354 : loss => 5.964744368827343,  accuracy: 0.3447
[2025-09-16 01:01:05,715][flp2p.graph_runner][INFO] - Train, Round 355 : loss => 0.0006765992524257548,  accuracy: 1.0, gradient_norm : 0.03723688672471961
[2025-09-16 01:01:26,423][flp2p.graph_runner][INFO] - Test, Round 355 : loss => 5.967116460883617,  accuracy: 0.3445
[2025-09-16 01:02:15,188][flp2p.graph_runner][INFO] - Train, Round 356 : loss => 0.000673553642227489,  accuracy: 1.0, gradient_norm : 0.03688544735219268
[2025-09-16 01:02:35,314][flp2p.graph_runner][INFO] - Test, Round 356 : loss => 5.970009799194336,  accuracy: 0.3447
[2025-09-16 01:03:23,698][flp2p.graph_runner][INFO] - Train, Round 357 : loss => 0.0006704996787690713,  accuracy: 1.0, gradient_norm : 0.03641806653996541
[2025-09-16 01:03:43,925][flp2p.graph_runner][INFO] - Test, Round 357 : loss => 5.97210037740469,  accuracy: 0.3447
[2025-09-16 01:04:31,972][flp2p.graph_runner][INFO] - Train, Round 358 : loss => 0.0006680996927267799,  accuracy: 1.0, gradient_norm : 0.03682080159768977
[2025-09-16 01:04:52,187][flp2p.graph_runner][INFO] - Test, Round 358 : loss => 5.974355589854717,  accuracy: 0.345
[2025-09-16 01:05:40,766][flp2p.graph_runner][INFO] - Train, Round 359 : loss => 0.0006652489082383305,  accuracy: 1.0, gradient_norm : 0.03565394475045331
[2025-09-16 01:06:01,077][flp2p.graph_runner][INFO] - Test, Round 359 : loss => 5.976922692596912,  accuracy: 0.3448
[2025-09-16 01:06:49,657][flp2p.graph_runner][INFO] - Train, Round 360 : loss => 0.0006621158400715406,  accuracy: 1.0, gradient_norm : 0.035898207331327134
[2025-09-16 01:07:09,614][flp2p.graph_runner][INFO] - Test, Round 360 : loss => 5.979927298676968,  accuracy: 0.3447
[2025-09-16 01:07:56,882][flp2p.graph_runner][INFO] - Train, Round 361 : loss => 0.0006595565663762195,  accuracy: 1.0, gradient_norm : 0.03566613220972737
[2025-09-16 01:08:17,393][flp2p.graph_runner][INFO] - Test, Round 361 : loss => 5.982136883580685,  accuracy: 0.3447
[2025-09-16 01:09:05,649][flp2p.graph_runner][INFO] - Train, Round 362 : loss => 0.0006570374218305609,  accuracy: 1.0, gradient_norm : 0.03662625746162457
[2025-09-16 01:09:25,957][flp2p.graph_runner][INFO] - Test, Round 362 : loss => 5.984682230508327,  accuracy: 0.3445
[2025-09-16 01:10:14,537][flp2p.graph_runner][INFO] - Train, Round 363 : loss => 0.0006540825562478858,  accuracy: 1.0, gradient_norm : 0.03694168409664434
[2025-09-16 01:10:34,902][flp2p.graph_runner][INFO] - Test, Round 363 : loss => 5.9866085838079455,  accuracy: 0.3446
[2025-09-16 01:11:23,401][flp2p.graph_runner][INFO] - Train, Round 364 : loss => 0.0006516528308323662,  accuracy: 1.0, gradient_norm : 0.03727714637552259
[2025-09-16 01:11:43,720][flp2p.graph_runner][INFO] - Test, Round 364 : loss => 5.98964606897831,  accuracy: 0.3449
[2025-09-16 01:12:32,386][flp2p.graph_runner][INFO] - Train, Round 365 : loss => 0.0006489030117639537,  accuracy: 1.0, gradient_norm : 0.036075609287803345
[2025-09-16 01:12:52,385][flp2p.graph_runner][INFO] - Test, Round 365 : loss => 5.991495959150791,  accuracy: 0.3442
[2025-09-16 01:13:40,053][flp2p.graph_runner][INFO] - Train, Round 366 : loss => 0.000646283433792026,  accuracy: 1.0, gradient_norm : 0.03677220087591651
[2025-09-16 01:14:00,323][flp2p.graph_runner][INFO] - Test, Round 366 : loss => 5.99413099796772,  accuracy: 0.3448
[2025-09-16 01:14:48,764][flp2p.graph_runner][INFO] - Train, Round 367 : loss => 0.0006434604542300806,  accuracy: 1.0, gradient_norm : 0.034785945077162095
[2025-09-16 01:15:09,035][flp2p.graph_runner][INFO] - Test, Round 367 : loss => 5.99650312063694,  accuracy: 0.3445
[2025-09-16 01:15:58,085][flp2p.graph_runner][INFO] - Train, Round 368 : loss => 0.0006411421736629563,  accuracy: 1.0, gradient_norm : 0.03485584218225826
[2025-09-16 01:16:18,109][flp2p.graph_runner][INFO] - Test, Round 368 : loss => 5.999003909361362,  accuracy: 0.3447
[2025-09-16 01:17:05,938][flp2p.graph_runner][INFO] - Train, Round 369 : loss => 0.0006382481259061023,  accuracy: 1.0, gradient_norm : 0.03450679773440427
[2025-09-16 01:17:26,344][flp2p.graph_runner][INFO] - Test, Round 369 : loss => 6.001071754539013,  accuracy: 0.3444
[2025-09-16 01:18:15,091][flp2p.graph_runner][INFO] - Train, Round 370 : loss => 0.0006357381798443386,  accuracy: 1.0, gradient_norm : 0.034892791016969425
[2025-09-16 01:18:35,488][flp2p.graph_runner][INFO] - Test, Round 370 : loss => 6.003425981354713,  accuracy: 0.3444
[2025-09-16 01:19:24,083][flp2p.graph_runner][INFO] - Train, Round 371 : loss => 0.0006331010862777475,  accuracy: 1.0, gradient_norm : 0.035162474578337315
[2025-09-16 01:19:44,747][flp2p.graph_runner][INFO] - Test, Round 371 : loss => 6.005767171859741,  accuracy: 0.3446
[2025-09-16 01:20:34,258][flp2p.graph_runner][INFO] - Train, Round 372 : loss => 0.0006306077505238741,  accuracy: 1.0, gradient_norm : 0.03485441189803264
[2025-09-16 01:20:54,784][flp2p.graph_runner][INFO] - Test, Round 372 : loss => 6.007975206327439,  accuracy: 0.3451
[2025-09-16 01:21:43,795][flp2p.graph_runner][INFO] - Train, Round 373 : loss => 0.0006282320974059983,  accuracy: 1.0, gradient_norm : 0.03514081784906765
[2025-09-16 01:22:04,237][flp2p.graph_runner][INFO] - Test, Round 373 : loss => 6.010149810969829,  accuracy: 0.3444
[2025-09-16 01:22:52,255][flp2p.graph_runner][INFO] - Train, Round 374 : loss => 0.0006259514961978615,  accuracy: 1.0, gradient_norm : 0.035048009328044326
[2025-09-16 01:23:12,584][flp2p.graph_runner][INFO] - Test, Round 374 : loss => 6.012759679722786,  accuracy: 0.3442
[2025-09-16 01:24:00,752][flp2p.graph_runner][INFO] - Train, Round 375 : loss => 0.0006232937708288471,  accuracy: 1.0, gradient_norm : 0.034095053646086476
[2025-09-16 01:24:20,529][flp2p.graph_runner][INFO] - Test, Round 375 : loss => 6.0154265138268475,  accuracy: 0.3445
[2025-09-16 01:25:08,294][flp2p.graph_runner][INFO] - Train, Round 376 : loss => 0.0006206333763960478,  accuracy: 1.0, gradient_norm : 0.03511904373909702
[2025-09-16 01:25:28,438][flp2p.graph_runner][INFO] - Test, Round 376 : loss => 6.017076001572609,  accuracy: 0.3442
[2025-09-16 01:26:16,810][flp2p.graph_runner][INFO] - Train, Round 377 : loss => 0.0006183873074284443,  accuracy: 1.0, gradient_norm : 0.03259515111525122
[2025-09-16 01:26:37,150][flp2p.graph_runner][INFO] - Test, Round 377 : loss => 6.019246303701401,  accuracy: 0.3445
[2025-09-16 01:27:25,919][flp2p.graph_runner][INFO] - Train, Round 378 : loss => 0.0006157433100937245,  accuracy: 1.0, gradient_norm : 0.03364463634630022
[2025-09-16 01:27:46,119][flp2p.graph_runner][INFO] - Test, Round 378 : loss => 6.022006939649582,  accuracy: 0.3449
[2025-09-16 01:28:35,268][flp2p.graph_runner][INFO] - Train, Round 379 : loss => 0.0006134066424419868,  accuracy: 1.0, gradient_norm : 0.03494777580990278
[2025-09-16 01:28:55,442][flp2p.graph_runner][INFO] - Test, Round 379 : loss => 6.02432296500206,  accuracy: 0.3446
[2025-09-16 01:29:43,932][flp2p.graph_runner][INFO] - Train, Round 380 : loss => 0.0006113086939997934,  accuracy: 1.0, gradient_norm : 0.03462875535071575
[2025-09-16 01:30:04,265][flp2p.graph_runner][INFO] - Test, Round 380 : loss => 6.025924233067036,  accuracy: 0.3446
[2025-09-16 01:30:53,241][flp2p.graph_runner][INFO] - Train, Round 381 : loss => 0.0006088010897534938,  accuracy: 1.0, gradient_norm : 0.03408484486110304
[2025-09-16 01:31:13,164][flp2p.graph_runner][INFO] - Test, Round 381 : loss => 6.02905683529377,  accuracy: 0.3447
[2025-09-16 01:32:01,136][flp2p.graph_runner][INFO] - Train, Round 382 : loss => 0.0006065643687300812,  accuracy: 1.0, gradient_norm : 0.03369310703746227
[2025-09-16 01:32:21,789][flp2p.graph_runner][INFO] - Test, Round 382 : loss => 6.031189264237881,  accuracy: 0.3445
[2025-09-16 01:33:10,167][flp2p.graph_runner][INFO] - Train, Round 383 : loss => 0.0006042387855874648,  accuracy: 1.0, gradient_norm : 0.03401310754546329
[2025-09-16 01:33:30,421][flp2p.graph_runner][INFO] - Test, Round 383 : loss => 6.033101710021496,  accuracy: 0.3444
[2025-09-16 01:34:19,387][flp2p.graph_runner][INFO] - Train, Round 384 : loss => 0.000601857836675966,  accuracy: 1.0, gradient_norm : 0.03367995678336295
[2025-09-16 01:34:39,800][flp2p.graph_runner][INFO] - Test, Round 384 : loss => 6.035506998181343,  accuracy: 0.3444
[2025-09-16 01:35:28,474][flp2p.graph_runner][INFO] - Train, Round 385 : loss => 0.000599812348179209,  accuracy: 1.0, gradient_norm : 0.033472379706672425
[2025-09-16 01:35:48,218][flp2p.graph_runner][INFO] - Test, Round 385 : loss => 6.037300649690628,  accuracy: 0.3442
[2025-09-16 01:36:35,712][flp2p.graph_runner][INFO] - Train, Round 386 : loss => 0.0005975831869970231,  accuracy: 1.0, gradient_norm : 0.03349356154339792
[2025-09-16 01:36:55,730][flp2p.graph_runner][INFO] - Test, Round 386 : loss => 6.039776848769188,  accuracy: 0.3447
[2025-09-16 01:37:44,701][flp2p.graph_runner][INFO] - Train, Round 387 : loss => 0.0005952621497393314,  accuracy: 1.0, gradient_norm : 0.034266372084783746
[2025-09-16 01:38:05,173][flp2p.graph_runner][INFO] - Test, Round 387 : loss => 6.041810965931416,  accuracy: 0.3446
[2025-09-16 01:38:53,890][flp2p.graph_runner][INFO] - Train, Round 388 : loss => 0.0005928399640712693,  accuracy: 1.0, gradient_norm : 0.0337553548971541
[2025-09-16 01:39:13,837][flp2p.graph_runner][INFO] - Test, Round 388 : loss => 6.043867478454113,  accuracy: 0.3448
[2025-09-16 01:40:01,612][flp2p.graph_runner][INFO] - Train, Round 389 : loss => 0.0005907713613123632,  accuracy: 1.0, gradient_norm : 0.03243881043599016
[2025-09-16 01:40:21,512][flp2p.graph_runner][INFO] - Test, Round 389 : loss => 6.046281107532978,  accuracy: 0.3444
[2025-09-16 01:41:09,952][flp2p.graph_runner][INFO] - Train, Round 390 : loss => 0.0005885143395668515,  accuracy: 1.0, gradient_norm : 0.03292001295134753
[2025-09-16 01:41:29,967][flp2p.graph_runner][INFO] - Test, Round 390 : loss => 6.048699105989933,  accuracy: 0.3445
[2025-09-16 01:42:18,133][flp2p.graph_runner][INFO] - Train, Round 391 : loss => 0.0005865040246135322,  accuracy: 1.0, gradient_norm : 0.03340576851824556
[2025-09-16 01:42:38,608][flp2p.graph_runner][INFO] - Test, Round 391 : loss => 6.050267285811901,  accuracy: 0.3442
[2025-09-16 01:43:26,571][flp2p.graph_runner][INFO] - Train, Round 392 : loss => 0.0005841918706209981,  accuracy: 1.0, gradient_norm : 0.03162308830323294
[2025-09-16 01:43:46,380][flp2p.graph_runner][INFO] - Test, Round 392 : loss => 6.05280877392292,  accuracy: 0.3441
[2025-09-16 01:44:34,918][flp2p.graph_runner][INFO] - Train, Round 393 : loss => 0.0005820336224375447,  accuracy: 1.0, gradient_norm : 0.031591096917342104
[2025-09-16 01:44:55,216][flp2p.graph_runner][INFO] - Test, Round 393 : loss => 6.054351704514026,  accuracy: 0.3443
[2025-09-16 01:45:43,351][flp2p.graph_runner][INFO] - Train, Round 394 : loss => 0.0005801568046808824,  accuracy: 1.0, gradient_norm : 0.03278951953733945
[2025-09-16 01:46:03,579][flp2p.graph_runner][INFO] - Test, Round 394 : loss => 6.056971183955669,  accuracy: 0.344
[2025-09-16 01:46:51,359][flp2p.graph_runner][INFO] - Train, Round 395 : loss => 0.0005779226517552161,  accuracy: 1.0, gradient_norm : 0.03234413475395081
[2025-09-16 01:47:11,134][flp2p.graph_runner][INFO] - Test, Round 395 : loss => 6.0590662644147875,  accuracy: 0.3445
[2025-09-16 01:47:59,565][flp2p.graph_runner][INFO] - Train, Round 396 : loss => 0.0005759171419898244,  accuracy: 1.0, gradient_norm : 0.033109187795171906
[2025-09-16 01:48:19,756][flp2p.graph_runner][INFO] - Test, Round 396 : loss => 6.0606292148709295,  accuracy: 0.3445
[2025-09-16 01:49:08,148][flp2p.graph_runner][INFO] - Train, Round 397 : loss => 0.0005736943755618995,  accuracy: 1.0, gradient_norm : 0.029963809182841097
[2025-09-16 01:49:28,108][flp2p.graph_runner][INFO] - Test, Round 397 : loss => 6.063082318639755,  accuracy: 0.3447
[2025-09-16 01:50:16,101][flp2p.graph_runner][INFO] - Train, Round 398 : loss => 0.0005718377330049408,  accuracy: 1.0, gradient_norm : 0.03251985963258321
[2025-09-16 01:50:36,853][flp2p.graph_runner][INFO] - Test, Round 398 : loss => 6.064775937211514,  accuracy: 0.3445
[2025-09-16 01:51:25,529][flp2p.graph_runner][INFO] - Train, Round 399 : loss => 0.0005694653904599061,  accuracy: 1.0, gradient_norm : 0.03142867282052064
[2025-09-16 01:51:45,607][flp2p.graph_runner][INFO] - Test, Round 399 : loss => 6.067291239130497,  accuracy: 0.345
[2025-09-16 01:52:34,282][flp2p.graph_runner][INFO] - Train, Round 400 : loss => 0.0005674963177443714,  accuracy: 1.0, gradient_norm : 0.03150837336238516
[2025-09-16 01:52:54,188][flp2p.graph_runner][INFO] - Test, Round 400 : loss => 6.069150628292561,  accuracy: 0.3447
[2025-09-16 01:53:41,998][flp2p.graph_runner][INFO] - Train, Round 401 : loss => 0.0005655100885269349,  accuracy: 1.0, gradient_norm : 0.030835152725209272
[2025-09-16 01:54:01,951][flp2p.graph_runner][INFO] - Test, Round 401 : loss => 6.071565180778504,  accuracy: 0.3442
[2025-09-16 01:54:49,552][flp2p.graph_runner][INFO] - Train, Round 402 : loss => 0.000563600729353008,  accuracy: 1.0, gradient_norm : 0.032009223534559685
[2025-09-16 01:55:09,669][flp2p.graph_runner][INFO] - Test, Round 402 : loss => 6.0727694331407545,  accuracy: 0.3445
[2025-09-16 01:55:57,911][flp2p.graph_runner][INFO] - Train, Round 403 : loss => 0.0005616770404473454,  accuracy: 1.0, gradient_norm : 0.03150939784804007
[2025-09-16 01:56:18,255][flp2p.graph_runner][INFO] - Test, Round 403 : loss => 6.074983909153938,  accuracy: 0.3444
[2025-09-16 01:57:06,044][flp2p.graph_runner][INFO] - Train, Round 404 : loss => 0.0005598581649974221,  accuracy: 1.0, gradient_norm : 0.03203967292007695
[2025-09-16 01:57:26,139][flp2p.graph_runner][INFO] - Test, Round 404 : loss => 6.077613703465461,  accuracy: 0.3444
[2025-09-16 01:58:14,215][flp2p.graph_runner][INFO] - Train, Round 405 : loss => 0.0005577651590889825,  accuracy: 1.0, gradient_norm : 0.030536054965614737
[2025-09-16 01:58:34,418][flp2p.graph_runner][INFO] - Test, Round 405 : loss => 6.079779155600071,  accuracy: 0.3447
[2025-09-16 01:59:23,143][flp2p.graph_runner][INFO] - Train, Round 406 : loss => 0.0005555369395733575,  accuracy: 1.0, gradient_norm : 0.030998835038035535
[2025-09-16 01:59:43,018][flp2p.graph_runner][INFO] - Test, Round 406 : loss => 6.081607750928402,  accuracy: 0.3442
[2025-09-16 02:00:31,449][flp2p.graph_runner][INFO] - Train, Round 407 : loss => 0.0005536640309583162,  accuracy: 1.0, gradient_norm : 0.03168325795880379
[2025-09-16 02:00:51,713][flp2p.graph_runner][INFO] - Test, Round 407 : loss => 6.083467590487003,  accuracy: 0.3441
[2025-09-16 02:01:39,999][flp2p.graph_runner][INFO] - Train, Round 408 : loss => 0.0005516284916908869,  accuracy: 1.0, gradient_norm : 0.03138445063643239
[2025-09-16 02:02:00,120][flp2p.graph_runner][INFO] - Test, Round 408 : loss => 6.085581522631645,  accuracy: 0.3443
[2025-09-16 02:02:48,534][flp2p.graph_runner][INFO] - Train, Round 409 : loss => 0.0005499748288275439,  accuracy: 1.0, gradient_norm : 0.030808875908669636
[2025-09-16 02:03:09,085][flp2p.graph_runner][INFO] - Test, Round 409 : loss => 6.08787567281723,  accuracy: 0.3444
[2025-09-16 02:03:58,444][flp2p.graph_runner][INFO] - Train, Round 410 : loss => 0.0005478495698722932,  accuracy: 1.0, gradient_norm : 0.031019842210604023
[2025-09-16 02:04:18,397][flp2p.graph_runner][INFO] - Test, Round 410 : loss => 6.089681899833679,  accuracy: 0.344
[2025-09-16 02:05:07,381][flp2p.graph_runner][INFO] - Train, Round 411 : loss => 0.0005459986867329764,  accuracy: 1.0, gradient_norm : 0.03063255098358938
[2025-09-16 02:05:27,541][flp2p.graph_runner][INFO] - Test, Round 411 : loss => 6.091866235911846,  accuracy: 0.3443
[2025-09-16 02:06:15,061][flp2p.graph_runner][INFO] - Train, Round 412 : loss => 0.0005442525394270584,  accuracy: 1.0, gradient_norm : 0.03146479539000355
[2025-09-16 02:06:35,302][flp2p.graph_runner][INFO] - Test, Round 412 : loss => 6.09373088196516,  accuracy: 0.3442
[2025-09-16 02:07:23,888][flp2p.graph_runner][INFO] - Train, Round 413 : loss => 0.0005420833640164345,  accuracy: 1.0, gradient_norm : 0.028777057956630027
[2025-09-16 02:07:44,107][flp2p.graph_runner][INFO] - Test, Round 413 : loss => 6.095466794872284,  accuracy: 0.3442
[2025-09-16 02:08:32,678][flp2p.graph_runner][INFO] - Train, Round 414 : loss => 0.0005405046499314875,  accuracy: 1.0, gradient_norm : 0.030757388210854466
[2025-09-16 02:08:52,777][flp2p.graph_runner][INFO] - Test, Round 414 : loss => 6.097332377684117,  accuracy: 0.3442
[2025-09-16 02:09:41,288][flp2p.graph_runner][INFO] - Train, Round 415 : loss => 0.0005387438872518638,  accuracy: 1.0, gradient_norm : 0.029231635170223143
[2025-09-16 02:10:01,718][flp2p.graph_runner][INFO] - Test, Round 415 : loss => 6.099789597868919,  accuracy: 0.3447
[2025-09-16 02:10:50,203][flp2p.graph_runner][INFO] - Train, Round 416 : loss => 0.0005369851554254031,  accuracy: 1.0, gradient_norm : 0.03082988560031509
[2025-09-16 02:11:10,574][flp2p.graph_runner][INFO] - Test, Round 416 : loss => 6.10118817807436,  accuracy: 0.3441
[2025-09-16 02:11:59,278][flp2p.graph_runner][INFO] - Train, Round 417 : loss => 0.0005352686153491958,  accuracy: 1.0, gradient_norm : 0.029971707714621206
[2025-09-16 02:12:19,249][flp2p.graph_runner][INFO] - Test, Round 417 : loss => 6.1033587786555294,  accuracy: 0.3444
[2025-09-16 02:13:07,802][flp2p.graph_runner][INFO] - Train, Round 418 : loss => 0.0005332872229337226,  accuracy: 1.0, gradient_norm : 0.0300856326223641
[2025-09-16 02:13:27,769][flp2p.graph_runner][INFO] - Test, Round 418 : loss => 6.1054960150599475,  accuracy: 0.3443
[2025-09-16 02:14:16,326][flp2p.graph_runner][INFO] - Train, Round 419 : loss => 0.0005313745205785381,  accuracy: 1.0, gradient_norm : 0.02861832564311311
[2025-09-16 02:14:36,349][flp2p.graph_runner][INFO] - Test, Round 419 : loss => 6.107346562302112,  accuracy: 0.3446
[2025-09-16 02:15:24,546][flp2p.graph_runner][INFO] - Train, Round 420 : loss => 0.0005297029233770446,  accuracy: 1.0, gradient_norm : 0.029167428600368944
[2025-09-16 02:15:45,051][flp2p.graph_runner][INFO] - Test, Round 420 : loss => 6.1091565330505375,  accuracy: 0.3449
[2025-09-16 02:16:33,926][flp2p.graph_runner][INFO] - Train, Round 421 : loss => 0.000528170334000606,  accuracy: 1.0, gradient_norm : 0.02912990536211716
[2025-09-16 02:16:54,187][flp2p.graph_runner][INFO] - Test, Round 421 : loss => 6.111247426283359,  accuracy: 0.3446
[2025-09-16 02:17:42,959][flp2p.graph_runner][INFO] - Train, Round 422 : loss => 0.0005262171355449633,  accuracy: 1.0, gradient_norm : 0.029921084974847134
[2025-09-16 02:18:03,680][flp2p.graph_runner][INFO] - Test, Round 422 : loss => 6.112905234360695,  accuracy: 0.3446
[2025-09-16 02:18:53,022][flp2p.graph_runner][INFO] - Train, Round 423 : loss => 0.0005245456146197587,  accuracy: 1.0, gradient_norm : 0.029744702711463032
[2025-09-16 02:19:13,003][flp2p.graph_runner][INFO] - Test, Round 423 : loss => 6.115087820613384,  accuracy: 0.3444
[2025-09-16 02:19:59,937][flp2p.graph_runner][INFO] - Train, Round 424 : loss => 0.0005228882819816741,  accuracy: 1.0, gradient_norm : 0.02976652606114103
[2025-09-16 02:20:19,983][flp2p.graph_runner][INFO] - Test, Round 424 : loss => 6.116973630511761,  accuracy: 0.3446
[2025-09-16 02:21:07,773][flp2p.graph_runner][INFO] - Train, Round 425 : loss => 0.0005209903899837324,  accuracy: 1.0, gradient_norm : 0.028805247553603655
[2025-09-16 02:21:27,980][flp2p.graph_runner][INFO] - Test, Round 425 : loss => 6.118239606642723,  accuracy: 0.3442
[2025-09-16 02:22:16,389][flp2p.graph_runner][INFO] - Train, Round 426 : loss => 0.0005194194779808946,  accuracy: 1.0, gradient_norm : 0.028693248085723853
[2025-09-16 02:22:36,340][flp2p.graph_runner][INFO] - Test, Round 426 : loss => 6.120360677719116,  accuracy: 0.3448
[2025-09-16 02:23:24,955][flp2p.graph_runner][INFO] - Train, Round 427 : loss => 0.0005177063718171363,  accuracy: 1.0, gradient_norm : 0.028736242966938327
[2025-09-16 02:23:45,160][flp2p.graph_runner][INFO] - Test, Round 427 : loss => 6.122239542365074,  accuracy: 0.3443
[2025-09-16 02:24:33,113][flp2p.graph_runner][INFO] - Train, Round 428 : loss => 0.0005161636047826807,  accuracy: 1.0, gradient_norm : 0.02860031067485072
[2025-09-16 02:24:53,234][flp2p.graph_runner][INFO] - Test, Round 428 : loss => 6.123996899664402,  accuracy: 0.3443
[2025-09-16 02:25:42,068][flp2p.graph_runner][INFO] - Train, Round 429 : loss => 0.0005143123436573659,  accuracy: 1.0, gradient_norm : 0.028667522896468046
[2025-09-16 02:26:02,125][flp2p.graph_runner][INFO] - Test, Round 429 : loss => 6.126136775267124,  accuracy: 0.3444
[2025-09-16 02:26:50,557][flp2p.graph_runner][INFO] - Train, Round 430 : loss => 0.0005126875670733474,  accuracy: 1.0, gradient_norm : 0.02837019864213062
[2025-09-16 02:27:10,227][flp2p.graph_runner][INFO] - Test, Round 430 : loss => 6.127746115386486,  accuracy: 0.3444
[2025-09-16 02:27:58,427][flp2p.graph_runner][INFO] - Train, Round 431 : loss => 0.0005111356863199035,  accuracy: 1.0, gradient_norm : 0.02912659346960149
[2025-09-16 02:28:18,700][flp2p.graph_runner][INFO] - Test, Round 431 : loss => 6.129688471412659,  accuracy: 0.3444
[2025-09-16 02:29:07,816][flp2p.graph_runner][INFO] - Train, Round 432 : loss => 0.0005093456217340037,  accuracy: 1.0, gradient_norm : 0.02842531491903913
[2025-09-16 02:29:28,130][flp2p.graph_runner][INFO] - Test, Round 432 : loss => 6.131987680578232,  accuracy: 0.3444
[2025-09-16 02:30:16,154][flp2p.graph_runner][INFO] - Train, Round 433 : loss => 0.000507999794887534,  accuracy: 1.0, gradient_norm : 0.029392390027702785
[2025-09-16 02:30:36,589][flp2p.graph_runner][INFO] - Test, Round 433 : loss => 6.133503855383396,  accuracy: 0.3442
[2025-09-16 02:31:25,042][flp2p.graph_runner][INFO] - Train, Round 434 : loss => 0.0005060756444436265,  accuracy: 1.0, gradient_norm : 0.02828815000718151
[2025-09-16 02:31:45,265][flp2p.graph_runner][INFO] - Test, Round 434 : loss => 6.135187001812458,  accuracy: 0.3445
[2025-09-16 02:32:33,629][flp2p.graph_runner][INFO] - Train, Round 435 : loss => 0.0005046696951103511,  accuracy: 1.0, gradient_norm : 0.02801243472248996
[2025-09-16 02:32:53,526][flp2p.graph_runner][INFO] - Test, Round 435 : loss => 6.136867361581325,  accuracy: 0.3443
[2025-09-16 02:33:41,543][flp2p.graph_runner][INFO] - Train, Round 436 : loss => 0.0005031670878694664,  accuracy: 1.0, gradient_norm : 0.028547764829611286
[2025-09-16 02:34:01,081][flp2p.graph_runner][INFO] - Test, Round 436 : loss => 6.138778577065468,  accuracy: 0.3447
[2025-09-16 02:34:48,070][flp2p.graph_runner][INFO] - Train, Round 437 : loss => 0.000501485602920487,  accuracy: 1.0, gradient_norm : 0.02758684142946905
[2025-09-16 02:35:08,291][flp2p.graph_runner][INFO] - Test, Round 437 : loss => 6.140734225666523,  accuracy: 0.3444
[2025-09-16 02:35:56,948][flp2p.graph_runner][INFO] - Train, Round 438 : loss => 0.0004998109708200596,  accuracy: 1.0, gradient_norm : 0.02722140544463436
[2025-09-16 02:36:17,351][flp2p.graph_runner][INFO] - Test, Round 438 : loss => 6.142480246794224,  accuracy: 0.3445
[2025-09-16 02:37:05,863][flp2p.graph_runner][INFO] - Train, Round 439 : loss => 0.0004982477449326931,  accuracy: 1.0, gradient_norm : 0.02843726308266084
[2025-09-16 02:37:26,278][flp2p.graph_runner][INFO] - Test, Round 439 : loss => 6.144230336833,  accuracy: 0.3443
[2025-09-16 02:38:15,420][flp2p.graph_runner][INFO] - Train, Round 440 : loss => 0.0004966241221094002,  accuracy: 1.0, gradient_norm : 0.027482023347230423
[2025-09-16 02:38:35,363][flp2p.graph_runner][INFO] - Test, Round 440 : loss => 6.145893142437935,  accuracy: 0.3442
[2025-09-16 02:39:23,729][flp2p.graph_runner][INFO] - Train, Round 441 : loss => 0.0004953462569877349,  accuracy: 1.0, gradient_norm : 0.02862163354057014
[2025-09-16 02:39:44,273][flp2p.graph_runner][INFO] - Test, Round 441 : loss => 6.147875417351723,  accuracy: 0.3441
[2025-09-16 02:40:32,979][flp2p.graph_runner][INFO] - Train, Round 442 : loss => 0.0004938494113108998,  accuracy: 1.0, gradient_norm : 0.028539807829153836
[2025-09-16 02:40:53,092][flp2p.graph_runner][INFO] - Test, Round 442 : loss => 6.149160402846336,  accuracy: 0.3447
[2025-09-16 02:41:41,502][flp2p.graph_runner][INFO] - Train, Round 443 : loss => 0.0004921856416573669,  accuracy: 1.0, gradient_norm : 0.02710489085072233
[2025-09-16 02:42:02,171][flp2p.graph_runner][INFO] - Test, Round 443 : loss => 6.151121524941921,  accuracy: 0.3441
[2025-09-16 02:42:51,013][flp2p.graph_runner][INFO] - Train, Round 444 : loss => 0.0004909127067609614,  accuracy: 1.0, gradient_norm : 0.028135926203228273
[2025-09-16 02:43:10,914][flp2p.graph_runner][INFO] - Test, Round 444 : loss => 6.152653314995765,  accuracy: 0.3444
[2025-09-16 02:43:58,451][flp2p.graph_runner][INFO] - Train, Round 445 : loss => 0.0004893859443230513,  accuracy: 1.0, gradient_norm : 0.028452441328849557
[2025-09-16 02:44:18,753][flp2p.graph_runner][INFO] - Test, Round 445 : loss => 6.1546739397645,  accuracy: 0.3444
[2025-09-16 02:45:07,679][flp2p.graph_runner][INFO] - Train, Round 446 : loss => 0.0004877453524143978,  accuracy: 1.0, gradient_norm : 0.02679410766646554
[2025-09-16 02:45:28,081][flp2p.graph_runner][INFO] - Test, Round 446 : loss => 6.156824131965637,  accuracy: 0.3447
[2025-09-16 02:46:17,377][flp2p.graph_runner][INFO] - Train, Round 447 : loss => 0.00048623345492766626,  accuracy: 1.0, gradient_norm : 0.027178532866443558
[2025-09-16 02:46:37,502][flp2p.graph_runner][INFO] - Test, Round 447 : loss => 6.15817934346199,  accuracy: 0.3441
[2025-09-16 02:47:25,746][flp2p.graph_runner][INFO] - Train, Round 448 : loss => 0.00048480955077441967,  accuracy: 1.0, gradient_norm : 0.0268694395456923
[2025-09-16 02:47:46,325][flp2p.graph_runner][INFO] - Test, Round 448 : loss => 6.159854864108563,  accuracy: 0.3449
[2025-09-16 02:48:35,196][flp2p.graph_runner][INFO] - Train, Round 449 : loss => 0.0004832948436402753,  accuracy: 1.0, gradient_norm : 0.026960068375495333
[2025-09-16 02:48:55,602][flp2p.graph_runner][INFO] - Test, Round 449 : loss => 6.161733891618252,  accuracy: 0.3444
[2025-09-16 02:49:44,411][flp2p.graph_runner][INFO] - Train, Round 450 : loss => 0.00048186141799912233,  accuracy: 1.0, gradient_norm : 0.02646091059652593
[2025-09-16 02:50:05,123][flp2p.graph_runner][INFO] - Test, Round 450 : loss => 6.163273324072361,  accuracy: 0.3445
[2025-09-16 02:50:54,314][flp2p.graph_runner][INFO] - Train, Round 451 : loss => 0.0004803696456171263,  accuracy: 1.0, gradient_norm : 0.02711845160377939
[2025-09-16 02:51:14,709][flp2p.graph_runner][INFO] - Test, Round 451 : loss => 6.1646971333146094,  accuracy: 0.3445
[2025-09-16 02:52:03,908][flp2p.graph_runner][INFO] - Train, Round 452 : loss => 0.0004791352388929226,  accuracy: 1.0, gradient_norm : 0.026728977210885473
[2025-09-16 02:52:24,291][flp2p.graph_runner][INFO] - Test, Round 452 : loss => 6.1666335762023925,  accuracy: 0.3441
[2025-09-16 02:53:12,414][flp2p.graph_runner][INFO] - Train, Round 453 : loss => 0.0004776342247108307,  accuracy: 1.0, gradient_norm : 0.027400825782796866
[2025-09-16 02:53:32,840][flp2p.graph_runner][INFO] - Test, Round 453 : loss => 6.168363208866119,  accuracy: 0.3448
[2025-09-16 02:54:21,495][flp2p.graph_runner][INFO] - Train, Round 454 : loss => 0.0004762127630965552,  accuracy: 1.0, gradient_norm : 0.02691033493112931
[2025-09-16 02:54:41,991][flp2p.graph_runner][INFO] - Test, Round 454 : loss => 6.1698276267290115,  accuracy: 0.3444
[2025-09-16 02:55:30,689][flp2p.graph_runner][INFO] - Train, Round 455 : loss => 0.00047467840860917934,  accuracy: 1.0, gradient_norm : 0.026489154166944324
[2025-09-16 02:55:51,108][flp2p.graph_runner][INFO] - Test, Round 455 : loss => 6.171666490387916,  accuracy: 0.3446
[2025-09-16 02:56:39,474][flp2p.graph_runner][INFO] - Train, Round 456 : loss => 0.00047339355821653334,  accuracy: 1.0, gradient_norm : 0.027245650701331687
[2025-09-16 02:56:59,864][flp2p.graph_runner][INFO] - Test, Round 456 : loss => 6.173451126837731,  accuracy: 0.3442
[2025-09-16 02:57:48,205][flp2p.graph_runner][INFO] - Train, Round 457 : loss => 0.0004720656862991745,  accuracy: 1.0, gradient_norm : 0.026242968873171048
[2025-09-16 02:58:08,445][flp2p.graph_runner][INFO] - Test, Round 457 : loss => 6.174797503638268,  accuracy: 0.3443
[2025-09-16 02:58:57,188][flp2p.graph_runner][INFO] - Train, Round 458 : loss => 0.00047067287045744413,  accuracy: 1.0, gradient_norm : 0.025846463754222473
[2025-09-16 02:59:17,566][flp2p.graph_runner][INFO] - Test, Round 458 : loss => 6.176223125171662,  accuracy: 0.3443
[2025-09-16 03:00:05,926][flp2p.graph_runner][INFO] - Train, Round 459 : loss => 0.00046929283636321393,  accuracy: 1.0, gradient_norm : 0.027232025719252363
[2025-09-16 03:00:26,187][flp2p.graph_runner][INFO] - Test, Round 459 : loss => 6.178429617130757,  accuracy: 0.3446
[2025-09-16 03:01:14,102][flp2p.graph_runner][INFO] - Train, Round 460 : loss => 0.00046786683805597314,  accuracy: 1.0, gradient_norm : 0.025882401499837327
[2025-09-16 03:01:34,418][flp2p.graph_runner][INFO] - Test, Round 460 : loss => 6.179773674499988,  accuracy: 0.3443
[2025-09-16 03:02:22,520][flp2p.graph_runner][INFO] - Train, Round 461 : loss => 0.0004664942037925357,  accuracy: 1.0, gradient_norm : 0.02642977764090738
[2025-09-16 03:02:41,843][flp2p.graph_runner][INFO] - Test, Round 461 : loss => 6.181782823204994,  accuracy: 0.3447
[2025-09-16 03:03:28,502][flp2p.graph_runner][INFO] - Train, Round 462 : loss => 0.0004652765896995941,  accuracy: 1.0, gradient_norm : 0.025759660147434006
[2025-09-16 03:03:48,585][flp2p.graph_runner][INFO] - Test, Round 462 : loss => 6.183107430994511,  accuracy: 0.3447
[2025-09-16 03:04:36,480][flp2p.graph_runner][INFO] - Train, Round 463 : loss => 0.0004638775247682738,  accuracy: 1.0, gradient_norm : 0.02558856143070452
[2025-09-16 03:04:56,954][flp2p.graph_runner][INFO] - Test, Round 463 : loss => 6.184816365253925,  accuracy: 0.3446
[2025-09-16 03:05:45,580][flp2p.graph_runner][INFO] - Train, Round 464 : loss => 0.00046272169039487677,  accuracy: 1.0, gradient_norm : 0.025888748832609432
[2025-09-16 03:06:05,851][flp2p.graph_runner][INFO] - Test, Round 464 : loss => 6.18637193132639,  accuracy: 0.3444
[2025-09-16 03:06:54,030][flp2p.graph_runner][INFO] - Train, Round 465 : loss => 0.0004612740280087262,  accuracy: 1.0, gradient_norm : 0.026423648324835153
[2025-09-16 03:07:14,442][flp2p.graph_runner][INFO] - Test, Round 465 : loss => 6.188420319616794,  accuracy: 0.3445
[2025-09-16 03:08:03,479][flp2p.graph_runner][INFO] - Train, Round 466 : loss => 0.00046000902002560915,  accuracy: 1.0, gradient_norm : 0.026449439960905242
[2025-09-16 03:08:23,155][flp2p.graph_runner][INFO] - Test, Round 466 : loss => 6.189778110074997,  accuracy: 0.344
[2025-09-16 03:09:10,877][flp2p.graph_runner][INFO] - Train, Round 467 : loss => 0.0004587204824080498,  accuracy: 1.0, gradient_norm : 0.025277073977300012
[2025-09-16 03:09:31,178][flp2p.graph_runner][INFO] - Test, Round 467 : loss => 6.191652905976772,  accuracy: 0.3446
[2025-09-16 03:10:19,908][flp2p.graph_runner][INFO] - Train, Round 468 : loss => 0.00045744472196626395,  accuracy: 1.0, gradient_norm : 0.027118666473314817
[2025-09-16 03:10:40,584][flp2p.graph_runner][INFO] - Test, Round 468 : loss => 6.193540039348602,  accuracy: 0.3446
[2025-09-16 03:11:29,525][flp2p.graph_runner][INFO] - Train, Round 469 : loss => 0.0004559213071600729,  accuracy: 1.0, gradient_norm : 0.025535810959941948
[2025-09-16 03:11:49,806][flp2p.graph_runner][INFO] - Test, Round 469 : loss => 6.194774361026287,  accuracy: 0.3446
[2025-09-16 03:12:37,991][flp2p.graph_runner][INFO] - Train, Round 470 : loss => 0.00045495149245349827,  accuracy: 1.0, gradient_norm : 0.026239213186788363
[2025-09-16 03:12:58,266][flp2p.graph_runner][INFO] - Test, Round 470 : loss => 6.19666511490345,  accuracy: 0.3445
[2025-09-16 03:13:46,507][flp2p.graph_runner][INFO] - Train, Round 471 : loss => 0.0004534235013488797,  accuracy: 1.0, gradient_norm : 0.026822835765622454
[2025-09-16 03:14:07,042][flp2p.graph_runner][INFO] - Test, Round 471 : loss => 6.197820777964592,  accuracy: 0.3446
[2025-09-16 03:14:55,574][flp2p.graph_runner][INFO] - Train, Round 472 : loss => 0.0004521983845673578,  accuracy: 1.0, gradient_norm : 0.025799398805209716
[2025-09-16 03:15:15,930][flp2p.graph_runner][INFO] - Test, Round 472 : loss => 6.199595414876938,  accuracy: 0.3443
[2025-09-16 03:16:04,357][flp2p.graph_runner][INFO] - Train, Round 473 : loss => 0.0004510381228586387,  accuracy: 1.0, gradient_norm : 0.025421766995945037
[2025-09-16 03:16:24,789][flp2p.graph_runner][INFO] - Test, Round 473 : loss => 6.201173302578926,  accuracy: 0.3442
[2025-09-16 03:17:13,288][flp2p.graph_runner][INFO] - Train, Round 474 : loss => 0.00044974093850517734,  accuracy: 1.0, gradient_norm : 0.025632145129223294
[2025-09-16 03:17:33,728][flp2p.graph_runner][INFO] - Test, Round 474 : loss => 6.202658941936493,  accuracy: 0.3442
[2025-09-16 03:18:22,537][flp2p.graph_runner][INFO] - Train, Round 475 : loss => 0.0004484138155263887,  accuracy: 1.0, gradient_norm : 0.026166368429406117
[2025-09-16 03:18:42,893][flp2p.graph_runner][INFO] - Test, Round 475 : loss => 6.204062306118011,  accuracy: 0.3443
[2025-09-16 03:19:30,938][flp2p.graph_runner][INFO] - Train, Round 476 : loss => 0.00044720135432726267,  accuracy: 1.0, gradient_norm : 0.02547041197150073
[2025-09-16 03:19:51,185][flp2p.graph_runner][INFO] - Test, Round 476 : loss => 6.205611394572258,  accuracy: 0.3443
[2025-09-16 03:20:39,579][flp2p.graph_runner][INFO] - Train, Round 477 : loss => 0.00044590140614673155,  accuracy: 1.0, gradient_norm : 0.02542259709968552
[2025-09-16 03:21:00,089][flp2p.graph_runner][INFO] - Test, Round 477 : loss => 6.207314180898666,  accuracy: 0.3443
[2025-09-16 03:21:48,231][flp2p.graph_runner][INFO] - Train, Round 478 : loss => 0.00044484579381484456,  accuracy: 1.0, gradient_norm : 0.024154464184547874
[2025-09-16 03:22:08,269][flp2p.graph_runner][INFO] - Test, Round 478 : loss => 6.208650181722641,  accuracy: 0.3444
[2025-09-16 03:22:55,892][flp2p.graph_runner][INFO] - Train, Round 479 : loss => 0.0004437465064620482,  accuracy: 1.0, gradient_norm : 0.025046470212573283
[2025-09-16 03:23:16,088][flp2p.graph_runner][INFO] - Test, Round 479 : loss => 6.210003726768494,  accuracy: 0.3442
[2025-09-16 03:24:03,855][flp2p.graph_runner][INFO] - Train, Round 480 : loss => 0.00044247440275891356,  accuracy: 1.0, gradient_norm : 0.025448395478891196
[2025-09-16 03:24:24,342][flp2p.graph_runner][INFO] - Test, Round 480 : loss => 6.2119887680292125,  accuracy: 0.3441
[2025-09-16 03:25:12,528][flp2p.graph_runner][INFO] - Train, Round 481 : loss => 0.00044128226370958144,  accuracy: 1.0, gradient_norm : 0.025434025534140176
[2025-09-16 03:25:32,693][flp2p.graph_runner][INFO] - Test, Round 481 : loss => 6.213609978604317,  accuracy: 0.3445
[2025-09-16 03:26:21,865][flp2p.graph_runner][INFO] - Train, Round 482 : loss => 0.0004399595159581321,  accuracy: 1.0, gradient_norm : 0.02511111753758031
[2025-09-16 03:26:42,485][flp2p.graph_runner][INFO] - Test, Round 482 : loss => 6.214850297284126,  accuracy: 0.3445
[2025-09-16 03:27:31,219][flp2p.graph_runner][INFO] - Train, Round 483 : loss => 0.00043871449020419596,  accuracy: 1.0, gradient_norm : 0.02540163900808533
[2025-09-16 03:27:51,813][flp2p.graph_runner][INFO] - Test, Round 483 : loss => 6.216411711335182,  accuracy: 0.3442
[2025-09-16 03:28:39,764][flp2p.graph_runner][INFO] - Train, Round 484 : loss => 0.0004376764847499241,  accuracy: 1.0, gradient_norm : 0.024076807377612496
[2025-09-16 03:28:59,920][flp2p.graph_runner][INFO] - Test, Round 484 : loss => 6.217979742574692,  accuracy: 0.3445
[2025-09-16 03:29:48,214][flp2p.graph_runner][INFO] - Train, Round 485 : loss => 0.0004364725627965526,  accuracy: 1.0, gradient_norm : 0.02371424764962396
[2025-09-16 03:30:08,958][flp2p.graph_runner][INFO] - Test, Round 485 : loss => 6.219710328483582,  accuracy: 0.3444
[2025-09-16 03:30:58,010][flp2p.graph_runner][INFO] - Train, Round 486 : loss => 0.00043538707165377343,  accuracy: 1.0, gradient_norm : 0.024780253598994745
[2025-09-16 03:31:18,890][flp2p.graph_runner][INFO] - Test, Round 486 : loss => 6.221535520243645,  accuracy: 0.3443
[2025-09-16 03:32:08,008][flp2p.graph_runner][INFO] - Train, Round 487 : loss => 0.0004341222392334506,  accuracy: 1.0, gradient_norm : 0.024447581649751957
[2025-09-16 03:32:28,691][flp2p.graph_runner][INFO] - Test, Round 487 : loss => 6.222768351101875,  accuracy: 0.3445
[2025-09-16 03:33:17,354][flp2p.graph_runner][INFO] - Train, Round 488 : loss => 0.0004331081267567545,  accuracy: 1.0, gradient_norm : 0.024872322350325837
[2025-09-16 03:33:37,676][flp2p.graph_runner][INFO] - Test, Round 488 : loss => 6.224262097859382,  accuracy: 0.3444
[2025-09-16 03:34:25,751][flp2p.graph_runner][INFO] - Train, Round 489 : loss => 0.0004318264230399412,  accuracy: 1.0, gradient_norm : 0.023536552428505275
[2025-09-16 03:34:46,106][flp2p.graph_runner][INFO] - Test, Round 489 : loss => 6.225752405381202,  accuracy: 0.3443
[2025-09-16 03:35:34,814][flp2p.graph_runner][INFO] - Train, Round 490 : loss => 0.0004307678007171489,  accuracy: 1.0, gradient_norm : 0.024281617754070936
[2025-09-16 03:35:55,252][flp2p.graph_runner][INFO] - Test, Round 490 : loss => 6.227323976945877,  accuracy: 0.3446
[2025-09-16 03:36:43,101][flp2p.graph_runner][INFO] - Train, Round 491 : loss => 0.0004295523381127473,  accuracy: 1.0, gradient_norm : 0.025094260250405957
[2025-09-16 03:37:03,733][flp2p.graph_runner][INFO] - Test, Round 491 : loss => 6.228852724766731,  accuracy: 0.3443
[2025-09-16 03:37:52,640][flp2p.graph_runner][INFO] - Train, Round 492 : loss => 0.0004285503321261785,  accuracy: 1.0, gradient_norm : 0.024387949559668262
[2025-09-16 03:38:13,303][flp2p.graph_runner][INFO] - Test, Round 492 : loss => 6.230229641866684,  accuracy: 0.3446
[2025-09-16 03:39:02,338][flp2p.graph_runner][INFO] - Train, Round 493 : loss => 0.00042740305512779743,  accuracy: 1.0, gradient_norm : 0.024446790618991868
[2025-09-16 03:39:22,795][flp2p.graph_runner][INFO] - Test, Round 493 : loss => 6.231822508859635,  accuracy: 0.3447
[2025-09-16 03:40:11,569][flp2p.graph_runner][INFO] - Train, Round 494 : loss => 0.00042603579477145097,  accuracy: 1.0, gradient_norm : 0.024370210672704576
[2025-09-16 03:40:32,283][flp2p.graph_runner][INFO] - Test, Round 494 : loss => 6.233244632577896,  accuracy: 0.3446
[2025-09-16 03:41:21,472][flp2p.graph_runner][INFO] - Train, Round 495 : loss => 0.0004251275981005164,  accuracy: 1.0, gradient_norm : 0.02464118569566438
[2025-09-16 03:41:41,578][flp2p.graph_runner][INFO] - Test, Round 495 : loss => 6.234302220034599,  accuracy: 0.3445
[2025-09-16 03:42:29,770][flp2p.graph_runner][INFO] - Train, Round 496 : loss => 0.0004240521928659292,  accuracy: 1.0, gradient_norm : 0.02465146440476106
[2025-09-16 03:42:50,252][flp2p.graph_runner][INFO] - Test, Round 496 : loss => 6.236118684959411,  accuracy: 0.3443
[2025-09-16 03:43:37,688][flp2p.graph_runner][INFO] - Train, Round 497 : loss => 0.0004228100731537171,  accuracy: 1.0, gradient_norm : 0.025315470623139026
[2025-09-16 03:43:57,778][flp2p.graph_runner][INFO] - Test, Round 497 : loss => 6.237794858193397,  accuracy: 0.3446
[2025-09-16 03:44:45,619][flp2p.graph_runner][INFO] - Train, Round 498 : loss => 0.00042183450800318184,  accuracy: 1.0, gradient_norm : 0.02437439551654749
[2025-09-16 03:45:05,956][flp2p.graph_runner][INFO] - Test, Round 498 : loss => 6.238835051774979,  accuracy: 0.3443
[2025-09-16 03:45:55,058][flp2p.graph_runner][INFO] - Train, Round 499 : loss => 0.0004207618651344091,  accuracy: 1.0, gradient_norm : 0.023600264225952107
[2025-09-16 03:46:15,508][flp2p.graph_runner][INFO] - Test, Round 499 : loss => 6.2408002982139585,  accuracy: 0.345
[2025-09-16 03:46:15,513][__main__][INFO] - Train, Round 001: loss=2.3039, accuracy=0.1132, gradient_norm=0.2387, 
[2025-09-16 03:46:15,513][__main__][INFO] - Train, Round 002: loss=2.3013, accuracy=0.1310, gradient_norm=0.2436, 
[2025-09-16 03:46:15,513][__main__][INFO] - Train, Round 003: loss=2.2988, accuracy=0.1359, gradient_norm=0.2425, 
[2025-09-16 03:46:15,513][__main__][INFO] - Train, Round 004: loss=2.2963, accuracy=0.1374, gradient_norm=0.2563, 
[2025-09-16 03:46:15,513][__main__][INFO] - Train, Round 005: loss=2.2937, accuracy=0.1404, gradient_norm=0.2570, 
[2025-09-16 03:46:15,513][__main__][INFO] - Train, Round 006: loss=2.2909, accuracy=0.1430, gradient_norm=0.2679, 
[2025-09-16 03:46:15,513][__main__][INFO] - Train, Round 007: loss=2.2878, accuracy=0.1467, gradient_norm=0.2841, 
[2025-09-16 03:46:15,514][__main__][INFO] - Train, Round 008: loss=2.2843, accuracy=0.1510, gradient_norm=0.3059, 
[2025-09-16 03:46:15,514][__main__][INFO] - Train, Round 009: loss=2.2802, accuracy=0.1552, gradient_norm=0.3257, 
[2025-09-16 03:46:15,514][__main__][INFO] - Train, Round 010: loss=2.2753, accuracy=0.1599, gradient_norm=0.3563, 
[2025-09-16 03:46:15,514][__main__][INFO] - Train, Round 011: loss=2.2694, accuracy=0.1651, gradient_norm=0.3870, 
[2025-09-16 03:46:15,514][__main__][INFO] - Train, Round 012: loss=2.2620, accuracy=0.1730, gradient_norm=0.4310, 
[2025-09-16 03:46:15,514][__main__][INFO] - Train, Round 013: loss=2.2530, accuracy=0.1813, gradient_norm=0.4967, 
[2025-09-16 03:46:15,514][__main__][INFO] - Train, Round 014: loss=2.2417, accuracy=0.1897, gradient_norm=0.5635, 
[2025-09-16 03:46:15,514][__main__][INFO] - Train, Round 015: loss=2.2274, accuracy=0.2001, gradient_norm=0.6263, 
[2025-09-16 03:46:15,514][__main__][INFO] - Train, Round 016: loss=2.2092, accuracy=0.2099, gradient_norm=0.7281, 
[2025-09-16 03:46:15,514][__main__][INFO] - Train, Round 017: loss=2.1870, accuracy=0.2189, gradient_norm=0.8632, 
[2025-09-16 03:46:15,514][__main__][INFO] - Train, Round 018: loss=2.1611, accuracy=0.2295, gradient_norm=0.9951, 
[2025-09-16 03:46:15,514][__main__][INFO] - Train, Round 019: loss=2.1320, accuracy=0.2395, gradient_norm=1.1938, 
[2025-09-16 03:46:15,514][__main__][INFO] - Train, Round 020: loss=2.1027, accuracy=0.2525, gradient_norm=1.3599, 
[2025-09-16 03:46:15,514][__main__][INFO] - Train, Round 021: loss=2.0747, accuracy=0.2621, gradient_norm=1.5214, 
[2025-09-16 03:46:15,514][__main__][INFO] - Train, Round 022: loss=2.0482, accuracy=0.2748, gradient_norm=1.6135, 
[2025-09-16 03:46:15,514][__main__][INFO] - Train, Round 023: loss=2.0217, accuracy=0.2873, gradient_norm=1.8862, 
[2025-09-16 03:46:15,514][__main__][INFO] - Train, Round 024: loss=1.9961, accuracy=0.2945, gradient_norm=2.0680, 
[2025-09-16 03:46:15,514][__main__][INFO] - Train, Round 025: loss=1.9694, accuracy=0.3059, gradient_norm=2.0902, 
[2025-09-16 03:46:15,514][__main__][INFO] - Train, Round 026: loss=1.9424, accuracy=0.3141, gradient_norm=2.3357, 
[2025-09-16 03:46:15,514][__main__][INFO] - Train, Round 027: loss=1.9160, accuracy=0.3222, gradient_norm=2.4449, 
[2025-09-16 03:46:15,514][__main__][INFO] - Train, Round 028: loss=1.8906, accuracy=0.3311, gradient_norm=2.5351, 
[2025-09-16 03:46:15,514][__main__][INFO] - Train, Round 029: loss=1.8656, accuracy=0.3397, gradient_norm=2.7985, 
[2025-09-16 03:46:15,514][__main__][INFO] - Train, Round 030: loss=1.8434, accuracy=0.3458, gradient_norm=3.0380, 
[2025-09-16 03:46:15,514][__main__][INFO] - Train, Round 031: loss=1.8221, accuracy=0.3523, gradient_norm=3.0287, 
[2025-09-16 03:46:15,514][__main__][INFO] - Train, Round 032: loss=1.8015, accuracy=0.3603, gradient_norm=3.1381, 
[2025-09-16 03:46:15,514][__main__][INFO] - Train, Round 033: loss=1.7831, accuracy=0.3663, gradient_norm=3.2599, 
[2025-09-16 03:46:15,514][__main__][INFO] - Train, Round 034: loss=1.7636, accuracy=0.3739, gradient_norm=3.4121, 
[2025-09-16 03:46:15,514][__main__][INFO] - Train, Round 035: loss=1.7431, accuracy=0.3826, gradient_norm=3.4371, 
[2025-09-16 03:46:15,514][__main__][INFO] - Train, Round 036: loss=1.7236, accuracy=0.3888, gradient_norm=3.5569, 
[2025-09-16 03:46:15,514][__main__][INFO] - Train, Round 037: loss=1.7014, accuracy=0.3972, gradient_norm=3.5899, 
[2025-09-16 03:46:15,514][__main__][INFO] - Train, Round 038: loss=1.6771, accuracy=0.4064, gradient_norm=3.7210, 
[2025-09-16 03:46:15,514][__main__][INFO] - Train, Round 039: loss=1.6552, accuracy=0.4153, gradient_norm=3.8976, 
[2025-09-16 03:46:15,514][__main__][INFO] - Train, Round 040: loss=1.6336, accuracy=0.4235, gradient_norm=4.0702, 
[2025-09-16 03:46:15,514][__main__][INFO] - Train, Round 041: loss=1.6109, accuracy=0.4323, gradient_norm=4.2488, 
[2025-09-16 03:46:15,514][__main__][INFO] - Train, Round 042: loss=1.5849, accuracy=0.4419, gradient_norm=4.4290, 
[2025-09-16 03:46:15,514][__main__][INFO] - Train, Round 043: loss=1.5669, accuracy=0.4486, gradient_norm=4.3756, 
[2025-09-16 03:46:15,514][__main__][INFO] - Train, Round 044: loss=1.5446, accuracy=0.4588, gradient_norm=4.5398, 
[2025-09-16 03:46:15,514][__main__][INFO] - Train, Round 045: loss=1.5098, accuracy=0.4707, gradient_norm=4.8090, 
[2025-09-16 03:46:15,514][__main__][INFO] - Train, Round 046: loss=1.4873, accuracy=0.4772, gradient_norm=4.8284, 
[2025-09-16 03:46:15,514][__main__][INFO] - Train, Round 047: loss=1.4579, accuracy=0.4881, gradient_norm=4.8438, 
[2025-09-16 03:46:15,514][__main__][INFO] - Train, Round 048: loss=1.4259, accuracy=0.5000, gradient_norm=5.1691, 
[2025-09-16 03:46:15,514][__main__][INFO] - Train, Round 049: loss=1.4011, accuracy=0.5090, gradient_norm=5.3256, 
[2025-09-16 03:46:15,514][__main__][INFO] - Train, Round 050: loss=1.3714, accuracy=0.5200, gradient_norm=5.7291, 
[2025-09-16 03:46:15,514][__main__][INFO] - Train, Round 051: loss=1.3427, accuracy=0.5302, gradient_norm=5.2751, 
[2025-09-16 03:46:15,514][__main__][INFO] - Train, Round 052: loss=1.3182, accuracy=0.5404, gradient_norm=5.4682, 
[2025-09-16 03:46:15,514][__main__][INFO] - Train, Round 053: loss=1.2834, accuracy=0.5531, gradient_norm=5.9615, 
[2025-09-16 03:46:15,514][__main__][INFO] - Train, Round 054: loss=1.2567, accuracy=0.5620, gradient_norm=5.9953, 
[2025-09-16 03:46:15,514][__main__][INFO] - Train, Round 055: loss=1.2286, accuracy=0.5716, gradient_norm=6.1135, 
[2025-09-16 03:46:15,514][__main__][INFO] - Train, Round 056: loss=1.1904, accuracy=0.5872, gradient_norm=6.3841, 
[2025-09-16 03:46:15,514][__main__][INFO] - Train, Round 057: loss=1.1548, accuracy=0.6012, gradient_norm=6.4064, 
[2025-09-16 03:46:15,514][__main__][INFO] - Train, Round 058: loss=1.1281, accuracy=0.6121, gradient_norm=6.8338, 
[2025-09-16 03:46:15,514][__main__][INFO] - Train, Round 059: loss=1.0894, accuracy=0.6248, gradient_norm=6.5793, 
[2025-09-16 03:46:15,514][__main__][INFO] - Train, Round 060: loss=1.0603, accuracy=0.6380, gradient_norm=6.6830, 
[2025-09-16 03:46:15,514][__main__][INFO] - Train, Round 061: loss=1.0232, accuracy=0.6499, gradient_norm=6.8192, 
[2025-09-16 03:46:15,514][__main__][INFO] - Train, Round 062: loss=0.9796, accuracy=0.6656, gradient_norm=7.2272, 
[2025-09-16 03:46:15,514][__main__][INFO] - Train, Round 063: loss=0.9457, accuracy=0.6790, gradient_norm=7.0320, 
[2025-09-16 03:46:15,514][__main__][INFO] - Train, Round 064: loss=0.9395, accuracy=0.6848, gradient_norm=7.0384, 
[2025-09-16 03:46:15,514][__main__][INFO] - Train, Round 065: loss=0.8744, accuracy=0.7070, gradient_norm=6.9736, 
[2025-09-16 03:46:15,514][__main__][INFO] - Train, Round 066: loss=0.8329, accuracy=0.7230, gradient_norm=7.1608, 
[2025-09-16 03:46:15,514][__main__][INFO] - Train, Round 067: loss=0.8127, accuracy=0.7334, gradient_norm=7.5901, 
[2025-09-16 03:46:15,515][__main__][INFO] - Train, Round 068: loss=0.7746, accuracy=0.7493, gradient_norm=7.5982, 
[2025-09-16 03:46:15,515][__main__][INFO] - Train, Round 069: loss=0.7128, accuracy=0.7715, gradient_norm=6.7936, 
[2025-09-16 03:46:15,515][__main__][INFO] - Train, Round 070: loss=0.6851, accuracy=0.7809, gradient_norm=7.6837, 
[2025-09-16 03:46:15,515][__main__][INFO] - Train, Round 071: loss=0.6384, accuracy=0.8008, gradient_norm=7.7958, 
[2025-09-16 03:46:15,515][__main__][INFO] - Train, Round 072: loss=0.5971, accuracy=0.8168, gradient_norm=7.5707, 
[2025-09-16 03:46:15,515][__main__][INFO] - Train, Round 073: loss=0.5638, accuracy=0.8288, gradient_norm=7.3428, 
[2025-09-16 03:46:15,515][__main__][INFO] - Train, Round 074: loss=0.5459, accuracy=0.8367, gradient_norm=7.1760, 
[2025-09-16 03:46:15,515][__main__][INFO] - Train, Round 075: loss=0.5315, accuracy=0.8491, gradient_norm=6.4497, 
[2025-09-16 03:46:15,515][__main__][INFO] - Train, Round 076: loss=0.4428, accuracy=0.8766, gradient_norm=7.0907, 
[2025-09-16 03:46:15,515][__main__][INFO] - Train, Round 077: loss=0.4392, accuracy=0.8793, gradient_norm=6.0218, 
[2025-09-16 03:46:15,515][__main__][INFO] - Train, Round 078: loss=0.3665, accuracy=0.9053, gradient_norm=5.7639, 
[2025-09-16 03:46:15,516][__main__][INFO] - Train, Round 079: loss=0.3351, accuracy=0.9179, gradient_norm=6.1365, 
[2025-09-16 03:46:15,516][__main__][INFO] - Train, Round 080: loss=0.3127, accuracy=0.9237, gradient_norm=5.4853, 
[2025-09-16 03:46:15,516][__main__][INFO] - Train, Round 081: loss=0.2910, accuracy=0.9323, gradient_norm=4.6854, 
[2025-09-16 03:46:15,516][__main__][INFO] - Train, Round 082: loss=0.2580, accuracy=0.9444, gradient_norm=5.0315, 
[2025-09-16 03:46:15,516][__main__][INFO] - Train, Round 083: loss=0.2335, accuracy=0.9518, gradient_norm=4.1413, 
[2025-09-16 03:46:15,516][__main__][INFO] - Train, Round 084: loss=0.2026, accuracy=0.9629, gradient_norm=4.5417, 
[2025-09-16 03:46:15,516][__main__][INFO] - Train, Round 085: loss=0.2162, accuracy=0.9565, gradient_norm=3.5380, 
[2025-09-16 03:46:15,516][__main__][INFO] - Train, Round 086: loss=0.1746, accuracy=0.9701, gradient_norm=3.2801, 
[2025-09-16 03:46:15,516][__main__][INFO] - Train, Round 087: loss=0.1517, accuracy=0.9787, gradient_norm=3.3983, 
[2025-09-16 03:46:15,516][__main__][INFO] - Train, Round 088: loss=0.1489, accuracy=0.9775, gradient_norm=2.6130, 
[2025-09-16 03:46:15,516][__main__][INFO] - Train, Round 089: loss=0.1050, accuracy=0.9895, gradient_norm=2.2477, 
[2025-09-16 03:46:15,516][__main__][INFO] - Train, Round 090: loss=0.0985, accuracy=0.9904, gradient_norm=2.3200, 
[2025-09-16 03:46:15,516][__main__][INFO] - Train, Round 091: loss=0.0715, accuracy=0.9959, gradient_norm=1.9888, 
[2025-09-16 03:46:15,516][__main__][INFO] - Train, Round 092: loss=0.0768, accuracy=0.9937, gradient_norm=2.0927, 
[2025-09-16 03:46:15,516][__main__][INFO] - Train, Round 093: loss=0.0610, accuracy=0.9964, gradient_norm=1.7430, 
[2025-09-16 03:46:15,516][__main__][INFO] - Train, Round 094: loss=0.0519, accuracy=0.9977, gradient_norm=1.7774, 
[2025-09-16 03:46:15,517][__main__][INFO] - Train, Round 095: loss=0.0580, accuracy=0.9962, gradient_norm=1.4117, 
[2025-09-16 03:46:15,517][__main__][INFO] - Train, Round 096: loss=0.0414, accuracy=0.9990, gradient_norm=1.3335, 
[2025-09-16 03:46:15,517][__main__][INFO] - Train, Round 097: loss=0.0412, accuracy=0.9982, gradient_norm=1.2749, 
[2025-09-16 03:46:15,517][__main__][INFO] - Train, Round 098: loss=0.0424, accuracy=0.9977, gradient_norm=1.1582, 
[2025-09-16 03:46:15,517][__main__][INFO] - Train, Round 099: loss=0.0329, accuracy=0.9991, gradient_norm=1.2241, 
[2025-09-16 03:46:15,517][__main__][INFO] - Train, Round 100: loss=0.0447, accuracy=0.9966, gradient_norm=0.9639, 
[2025-09-16 03:46:15,517][__main__][INFO] - Train, Round 101: loss=0.0356, accuracy=0.9987, gradient_norm=1.0934, 
[2025-09-16 03:46:15,517][__main__][INFO] - Train, Round 102: loss=0.0292, accuracy=0.9988, gradient_norm=0.8913, 
[2025-09-16 03:46:15,517][__main__][INFO] - Train, Round 103: loss=0.0231, accuracy=0.9999, gradient_norm=0.8122, 
[2025-09-16 03:46:15,517][__main__][INFO] - Train, Round 104: loss=0.0211, accuracy=1.0000, gradient_norm=0.8070, 
[2025-09-16 03:46:15,517][__main__][INFO] - Train, Round 105: loss=0.0197, accuracy=0.9999, gradient_norm=0.7347, 
[2025-09-16 03:46:15,517][__main__][INFO] - Train, Round 106: loss=0.0185, accuracy=1.0000, gradient_norm=0.7020, 
[2025-09-16 03:46:15,517][__main__][INFO] - Train, Round 107: loss=0.0172, accuracy=1.0000, gradient_norm=0.6407, 
[2025-09-16 03:46:15,517][__main__][INFO] - Train, Round 108: loss=0.0163, accuracy=1.0000, gradient_norm=0.6141, 
[2025-09-16 03:46:15,517][__main__][INFO] - Train, Round 109: loss=0.0153, accuracy=1.0000, gradient_norm=0.6041, 
[2025-09-16 03:46:15,517][__main__][INFO] - Train, Round 110: loss=0.0145, accuracy=1.0000, gradient_norm=0.5838, 
[2025-09-16 03:46:15,517][__main__][INFO] - Train, Round 111: loss=0.0138, accuracy=1.0000, gradient_norm=0.5286, 
[2025-09-16 03:46:15,517][__main__][INFO] - Train, Round 112: loss=0.0131, accuracy=1.0000, gradient_norm=0.5152, 
[2025-09-16 03:46:15,517][__main__][INFO] - Train, Round 113: loss=0.0125, accuracy=1.0000, gradient_norm=0.4952, 
[2025-09-16 03:46:15,517][__main__][INFO] - Train, Round 114: loss=0.0119, accuracy=1.0000, gradient_norm=0.4991, 
[2025-09-16 03:46:15,517][__main__][INFO] - Train, Round 115: loss=0.0113, accuracy=1.0000, gradient_norm=0.4251, 
[2025-09-16 03:46:15,517][__main__][INFO] - Train, Round 116: loss=0.0108, accuracy=1.0000, gradient_norm=0.4506, 
[2025-09-16 03:46:15,517][__main__][INFO] - Train, Round 117: loss=0.0104, accuracy=1.0000, gradient_norm=0.4118, 
[2025-09-16 03:46:15,517][__main__][INFO] - Train, Round 118: loss=0.0100, accuracy=1.0000, gradient_norm=0.3968, 
[2025-09-16 03:46:15,517][__main__][INFO] - Train, Round 119: loss=0.0096, accuracy=1.0000, gradient_norm=0.3985, 
[2025-09-16 03:46:15,517][__main__][INFO] - Train, Round 120: loss=0.0092, accuracy=1.0000, gradient_norm=0.3886, 
[2025-09-16 03:46:15,517][__main__][INFO] - Train, Round 121: loss=0.0090, accuracy=1.0000, gradient_norm=0.3628, 
[2025-09-16 03:46:15,517][__main__][INFO] - Train, Round 122: loss=0.0086, accuracy=1.0000, gradient_norm=0.3495, 
[2025-09-16 03:46:15,517][__main__][INFO] - Train, Round 123: loss=0.0083, accuracy=1.0000, gradient_norm=0.3672, 
[2025-09-16 03:46:15,517][__main__][INFO] - Train, Round 124: loss=0.0080, accuracy=1.0000, gradient_norm=0.3366, 
[2025-09-16 03:46:15,517][__main__][INFO] - Train, Round 125: loss=0.0077, accuracy=1.0000, gradient_norm=0.3391, 
[2025-09-16 03:46:15,517][__main__][INFO] - Train, Round 126: loss=0.0075, accuracy=1.0000, gradient_norm=0.3216, 
[2025-09-16 03:46:15,517][__main__][INFO] - Train, Round 127: loss=0.0072, accuracy=1.0000, gradient_norm=0.3161, 
[2025-09-16 03:46:15,517][__main__][INFO] - Train, Round 128: loss=0.0070, accuracy=1.0000, gradient_norm=0.2910, 
[2025-09-16 03:46:15,517][__main__][INFO] - Train, Round 129: loss=0.0068, accuracy=1.0000, gradient_norm=0.2896, 
[2025-09-16 03:46:15,517][__main__][INFO] - Train, Round 130: loss=0.0066, accuracy=1.0000, gradient_norm=0.2834, 
[2025-09-16 03:46:15,517][__main__][INFO] - Train, Round 131: loss=0.0064, accuracy=1.0000, gradient_norm=0.2791, 
[2025-09-16 03:46:15,517][__main__][INFO] - Train, Round 132: loss=0.0062, accuracy=1.0000, gradient_norm=0.2578, 
[2025-09-16 03:46:15,517][__main__][INFO] - Train, Round 133: loss=0.0060, accuracy=1.0000, gradient_norm=0.2595, 
[2025-09-16 03:46:15,517][__main__][INFO] - Train, Round 134: loss=0.0059, accuracy=1.0000, gradient_norm=0.2585, 
[2025-09-16 03:46:15,517][__main__][INFO] - Train, Round 135: loss=0.0057, accuracy=1.0000, gradient_norm=0.2495, 
[2025-09-16 03:46:15,517][__main__][INFO] - Train, Round 136: loss=0.0056, accuracy=1.0000, gradient_norm=0.2453, 
[2025-09-16 03:46:15,517][__main__][INFO] - Train, Round 137: loss=0.0054, accuracy=1.0000, gradient_norm=0.2420, 
[2025-09-16 03:46:15,517][__main__][INFO] - Train, Round 138: loss=0.0053, accuracy=1.0000, gradient_norm=0.2381, 
[2025-09-16 03:46:15,517][__main__][INFO] - Train, Round 139: loss=0.0052, accuracy=1.0000, gradient_norm=0.2311, 
[2025-09-16 03:46:15,517][__main__][INFO] - Train, Round 140: loss=0.0050, accuracy=1.0000, gradient_norm=0.2187, 
[2025-09-16 03:46:15,517][__main__][INFO] - Train, Round 141: loss=0.0049, accuracy=1.0000, gradient_norm=0.2219, 
[2025-09-16 03:46:15,517][__main__][INFO] - Train, Round 142: loss=0.0048, accuracy=1.0000, gradient_norm=0.2293, 
[2025-09-16 03:46:15,517][__main__][INFO] - Train, Round 143: loss=0.0047, accuracy=1.0000, gradient_norm=0.2213, 
[2025-09-16 03:46:15,517][__main__][INFO] - Train, Round 144: loss=0.0046, accuracy=1.0000, gradient_norm=0.2028, 
[2025-09-16 03:46:15,517][__main__][INFO] - Train, Round 145: loss=0.0045, accuracy=1.0000, gradient_norm=0.2059, 
[2025-09-16 03:46:15,517][__main__][INFO] - Train, Round 146: loss=0.0044, accuracy=1.0000, gradient_norm=0.1858, 
[2025-09-16 03:46:15,517][__main__][INFO] - Train, Round 147: loss=0.0043, accuracy=1.0000, gradient_norm=0.2057, 
[2025-09-16 03:46:15,517][__main__][INFO] - Train, Round 148: loss=0.0042, accuracy=1.0000, gradient_norm=0.1863, 
[2025-09-16 03:46:15,517][__main__][INFO] - Train, Round 149: loss=0.0041, accuracy=1.0000, gradient_norm=0.1963, 
[2025-09-16 03:46:15,517][__main__][INFO] - Train, Round 150: loss=0.0040, accuracy=1.0000, gradient_norm=0.1861, 
[2025-09-16 03:46:15,517][__main__][INFO] - Train, Round 151: loss=0.0040, accuracy=1.0000, gradient_norm=0.1790, 
[2025-09-16 03:46:15,518][__main__][INFO] - Train, Round 152: loss=0.0039, accuracy=1.0000, gradient_norm=0.1814, 
[2025-09-16 03:46:15,518][__main__][INFO] - Train, Round 153: loss=0.0038, accuracy=1.0000, gradient_norm=0.1750, 
[2025-09-16 03:46:15,518][__main__][INFO] - Train, Round 154: loss=0.0037, accuracy=1.0000, gradient_norm=0.1681, 
[2025-09-16 03:46:15,518][__main__][INFO] - Train, Round 155: loss=0.0037, accuracy=1.0000, gradient_norm=0.1719, 
[2025-09-16 03:46:15,518][__main__][INFO] - Train, Round 156: loss=0.0036, accuracy=1.0000, gradient_norm=0.1715, 
[2025-09-16 03:46:15,518][__main__][INFO] - Train, Round 157: loss=0.0035, accuracy=1.0000, gradient_norm=0.1689, 
[2025-09-16 03:46:15,518][__main__][INFO] - Train, Round 158: loss=0.0035, accuracy=1.0000, gradient_norm=0.1627, 
[2025-09-16 03:46:15,518][__main__][INFO] - Train, Round 159: loss=0.0034, accuracy=1.0000, gradient_norm=0.1556, 
[2025-09-16 03:46:15,518][__main__][INFO] - Train, Round 160: loss=0.0034, accuracy=1.0000, gradient_norm=0.1540, 
[2025-09-16 03:46:15,518][__main__][INFO] - Train, Round 161: loss=0.0033, accuracy=1.0000, gradient_norm=0.1596, 
[2025-09-16 03:46:15,518][__main__][INFO] - Train, Round 162: loss=0.0032, accuracy=1.0000, gradient_norm=0.1453, 
[2025-09-16 03:46:15,518][__main__][INFO] - Train, Round 163: loss=0.0032, accuracy=1.0000, gradient_norm=0.1459, 
[2025-09-16 03:46:15,518][__main__][INFO] - Train, Round 164: loss=0.0031, accuracy=1.0000, gradient_norm=0.1425, 
[2025-09-16 03:46:15,518][__main__][INFO] - Train, Round 165: loss=0.0031, accuracy=1.0000, gradient_norm=0.1382, 
[2025-09-16 03:46:15,518][__main__][INFO] - Train, Round 166: loss=0.0030, accuracy=1.0000, gradient_norm=0.1407, 
[2025-09-16 03:46:15,518][__main__][INFO] - Train, Round 167: loss=0.0030, accuracy=1.0000, gradient_norm=0.1393, 
[2025-09-16 03:46:15,518][__main__][INFO] - Train, Round 168: loss=0.0029, accuracy=1.0000, gradient_norm=0.1357, 
[2025-09-16 03:46:15,518][__main__][INFO] - Train, Round 169: loss=0.0029, accuracy=1.0000, gradient_norm=0.1353, 
[2025-09-16 03:46:15,518][__main__][INFO] - Train, Round 170: loss=0.0028, accuracy=1.0000, gradient_norm=0.1382, 
[2025-09-16 03:46:15,518][__main__][INFO] - Train, Round 171: loss=0.0028, accuracy=1.0000, gradient_norm=0.1331, 
[2025-09-16 03:46:15,518][__main__][INFO] - Train, Round 172: loss=0.0028, accuracy=1.0000, gradient_norm=0.1258, 
[2025-09-16 03:46:15,518][__main__][INFO] - Train, Round 173: loss=0.0027, accuracy=1.0000, gradient_norm=0.1321, 
[2025-09-16 03:46:15,518][__main__][INFO] - Train, Round 174: loss=0.0027, accuracy=1.0000, gradient_norm=0.1242, 
[2025-09-16 03:46:15,518][__main__][INFO] - Train, Round 175: loss=0.0026, accuracy=1.0000, gradient_norm=0.1245, 
[2025-09-16 03:46:15,518][__main__][INFO] - Train, Round 176: loss=0.0026, accuracy=1.0000, gradient_norm=0.1227, 
[2025-09-16 03:46:15,518][__main__][INFO] - Train, Round 177: loss=0.0026, accuracy=1.0000, gradient_norm=0.1188, 
[2025-09-16 03:46:15,518][__main__][INFO] - Train, Round 178: loss=0.0025, accuracy=1.0000, gradient_norm=0.1243, 
[2025-09-16 03:46:15,518][__main__][INFO] - Train, Round 179: loss=0.0025, accuracy=1.0000, gradient_norm=0.1228, 
[2025-09-16 03:46:15,518][__main__][INFO] - Train, Round 180: loss=0.0025, accuracy=1.0000, gradient_norm=0.1145, 
[2025-09-16 03:46:15,518][__main__][INFO] - Train, Round 181: loss=0.0024, accuracy=1.0000, gradient_norm=0.1177, 
[2025-09-16 03:46:15,518][__main__][INFO] - Train, Round 182: loss=0.0024, accuracy=1.0000, gradient_norm=0.1170, 
[2025-09-16 03:46:15,518][__main__][INFO] - Train, Round 183: loss=0.0024, accuracy=1.0000, gradient_norm=0.1108, 
[2025-09-16 03:46:15,518][__main__][INFO] - Train, Round 184: loss=0.0023, accuracy=1.0000, gradient_norm=0.1149, 
[2025-09-16 03:46:15,518][__main__][INFO] - Train, Round 185: loss=0.0023, accuracy=1.0000, gradient_norm=0.1110, 
[2025-09-16 03:46:15,518][__main__][INFO] - Train, Round 186: loss=0.0023, accuracy=1.0000, gradient_norm=0.1176, 
[2025-09-16 03:46:15,518][__main__][INFO] - Train, Round 187: loss=0.0023, accuracy=1.0000, gradient_norm=0.1066, 
[2025-09-16 03:46:15,518][__main__][INFO] - Train, Round 188: loss=0.0022, accuracy=1.0000, gradient_norm=0.1065, 
[2025-09-16 03:46:15,518][__main__][INFO] - Train, Round 189: loss=0.0022, accuracy=1.0000, gradient_norm=0.1066, 
[2025-09-16 03:46:15,518][__main__][INFO] - Train, Round 190: loss=0.0022, accuracy=1.0000, gradient_norm=0.1052, 
[2025-09-16 03:46:15,518][__main__][INFO] - Train, Round 191: loss=0.0021, accuracy=1.0000, gradient_norm=0.1040, 
[2025-09-16 03:46:15,518][__main__][INFO] - Train, Round 192: loss=0.0021, accuracy=1.0000, gradient_norm=0.1044, 
[2025-09-16 03:46:15,518][__main__][INFO] - Train, Round 193: loss=0.0021, accuracy=1.0000, gradient_norm=0.0979, 
[2025-09-16 03:46:15,518][__main__][INFO] - Train, Round 194: loss=0.0021, accuracy=1.0000, gradient_norm=0.1049, 
[2025-09-16 03:46:15,518][__main__][INFO] - Train, Round 195: loss=0.0020, accuracy=1.0000, gradient_norm=0.1044, 
[2025-09-16 03:46:15,518][__main__][INFO] - Train, Round 196: loss=0.0020, accuracy=1.0000, gradient_norm=0.1027, 
[2025-09-16 03:46:15,518][__main__][INFO] - Train, Round 197: loss=0.0020, accuracy=1.0000, gradient_norm=0.0963, 
[2025-09-16 03:46:15,518][__main__][INFO] - Train, Round 198: loss=0.0020, accuracy=1.0000, gradient_norm=0.0913, 
[2025-09-16 03:46:15,518][__main__][INFO] - Train, Round 199: loss=0.0020, accuracy=1.0000, gradient_norm=0.0961, 
[2025-09-16 03:46:15,518][__main__][INFO] - Train, Round 200: loss=0.0019, accuracy=1.0000, gradient_norm=0.0941, 
[2025-09-16 03:46:15,518][__main__][INFO] - Train, Round 201: loss=0.0019, accuracy=1.0000, gradient_norm=0.0960, 
[2025-09-16 03:46:15,518][__main__][INFO] - Train, Round 202: loss=0.0019, accuracy=1.0000, gradient_norm=0.0957, 
[2025-09-16 03:46:15,518][__main__][INFO] - Train, Round 203: loss=0.0019, accuracy=1.0000, gradient_norm=0.0921, 
[2025-09-16 03:46:15,518][__main__][INFO] - Train, Round 204: loss=0.0018, accuracy=1.0000, gradient_norm=0.0923, 
[2025-09-16 03:46:15,518][__main__][INFO] - Train, Round 205: loss=0.0018, accuracy=1.0000, gradient_norm=0.0898, 
[2025-09-16 03:46:15,518][__main__][INFO] - Train, Round 206: loss=0.0018, accuracy=1.0000, gradient_norm=0.0871, 
[2025-09-16 03:46:15,518][__main__][INFO] - Train, Round 207: loss=0.0018, accuracy=1.0000, gradient_norm=0.0865, 
[2025-09-16 03:46:15,518][__main__][INFO] - Train, Round 208: loss=0.0018, accuracy=1.0000, gradient_norm=0.0901, 
[2025-09-16 03:46:15,518][__main__][INFO] - Train, Round 209: loss=0.0018, accuracy=1.0000, gradient_norm=0.0900, 
[2025-09-16 03:46:15,518][__main__][INFO] - Train, Round 210: loss=0.0017, accuracy=1.0000, gradient_norm=0.0862, 
[2025-09-16 03:46:15,519][__main__][INFO] - Train, Round 211: loss=0.0017, accuracy=1.0000, gradient_norm=0.0837, 
[2025-09-16 03:46:15,519][__main__][INFO] - Train, Round 212: loss=0.0017, accuracy=1.0000, gradient_norm=0.0887, 
[2025-09-16 03:46:15,519][__main__][INFO] - Train, Round 213: loss=0.0017, accuracy=1.0000, gradient_norm=0.0852, 
[2025-09-16 03:46:15,519][__main__][INFO] - Train, Round 214: loss=0.0017, accuracy=1.0000, gradient_norm=0.0808, 
[2025-09-16 03:46:15,519][__main__][INFO] - Train, Round 215: loss=0.0017, accuracy=1.0000, gradient_norm=0.0859, 
[2025-09-16 03:46:15,519][__main__][INFO] - Train, Round 216: loss=0.0016, accuracy=1.0000, gradient_norm=0.0816, 
[2025-09-16 03:46:15,519][__main__][INFO] - Train, Round 217: loss=0.0016, accuracy=1.0000, gradient_norm=0.0818, 
[2025-09-16 03:46:15,519][__main__][INFO] - Train, Round 218: loss=0.0016, accuracy=1.0000, gradient_norm=0.0815, 
[2025-09-16 03:46:15,519][__main__][INFO] - Train, Round 219: loss=0.0016, accuracy=1.0000, gradient_norm=0.0780, 
[2025-09-16 03:46:15,519][__main__][INFO] - Train, Round 220: loss=0.0016, accuracy=1.0000, gradient_norm=0.0775, 
[2025-09-16 03:46:15,519][__main__][INFO] - Train, Round 221: loss=0.0016, accuracy=1.0000, gradient_norm=0.0781, 
[2025-09-16 03:46:15,519][__main__][INFO] - Train, Round 222: loss=0.0015, accuracy=1.0000, gradient_norm=0.0754, 
[2025-09-16 03:46:15,519][__main__][INFO] - Train, Round 223: loss=0.0015, accuracy=1.0000, gradient_norm=0.0744, 
[2025-09-16 03:46:15,519][__main__][INFO] - Train, Round 224: loss=0.0015, accuracy=1.0000, gradient_norm=0.0758, 
[2025-09-16 03:46:15,519][__main__][INFO] - Train, Round 225: loss=0.0015, accuracy=1.0000, gradient_norm=0.0780, 
[2025-09-16 03:46:15,519][__main__][INFO] - Train, Round 226: loss=0.0015, accuracy=1.0000, gradient_norm=0.0763, 
[2025-09-16 03:46:15,519][__main__][INFO] - Train, Round 227: loss=0.0015, accuracy=1.0000, gradient_norm=0.0761, 
[2025-09-16 03:46:15,519][__main__][INFO] - Train, Round 228: loss=0.0015, accuracy=1.0000, gradient_norm=0.0744, 
[2025-09-16 03:46:15,519][__main__][INFO] - Train, Round 229: loss=0.0015, accuracy=1.0000, gradient_norm=0.0705, 
[2025-09-16 03:46:15,519][__main__][INFO] - Train, Round 230: loss=0.0014, accuracy=1.0000, gradient_norm=0.0728, 
[2025-09-16 03:46:15,519][__main__][INFO] - Train, Round 231: loss=0.0014, accuracy=1.0000, gradient_norm=0.0741, 
[2025-09-16 03:46:15,519][__main__][INFO] - Train, Round 232: loss=0.0014, accuracy=1.0000, gradient_norm=0.0738, 
[2025-09-16 03:46:15,519][__main__][INFO] - Train, Round 233: loss=0.0014, accuracy=1.0000, gradient_norm=0.0716, 
[2025-09-16 03:46:15,519][__main__][INFO] - Train, Round 234: loss=0.0014, accuracy=1.0000, gradient_norm=0.0671, 
[2025-09-16 03:46:15,519][__main__][INFO] - Train, Round 235: loss=0.0014, accuracy=1.0000, gradient_norm=0.0710, 
[2025-09-16 03:46:15,519][__main__][INFO] - Train, Round 236: loss=0.0014, accuracy=1.0000, gradient_norm=0.0704, 
[2025-09-16 03:46:15,519][__main__][INFO] - Train, Round 237: loss=0.0014, accuracy=1.0000, gradient_norm=0.0721, 
[2025-09-16 03:46:15,519][__main__][INFO] - Train, Round 238: loss=0.0013, accuracy=1.0000, gradient_norm=0.0694, 
[2025-09-16 03:46:15,519][__main__][INFO] - Train, Round 239: loss=0.0013, accuracy=1.0000, gradient_norm=0.0661, 
[2025-09-16 03:46:15,519][__main__][INFO] - Train, Round 240: loss=0.0013, accuracy=1.0000, gradient_norm=0.0687, 
[2025-09-16 03:46:15,519][__main__][INFO] - Train, Round 241: loss=0.0013, accuracy=1.0000, gradient_norm=0.0649, 
[2025-09-16 03:46:15,519][__main__][INFO] - Train, Round 242: loss=0.0013, accuracy=1.0000, gradient_norm=0.0661, 
[2025-09-16 03:46:15,519][__main__][INFO] - Train, Round 243: loss=0.0013, accuracy=1.0000, gradient_norm=0.0641, 
[2025-09-16 03:46:15,519][__main__][INFO] - Train, Round 244: loss=0.0013, accuracy=1.0000, gradient_norm=0.0652, 
[2025-09-16 03:46:15,519][__main__][INFO] - Train, Round 245: loss=0.0013, accuracy=1.0000, gradient_norm=0.0662, 
[2025-09-16 03:46:15,519][__main__][INFO] - Train, Round 246: loss=0.0013, accuracy=1.0000, gradient_norm=0.0670, 
[2025-09-16 03:46:15,519][__main__][INFO] - Train, Round 247: loss=0.0013, accuracy=1.0000, gradient_norm=0.0658, 
[2025-09-16 03:46:15,519][__main__][INFO] - Train, Round 248: loss=0.0012, accuracy=1.0000, gradient_norm=0.0656, 
[2025-09-16 03:46:15,519][__main__][INFO] - Train, Round 249: loss=0.0012, accuracy=1.0000, gradient_norm=0.0643, 
[2025-09-16 03:46:15,519][__main__][INFO] - Train, Round 250: loss=0.0012, accuracy=1.0000, gradient_norm=0.0636, 
[2025-09-16 03:46:15,519][__main__][INFO] - Train, Round 251: loss=0.0012, accuracy=1.0000, gradient_norm=0.0644, 
[2025-09-16 03:46:15,519][__main__][INFO] - Train, Round 252: loss=0.0012, accuracy=1.0000, gradient_norm=0.0655, 
[2025-09-16 03:46:15,519][__main__][INFO] - Train, Round 253: loss=0.0012, accuracy=1.0000, gradient_norm=0.0621, 
[2025-09-16 03:46:15,519][__main__][INFO] - Train, Round 254: loss=0.0012, accuracy=1.0000, gradient_norm=0.0661, 
[2025-09-16 03:46:15,519][__main__][INFO] - Train, Round 255: loss=0.0012, accuracy=1.0000, gradient_norm=0.0609, 
[2025-09-16 03:46:15,519][__main__][INFO] - Train, Round 256: loss=0.0012, accuracy=1.0000, gradient_norm=0.0597, 
[2025-09-16 03:46:15,519][__main__][INFO] - Train, Round 257: loss=0.0012, accuracy=1.0000, gradient_norm=0.0608, 
[2025-09-16 03:46:15,519][__main__][INFO] - Train, Round 258: loss=0.0012, accuracy=1.0000, gradient_norm=0.0613, 
[2025-09-16 03:46:15,519][__main__][INFO] - Train, Round 259: loss=0.0011, accuracy=1.0000, gradient_norm=0.0600, 
[2025-09-16 03:46:15,519][__main__][INFO] - Train, Round 260: loss=0.0011, accuracy=1.0000, gradient_norm=0.0596, 
[2025-09-16 03:46:15,519][__main__][INFO] - Train, Round 261: loss=0.0011, accuracy=1.0000, gradient_norm=0.0617, 
[2025-09-16 03:46:15,519][__main__][INFO] - Train, Round 262: loss=0.0011, accuracy=1.0000, gradient_norm=0.0570, 
[2025-09-16 03:46:15,519][__main__][INFO] - Train, Round 263: loss=0.0011, accuracy=1.0000, gradient_norm=0.0612, 
[2025-09-16 03:46:15,519][__main__][INFO] - Train, Round 264: loss=0.0011, accuracy=1.0000, gradient_norm=0.0554, 
[2025-09-16 03:46:15,519][__main__][INFO] - Train, Round 265: loss=0.0011, accuracy=1.0000, gradient_norm=0.0590, 
[2025-09-16 03:46:15,519][__main__][INFO] - Train, Round 266: loss=0.0011, accuracy=1.0000, gradient_norm=0.0553, 
[2025-09-16 03:46:15,519][__main__][INFO] - Train, Round 267: loss=0.0011, accuracy=1.0000, gradient_norm=0.0571, 
[2025-09-16 03:46:15,519][__main__][INFO] - Train, Round 268: loss=0.0011, accuracy=1.0000, gradient_norm=0.0558, 
[2025-09-16 03:46:15,519][__main__][INFO] - Train, Round 269: loss=0.0011, accuracy=1.0000, gradient_norm=0.0535, 
[2025-09-16 03:46:15,519][__main__][INFO] - Train, Round 270: loss=0.0011, accuracy=1.0000, gradient_norm=0.0553, 
[2025-09-16 03:46:15,520][__main__][INFO] - Train, Round 271: loss=0.0011, accuracy=1.0000, gradient_norm=0.0563, 
[2025-09-16 03:46:15,520][__main__][INFO] - Train, Round 272: loss=0.0011, accuracy=1.0000, gradient_norm=0.0551, 
[2025-09-16 03:46:15,520][__main__][INFO] - Train, Round 273: loss=0.0010, accuracy=1.0000, gradient_norm=0.0550, 
[2025-09-16 03:46:15,520][__main__][INFO] - Train, Round 274: loss=0.0010, accuracy=1.0000, gradient_norm=0.0546, 
[2025-09-16 03:46:15,520][__main__][INFO] - Train, Round 275: loss=0.0010, accuracy=1.0000, gradient_norm=0.0542, 
[2025-09-16 03:46:15,520][__main__][INFO] - Train, Round 276: loss=0.0010, accuracy=1.0000, gradient_norm=0.0526, 
[2025-09-16 03:46:15,520][__main__][INFO] - Train, Round 277: loss=0.0010, accuracy=1.0000, gradient_norm=0.0517, 
[2025-09-16 03:46:15,520][__main__][INFO] - Train, Round 278: loss=0.0010, accuracy=1.0000, gradient_norm=0.0528, 
[2025-09-16 03:46:15,520][__main__][INFO] - Train, Round 279: loss=0.0010, accuracy=1.0000, gradient_norm=0.0522, 
[2025-09-16 03:46:15,520][__main__][INFO] - Train, Round 280: loss=0.0010, accuracy=1.0000, gradient_norm=0.0517, 
[2025-09-16 03:46:15,520][__main__][INFO] - Train, Round 281: loss=0.0010, accuracy=1.0000, gradient_norm=0.0526, 
[2025-09-16 03:46:15,520][__main__][INFO] - Train, Round 282: loss=0.0010, accuracy=1.0000, gradient_norm=0.0520, 
[2025-09-16 03:46:15,520][__main__][INFO] - Train, Round 283: loss=0.0010, accuracy=1.0000, gradient_norm=0.0511, 
[2025-09-16 03:46:15,520][__main__][INFO] - Train, Round 284: loss=0.0010, accuracy=1.0000, gradient_norm=0.0512, 
[2025-09-16 03:46:15,520][__main__][INFO] - Train, Round 285: loss=0.0010, accuracy=1.0000, gradient_norm=0.0506, 
[2025-09-16 03:46:15,520][__main__][INFO] - Train, Round 286: loss=0.0010, accuracy=1.0000, gradient_norm=0.0508, 
[2025-09-16 03:46:15,520][__main__][INFO] - Train, Round 287: loss=0.0010, accuracy=1.0000, gradient_norm=0.0519, 
[2025-09-16 03:46:15,520][__main__][INFO] - Train, Round 288: loss=0.0010, accuracy=1.0000, gradient_norm=0.0486, 
[2025-09-16 03:46:15,520][__main__][INFO] - Train, Round 289: loss=0.0009, accuracy=1.0000, gradient_norm=0.0495, 
[2025-09-16 03:46:15,520][__main__][INFO] - Train, Round 290: loss=0.0009, accuracy=1.0000, gradient_norm=0.0471, 
[2025-09-16 03:46:15,520][__main__][INFO] - Train, Round 291: loss=0.0009, accuracy=1.0000, gradient_norm=0.0503, 
[2025-09-16 03:46:15,520][__main__][INFO] - Train, Round 292: loss=0.0009, accuracy=1.0000, gradient_norm=0.0493, 
[2025-09-16 03:46:15,520][__main__][INFO] - Train, Round 293: loss=0.0009, accuracy=1.0000, gradient_norm=0.0492, 
[2025-09-16 03:46:15,520][__main__][INFO] - Train, Round 294: loss=0.0009, accuracy=1.0000, gradient_norm=0.0493, 
[2025-09-16 03:46:15,520][__main__][INFO] - Train, Round 295: loss=0.0009, accuracy=1.0000, gradient_norm=0.0481, 
[2025-09-16 03:46:15,520][__main__][INFO] - Train, Round 296: loss=0.0009, accuracy=1.0000, gradient_norm=0.0496, 
[2025-09-16 03:46:15,520][__main__][INFO] - Train, Round 297: loss=0.0009, accuracy=1.0000, gradient_norm=0.0478, 
[2025-09-16 03:46:15,520][__main__][INFO] - Train, Round 298: loss=0.0009, accuracy=1.0000, gradient_norm=0.0474, 
[2025-09-16 03:46:15,520][__main__][INFO] - Train, Round 299: loss=0.0009, accuracy=1.0000, gradient_norm=0.0488, 
[2025-09-16 03:46:15,520][__main__][INFO] - Train, Round 300: loss=0.0009, accuracy=1.0000, gradient_norm=0.0472, 
[2025-09-16 03:46:15,520][__main__][INFO] - Train, Round 301: loss=0.0009, accuracy=1.0000, gradient_norm=0.0449, 
[2025-09-16 03:46:15,520][__main__][INFO] - Train, Round 302: loss=0.0009, accuracy=1.0000, gradient_norm=0.0471, 
[2025-09-16 03:46:15,520][__main__][INFO] - Train, Round 303: loss=0.0009, accuracy=1.0000, gradient_norm=0.0454, 
[2025-09-16 03:46:15,520][__main__][INFO] - Train, Round 304: loss=0.0009, accuracy=1.0000, gradient_norm=0.0451, 
[2025-09-16 03:46:15,520][__main__][INFO] - Train, Round 305: loss=0.0009, accuracy=1.0000, gradient_norm=0.0452, 
[2025-09-16 03:46:15,520][__main__][INFO] - Train, Round 306: loss=0.0009, accuracy=1.0000, gradient_norm=0.0457, 
[2025-09-16 03:46:15,520][__main__][INFO] - Train, Round 307: loss=0.0009, accuracy=1.0000, gradient_norm=0.0484, 
[2025-09-16 03:46:15,520][__main__][INFO] - Train, Round 308: loss=0.0009, accuracy=1.0000, gradient_norm=0.0462, 
[2025-09-16 03:46:15,520][__main__][INFO] - Train, Round 309: loss=0.0008, accuracy=1.0000, gradient_norm=0.0475, 
[2025-09-16 03:46:15,520][__main__][INFO] - Train, Round 310: loss=0.0008, accuracy=1.0000, gradient_norm=0.0431, 
[2025-09-16 03:46:15,520][__main__][INFO] - Train, Round 311: loss=0.0008, accuracy=1.0000, gradient_norm=0.0453, 
[2025-09-16 03:46:15,520][__main__][INFO] - Train, Round 312: loss=0.0008, accuracy=1.0000, gradient_norm=0.0449, 
[2025-09-16 03:46:15,520][__main__][INFO] - Train, Round 313: loss=0.0008, accuracy=1.0000, gradient_norm=0.0444, 
[2025-09-16 03:46:15,520][__main__][INFO] - Train, Round 314: loss=0.0008, accuracy=1.0000, gradient_norm=0.0436, 
[2025-09-16 03:46:15,520][__main__][INFO] - Train, Round 315: loss=0.0008, accuracy=1.0000, gradient_norm=0.0436, 
[2025-09-16 03:46:15,520][__main__][INFO] - Train, Round 316: loss=0.0008, accuracy=1.0000, gradient_norm=0.0433, 
[2025-09-16 03:46:15,520][__main__][INFO] - Train, Round 317: loss=0.0008, accuracy=1.0000, gradient_norm=0.0436, 
[2025-09-16 03:46:15,520][__main__][INFO] - Train, Round 318: loss=0.0008, accuracy=1.0000, gradient_norm=0.0436, 
[2025-09-16 03:46:15,520][__main__][INFO] - Train, Round 319: loss=0.0008, accuracy=1.0000, gradient_norm=0.0422, 
[2025-09-16 03:46:15,520][__main__][INFO] - Train, Round 320: loss=0.0008, accuracy=1.0000, gradient_norm=0.0439, 
[2025-09-16 03:46:15,520][__main__][INFO] - Train, Round 321: loss=0.0008, accuracy=1.0000, gradient_norm=0.0423, 
[2025-09-16 03:46:15,520][__main__][INFO] - Train, Round 322: loss=0.0008, accuracy=1.0000, gradient_norm=0.0443, 
[2025-09-16 03:46:15,520][__main__][INFO] - Train, Round 323: loss=0.0008, accuracy=1.0000, gradient_norm=0.0424, 
[2025-09-16 03:46:15,520][__main__][INFO] - Train, Round 324: loss=0.0008, accuracy=1.0000, gradient_norm=0.0417, 
[2025-09-16 03:46:15,520][__main__][INFO] - Train, Round 325: loss=0.0008, accuracy=1.0000, gradient_norm=0.0426, 
[2025-09-16 03:46:15,520][__main__][INFO] - Train, Round 326: loss=0.0008, accuracy=1.0000, gradient_norm=0.0416, 
[2025-09-16 03:46:15,520][__main__][INFO] - Train, Round 327: loss=0.0008, accuracy=1.0000, gradient_norm=0.0428, 
[2025-09-16 03:46:15,520][__main__][INFO] - Train, Round 328: loss=0.0008, accuracy=1.0000, gradient_norm=0.0416, 
[2025-09-16 03:46:15,520][__main__][INFO] - Train, Round 329: loss=0.0008, accuracy=1.0000, gradient_norm=0.0406, 
[2025-09-16 03:46:15,520][__main__][INFO] - Train, Round 330: loss=0.0008, accuracy=1.0000, gradient_norm=0.0398, 
[2025-09-16 03:46:15,520][__main__][INFO] - Train, Round 331: loss=0.0008, accuracy=1.0000, gradient_norm=0.0408, 
[2025-09-16 03:46:15,520][__main__][INFO] - Train, Round 332: loss=0.0008, accuracy=1.0000, gradient_norm=0.0404, 
[2025-09-16 03:46:15,520][__main__][INFO] - Train, Round 333: loss=0.0007, accuracy=1.0000, gradient_norm=0.0391, 
[2025-09-16 03:46:15,520][__main__][INFO] - Train, Round 334: loss=0.0007, accuracy=1.0000, gradient_norm=0.0408, 
[2025-09-16 03:46:15,520][__main__][INFO] - Train, Round 335: loss=0.0007, accuracy=1.0000, gradient_norm=0.0406, 
[2025-09-16 03:46:15,520][__main__][INFO] - Train, Round 336: loss=0.0007, accuracy=1.0000, gradient_norm=0.0411, 
[2025-09-16 03:46:15,520][__main__][INFO] - Train, Round 337: loss=0.0007, accuracy=1.0000, gradient_norm=0.0408, 
[2025-09-16 03:46:15,520][__main__][INFO] - Train, Round 338: loss=0.0007, accuracy=1.0000, gradient_norm=0.0399, 
[2025-09-16 03:46:15,520][__main__][INFO] - Train, Round 339: loss=0.0007, accuracy=1.0000, gradient_norm=0.0391, 
[2025-09-16 03:46:15,521][__main__][INFO] - Train, Round 340: loss=0.0007, accuracy=1.0000, gradient_norm=0.0381, 
[2025-09-16 03:46:15,521][__main__][INFO] - Train, Round 341: loss=0.0007, accuracy=1.0000, gradient_norm=0.0388, 
[2025-09-16 03:46:15,521][__main__][INFO] - Train, Round 342: loss=0.0007, accuracy=1.0000, gradient_norm=0.0372, 
[2025-09-16 03:46:15,521][__main__][INFO] - Train, Round 343: loss=0.0007, accuracy=1.0000, gradient_norm=0.0398, 
[2025-09-16 03:46:15,521][__main__][INFO] - Train, Round 344: loss=0.0007, accuracy=1.0000, gradient_norm=0.0390, 
[2025-09-16 03:46:15,521][__main__][INFO] - Train, Round 345: loss=0.0007, accuracy=1.0000, gradient_norm=0.0382, 
[2025-09-16 03:46:15,521][__main__][INFO] - Train, Round 346: loss=0.0007, accuracy=1.0000, gradient_norm=0.0385, 
[2025-09-16 03:46:15,521][__main__][INFO] - Train, Round 347: loss=0.0007, accuracy=1.0000, gradient_norm=0.0390, 
[2025-09-16 03:46:15,521][__main__][INFO] - Train, Round 348: loss=0.0007, accuracy=1.0000, gradient_norm=0.0390, 
[2025-09-16 03:46:15,521][__main__][INFO] - Train, Round 349: loss=0.0007, accuracy=1.0000, gradient_norm=0.0379, 
[2025-09-16 03:46:15,521][__main__][INFO] - Train, Round 350: loss=0.0007, accuracy=1.0000, gradient_norm=0.0385, 
[2025-09-16 03:46:15,521][__main__][INFO] - Train, Round 351: loss=0.0007, accuracy=1.0000, gradient_norm=0.0379, 
[2025-09-16 03:46:15,521][__main__][INFO] - Train, Round 352: loss=0.0007, accuracy=1.0000, gradient_norm=0.0389, 
[2025-09-16 03:46:15,521][__main__][INFO] - Train, Round 353: loss=0.0007, accuracy=1.0000, gradient_norm=0.0401, 
[2025-09-16 03:46:15,521][__main__][INFO] - Train, Round 354: loss=0.0007, accuracy=1.0000, gradient_norm=0.0375, 
[2025-09-16 03:46:15,521][__main__][INFO] - Train, Round 355: loss=0.0007, accuracy=1.0000, gradient_norm=0.0379, 
[2025-09-16 03:46:15,521][__main__][INFO] - Train, Round 356: loss=0.0007, accuracy=1.0000, gradient_norm=0.0372, 
[2025-09-16 03:46:15,521][__main__][INFO] - Train, Round 357: loss=0.0007, accuracy=1.0000, gradient_norm=0.0369, 
[2025-09-16 03:46:15,521][__main__][INFO] - Train, Round 358: loss=0.0007, accuracy=1.0000, gradient_norm=0.0364, 
[2025-09-16 03:46:15,521][__main__][INFO] - Train, Round 359: loss=0.0007, accuracy=1.0000, gradient_norm=0.0368, 
[2025-09-16 03:46:15,521][__main__][INFO] - Train, Round 360: loss=0.0007, accuracy=1.0000, gradient_norm=0.0357, 
[2025-09-16 03:46:15,521][__main__][INFO] - Train, Round 361: loss=0.0007, accuracy=1.0000, gradient_norm=0.0359, 
[2025-09-16 03:46:15,521][__main__][INFO] - Train, Round 362: loss=0.0007, accuracy=1.0000, gradient_norm=0.0357, 
[2025-09-16 03:46:15,521][__main__][INFO] - Train, Round 363: loss=0.0007, accuracy=1.0000, gradient_norm=0.0366, 
[2025-09-16 03:46:15,521][__main__][INFO] - Train, Round 364: loss=0.0007, accuracy=1.0000, gradient_norm=0.0369, 
[2025-09-16 03:46:15,521][__main__][INFO] - Train, Round 365: loss=0.0007, accuracy=1.0000, gradient_norm=0.0373, 
[2025-09-16 03:46:15,521][__main__][INFO] - Train, Round 366: loss=0.0006, accuracy=1.0000, gradient_norm=0.0361, 
[2025-09-16 03:46:15,521][__main__][INFO] - Train, Round 367: loss=0.0006, accuracy=1.0000, gradient_norm=0.0368, 
[2025-09-16 03:46:15,521][__main__][INFO] - Train, Round 368: loss=0.0006, accuracy=1.0000, gradient_norm=0.0348, 
[2025-09-16 03:46:15,521][__main__][INFO] - Train, Round 369: loss=0.0006, accuracy=1.0000, gradient_norm=0.0349, 
[2025-09-16 03:46:15,521][__main__][INFO] - Train, Round 370: loss=0.0006, accuracy=1.0000, gradient_norm=0.0345, 
[2025-09-16 03:46:15,521][__main__][INFO] - Train, Round 371: loss=0.0006, accuracy=1.0000, gradient_norm=0.0349, 
[2025-09-16 03:46:15,521][__main__][INFO] - Train, Round 372: loss=0.0006, accuracy=1.0000, gradient_norm=0.0352, 
[2025-09-16 03:46:15,521][__main__][INFO] - Train, Round 373: loss=0.0006, accuracy=1.0000, gradient_norm=0.0349, 
[2025-09-16 03:46:15,521][__main__][INFO] - Train, Round 374: loss=0.0006, accuracy=1.0000, gradient_norm=0.0351, 
[2025-09-16 03:46:15,521][__main__][INFO] - Train, Round 375: loss=0.0006, accuracy=1.0000, gradient_norm=0.0350, 
[2025-09-16 03:46:15,521][__main__][INFO] - Train, Round 376: loss=0.0006, accuracy=1.0000, gradient_norm=0.0341, 
[2025-09-16 03:46:15,521][__main__][INFO] - Train, Round 377: loss=0.0006, accuracy=1.0000, gradient_norm=0.0351, 
[2025-09-16 03:46:15,521][__main__][INFO] - Train, Round 378: loss=0.0006, accuracy=1.0000, gradient_norm=0.0326, 
[2025-09-16 03:46:15,521][__main__][INFO] - Train, Round 379: loss=0.0006, accuracy=1.0000, gradient_norm=0.0336, 
[2025-09-16 03:46:15,521][__main__][INFO] - Train, Round 380: loss=0.0006, accuracy=1.0000, gradient_norm=0.0349, 
[2025-09-16 03:46:15,521][__main__][INFO] - Train, Round 381: loss=0.0006, accuracy=1.0000, gradient_norm=0.0346, 
[2025-09-16 03:46:15,521][__main__][INFO] - Train, Round 382: loss=0.0006, accuracy=1.0000, gradient_norm=0.0341, 
[2025-09-16 03:46:15,521][__main__][INFO] - Train, Round 383: loss=0.0006, accuracy=1.0000, gradient_norm=0.0337, 
[2025-09-16 03:46:15,521][__main__][INFO] - Train, Round 384: loss=0.0006, accuracy=1.0000, gradient_norm=0.0340, 
[2025-09-16 03:46:15,521][__main__][INFO] - Train, Round 385: loss=0.0006, accuracy=1.0000, gradient_norm=0.0337, 
[2025-09-16 03:46:15,521][__main__][INFO] - Train, Round 386: loss=0.0006, accuracy=1.0000, gradient_norm=0.0335, 
[2025-09-16 03:46:15,521][__main__][INFO] - Train, Round 387: loss=0.0006, accuracy=1.0000, gradient_norm=0.0335, 
[2025-09-16 03:46:15,521][__main__][INFO] - Train, Round 388: loss=0.0006, accuracy=1.0000, gradient_norm=0.0343, 
[2025-09-16 03:46:15,521][__main__][INFO] - Train, Round 389: loss=0.0006, accuracy=1.0000, gradient_norm=0.0338, 
[2025-09-16 03:46:15,521][__main__][INFO] - Train, Round 390: loss=0.0006, accuracy=1.0000, gradient_norm=0.0324, 
[2025-09-16 03:46:15,521][__main__][INFO] - Train, Round 391: loss=0.0006, accuracy=1.0000, gradient_norm=0.0329, 
[2025-09-16 03:46:15,521][__main__][INFO] - Train, Round 392: loss=0.0006, accuracy=1.0000, gradient_norm=0.0334, 
[2025-09-16 03:46:15,521][__main__][INFO] - Train, Round 393: loss=0.0006, accuracy=1.0000, gradient_norm=0.0316, 
[2025-09-16 03:46:15,521][__main__][INFO] - Train, Round 394: loss=0.0006, accuracy=1.0000, gradient_norm=0.0316, 
[2025-09-16 03:46:15,521][__main__][INFO] - Train, Round 395: loss=0.0006, accuracy=1.0000, gradient_norm=0.0328, 
[2025-09-16 03:46:15,521][__main__][INFO] - Train, Round 396: loss=0.0006, accuracy=1.0000, gradient_norm=0.0323, 
[2025-09-16 03:46:15,521][__main__][INFO] - Train, Round 397: loss=0.0006, accuracy=1.0000, gradient_norm=0.0331, 
[2025-09-16 03:46:15,521][__main__][INFO] - Train, Round 398: loss=0.0006, accuracy=1.0000, gradient_norm=0.0300, 
[2025-09-16 03:46:15,521][__main__][INFO] - Train, Round 399: loss=0.0006, accuracy=1.0000, gradient_norm=0.0325, 
[2025-09-16 03:46:15,521][__main__][INFO] - Train, Round 400: loss=0.0006, accuracy=1.0000, gradient_norm=0.0314, 
[2025-09-16 03:46:15,521][__main__][INFO] - Train, Round 401: loss=0.0006, accuracy=1.0000, gradient_norm=0.0315, 
[2025-09-16 03:46:15,521][__main__][INFO] - Train, Round 402: loss=0.0006, accuracy=1.0000, gradient_norm=0.0308, 
[2025-09-16 03:46:15,521][__main__][INFO] - Train, Round 403: loss=0.0006, accuracy=1.0000, gradient_norm=0.0320, 
[2025-09-16 03:46:15,521][__main__][INFO] - Train, Round 404: loss=0.0006, accuracy=1.0000, gradient_norm=0.0315, 
[2025-09-16 03:46:15,521][__main__][INFO] - Train, Round 405: loss=0.0006, accuracy=1.0000, gradient_norm=0.0320, 
[2025-09-16 03:46:15,521][__main__][INFO] - Train, Round 406: loss=0.0006, accuracy=1.0000, gradient_norm=0.0305, 
[2025-09-16 03:46:15,521][__main__][INFO] - Train, Round 407: loss=0.0006, accuracy=1.0000, gradient_norm=0.0310, 
[2025-09-16 03:46:15,521][__main__][INFO] - Train, Round 408: loss=0.0006, accuracy=1.0000, gradient_norm=0.0317, 
[2025-09-16 03:46:15,521][__main__][INFO] - Train, Round 409: loss=0.0006, accuracy=1.0000, gradient_norm=0.0314, 
[2025-09-16 03:46:15,521][__main__][INFO] - Train, Round 410: loss=0.0005, accuracy=1.0000, gradient_norm=0.0308, 
[2025-09-16 03:46:15,521][__main__][INFO] - Train, Round 411: loss=0.0005, accuracy=1.0000, gradient_norm=0.0310, 
[2025-09-16 03:46:15,521][__main__][INFO] - Train, Round 412: loss=0.0005, accuracy=1.0000, gradient_norm=0.0306, 
[2025-09-16 03:46:15,521][__main__][INFO] - Train, Round 413: loss=0.0005, accuracy=1.0000, gradient_norm=0.0315, 
[2025-09-16 03:46:15,521][__main__][INFO] - Train, Round 414: loss=0.0005, accuracy=1.0000, gradient_norm=0.0288, 
[2025-09-16 03:46:15,521][__main__][INFO] - Train, Round 415: loss=0.0005, accuracy=1.0000, gradient_norm=0.0308, 
[2025-09-16 03:46:15,521][__main__][INFO] - Train, Round 416: loss=0.0005, accuracy=1.0000, gradient_norm=0.0292, 
[2025-09-16 03:46:15,521][__main__][INFO] - Train, Round 417: loss=0.0005, accuracy=1.0000, gradient_norm=0.0308, 
[2025-09-16 03:46:15,521][__main__][INFO] - Train, Round 418: loss=0.0005, accuracy=1.0000, gradient_norm=0.0300, 
[2025-09-16 03:46:15,522][__main__][INFO] - Train, Round 419: loss=0.0005, accuracy=1.0000, gradient_norm=0.0301, 
[2025-09-16 03:46:15,522][__main__][INFO] - Train, Round 420: loss=0.0005, accuracy=1.0000, gradient_norm=0.0286, 
[2025-09-16 03:46:15,522][__main__][INFO] - Train, Round 421: loss=0.0005, accuracy=1.0000, gradient_norm=0.0292, 
[2025-09-16 03:46:15,522][__main__][INFO] - Train, Round 422: loss=0.0005, accuracy=1.0000, gradient_norm=0.0291, 
[2025-09-16 03:46:15,522][__main__][INFO] - Train, Round 423: loss=0.0005, accuracy=1.0000, gradient_norm=0.0299, 
[2025-09-16 03:46:15,522][__main__][INFO] - Train, Round 424: loss=0.0005, accuracy=1.0000, gradient_norm=0.0297, 
[2025-09-16 03:46:15,522][__main__][INFO] - Train, Round 425: loss=0.0005, accuracy=1.0000, gradient_norm=0.0298, 
[2025-09-16 03:46:15,522][__main__][INFO] - Train, Round 426: loss=0.0005, accuracy=1.0000, gradient_norm=0.0288, 
[2025-09-16 03:46:15,522][__main__][INFO] - Train, Round 427: loss=0.0005, accuracy=1.0000, gradient_norm=0.0287, 
[2025-09-16 03:46:15,522][__main__][INFO] - Train, Round 428: loss=0.0005, accuracy=1.0000, gradient_norm=0.0287, 
[2025-09-16 03:46:15,522][__main__][INFO] - Train, Round 429: loss=0.0005, accuracy=1.0000, gradient_norm=0.0286, 
[2025-09-16 03:46:15,522][__main__][INFO] - Train, Round 430: loss=0.0005, accuracy=1.0000, gradient_norm=0.0287, 
[2025-09-16 03:46:15,522][__main__][INFO] - Train, Round 431: loss=0.0005, accuracy=1.0000, gradient_norm=0.0284, 
[2025-09-16 03:46:15,522][__main__][INFO] - Train, Round 432: loss=0.0005, accuracy=1.0000, gradient_norm=0.0291, 
[2025-09-16 03:46:15,522][__main__][INFO] - Train, Round 433: loss=0.0005, accuracy=1.0000, gradient_norm=0.0284, 
[2025-09-16 03:46:15,522][__main__][INFO] - Train, Round 434: loss=0.0005, accuracy=1.0000, gradient_norm=0.0294, 
[2025-09-16 03:46:15,522][__main__][INFO] - Train, Round 435: loss=0.0005, accuracy=1.0000, gradient_norm=0.0283, 
[2025-09-16 03:46:15,522][__main__][INFO] - Train, Round 436: loss=0.0005, accuracy=1.0000, gradient_norm=0.0280, 
[2025-09-16 03:46:15,522][__main__][INFO] - Train, Round 437: loss=0.0005, accuracy=1.0000, gradient_norm=0.0285, 
[2025-09-16 03:46:15,522][__main__][INFO] - Train, Round 438: loss=0.0005, accuracy=1.0000, gradient_norm=0.0276, 
[2025-09-16 03:46:15,522][__main__][INFO] - Train, Round 439: loss=0.0005, accuracy=1.0000, gradient_norm=0.0272, 
[2025-09-16 03:46:15,522][__main__][INFO] - Train, Round 440: loss=0.0005, accuracy=1.0000, gradient_norm=0.0284, 
[2025-09-16 03:46:15,522][__main__][INFO] - Train, Round 441: loss=0.0005, accuracy=1.0000, gradient_norm=0.0275, 
[2025-09-16 03:46:15,522][__main__][INFO] - Train, Round 442: loss=0.0005, accuracy=1.0000, gradient_norm=0.0286, 
[2025-09-16 03:46:15,522][__main__][INFO] - Train, Round 443: loss=0.0005, accuracy=1.0000, gradient_norm=0.0285, 
[2025-09-16 03:46:15,522][__main__][INFO] - Train, Round 444: loss=0.0005, accuracy=1.0000, gradient_norm=0.0271, 
[2025-09-16 03:46:15,522][__main__][INFO] - Train, Round 445: loss=0.0005, accuracy=1.0000, gradient_norm=0.0281, 
[2025-09-16 03:46:15,522][__main__][INFO] - Train, Round 446: loss=0.0005, accuracy=1.0000, gradient_norm=0.0285, 
[2025-09-16 03:46:15,522][__main__][INFO] - Train, Round 447: loss=0.0005, accuracy=1.0000, gradient_norm=0.0268, 
[2025-09-16 03:46:15,522][__main__][INFO] - Train, Round 448: loss=0.0005, accuracy=1.0000, gradient_norm=0.0272, 
[2025-09-16 03:46:15,522][__main__][INFO] - Train, Round 449: loss=0.0005, accuracy=1.0000, gradient_norm=0.0269, 
[2025-09-16 03:46:15,522][__main__][INFO] - Train, Round 450: loss=0.0005, accuracy=1.0000, gradient_norm=0.0270, 
[2025-09-16 03:46:15,522][__main__][INFO] - Train, Round 451: loss=0.0005, accuracy=1.0000, gradient_norm=0.0265, 
[2025-09-16 03:46:15,522][__main__][INFO] - Train, Round 452: loss=0.0005, accuracy=1.0000, gradient_norm=0.0271, 
[2025-09-16 03:46:15,522][__main__][INFO] - Train, Round 453: loss=0.0005, accuracy=1.0000, gradient_norm=0.0267, 
[2025-09-16 03:46:15,522][__main__][INFO] - Train, Round 454: loss=0.0005, accuracy=1.0000, gradient_norm=0.0274, 
[2025-09-16 03:46:15,522][__main__][INFO] - Train, Round 455: loss=0.0005, accuracy=1.0000, gradient_norm=0.0269, 
[2025-09-16 03:46:15,522][__main__][INFO] - Train, Round 456: loss=0.0005, accuracy=1.0000, gradient_norm=0.0265, 
[2025-09-16 03:46:15,522][__main__][INFO] - Train, Round 457: loss=0.0005, accuracy=1.0000, gradient_norm=0.0272, 
[2025-09-16 03:46:15,522][__main__][INFO] - Train, Round 458: loss=0.0005, accuracy=1.0000, gradient_norm=0.0262, 
[2025-09-16 03:46:15,522][__main__][INFO] - Train, Round 459: loss=0.0005, accuracy=1.0000, gradient_norm=0.0258, 
[2025-09-16 03:46:15,522][__main__][INFO] - Train, Round 460: loss=0.0005, accuracy=1.0000, gradient_norm=0.0272, 
[2025-09-16 03:46:15,522][__main__][INFO] - Train, Round 461: loss=0.0005, accuracy=1.0000, gradient_norm=0.0259, 
[2025-09-16 03:46:15,522][__main__][INFO] - Train, Round 462: loss=0.0005, accuracy=1.0000, gradient_norm=0.0264, 
[2025-09-16 03:46:15,522][__main__][INFO] - Train, Round 463: loss=0.0005, accuracy=1.0000, gradient_norm=0.0258, 
[2025-09-16 03:46:15,522][__main__][INFO] - Train, Round 464: loss=0.0005, accuracy=1.0000, gradient_norm=0.0256, 
[2025-09-16 03:46:15,522][__main__][INFO] - Train, Round 465: loss=0.0005, accuracy=1.0000, gradient_norm=0.0259, 
[2025-09-16 03:46:15,522][__main__][INFO] - Train, Round 466: loss=0.0005, accuracy=1.0000, gradient_norm=0.0264, 
[2025-09-16 03:46:15,522][__main__][INFO] - Train, Round 467: loss=0.0005, accuracy=1.0000, gradient_norm=0.0264, 
[2025-09-16 03:46:15,522][__main__][INFO] - Train, Round 468: loss=0.0005, accuracy=1.0000, gradient_norm=0.0253, 
[2025-09-16 03:46:15,522][__main__][INFO] - Train, Round 469: loss=0.0005, accuracy=1.0000, gradient_norm=0.0271, 
[2025-09-16 03:46:15,522][__main__][INFO] - Train, Round 470: loss=0.0005, accuracy=1.0000, gradient_norm=0.0255, 
[2025-09-16 03:46:15,522][__main__][INFO] - Train, Round 471: loss=0.0005, accuracy=1.0000, gradient_norm=0.0262, 
[2025-09-16 03:46:15,522][__main__][INFO] - Train, Round 472: loss=0.0005, accuracy=1.0000, gradient_norm=0.0268, 
[2025-09-16 03:46:15,522][__main__][INFO] - Train, Round 473: loss=0.0005, accuracy=1.0000, gradient_norm=0.0258, 
[2025-09-16 03:46:15,522][__main__][INFO] - Train, Round 474: loss=0.0005, accuracy=1.0000, gradient_norm=0.0254, 
[2025-09-16 03:46:15,522][__main__][INFO] - Train, Round 475: loss=0.0004, accuracy=1.0000, gradient_norm=0.0256, 
[2025-09-16 03:46:15,522][__main__][INFO] - Train, Round 476: loss=0.0004, accuracy=1.0000, gradient_norm=0.0262, 
[2025-09-16 03:46:15,522][__main__][INFO] - Train, Round 477: loss=0.0004, accuracy=1.0000, gradient_norm=0.0255, 
[2025-09-16 03:46:15,522][__main__][INFO] - Train, Round 478: loss=0.0004, accuracy=1.0000, gradient_norm=0.0254, 
[2025-09-16 03:46:15,522][__main__][INFO] - Train, Round 479: loss=0.0004, accuracy=1.0000, gradient_norm=0.0242, 
[2025-09-16 03:46:15,522][__main__][INFO] - Train, Round 480: loss=0.0004, accuracy=1.0000, gradient_norm=0.0250, 
[2025-09-16 03:46:15,522][__main__][INFO] - Train, Round 481: loss=0.0004, accuracy=1.0000, gradient_norm=0.0254, 
[2025-09-16 03:46:15,522][__main__][INFO] - Train, Round 482: loss=0.0004, accuracy=1.0000, gradient_norm=0.0254, 
[2025-09-16 03:46:15,522][__main__][INFO] - Train, Round 483: loss=0.0004, accuracy=1.0000, gradient_norm=0.0251, 
[2025-09-16 03:46:15,522][__main__][INFO] - Train, Round 484: loss=0.0004, accuracy=1.0000, gradient_norm=0.0254, 
[2025-09-16 03:46:15,522][__main__][INFO] - Train, Round 485: loss=0.0004, accuracy=1.0000, gradient_norm=0.0241, 
[2025-09-16 03:46:15,522][__main__][INFO] - Train, Round 486: loss=0.0004, accuracy=1.0000, gradient_norm=0.0237, 
[2025-09-16 03:46:15,522][__main__][INFO] - Train, Round 487: loss=0.0004, accuracy=1.0000, gradient_norm=0.0248, 
[2025-09-16 03:46:15,522][__main__][INFO] - Train, Round 488: loss=0.0004, accuracy=1.0000, gradient_norm=0.0244, 
[2025-09-16 03:46:15,522][__main__][INFO] - Train, Round 489: loss=0.0004, accuracy=1.0000, gradient_norm=0.0249, 
[2025-09-16 03:46:15,522][__main__][INFO] - Train, Round 490: loss=0.0004, accuracy=1.0000, gradient_norm=0.0235, 
[2025-09-16 03:46:15,522][__main__][INFO] - Train, Round 491: loss=0.0004, accuracy=1.0000, gradient_norm=0.0243, 
[2025-09-16 03:46:15,522][__main__][INFO] - Train, Round 492: loss=0.0004, accuracy=1.0000, gradient_norm=0.0251, 
[2025-09-16 03:46:15,522][__main__][INFO] - Train, Round 493: loss=0.0004, accuracy=1.0000, gradient_norm=0.0244, 
[2025-09-16 03:46:15,522][__main__][INFO] - Train, Round 494: loss=0.0004, accuracy=1.0000, gradient_norm=0.0244, 
[2025-09-16 03:46:15,522][__main__][INFO] - Train, Round 495: loss=0.0004, accuracy=1.0000, gradient_norm=0.0244, 
[2025-09-16 03:46:15,522][__main__][INFO] - Train, Round 496: loss=0.0004, accuracy=1.0000, gradient_norm=0.0246, 
[2025-09-16 03:46:15,522][__main__][INFO] - Train, Round 497: loss=0.0004, accuracy=1.0000, gradient_norm=0.0247, 
[2025-09-16 03:46:15,523][__main__][INFO] - Train, Round 498: loss=0.0004, accuracy=1.0000, gradient_norm=0.0253, 
[2025-09-16 03:46:15,523][__main__][INFO] - Train, Round 499: loss=0.0004, accuracy=1.0000, gradient_norm=0.0244, 
[2025-09-16 03:46:15,523][__main__][INFO] - Train, Round 500: loss=0.0004, accuracy=1.0000, gradient_norm=0.0236, 
[2025-09-16 03:46:15,523][__main__][INFO] - Test, Round 001: loss=2.3024, accuracy=0.1311, 
[2025-09-16 03:46:15,523][__main__][INFO] - Test, Round 002: loss=2.3003, accuracy=0.1346, 
[2025-09-16 03:46:15,523][__main__][INFO] - Test, Round 003: loss=2.2981, accuracy=0.1325, 
[2025-09-16 03:46:15,523][__main__][INFO] - Test, Round 004: loss=2.2959, accuracy=0.1352, 
[2025-09-16 03:46:15,523][__main__][INFO] - Test, Round 005: loss=2.2936, accuracy=0.1368, 
[2025-09-16 03:46:15,523][__main__][INFO] - Test, Round 006: loss=2.2911, accuracy=0.1386, 
[2025-09-16 03:46:15,523][__main__][INFO] - Test, Round 007: loss=2.2882, accuracy=0.1437, 
[2025-09-16 03:46:15,523][__main__][INFO] - Test, Round 008: loss=2.2849, accuracy=0.1475, 
[2025-09-16 03:46:15,523][__main__][INFO] - Test, Round 009: loss=2.2809, accuracy=0.1535, 
[2025-09-16 03:46:15,523][__main__][INFO] - Test, Round 010: loss=2.2762, accuracy=0.1559, 
[2025-09-16 03:46:15,523][__main__][INFO] - Test, Round 011: loss=2.2703, accuracy=0.1609, 
[2025-09-16 03:46:15,523][__main__][INFO] - Test, Round 012: loss=2.2631, accuracy=0.1687, 
[2025-09-16 03:46:15,523][__main__][INFO] - Test, Round 013: loss=2.2543, accuracy=0.1799, 
[2025-09-16 03:46:15,523][__main__][INFO] - Test, Round 014: loss=2.2431, accuracy=0.1872, 
[2025-09-16 03:46:15,523][__main__][INFO] - Test, Round 015: loss=2.2289, accuracy=0.1923, 
[2025-09-16 03:46:15,523][__main__][INFO] - Test, Round 016: loss=2.2117, accuracy=0.2008, 
[2025-09-16 03:46:15,523][__main__][INFO] - Test, Round 017: loss=2.1915, accuracy=0.2116, 
[2025-09-16 03:46:15,523][__main__][INFO] - Test, Round 018: loss=2.1690, accuracy=0.2146, 
[2025-09-16 03:46:15,523][__main__][INFO] - Test, Round 019: loss=2.1471, accuracy=0.2259, 
[2025-09-16 03:46:15,523][__main__][INFO] - Test, Round 020: loss=2.1263, accuracy=0.2297, 
[2025-09-16 03:46:15,523][__main__][INFO] - Test, Round 021: loss=2.1081, accuracy=0.2380, 
[2025-09-16 03:46:15,523][__main__][INFO] - Test, Round 022: loss=2.0887, accuracy=0.2487, 
[2025-09-16 03:46:15,523][__main__][INFO] - Test, Round 023: loss=2.0732, accuracy=0.2556, 
[2025-09-16 03:46:15,523][__main__][INFO] - Test, Round 024: loss=2.0551, accuracy=0.2665, 
[2025-09-16 03:46:15,523][__main__][INFO] - Test, Round 025: loss=2.0381, accuracy=0.2758, 
[2025-09-16 03:46:15,523][__main__][INFO] - Test, Round 026: loss=2.0225, accuracy=0.2750, 
[2025-09-16 03:46:15,523][__main__][INFO] - Test, Round 027: loss=2.0098, accuracy=0.2765, 
[2025-09-16 03:46:15,523][__main__][INFO] - Test, Round 028: loss=1.9938, accuracy=0.2795, 
[2025-09-16 03:46:15,523][__main__][INFO] - Test, Round 029: loss=1.9838, accuracy=0.2871, 
[2025-09-16 03:46:15,523][__main__][INFO] - Test, Round 030: loss=1.9691, accuracy=0.2933, 
[2025-09-16 03:46:15,523][__main__][INFO] - Test, Round 031: loss=1.9750, accuracy=0.2849, 
[2025-09-16 03:46:15,523][__main__][INFO] - Test, Round 032: loss=1.9840, accuracy=0.2854, 
[2025-09-16 03:46:15,523][__main__][INFO] - Test, Round 033: loss=2.0116, accuracy=0.2917, 
[2025-09-16 03:46:15,523][__main__][INFO] - Test, Round 034: loss=1.9539, accuracy=0.2999, 
[2025-09-16 03:46:15,523][__main__][INFO] - Test, Round 035: loss=1.9791, accuracy=0.2976, 
[2025-09-16 03:46:15,523][__main__][INFO] - Test, Round 036: loss=1.9705, accuracy=0.2934, 
[2025-09-16 03:46:15,523][__main__][INFO] - Test, Round 037: loss=1.9252, accuracy=0.3055, 
[2025-09-16 03:46:15,523][__main__][INFO] - Test, Round 038: loss=1.9269, accuracy=0.3045, 
[2025-09-16 03:46:15,523][__main__][INFO] - Test, Round 039: loss=1.9316, accuracy=0.3126, 
[2025-09-16 03:46:15,523][__main__][INFO] - Test, Round 040: loss=1.9240, accuracy=0.3107, 
[2025-09-16 03:46:15,523][__main__][INFO] - Test, Round 041: loss=1.9190, accuracy=0.3212, 
[2025-09-16 03:46:15,523][__main__][INFO] - Test, Round 042: loss=1.9942, accuracy=0.3061, 
[2025-09-16 03:46:15,523][__main__][INFO] - Test, Round 043: loss=2.0619, accuracy=0.2875, 
[2025-09-16 03:46:15,523][__main__][INFO] - Test, Round 044: loss=1.8943, accuracy=0.3261, 
[2025-09-16 03:46:15,523][__main__][INFO] - Test, Round 045: loss=2.0147, accuracy=0.3044, 
[2025-09-16 03:46:15,523][__main__][INFO] - Test, Round 046: loss=1.9327, accuracy=0.3199, 
[2025-09-16 03:46:15,523][__main__][INFO] - Test, Round 047: loss=1.8866, accuracy=0.3355, 
[2025-09-16 03:46:15,523][__main__][INFO] - Test, Round 048: loss=1.9506, accuracy=0.3233, 
[2025-09-16 03:46:15,523][__main__][INFO] - Test, Round 049: loss=1.8991, accuracy=0.3359, 
[2025-09-16 03:46:15,523][__main__][INFO] - Test, Round 050: loss=1.9496, accuracy=0.3285, 
[2025-09-16 03:46:15,523][__main__][INFO] - Test, Round 051: loss=2.0542, accuracy=0.3132, 
[2025-09-16 03:46:15,523][__main__][INFO] - Test, Round 052: loss=1.9814, accuracy=0.3306, 
[2025-09-16 03:46:15,523][__main__][INFO] - Test, Round 053: loss=2.0266, accuracy=0.3259, 
[2025-09-16 03:46:15,523][__main__][INFO] - Test, Round 054: loss=2.1517, accuracy=0.3176, 
[2025-09-16 03:46:15,523][__main__][INFO] - Test, Round 055: loss=2.0337, accuracy=0.3227, 
[2025-09-16 03:46:15,523][__main__][INFO] - Test, Round 056: loss=2.0267, accuracy=0.3334, 
[2025-09-16 03:46:15,523][__main__][INFO] - Test, Round 057: loss=2.0861, accuracy=0.3277, 
[2025-09-16 03:46:15,523][__main__][INFO] - Test, Round 058: loss=2.0837, accuracy=0.3317, 
[2025-09-16 03:46:15,523][__main__][INFO] - Test, Round 059: loss=2.2180, accuracy=0.3071, 
[2025-09-16 03:46:15,523][__main__][INFO] - Test, Round 060: loss=2.1159, accuracy=0.3304, 
[2025-09-16 03:46:15,523][__main__][INFO] - Test, Round 061: loss=2.1056, accuracy=0.3237, 
[2025-09-16 03:46:15,523][__main__][INFO] - Test, Round 062: loss=2.1974, accuracy=0.3273, 
[2025-09-16 03:46:15,523][__main__][INFO] - Test, Round 063: loss=2.6963, accuracy=0.2730, 
[2025-09-16 03:46:15,523][__main__][INFO] - Test, Round 064: loss=2.2252, accuracy=0.3285, 
[2025-09-16 03:46:15,523][__main__][INFO] - Test, Round 065: loss=2.2732, accuracy=0.3321, 
[2025-09-16 03:46:15,523][__main__][INFO] - Test, Round 066: loss=2.5635, accuracy=0.2922, 
[2025-09-16 03:46:15,523][__main__][INFO] - Test, Round 067: loss=2.6366, accuracy=0.2973, 
[2025-09-16 03:46:15,523][__main__][INFO] - Test, Round 068: loss=2.3464, accuracy=0.3180, 
[2025-09-16 03:46:15,523][__main__][INFO] - Test, Round 069: loss=2.4054, accuracy=0.3237, 
[2025-09-16 03:46:15,523][__main__][INFO] - Test, Round 070: loss=2.5989, accuracy=0.3215, 
[2025-09-16 03:46:15,523][__main__][INFO] - Test, Round 071: loss=2.4459, accuracy=0.3342, 
[2025-09-16 03:46:15,523][__main__][INFO] - Test, Round 072: loss=2.6438, accuracy=0.3337, 
[2025-09-16 03:46:15,523][__main__][INFO] - Test, Round 073: loss=2.9587, accuracy=0.3036, 
[2025-09-16 03:46:15,523][__main__][INFO] - Test, Round 074: loss=3.2242, accuracy=0.2968, 
[2025-09-16 03:46:15,523][__main__][INFO] - Test, Round 075: loss=2.7377, accuracy=0.3388, 
[2025-09-16 03:46:15,523][__main__][INFO] - Test, Round 076: loss=2.9164, accuracy=0.3169, 
[2025-09-16 03:46:15,523][__main__][INFO] - Test, Round 077: loss=2.7297, accuracy=0.3332, 
[2025-09-16 03:46:15,523][__main__][INFO] - Test, Round 078: loss=2.7179, accuracy=0.3363, 
[2025-09-16 03:46:15,524][__main__][INFO] - Test, Round 079: loss=2.9055, accuracy=0.3315, 
[2025-09-16 03:46:15,524][__main__][INFO] - Test, Round 080: loss=2.9130, accuracy=0.3465, 
[2025-09-16 03:46:15,524][__main__][INFO] - Test, Round 081: loss=2.9654, accuracy=0.3429, 
[2025-09-16 03:46:15,524][__main__][INFO] - Test, Round 082: loss=3.0438, accuracy=0.3243, 
[2025-09-16 03:46:15,524][__main__][INFO] - Test, Round 083: loss=3.0628, accuracy=0.3393, 
[2025-09-16 03:46:15,524][__main__][INFO] - Test, Round 084: loss=3.2871, accuracy=0.3370, 
[2025-09-16 03:46:15,524][__main__][INFO] - Test, Round 085: loss=3.2053, accuracy=0.3393, 
[2025-09-16 03:46:15,524][__main__][INFO] - Test, Round 086: loss=3.1306, accuracy=0.3444, 
[2025-09-16 03:46:15,524][__main__][INFO] - Test, Round 087: loss=3.3059, accuracy=0.3393, 
[2025-09-16 03:46:15,524][__main__][INFO] - Test, Round 088: loss=3.2383, accuracy=0.3541, 
[2025-09-16 03:46:15,524][__main__][INFO] - Test, Round 089: loss=3.2954, accuracy=0.3463, 
[2025-09-16 03:46:15,524][__main__][INFO] - Test, Round 090: loss=3.3872, accuracy=0.3440, 
[2025-09-16 03:46:15,524][__main__][INFO] - Test, Round 091: loss=3.4370, accuracy=0.3542, 
[2025-09-16 03:46:15,524][__main__][INFO] - Test, Round 092: loss=3.4812, accuracy=0.3502, 
[2025-09-16 03:46:15,524][__main__][INFO] - Test, Round 093: loss=3.5738, accuracy=0.3443, 
[2025-09-16 03:46:15,524][__main__][INFO] - Test, Round 094: loss=3.6349, accuracy=0.3489, 
[2025-09-16 03:46:15,524][__main__][INFO] - Test, Round 095: loss=3.6263, accuracy=0.3435, 
[2025-09-16 03:46:15,524][__main__][INFO] - Test, Round 096: loss=3.6758, accuracy=0.3454, 
[2025-09-16 03:46:15,524][__main__][INFO] - Test, Round 097: loss=3.7293, accuracy=0.3502, 
[2025-09-16 03:46:15,524][__main__][INFO] - Test, Round 098: loss=3.7471, accuracy=0.3512, 
[2025-09-16 03:46:15,524][__main__][INFO] - Test, Round 099: loss=3.8435, accuracy=0.3518, 
[2025-09-16 03:46:15,524][__main__][INFO] - Test, Round 100: loss=3.8290, accuracy=0.3511, 
[2025-09-16 03:46:15,524][__main__][INFO] - Test, Round 101: loss=3.8877, accuracy=0.3485, 
[2025-09-16 03:46:15,524][__main__][INFO] - Test, Round 102: loss=3.9043, accuracy=0.3481, 
[2025-09-16 03:46:15,524][__main__][INFO] - Test, Round 103: loss=3.9444, accuracy=0.3462, 
[2025-09-16 03:46:15,524][__main__][INFO] - Test, Round 104: loss=3.9879, accuracy=0.3467, 
[2025-09-16 03:46:15,524][__main__][INFO] - Test, Round 105: loss=4.0240, accuracy=0.3455, 
[2025-09-16 03:46:15,524][__main__][INFO] - Test, Round 106: loss=4.0638, accuracy=0.3493, 
[2025-09-16 03:46:15,524][__main__][INFO] - Test, Round 107: loss=4.0942, accuracy=0.3478, 
[2025-09-16 03:46:15,524][__main__][INFO] - Test, Round 108: loss=4.1216, accuracy=0.3472, 
[2025-09-16 03:46:15,524][__main__][INFO] - Test, Round 109: loss=4.1588, accuracy=0.3491, 
[2025-09-16 03:46:15,524][__main__][INFO] - Test, Round 110: loss=4.1850, accuracy=0.3509, 
[2025-09-16 03:46:15,524][__main__][INFO] - Test, Round 111: loss=4.2128, accuracy=0.3480, 
[2025-09-16 03:46:15,524][__main__][INFO] - Test, Round 112: loss=4.2503, accuracy=0.3448, 
[2025-09-16 03:46:15,524][__main__][INFO] - Test, Round 113: loss=4.2667, accuracy=0.3469, 
[2025-09-16 03:46:15,524][__main__][INFO] - Test, Round 114: loss=4.2937, accuracy=0.3474, 
[2025-09-16 03:46:15,524][__main__][INFO] - Test, Round 115: loss=4.3178, accuracy=0.3493, 
[2025-09-16 03:46:15,524][__main__][INFO] - Test, Round 116: loss=4.3453, accuracy=0.3469, 
[2025-09-16 03:46:15,524][__main__][INFO] - Test, Round 117: loss=4.3700, accuracy=0.3482, 
[2025-09-16 03:46:15,524][__main__][INFO] - Test, Round 118: loss=4.3904, accuracy=0.3482, 
[2025-09-16 03:46:15,524][__main__][INFO] - Test, Round 119: loss=4.4176, accuracy=0.3468, 
[2025-09-16 03:46:15,524][__main__][INFO] - Test, Round 120: loss=4.4397, accuracy=0.3500, 
[2025-09-16 03:46:15,524][__main__][INFO] - Test, Round 121: loss=4.4626, accuracy=0.3464, 
[2025-09-16 03:46:15,524][__main__][INFO] - Test, Round 122: loss=4.4806, accuracy=0.3478, 
[2025-09-16 03:46:15,524][__main__][INFO] - Test, Round 123: loss=4.5017, accuracy=0.3472, 
[2025-09-16 03:46:15,524][__main__][INFO] - Test, Round 124: loss=4.5201, accuracy=0.3482, 
[2025-09-16 03:46:15,524][__main__][INFO] - Test, Round 125: loss=4.5398, accuracy=0.3482, 
[2025-09-16 03:46:15,524][__main__][INFO] - Test, Round 126: loss=4.5571, accuracy=0.3487, 
[2025-09-16 03:46:15,524][__main__][INFO] - Test, Round 127: loss=4.5764, accuracy=0.3477, 
[2025-09-16 03:46:15,524][__main__][INFO] - Test, Round 128: loss=4.5937, accuracy=0.3477, 
[2025-09-16 03:46:15,524][__main__][INFO] - Test, Round 129: loss=4.6127, accuracy=0.3470, 
[2025-09-16 03:46:15,524][__main__][INFO] - Test, Round 130: loss=4.6284, accuracy=0.3479, 
[2025-09-16 03:46:15,524][__main__][INFO] - Test, Round 131: loss=4.6474, accuracy=0.3472, 
[2025-09-16 03:46:15,524][__main__][INFO] - Test, Round 132: loss=4.6596, accuracy=0.3462, 
[2025-09-16 03:46:15,524][__main__][INFO] - Test, Round 133: loss=4.6792, accuracy=0.3489, 
[2025-09-16 03:46:15,524][__main__][INFO] - Test, Round 134: loss=4.6949, accuracy=0.3485, 
[2025-09-16 03:46:15,524][__main__][INFO] - Test, Round 135: loss=4.7080, accuracy=0.3481, 
[2025-09-16 03:46:15,524][__main__][INFO] - Test, Round 136: loss=4.7231, accuracy=0.3464, 
[2025-09-16 03:46:15,524][__main__][INFO] - Test, Round 137: loss=4.7398, accuracy=0.3472, 
[2025-09-16 03:46:15,524][__main__][INFO] - Test, Round 138: loss=4.7559, accuracy=0.3488, 
[2025-09-16 03:46:15,524][__main__][INFO] - Test, Round 139: loss=4.7686, accuracy=0.3476, 
[2025-09-16 03:46:15,524][__main__][INFO] - Test, Round 140: loss=4.7845, accuracy=0.3478, 
[2025-09-16 03:46:15,524][__main__][INFO] - Test, Round 141: loss=4.7998, accuracy=0.3476, 
[2025-09-16 03:46:15,524][__main__][INFO] - Test, Round 142: loss=4.8135, accuracy=0.3469, 
[2025-09-16 03:46:15,524][__main__][INFO] - Test, Round 143: loss=4.8261, accuracy=0.3486, 
[2025-09-16 03:46:15,524][__main__][INFO] - Test, Round 144: loss=4.8384, accuracy=0.3476, 
[2025-09-16 03:46:15,524][__main__][INFO] - Test, Round 145: loss=4.8511, accuracy=0.3480, 
[2025-09-16 03:46:15,524][__main__][INFO] - Test, Round 146: loss=4.8629, accuracy=0.3475, 
[2025-09-16 03:46:15,524][__main__][INFO] - Test, Round 147: loss=4.8757, accuracy=0.3478, 
[2025-09-16 03:46:15,524][__main__][INFO] - Test, Round 148: loss=4.8894, accuracy=0.3477, 
[2025-09-16 03:46:15,524][__main__][INFO] - Test, Round 149: loss=4.9020, accuracy=0.3465, 
[2025-09-16 03:46:15,524][__main__][INFO] - Test, Round 150: loss=4.9114, accuracy=0.3475, 
[2025-09-16 03:46:15,524][__main__][INFO] - Test, Round 151: loss=4.9229, accuracy=0.3486, 
[2025-09-16 03:46:15,524][__main__][INFO] - Test, Round 152: loss=4.9370, accuracy=0.3483, 
[2025-09-16 03:46:15,524][__main__][INFO] - Test, Round 153: loss=4.9479, accuracy=0.3470, 
[2025-09-16 03:46:15,524][__main__][INFO] - Test, Round 154: loss=4.9594, accuracy=0.3473, 
[2025-09-16 03:46:15,524][__main__][INFO] - Test, Round 155: loss=4.9705, accuracy=0.3477, 
[2025-09-16 03:46:15,524][__main__][INFO] - Test, Round 156: loss=4.9793, accuracy=0.3479, 
[2025-09-16 03:46:15,524][__main__][INFO] - Test, Round 157: loss=4.9915, accuracy=0.3478, 
[2025-09-16 03:46:15,524][__main__][INFO] - Test, Round 158: loss=5.0006, accuracy=0.3482, 
[2025-09-16 03:46:15,524][__main__][INFO] - Test, Round 159: loss=5.0124, accuracy=0.3474, 
[2025-09-16 03:46:15,524][__main__][INFO] - Test, Round 160: loss=5.0260, accuracy=0.3471, 
[2025-09-16 03:46:15,525][__main__][INFO] - Test, Round 161: loss=5.0334, accuracy=0.3477, 
[2025-09-16 03:46:15,525][__main__][INFO] - Test, Round 162: loss=5.0438, accuracy=0.3481, 
[2025-09-16 03:46:15,525][__main__][INFO] - Test, Round 163: loss=5.0538, accuracy=0.3475, 
[2025-09-16 03:46:15,525][__main__][INFO] - Test, Round 164: loss=5.0628, accuracy=0.3481, 
[2025-09-16 03:46:15,525][__main__][INFO] - Test, Round 165: loss=5.0730, accuracy=0.3486, 
[2025-09-16 03:46:15,525][__main__][INFO] - Test, Round 166: loss=5.0822, accuracy=0.3475, 
[2025-09-16 03:46:15,525][__main__][INFO] - Test, Round 167: loss=5.0920, accuracy=0.3468, 
[2025-09-16 03:46:15,525][__main__][INFO] - Test, Round 168: loss=5.1004, accuracy=0.3472, 
[2025-09-16 03:46:15,525][__main__][INFO] - Test, Round 169: loss=5.1107, accuracy=0.3474, 
[2025-09-16 03:46:15,525][__main__][INFO] - Test, Round 170: loss=5.1199, accuracy=0.3477, 
[2025-09-16 03:46:15,525][__main__][INFO] - Test, Round 171: loss=5.1286, accuracy=0.3467, 
[2025-09-16 03:46:15,525][__main__][INFO] - Test, Round 172: loss=5.1369, accuracy=0.3465, 
[2025-09-16 03:46:15,525][__main__][INFO] - Test, Round 173: loss=5.1461, accuracy=0.3475, 
[2025-09-16 03:46:15,525][__main__][INFO] - Test, Round 174: loss=5.1559, accuracy=0.3479, 
[2025-09-16 03:46:15,525][__main__][INFO] - Test, Round 175: loss=5.1624, accuracy=0.3481, 
[2025-09-16 03:46:15,525][__main__][INFO] - Test, Round 176: loss=5.1707, accuracy=0.3474, 
[2025-09-16 03:46:15,525][__main__][INFO] - Test, Round 177: loss=5.1800, accuracy=0.3475, 
[2025-09-16 03:46:15,525][__main__][INFO] - Test, Round 178: loss=5.1863, accuracy=0.3474, 
[2025-09-16 03:46:15,525][__main__][INFO] - Test, Round 179: loss=5.1971, accuracy=0.3468, 
[2025-09-16 03:46:15,525][__main__][INFO] - Test, Round 180: loss=5.2045, accuracy=0.3464, 
[2025-09-16 03:46:15,525][__main__][INFO] - Test, Round 181: loss=5.2130, accuracy=0.3473, 
[2025-09-16 03:46:15,525][__main__][INFO] - Test, Round 182: loss=5.2204, accuracy=0.3469, 
[2025-09-16 03:46:15,525][__main__][INFO] - Test, Round 183: loss=5.2292, accuracy=0.3474, 
[2025-09-16 03:46:15,525][__main__][INFO] - Test, Round 184: loss=5.2344, accuracy=0.3470, 
[2025-09-16 03:46:15,525][__main__][INFO] - Test, Round 185: loss=5.2424, accuracy=0.3469, 
[2025-09-16 03:46:15,525][__main__][INFO] - Test, Round 186: loss=5.2505, accuracy=0.3474, 
[2025-09-16 03:46:15,525][__main__][INFO] - Test, Round 187: loss=5.2590, accuracy=0.3471, 
[2025-09-16 03:46:15,525][__main__][INFO] - Test, Round 188: loss=5.2651, accuracy=0.3470, 
[2025-09-16 03:46:15,525][__main__][INFO] - Test, Round 189: loss=5.2741, accuracy=0.3465, 
[2025-09-16 03:46:15,525][__main__][INFO] - Test, Round 190: loss=5.2798, accuracy=0.3474, 
[2025-09-16 03:46:15,525][__main__][INFO] - Test, Round 191: loss=5.2872, accuracy=0.3466, 
[2025-09-16 03:46:15,525][__main__][INFO] - Test, Round 192: loss=5.2946, accuracy=0.3475, 
[2025-09-16 03:46:15,525][__main__][INFO] - Test, Round 193: loss=5.3019, accuracy=0.3470, 
[2025-09-16 03:46:15,525][__main__][INFO] - Test, Round 194: loss=5.3084, accuracy=0.3462, 
[2025-09-16 03:46:15,525][__main__][INFO] - Test, Round 195: loss=5.3166, accuracy=0.3474, 
[2025-09-16 03:46:15,525][__main__][INFO] - Test, Round 196: loss=5.3223, accuracy=0.3466, 
[2025-09-16 03:46:15,525][__main__][INFO] - Test, Round 197: loss=5.3296, accuracy=0.3466, 
[2025-09-16 03:46:15,525][__main__][INFO] - Test, Round 198: loss=5.3357, accuracy=0.3467, 
[2025-09-16 03:46:15,525][__main__][INFO] - Test, Round 199: loss=5.3420, accuracy=0.3464, 
[2025-09-16 03:46:15,525][__main__][INFO] - Test, Round 200: loss=5.3496, accuracy=0.3470, 
[2025-09-16 03:46:15,525][__main__][INFO] - Test, Round 201: loss=5.3553, accuracy=0.3467, 
[2025-09-16 03:46:15,525][__main__][INFO] - Test, Round 202: loss=5.3621, accuracy=0.3459, 
[2025-09-16 03:46:15,525][__main__][INFO] - Test, Round 203: loss=5.3690, accuracy=0.3477, 
[2025-09-16 03:46:15,525][__main__][INFO] - Test, Round 204: loss=5.3763, accuracy=0.3466, 
[2025-09-16 03:46:15,525][__main__][INFO] - Test, Round 205: loss=5.3823, accuracy=0.3471, 
[2025-09-16 03:46:15,525][__main__][INFO] - Test, Round 206: loss=5.3877, accuracy=0.3463, 
[2025-09-16 03:46:15,525][__main__][INFO] - Test, Round 207: loss=5.3932, accuracy=0.3472, 
[2025-09-16 03:46:15,525][__main__][INFO] - Test, Round 208: loss=5.4007, accuracy=0.3465, 
[2025-09-16 03:46:15,525][__main__][INFO] - Test, Round 209: loss=5.4065, accuracy=0.3466, 
[2025-09-16 03:46:15,525][__main__][INFO] - Test, Round 210: loss=5.4122, accuracy=0.3470, 
[2025-09-16 03:46:15,525][__main__][INFO] - Test, Round 211: loss=5.4179, accuracy=0.3467, 
[2025-09-16 03:46:15,525][__main__][INFO] - Test, Round 212: loss=5.4244, accuracy=0.3464, 
[2025-09-16 03:46:15,525][__main__][INFO] - Test, Round 213: loss=5.4294, accuracy=0.3469, 
[2025-09-16 03:46:15,525][__main__][INFO] - Test, Round 214: loss=5.4358, accuracy=0.3467, 
[2025-09-16 03:46:15,525][__main__][INFO] - Test, Round 215: loss=5.4409, accuracy=0.3465, 
[2025-09-16 03:46:15,525][__main__][INFO] - Test, Round 216: loss=5.4473, accuracy=0.3459, 
[2025-09-16 03:46:15,525][__main__][INFO] - Test, Round 217: loss=5.4530, accuracy=0.3454, 
[2025-09-16 03:46:15,525][__main__][INFO] - Test, Round 218: loss=5.4599, accuracy=0.3463, 
[2025-09-16 03:46:15,525][__main__][INFO] - Test, Round 219: loss=5.4648, accuracy=0.3465, 
[2025-09-16 03:46:15,525][__main__][INFO] - Test, Round 220: loss=5.4697, accuracy=0.3470, 
[2025-09-16 03:46:15,525][__main__][INFO] - Test, Round 221: loss=5.4749, accuracy=0.3457, 
[2025-09-16 03:46:15,525][__main__][INFO] - Test, Round 222: loss=5.4795, accuracy=0.3462, 
[2025-09-16 03:46:15,525][__main__][INFO] - Test, Round 223: loss=5.4857, accuracy=0.3464, 
[2025-09-16 03:46:15,525][__main__][INFO] - Test, Round 224: loss=5.4902, accuracy=0.3461, 
[2025-09-16 03:46:15,525][__main__][INFO] - Test, Round 225: loss=5.4961, accuracy=0.3460, 
[2025-09-16 03:46:15,525][__main__][INFO] - Test, Round 226: loss=5.5006, accuracy=0.3466, 
[2025-09-16 03:46:15,525][__main__][INFO] - Test, Round 227: loss=5.5069, accuracy=0.3451, 
[2025-09-16 03:46:15,525][__main__][INFO] - Test, Round 228: loss=5.5128, accuracy=0.3459, 
[2025-09-16 03:46:15,525][__main__][INFO] - Test, Round 229: loss=5.5183, accuracy=0.3460, 
[2025-09-16 03:46:15,525][__main__][INFO] - Test, Round 230: loss=5.5234, accuracy=0.3465, 
[2025-09-16 03:46:15,525][__main__][INFO] - Test, Round 231: loss=5.5282, accuracy=0.3457, 
[2025-09-16 03:46:15,525][__main__][INFO] - Test, Round 232: loss=5.5318, accuracy=0.3454, 
[2025-09-16 03:46:15,525][__main__][INFO] - Test, Round 233: loss=5.5383, accuracy=0.3458, 
[2025-09-16 03:46:15,525][__main__][INFO] - Test, Round 234: loss=5.5427, accuracy=0.3460, 
[2025-09-16 03:46:15,525][__main__][INFO] - Test, Round 235: loss=5.5483, accuracy=0.3455, 
[2025-09-16 03:46:15,525][__main__][INFO] - Test, Round 236: loss=5.5531, accuracy=0.3456, 
[2025-09-16 03:46:15,525][__main__][INFO] - Test, Round 237: loss=5.5574, accuracy=0.3463, 
[2025-09-16 03:46:15,525][__main__][INFO] - Test, Round 238: loss=5.5613, accuracy=0.3457, 
[2025-09-16 03:46:15,525][__main__][INFO] - Test, Round 239: loss=5.5672, accuracy=0.3457, 
[2025-09-16 03:46:15,525][__main__][INFO] - Test, Round 240: loss=5.5729, accuracy=0.3462, 
[2025-09-16 03:46:15,525][__main__][INFO] - Test, Round 241: loss=5.5768, accuracy=0.3454, 
[2025-09-16 03:46:15,525][__main__][INFO] - Test, Round 242: loss=5.5819, accuracy=0.3456, 
[2025-09-16 03:46:15,525][__main__][INFO] - Test, Round 243: loss=5.5856, accuracy=0.3456, 
[2025-09-16 03:46:15,525][__main__][INFO] - Test, Round 244: loss=5.5907, accuracy=0.3460, 
[2025-09-16 03:46:15,526][__main__][INFO] - Test, Round 245: loss=5.5957, accuracy=0.3457, 
[2025-09-16 03:46:15,526][__main__][INFO] - Test, Round 246: loss=5.5998, accuracy=0.3455, 
[2025-09-16 03:46:15,526][__main__][INFO] - Test, Round 247: loss=5.6041, accuracy=0.3458, 
[2025-09-16 03:46:15,526][__main__][INFO] - Test, Round 248: loss=5.6091, accuracy=0.3461, 
[2025-09-16 03:46:15,526][__main__][INFO] - Test, Round 249: loss=5.6132, accuracy=0.3451, 
[2025-09-16 03:46:15,526][__main__][INFO] - Test, Round 250: loss=5.6171, accuracy=0.3460, 
[2025-09-16 03:46:15,526][__main__][INFO] - Test, Round 251: loss=5.6212, accuracy=0.3463, 
[2025-09-16 03:46:15,526][__main__][INFO] - Test, Round 252: loss=5.6255, accuracy=0.3457, 
[2025-09-16 03:46:15,526][__main__][INFO] - Test, Round 253: loss=5.6306, accuracy=0.3456, 
[2025-09-16 03:46:15,526][__main__][INFO] - Test, Round 254: loss=5.6351, accuracy=0.3452, 
[2025-09-16 03:46:15,526][__main__][INFO] - Test, Round 255: loss=5.6394, accuracy=0.3457, 
[2025-09-16 03:46:15,526][__main__][INFO] - Test, Round 256: loss=5.6430, accuracy=0.3454, 
[2025-09-16 03:46:15,526][__main__][INFO] - Test, Round 257: loss=5.6477, accuracy=0.3460, 
[2025-09-16 03:46:15,526][__main__][INFO] - Test, Round 258: loss=5.6528, accuracy=0.3457, 
[2025-09-16 03:46:15,526][__main__][INFO] - Test, Round 259: loss=5.6566, accuracy=0.3453, 
[2025-09-16 03:46:15,526][__main__][INFO] - Test, Round 260: loss=5.6601, accuracy=0.3454, 
[2025-09-16 03:46:15,526][__main__][INFO] - Test, Round 261: loss=5.6644, accuracy=0.3451, 
[2025-09-16 03:46:15,526][__main__][INFO] - Test, Round 262: loss=5.6682, accuracy=0.3452, 
[2025-09-16 03:46:15,526][__main__][INFO] - Test, Round 263: loss=5.6733, accuracy=0.3454, 
[2025-09-16 03:46:15,526][__main__][INFO] - Test, Round 264: loss=5.6766, accuracy=0.3450, 
[2025-09-16 03:46:15,526][__main__][INFO] - Test, Round 265: loss=5.6808, accuracy=0.3453, 
[2025-09-16 03:46:15,526][__main__][INFO] - Test, Round 266: loss=5.6844, accuracy=0.3451, 
[2025-09-16 03:46:15,526][__main__][INFO] - Test, Round 267: loss=5.6886, accuracy=0.3453, 
[2025-09-16 03:46:15,526][__main__][INFO] - Test, Round 268: loss=5.6930, accuracy=0.3453, 
[2025-09-16 03:46:15,526][__main__][INFO] - Test, Round 269: loss=5.6964, accuracy=0.3456, 
[2025-09-16 03:46:15,526][__main__][INFO] - Test, Round 270: loss=5.7014, accuracy=0.3452, 
[2025-09-16 03:46:15,526][__main__][INFO] - Test, Round 271: loss=5.7055, accuracy=0.3454, 
[2025-09-16 03:46:15,526][__main__][INFO] - Test, Round 272: loss=5.7092, accuracy=0.3454, 
[2025-09-16 03:46:15,526][__main__][INFO] - Test, Round 273: loss=5.7126, accuracy=0.3453, 
[2025-09-16 03:46:15,526][__main__][INFO] - Test, Round 274: loss=5.7155, accuracy=0.3451, 
[2025-09-16 03:46:15,526][__main__][INFO] - Test, Round 275: loss=5.7197, accuracy=0.3453, 
[2025-09-16 03:46:15,526][__main__][INFO] - Test, Round 276: loss=5.7237, accuracy=0.3455, 
[2025-09-16 03:46:15,526][__main__][INFO] - Test, Round 277: loss=5.7271, accuracy=0.3452, 
[2025-09-16 03:46:15,526][__main__][INFO] - Test, Round 278: loss=5.7315, accuracy=0.3455, 
[2025-09-16 03:46:15,526][__main__][INFO] - Test, Round 279: loss=5.7341, accuracy=0.3452, 
[2025-09-16 03:46:15,526][__main__][INFO] - Test, Round 280: loss=5.7380, accuracy=0.3447, 
[2025-09-16 03:46:15,526][__main__][INFO] - Test, Round 281: loss=5.7421, accuracy=0.3450, 
[2025-09-16 03:46:15,526][__main__][INFO] - Test, Round 282: loss=5.7458, accuracy=0.3450, 
[2025-09-16 03:46:15,526][__main__][INFO] - Test, Round 283: loss=5.7496, accuracy=0.3447, 
[2025-09-16 03:46:15,526][__main__][INFO] - Test, Round 284: loss=5.7531, accuracy=0.3454, 
[2025-09-16 03:46:15,526][__main__][INFO] - Test, Round 285: loss=5.7563, accuracy=0.3448, 
[2025-09-16 03:46:15,526][__main__][INFO] - Test, Round 286: loss=5.7600, accuracy=0.3446, 
[2025-09-16 03:46:15,526][__main__][INFO] - Test, Round 287: loss=5.7636, accuracy=0.3453, 
[2025-09-16 03:46:15,526][__main__][INFO] - Test, Round 288: loss=5.7671, accuracy=0.3449, 
[2025-09-16 03:46:15,526][__main__][INFO] - Test, Round 289: loss=5.7713, accuracy=0.3452, 
[2025-09-16 03:46:15,526][__main__][INFO] - Test, Round 290: loss=5.7729, accuracy=0.3451, 
[2025-09-16 03:46:15,526][__main__][INFO] - Test, Round 291: loss=5.7769, accuracy=0.3454, 
[2025-09-16 03:46:15,526][__main__][INFO] - Test, Round 292: loss=5.7809, accuracy=0.3450, 
[2025-09-16 03:46:15,526][__main__][INFO] - Test, Round 293: loss=5.7841, accuracy=0.3448, 
[2025-09-16 03:46:15,526][__main__][INFO] - Test, Round 294: loss=5.7875, accuracy=0.3450, 
[2025-09-16 03:46:15,526][__main__][INFO] - Test, Round 295: loss=5.7905, accuracy=0.3449, 
[2025-09-16 03:46:15,526][__main__][INFO] - Test, Round 296: loss=5.7945, accuracy=0.3449, 
[2025-09-16 03:46:15,526][__main__][INFO] - Test, Round 297: loss=5.7977, accuracy=0.3452, 
[2025-09-16 03:46:15,526][__main__][INFO] - Test, Round 298: loss=5.8011, accuracy=0.3451, 
[2025-09-16 03:46:15,526][__main__][INFO] - Test, Round 299: loss=5.8036, accuracy=0.3448, 
[2025-09-16 03:46:15,526][__main__][INFO] - Test, Round 300: loss=5.8075, accuracy=0.3449, 
[2025-09-16 03:46:15,526][__main__][INFO] - Test, Round 301: loss=5.8104, accuracy=0.3449, 
[2025-09-16 03:46:15,526][__main__][INFO] - Test, Round 302: loss=5.8139, accuracy=0.3451, 
[2025-09-16 03:46:15,526][__main__][INFO] - Test, Round 303: loss=5.8166, accuracy=0.3455, 
[2025-09-16 03:46:15,526][__main__][INFO] - Test, Round 304: loss=5.8204, accuracy=0.3450, 
[2025-09-16 03:46:15,526][__main__][INFO] - Test, Round 305: loss=5.8235, accuracy=0.3448, 
[2025-09-16 03:46:15,526][__main__][INFO] - Test, Round 306: loss=5.8267, accuracy=0.3452, 
[2025-09-16 03:46:15,526][__main__][INFO] - Test, Round 307: loss=5.8303, accuracy=0.3450, 
[2025-09-16 03:46:15,526][__main__][INFO] - Test, Round 308: loss=5.8327, accuracy=0.3453, 
[2025-09-16 03:46:15,526][__main__][INFO] - Test, Round 309: loss=5.8367, accuracy=0.3449, 
[2025-09-16 03:46:15,526][__main__][INFO] - Test, Round 310: loss=5.8402, accuracy=0.3452, 
[2025-09-16 03:46:15,526][__main__][INFO] - Test, Round 311: loss=5.8424, accuracy=0.3448, 
[2025-09-16 03:46:15,526][__main__][INFO] - Test, Round 312: loss=5.8460, accuracy=0.3451, 
[2025-09-16 03:46:15,526][__main__][INFO] - Test, Round 313: loss=5.8492, accuracy=0.3451, 
[2025-09-16 03:46:15,526][__main__][INFO] - Test, Round 314: loss=5.8517, accuracy=0.3450, 
[2025-09-16 03:46:15,526][__main__][INFO] - Test, Round 315: loss=5.8554, accuracy=0.3450, 
[2025-09-16 03:46:15,526][__main__][INFO] - Test, Round 316: loss=5.8584, accuracy=0.3452, 
[2025-09-16 03:46:15,526][__main__][INFO] - Test, Round 317: loss=5.8617, accuracy=0.3449, 
[2025-09-16 03:46:15,526][__main__][INFO] - Test, Round 318: loss=5.8645, accuracy=0.3452, 
[2025-09-16 03:46:15,526][__main__][INFO] - Test, Round 319: loss=5.8670, accuracy=0.3449, 
[2025-09-16 03:46:15,526][__main__][INFO] - Test, Round 320: loss=5.8697, accuracy=0.3449, 
[2025-09-16 03:46:15,526][__main__][INFO] - Test, Round 321: loss=5.8731, accuracy=0.3450, 
[2025-09-16 03:46:15,526][__main__][INFO] - Test, Round 322: loss=5.8762, accuracy=0.3450, 
[2025-09-16 03:46:15,526][__main__][INFO] - Test, Round 323: loss=5.8791, accuracy=0.3452, 
[2025-09-16 03:46:15,526][__main__][INFO] - Test, Round 324: loss=5.8817, accuracy=0.3446, 
[2025-09-16 03:46:15,526][__main__][INFO] - Test, Round 325: loss=5.8847, accuracy=0.3448, 
[2025-09-16 03:46:15,526][__main__][INFO] - Test, Round 326: loss=5.8874, accuracy=0.3449, 
[2025-09-16 03:46:15,526][__main__][INFO] - Test, Round 327: loss=5.8903, accuracy=0.3445, 
[2025-09-16 03:46:15,527][__main__][INFO] - Test, Round 328: loss=5.8927, accuracy=0.3449, 
[2025-09-16 03:46:15,527][__main__][INFO] - Test, Round 329: loss=5.8961, accuracy=0.3449, 
[2025-09-16 03:46:15,527][__main__][INFO] - Test, Round 330: loss=5.8991, accuracy=0.3451, 
[2025-09-16 03:46:15,527][__main__][INFO] - Test, Round 331: loss=5.9019, accuracy=0.3448, 
[2025-09-16 03:46:15,527][__main__][INFO] - Test, Round 332: loss=5.9045, accuracy=0.3451, 
[2025-09-16 03:46:15,527][__main__][INFO] - Test, Round 333: loss=5.9063, accuracy=0.3454, 
[2025-09-16 03:46:15,527][__main__][INFO] - Test, Round 334: loss=5.9098, accuracy=0.3448, 
[2025-09-16 03:46:15,527][__main__][INFO] - Test, Round 335: loss=5.9124, accuracy=0.3450, 
[2025-09-16 03:46:15,527][__main__][INFO] - Test, Round 336: loss=5.9156, accuracy=0.3451, 
[2025-09-16 03:46:15,527][__main__][INFO] - Test, Round 337: loss=5.9172, accuracy=0.3446, 
[2025-09-16 03:46:15,527][__main__][INFO] - Test, Round 338: loss=5.9209, accuracy=0.3447, 
[2025-09-16 03:46:15,527][__main__][INFO] - Test, Round 339: loss=5.9234, accuracy=0.3450, 
[2025-09-16 03:46:15,527][__main__][INFO] - Test, Round 340: loss=5.9260, accuracy=0.3453, 
[2025-09-16 03:46:15,527][__main__][INFO] - Test, Round 341: loss=5.9287, accuracy=0.3449, 
[2025-09-16 03:46:15,527][__main__][INFO] - Test, Round 342: loss=5.9314, accuracy=0.3448, 
[2025-09-16 03:46:15,527][__main__][INFO] - Test, Round 343: loss=5.9336, accuracy=0.3448, 
[2025-09-16 03:46:15,527][__main__][INFO] - Test, Round 344: loss=5.9362, accuracy=0.3450, 
[2025-09-16 03:46:15,527][__main__][INFO] - Test, Round 345: loss=5.9396, accuracy=0.3448, 
[2025-09-16 03:46:15,527][__main__][INFO] - Test, Round 346: loss=5.9422, accuracy=0.3447, 
[2025-09-16 03:46:15,527][__main__][INFO] - Test, Round 347: loss=5.9445, accuracy=0.3450, 
[2025-09-16 03:46:15,527][__main__][INFO] - Test, Round 348: loss=5.9467, accuracy=0.3448, 
[2025-09-16 03:46:15,527][__main__][INFO] - Test, Round 349: loss=5.9496, accuracy=0.3451, 
[2025-09-16 03:46:15,527][__main__][INFO] - Test, Round 350: loss=5.9526, accuracy=0.3450, 
[2025-09-16 03:46:15,527][__main__][INFO] - Test, Round 351: loss=5.9546, accuracy=0.3442, 
[2025-09-16 03:46:15,527][__main__][INFO] - Test, Round 352: loss=5.9569, accuracy=0.3449, 
[2025-09-16 03:46:15,527][__main__][INFO] - Test, Round 353: loss=5.9593, accuracy=0.3449, 
[2025-09-16 03:46:15,527][__main__][INFO] - Test, Round 354: loss=5.9624, accuracy=0.3451, 
[2025-09-16 03:46:15,527][__main__][INFO] - Test, Round 355: loss=5.9647, accuracy=0.3447, 
[2025-09-16 03:46:15,527][__main__][INFO] - Test, Round 356: loss=5.9671, accuracy=0.3445, 
[2025-09-16 03:46:15,527][__main__][INFO] - Test, Round 357: loss=5.9700, accuracy=0.3447, 
[2025-09-16 03:46:15,527][__main__][INFO] - Test, Round 358: loss=5.9721, accuracy=0.3447, 
[2025-09-16 03:46:15,527][__main__][INFO] - Test, Round 359: loss=5.9744, accuracy=0.3450, 
[2025-09-16 03:46:15,527][__main__][INFO] - Test, Round 360: loss=5.9769, accuracy=0.3448, 
[2025-09-16 03:46:15,527][__main__][INFO] - Test, Round 361: loss=5.9799, accuracy=0.3447, 
[2025-09-16 03:46:15,527][__main__][INFO] - Test, Round 362: loss=5.9821, accuracy=0.3447, 
[2025-09-16 03:46:15,527][__main__][INFO] - Test, Round 363: loss=5.9847, accuracy=0.3445, 
[2025-09-16 03:46:15,527][__main__][INFO] - Test, Round 364: loss=5.9866, accuracy=0.3446, 
[2025-09-16 03:46:15,527][__main__][INFO] - Test, Round 365: loss=5.9896, accuracy=0.3449, 
[2025-09-16 03:46:15,527][__main__][INFO] - Test, Round 366: loss=5.9915, accuracy=0.3442, 
[2025-09-16 03:46:15,527][__main__][INFO] - Test, Round 367: loss=5.9941, accuracy=0.3448, 
[2025-09-16 03:46:15,527][__main__][INFO] - Test, Round 368: loss=5.9965, accuracy=0.3445, 
[2025-09-16 03:46:15,527][__main__][INFO] - Test, Round 369: loss=5.9990, accuracy=0.3447, 
[2025-09-16 03:46:15,527][__main__][INFO] - Test, Round 370: loss=6.0011, accuracy=0.3444, 
[2025-09-16 03:46:15,527][__main__][INFO] - Test, Round 371: loss=6.0034, accuracy=0.3444, 
[2025-09-16 03:46:15,527][__main__][INFO] - Test, Round 372: loss=6.0058, accuracy=0.3446, 
[2025-09-16 03:46:15,527][__main__][INFO] - Test, Round 373: loss=6.0080, accuracy=0.3451, 
[2025-09-16 03:46:15,527][__main__][INFO] - Test, Round 374: loss=6.0101, accuracy=0.3444, 
[2025-09-16 03:46:15,527][__main__][INFO] - Test, Round 375: loss=6.0128, accuracy=0.3442, 
[2025-09-16 03:46:15,527][__main__][INFO] - Test, Round 376: loss=6.0154, accuracy=0.3445, 
[2025-09-16 03:46:15,527][__main__][INFO] - Test, Round 377: loss=6.0171, accuracy=0.3442, 
[2025-09-16 03:46:15,527][__main__][INFO] - Test, Round 378: loss=6.0192, accuracy=0.3445, 
[2025-09-16 03:46:15,527][__main__][INFO] - Test, Round 379: loss=6.0220, accuracy=0.3449, 
[2025-09-16 03:46:15,527][__main__][INFO] - Test, Round 380: loss=6.0243, accuracy=0.3446, 
[2025-09-16 03:46:15,527][__main__][INFO] - Test, Round 381: loss=6.0259, accuracy=0.3446, 
[2025-09-16 03:46:15,527][__main__][INFO] - Test, Round 382: loss=6.0291, accuracy=0.3447, 
[2025-09-16 03:46:15,527][__main__][INFO] - Test, Round 383: loss=6.0312, accuracy=0.3445, 
[2025-09-16 03:46:15,527][__main__][INFO] - Test, Round 384: loss=6.0331, accuracy=0.3444, 
[2025-09-16 03:46:15,527][__main__][INFO] - Test, Round 385: loss=6.0355, accuracy=0.3444, 
[2025-09-16 03:46:15,527][__main__][INFO] - Test, Round 386: loss=6.0373, accuracy=0.3442, 
[2025-09-16 03:46:15,527][__main__][INFO] - Test, Round 387: loss=6.0398, accuracy=0.3447, 
[2025-09-16 03:46:15,527][__main__][INFO] - Test, Round 388: loss=6.0418, accuracy=0.3446, 
[2025-09-16 03:46:15,527][__main__][INFO] - Test, Round 389: loss=6.0439, accuracy=0.3448, 
[2025-09-16 03:46:15,527][__main__][INFO] - Test, Round 390: loss=6.0463, accuracy=0.3444, 
[2025-09-16 03:46:15,527][__main__][INFO] - Test, Round 391: loss=6.0487, accuracy=0.3445, 
[2025-09-16 03:46:15,527][__main__][INFO] - Test, Round 392: loss=6.0503, accuracy=0.3442, 
[2025-09-16 03:46:15,527][__main__][INFO] - Test, Round 393: loss=6.0528, accuracy=0.3441, 
[2025-09-16 03:46:15,527][__main__][INFO] - Test, Round 394: loss=6.0544, accuracy=0.3443, 
[2025-09-16 03:46:15,527][__main__][INFO] - Test, Round 395: loss=6.0570, accuracy=0.3440, 
[2025-09-16 03:46:15,527][__main__][INFO] - Test, Round 396: loss=6.0591, accuracy=0.3445, 
[2025-09-16 03:46:15,527][__main__][INFO] - Test, Round 397: loss=6.0606, accuracy=0.3445, 
[2025-09-16 03:46:15,527][__main__][INFO] - Test, Round 398: loss=6.0631, accuracy=0.3447, 
[2025-09-16 03:46:15,527][__main__][INFO] - Test, Round 399: loss=6.0648, accuracy=0.3445, 
[2025-09-16 03:46:15,527][__main__][INFO] - Test, Round 400: loss=6.0673, accuracy=0.3450, 
[2025-09-16 03:46:15,527][__main__][INFO] - Test, Round 401: loss=6.0692, accuracy=0.3447, 
[2025-09-16 03:46:15,527][__main__][INFO] - Test, Round 402: loss=6.0716, accuracy=0.3442, 
[2025-09-16 03:46:15,527][__main__][INFO] - Test, Round 403: loss=6.0728, accuracy=0.3445, 
[2025-09-16 03:46:15,527][__main__][INFO] - Test, Round 404: loss=6.0750, accuracy=0.3444, 
[2025-09-16 03:46:15,527][__main__][INFO] - Test, Round 405: loss=6.0776, accuracy=0.3444, 
[2025-09-16 03:46:15,527][__main__][INFO] - Test, Round 406: loss=6.0798, accuracy=0.3447, 
[2025-09-16 03:46:15,527][__main__][INFO] - Test, Round 407: loss=6.0816, accuracy=0.3442, 
[2025-09-16 03:46:15,527][__main__][INFO] - Test, Round 408: loss=6.0835, accuracy=0.3441, 
[2025-09-16 03:46:15,527][__main__][INFO] - Test, Round 409: loss=6.0856, accuracy=0.3443, 
[2025-09-16 03:46:15,527][__main__][INFO] - Test, Round 410: loss=6.0879, accuracy=0.3444, 
[2025-09-16 03:46:15,528][__main__][INFO] - Test, Round 411: loss=6.0897, accuracy=0.3440, 
[2025-09-16 03:46:15,528][__main__][INFO] - Test, Round 412: loss=6.0919, accuracy=0.3443, 
[2025-09-16 03:46:15,528][__main__][INFO] - Test, Round 413: loss=6.0937, accuracy=0.3442, 
[2025-09-16 03:46:15,528][__main__][INFO] - Test, Round 414: loss=6.0955, accuracy=0.3442, 
[2025-09-16 03:46:15,528][__main__][INFO] - Test, Round 415: loss=6.0973, accuracy=0.3442, 
[2025-09-16 03:46:15,528][__main__][INFO] - Test, Round 416: loss=6.0998, accuracy=0.3447, 
[2025-09-16 03:46:15,528][__main__][INFO] - Test, Round 417: loss=6.1012, accuracy=0.3441, 
[2025-09-16 03:46:15,528][__main__][INFO] - Test, Round 418: loss=6.1034, accuracy=0.3444, 
[2025-09-16 03:46:15,528][__main__][INFO] - Test, Round 419: loss=6.1055, accuracy=0.3443, 
[2025-09-16 03:46:15,528][__main__][INFO] - Test, Round 420: loss=6.1073, accuracy=0.3446, 
[2025-09-16 03:46:15,528][__main__][INFO] - Test, Round 421: loss=6.1092, accuracy=0.3449, 
[2025-09-16 03:46:15,528][__main__][INFO] - Test, Round 422: loss=6.1112, accuracy=0.3446, 
[2025-09-16 03:46:15,528][__main__][INFO] - Test, Round 423: loss=6.1129, accuracy=0.3446, 
[2025-09-16 03:46:15,528][__main__][INFO] - Test, Round 424: loss=6.1151, accuracy=0.3444, 
[2025-09-16 03:46:15,528][__main__][INFO] - Test, Round 425: loss=6.1170, accuracy=0.3446, 
[2025-09-16 03:46:15,528][__main__][INFO] - Test, Round 426: loss=6.1182, accuracy=0.3442, 
[2025-09-16 03:46:15,528][__main__][INFO] - Test, Round 427: loss=6.1204, accuracy=0.3448, 
[2025-09-16 03:46:15,528][__main__][INFO] - Test, Round 428: loss=6.1222, accuracy=0.3443, 
[2025-09-16 03:46:15,528][__main__][INFO] - Test, Round 429: loss=6.1240, accuracy=0.3443, 
[2025-09-16 03:46:15,528][__main__][INFO] - Test, Round 430: loss=6.1261, accuracy=0.3444, 
[2025-09-16 03:46:15,528][__main__][INFO] - Test, Round 431: loss=6.1277, accuracy=0.3444, 
[2025-09-16 03:46:15,528][__main__][INFO] - Test, Round 432: loss=6.1297, accuracy=0.3444, 
[2025-09-16 03:46:15,528][__main__][INFO] - Test, Round 433: loss=6.1320, accuracy=0.3444, 
[2025-09-16 03:46:15,528][__main__][INFO] - Test, Round 434: loss=6.1335, accuracy=0.3442, 
[2025-09-16 03:46:15,528][__main__][INFO] - Test, Round 435: loss=6.1352, accuracy=0.3445, 
[2025-09-16 03:46:15,528][__main__][INFO] - Test, Round 436: loss=6.1369, accuracy=0.3443, 
[2025-09-16 03:46:15,528][__main__][INFO] - Test, Round 437: loss=6.1388, accuracy=0.3447, 
[2025-09-16 03:46:15,528][__main__][INFO] - Test, Round 438: loss=6.1407, accuracy=0.3444, 
[2025-09-16 03:46:15,528][__main__][INFO] - Test, Round 439: loss=6.1425, accuracy=0.3445, 
[2025-09-16 03:46:15,528][__main__][INFO] - Test, Round 440: loss=6.1442, accuracy=0.3443, 
[2025-09-16 03:46:15,528][__main__][INFO] - Test, Round 441: loss=6.1459, accuracy=0.3442, 
[2025-09-16 03:46:15,528][__main__][INFO] - Test, Round 442: loss=6.1479, accuracy=0.3441, 
[2025-09-16 03:46:15,528][__main__][INFO] - Test, Round 443: loss=6.1492, accuracy=0.3447, 
[2025-09-16 03:46:15,528][__main__][INFO] - Test, Round 444: loss=6.1511, accuracy=0.3441, 
[2025-09-16 03:46:15,528][__main__][INFO] - Test, Round 445: loss=6.1527, accuracy=0.3444, 
[2025-09-16 03:46:15,528][__main__][INFO] - Test, Round 446: loss=6.1547, accuracy=0.3444, 
[2025-09-16 03:46:15,528][__main__][INFO] - Test, Round 447: loss=6.1568, accuracy=0.3447, 
[2025-09-16 03:46:15,528][__main__][INFO] - Test, Round 448: loss=6.1582, accuracy=0.3441, 
[2025-09-16 03:46:15,528][__main__][INFO] - Test, Round 449: loss=6.1599, accuracy=0.3449, 
[2025-09-16 03:46:15,528][__main__][INFO] - Test, Round 450: loss=6.1617, accuracy=0.3444, 
[2025-09-16 03:46:15,528][__main__][INFO] - Test, Round 451: loss=6.1633, accuracy=0.3445, 
[2025-09-16 03:46:15,528][__main__][INFO] - Test, Round 452: loss=6.1647, accuracy=0.3445, 
[2025-09-16 03:46:15,528][__main__][INFO] - Test, Round 453: loss=6.1666, accuracy=0.3441, 
[2025-09-16 03:46:15,528][__main__][INFO] - Test, Round 454: loss=6.1684, accuracy=0.3448, 
[2025-09-16 03:46:15,528][__main__][INFO] - Test, Round 455: loss=6.1698, accuracy=0.3444, 
[2025-09-16 03:46:15,528][__main__][INFO] - Test, Round 456: loss=6.1717, accuracy=0.3446, 
[2025-09-16 03:46:15,528][__main__][INFO] - Test, Round 457: loss=6.1735, accuracy=0.3442, 
[2025-09-16 03:46:15,528][__main__][INFO] - Test, Round 458: loss=6.1748, accuracy=0.3443, 
[2025-09-16 03:46:15,528][__main__][INFO] - Test, Round 459: loss=6.1762, accuracy=0.3443, 
[2025-09-16 03:46:15,528][__main__][INFO] - Test, Round 460: loss=6.1784, accuracy=0.3446, 
[2025-09-16 03:46:15,528][__main__][INFO] - Test, Round 461: loss=6.1798, accuracy=0.3443, 
[2025-09-16 03:46:15,528][__main__][INFO] - Test, Round 462: loss=6.1818, accuracy=0.3447, 
[2025-09-16 03:46:15,528][__main__][INFO] - Test, Round 463: loss=6.1831, accuracy=0.3447, 
[2025-09-16 03:46:15,528][__main__][INFO] - Test, Round 464: loss=6.1848, accuracy=0.3446, 
[2025-09-16 03:46:15,528][__main__][INFO] - Test, Round 465: loss=6.1864, accuracy=0.3444, 
[2025-09-16 03:46:15,528][__main__][INFO] - Test, Round 466: loss=6.1884, accuracy=0.3445, 
[2025-09-16 03:46:15,528][__main__][INFO] - Test, Round 467: loss=6.1898, accuracy=0.3440, 
[2025-09-16 03:46:15,528][__main__][INFO] - Test, Round 468: loss=6.1917, accuracy=0.3446, 
[2025-09-16 03:46:15,528][__main__][INFO] - Test, Round 469: loss=6.1935, accuracy=0.3446, 
[2025-09-16 03:46:15,528][__main__][INFO] - Test, Round 470: loss=6.1948, accuracy=0.3446, 
[2025-09-16 03:46:15,528][__main__][INFO] - Test, Round 471: loss=6.1967, accuracy=0.3445, 
[2025-09-16 03:46:15,528][__main__][INFO] - Test, Round 472: loss=6.1978, accuracy=0.3446, 
[2025-09-16 03:46:15,528][__main__][INFO] - Test, Round 473: loss=6.1996, accuracy=0.3443, 
[2025-09-16 03:46:15,528][__main__][INFO] - Test, Round 474: loss=6.2012, accuracy=0.3442, 
[2025-09-16 03:46:15,528][__main__][INFO] - Test, Round 475: loss=6.2027, accuracy=0.3442, 
[2025-09-16 03:46:15,528][__main__][INFO] - Test, Round 476: loss=6.2041, accuracy=0.3443, 
[2025-09-16 03:46:15,528][__main__][INFO] - Test, Round 477: loss=6.2056, accuracy=0.3443, 
[2025-09-16 03:46:15,528][__main__][INFO] - Test, Round 478: loss=6.2073, accuracy=0.3443, 
[2025-09-16 03:46:15,528][__main__][INFO] - Test, Round 479: loss=6.2087, accuracy=0.3444, 
[2025-09-16 03:46:15,528][__main__][INFO] - Test, Round 480: loss=6.2100, accuracy=0.3442, 
[2025-09-16 03:46:15,528][__main__][INFO] - Test, Round 481: loss=6.2120, accuracy=0.3441, 
[2025-09-16 03:46:15,528][__main__][INFO] - Test, Round 482: loss=6.2136, accuracy=0.3445, 
[2025-09-16 03:46:15,528][__main__][INFO] - Test, Round 483: loss=6.2149, accuracy=0.3445, 
[2025-09-16 03:46:15,528][__main__][INFO] - Test, Round 484: loss=6.2164, accuracy=0.3442, 
[2025-09-16 03:46:15,528][__main__][INFO] - Test, Round 485: loss=6.2180, accuracy=0.3445, 
[2025-09-16 03:46:15,528][__main__][INFO] - Test, Round 486: loss=6.2197, accuracy=0.3444, 
[2025-09-16 03:46:15,528][__main__][INFO] - Test, Round 487: loss=6.2215, accuracy=0.3443, 
[2025-09-16 03:46:15,528][__main__][INFO] - Test, Round 488: loss=6.2228, accuracy=0.3445, 
[2025-09-16 03:46:15,528][__main__][INFO] - Test, Round 489: loss=6.2243, accuracy=0.3444, 
[2025-09-16 03:46:15,528][__main__][INFO] - Test, Round 490: loss=6.2258, accuracy=0.3443, 
[2025-09-16 03:46:15,528][__main__][INFO] - Test, Round 491: loss=6.2273, accuracy=0.3446, 
[2025-09-16 03:46:15,528][__main__][INFO] - Test, Round 492: loss=6.2289, accuracy=0.3443, 
[2025-09-16 03:46:15,528][__main__][INFO] - Test, Round 493: loss=6.2302, accuracy=0.3446, 
[2025-09-16 03:46:15,528][__main__][INFO] - Test, Round 494: loss=6.2318, accuracy=0.3447, 
[2025-09-16 03:46:15,529][__main__][INFO] - Test, Round 495: loss=6.2332, accuracy=0.3446, 
[2025-09-16 03:46:15,529][__main__][INFO] - Test, Round 496: loss=6.2343, accuracy=0.3445, 
[2025-09-16 03:46:15,529][__main__][INFO] - Test, Round 497: loss=6.2361, accuracy=0.3443, 
[2025-09-16 03:46:15,529][__main__][INFO] - Test, Round 498: loss=6.2378, accuracy=0.3446, 
[2025-09-16 03:46:15,529][__main__][INFO] - Test, Round 499: loss=6.2388, accuracy=0.3443, 
[2025-09-16 03:46:15,529][__main__][INFO] - Test, Round 500: loss=6.2408, accuracy=0.3450, 
